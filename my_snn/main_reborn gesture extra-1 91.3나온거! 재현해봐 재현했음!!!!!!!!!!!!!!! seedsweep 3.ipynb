{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35282/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 128.0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0*2, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0*2,\n",
    "#                 lif_layer_v_threshold2 = 128.0,\n",
    "#                 learning_rate2 = 1,\n",
    "#                 init_scaling = [10000+ 9,10000+ 9,10000+ 8],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v46y0e0r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 28186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251212_182043-v46y0e0r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v46y0e0r' target=\"_blank\">decent-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v46y0e0r' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v46y0e0r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251212_182052_705', 'my_seed': 28186, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 144.0\n",
      "lif layer 1 self.abs_max_v: 144.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 215.0\n",
      "lif layer 1 self.abs_max_v: 264.0\n",
      "fc layer 2 self.abs_max_out: 217.0\n",
      "lif layer 2 self.abs_max_v: 211.0\n",
      "fc layer 3 self.abs_max_out: 44.0\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "lif layer 1 self.abs_max_v: 309.0\n",
      "fc layer 2 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 327.0\n",
      "fc layer 3 self.abs_max_out: 61.0\n",
      "lif layer 2 self.abs_max_v: 349.0\n",
      "fc layer 3 self.abs_max_out: 63.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 1 self.abs_max_out: 305.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "lif layer 1 self.abs_max_v: 315.5\n",
      "lif layer 1 self.abs_max_v: 374.5\n",
      "fc layer 1 self.abs_max_out: 325.0\n",
      "lif layer 2 self.abs_max_v: 354.5\n",
      "fc layer 1 self.abs_max_out: 334.0\n",
      "lif layer 2 self.abs_max_v: 357.0\n",
      "lif layer 2 self.abs_max_v: 430.5\n",
      "fc layer 1 self.abs_max_out: 409.0\n",
      "lif layer 1 self.abs_max_v: 409.0\n",
      "fc layer 2 self.abs_max_out: 386.0\n",
      "fc layer 1 self.abs_max_out: 500.0\n",
      "lif layer 1 self.abs_max_v: 500.0\n",
      "lif layer 2 self.abs_max_v: 454.5\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "fc layer 1 self.abs_max_out: 634.0\n",
      "lif layer 1 self.abs_max_v: 634.0\n",
      "fc layer 2 self.abs_max_out: 405.0\n",
      "lif layer 2 self.abs_max_v: 499.0\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "fc layer 2 self.abs_max_out: 412.0\n",
      "lif layer 2 self.abs_max_v: 564.0\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "lif layer 1 self.abs_max_v: 669.5\n",
      "fc layer 2 self.abs_max_out: 425.0\n",
      "lif layer 2 self.abs_max_v: 633.5\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "fc layer 2 self.abs_max_out: 473.0\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "lif layer 2 self.abs_max_v: 716.5\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 3 self.abs_max_out: 226.0\n",
      "lif layer 2 self.abs_max_v: 757.5\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 2 self.abs_max_out: 562.0\n",
      "lif layer 2 self.abs_max_v: 846.5\n",
      "fc layer 2 self.abs_max_out: 586.0\n",
      "lif layer 2 self.abs_max_v: 936.0\n",
      "fc layer 1 self.abs_max_out: 647.0\n",
      "lif layer 1 self.abs_max_v: 695.0\n",
      "lif layer 1 self.abs_max_v: 773.5\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "fc layer 2 self.abs_max_out: 635.0\n",
      "fc layer 1 self.abs_max_out: 723.0\n",
      "fc layer 2 self.abs_max_out: 643.0\n",
      "lif layer 1 self.abs_max_v: 801.0\n",
      "fc layer 1 self.abs_max_out: 736.0\n",
      "fc layer 1 self.abs_max_out: 750.0\n",
      "fc layer 2 self.abs_max_out: 652.0\n",
      "fc layer 1 self.abs_max_out: 810.0\n",
      "lif layer 1 self.abs_max_v: 810.0\n",
      "fc layer 1 self.abs_max_out: 949.0\n",
      "lif layer 1 self.abs_max_v: 949.0\n",
      "fc layer 1 self.abs_max_out: 1080.0\n",
      "lif layer 1 self.abs_max_v: 1080.0\n",
      "fc layer 1 self.abs_max_out: 1094.0\n",
      "lif layer 1 self.abs_max_v: 1094.0\n",
      "fc layer 2 self.abs_max_out: 658.0\n",
      "fc layer 3 self.abs_max_out: 277.0\n",
      "fc layer 2 self.abs_max_out: 686.0\n",
      "fc layer 2 self.abs_max_out: 752.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 2 self.abs_max_out: 772.0\n",
      "fc layer 2 self.abs_max_out: 833.0\n",
      "fc layer 2 self.abs_max_out: 880.0\n",
      "lif layer 2 self.abs_max_v: 993.5\n",
      "fc layer 2 self.abs_max_out: 915.0\n",
      "lif layer 2 self.abs_max_v: 1017.0\n",
      "lif layer 2 self.abs_max_v: 1071.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "fc layer 3 self.abs_max_out: 299.0\n",
      "lif layer 2 self.abs_max_v: 1104.5\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "fc layer 3 self.abs_max_out: 319.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "lif layer 2 self.abs_max_v: 1195.5\n",
      "fc layer 2 self.abs_max_out: 1135.0\n",
      "fc layer 1 self.abs_max_out: 1293.0\n",
      "lif layer 1 self.abs_max_v: 1293.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "lif layer 1 self.abs_max_v: 1312.5\n",
      "lif layer 1 self.abs_max_v: 1435.0\n",
      "lif layer 1 self.abs_max_v: 1471.0\n",
      "lif layer 2 self.abs_max_v: 1217.0\n",
      "lif layer 2 self.abs_max_v: 1304.5\n",
      "lif layer 1 self.abs_max_v: 1567.5\n",
      "lif layer 1 self.abs_max_v: 1634.0\n",
      "lif layer 1 self.abs_max_v: 1786.0\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 1 self.abs_max_out: 1533.0\n",
      "lif layer 1 self.abs_max_v: 1912.0\n",
      "lif layer 1 self.abs_max_v: 2032.5\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 1 self.abs_max_out: 1671.0\n",
      "lif layer 1 self.abs_max_v: 2113.5\n",
      "lif layer 1 self.abs_max_v: 2284.0\n",
      "lif layer 1 self.abs_max_v: 2313.0\n",
      "lif layer 1 self.abs_max_v: 2428.5\n",
      "lif layer 1 self.abs_max_v: 2517.5\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "lif layer 2 self.abs_max_v: 1341.5\n",
      "lif layer 2 self.abs_max_v: 1372.0\n",
      "lif layer 2 self.abs_max_v: 1376.5\n",
      "lif layer 2 self.abs_max_v: 1398.5\n",
      "fc layer 1 self.abs_max_out: 1759.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "lif layer 2 self.abs_max_v: 1470.5\n",
      "lif layer 1 self.abs_max_v: 2619.0\n",
      "fc layer 2 self.abs_max_out: 1272.0\n",
      "fc layer 2 self.abs_max_out: 1284.0\n",
      "lif layer 1 self.abs_max_v: 2735.5\n",
      "lif layer 1 self.abs_max_v: 2772.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.734033/ 57.445370, val:  37.92%, val_best:  37.92%, tr:  96.83%, tr_best:  96.83%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2196  22.431%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1762.0\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "lif layer 2 self.abs_max_v: 1506.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "fc layer 1 self.abs_max_out: 1779.0\n",
      "lif layer 2 self.abs_max_v: 1539.5\n",
      "fc layer 1 self.abs_max_out: 1868.0\n",
      "lif layer 2 self.abs_max_v: 1549.5\n",
      "fc layer 1 self.abs_max_out: 1917.0\n",
      "lif layer 2 self.abs_max_v: 1554.5\n",
      "lif layer 2 self.abs_max_v: 1583.0\n",
      "lif layer 2 self.abs_max_v: 1664.5\n",
      "lif layer 2 self.abs_max_v: 1669.5\n",
      "fc layer 2 self.abs_max_out: 1290.0\n",
      "lif layer 1 self.abs_max_v: 3130.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.611786/ 53.968189, val:  45.42%, val_best:  45.42%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3772  19.265%\n",
      "lif layer 2 self.abs_max_v: 1762.0\n",
      "fc layer 1 self.abs_max_out: 1975.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "lif layer 2 self.abs_max_v: 1765.0\n",
      "lif layer 2 self.abs_max_v: 1769.5\n",
      "lif layer 2 self.abs_max_v: 1804.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "fc layer 3 self.abs_max_out: 496.0\n",
      "lif layer 2 self.abs_max_v: 1921.5\n",
      "lif layer 2 self.abs_max_v: 1986.0\n",
      "fc layer 2 self.abs_max_out: 1332.0\n",
      "fc layer 1 self.abs_max_out: 2005.0\n",
      "lif layer 2 self.abs_max_v: 2115.5\n",
      "lif layer 2 self.abs_max_v: 2171.5\n",
      "lif layer 2 self.abs_max_v: 2229.0\n",
      "fc layer 1 self.abs_max_out: 2026.0\n",
      "lif layer 1 self.abs_max_v: 3191.5\n",
      "lif layer 1 self.abs_max_v: 3244.0\n",
      "lif layer 1 self.abs_max_v: 3455.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.343085/ 50.767830, val:  52.50%, val_best:  52.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5237  17.831%\n",
      "fc layer 1 self.abs_max_out: 2090.0\n",
      "fc layer 1 self.abs_max_out: 2204.0\n",
      "lif layer 1 self.abs_max_v: 3556.0\n",
      "lif layer 1 self.abs_max_v: 3560.5\n",
      "lif layer 1 self.abs_max_v: 3564.5\n",
      "lif layer 1 self.abs_max_v: 3592.0\n",
      "fc layer 2 self.abs_max_out: 1373.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.731892/ 84.962463, val:  39.58%, val_best:  52.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6678  17.053%\n",
      "lif layer 1 self.abs_max_v: 3669.5\n",
      "lif layer 1 self.abs_max_v: 3810.0\n",
      "lif layer 1 self.abs_max_v: 3864.0\n",
      "lif layer 1 self.abs_max_v: 3920.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.959193/ 54.072872, val:  44.17%, val_best:  52.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1134%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8031  16.407%\n",
      "fc layer 1 self.abs_max_out: 2249.0\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 3 self.abs_max_out: 535.0\n",
      "fc layer 1 self.abs_max_out: 2389.0\n",
      "lif layer 1 self.abs_max_v: 4070.0\n",
      "lif layer 1 self.abs_max_v: 4334.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.473011/ 61.611008, val:  52.08%, val_best:  52.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9343  15.906%\n",
      "fc layer 2 self.abs_max_out: 1413.0\n",
      "fc layer 1 self.abs_max_out: 2494.0\n",
      "fc layer 2 self.abs_max_out: 1427.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.418174/ 89.447617, val:  32.08%, val_best:  52.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0412%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3722%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2866%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10601  15.469%\n",
      "fc layer 2 self.abs_max_out: 1463.0\n",
      "fc layer 1 self.abs_max_out: 2536.0\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "fc layer 1 self.abs_max_out: 2553.0\n",
      "lif layer 1 self.abs_max_v: 4549.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.426490/ 36.431000, val:  60.42%, val_best:  60.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1615%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11850  15.130%\n",
      "fc layer 3 self.abs_max_out: 538.0\n",
      "fc layer 3 self.abs_max_out: 545.0\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.997409/ 59.786469, val:  45.00%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0547%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13068  14.831%\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "fc layer 1 self.abs_max_out: 2565.0\n",
      "fc layer 1 self.abs_max_out: 2633.0\n",
      "lif layer 1 self.abs_max_v: 4717.5\n",
      "lif layer 2 self.abs_max_v: 2264.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.530721/ 64.924606, val:  50.42%, val_best:  60.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1253%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14328  14.635%\n",
      "fc layer 1 self.abs_max_out: 2651.0\n",
      "fc layer 1 self.abs_max_out: 2660.0\n",
      "fc layer 1 self.abs_max_out: 2739.0\n",
      "lif layer 1 self.abs_max_v: 4879.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.379989/ 44.375992, val:  56.25%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15540  14.430%\n",
      "fc layer 1 self.abs_max_out: 2797.0\n",
      "lif layer 1 self.abs_max_v: 4934.5\n",
      "lif layer 2 self.abs_max_v: 2271.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.721983/ 61.270782, val:  55.42%, val_best:  60.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16733  14.243%\n",
      "lif layer 1 self.abs_max_v: 4936.0\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.701485/ 45.144638, val:  61.25%, val_best:  61.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17910  14.072%\n",
      "fc layer 2 self.abs_max_out: 1685.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.307615/ 43.814487, val:  59.58%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19014  13.873%\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "fc layer 1 self.abs_max_out: 2873.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.684642/ 68.632362, val:  44.58%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8960%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20190  13.749%\n",
      "lif layer 2 self.abs_max_v: 2277.0\n",
      "fc layer 2 self.abs_max_out: 1688.0\n",
      "fc layer 1 self.abs_max_out: 2943.0\n",
      "lif layer 1 self.abs_max_v: 4941.0\n",
      "fc layer 1 self.abs_max_out: 2957.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.397210/ 46.124336, val:  57.50%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21307  13.603%\n",
      "fc layer 1 self.abs_max_out: 3113.0\n",
      "lif layer 1 self.abs_max_v: 4990.0\n",
      "lif layer 1 self.abs_max_v: 5197.0\n",
      "fc layer 2 self.abs_max_out: 1728.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.113949/ 67.110207, val:  56.25%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0408%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0647%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22517  13.529%\n",
      "lif layer 2 self.abs_max_v: 2359.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  6.855688/ 44.038704, val:  61.67%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23602  13.393%\n",
      "lif layer 2 self.abs_max_v: 2364.0\n",
      "fc layer 1 self.abs_max_out: 3192.0\n",
      "lif layer 1 self.abs_max_v: 5398.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.240598/ 53.679604, val:  65.00%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1241%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24700  13.279%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.161294/ 46.659355, val:  60.00%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25753  13.153%\n",
      "lif layer 2 self.abs_max_v: 2451.0\n",
      "fc layer 2 self.abs_max_out: 1742.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.930175/ 50.112709, val:  63.75%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26808  13.040%\n",
      "lif layer 2 self.abs_max_v: 2458.5\n",
      "lif layer 2 self.abs_max_v: 2561.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.640037/ 49.786156, val:  56.67%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1003%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27825  12.919%\n",
      "lif layer 2 self.abs_max_v: 2578.5\n",
      "lif layer 2 self.abs_max_v: 2614.5\n",
      "lif layer 2 self.abs_max_v: 2726.5\n",
      "fc layer 1 self.abs_max_out: 3244.0\n",
      "lif layer 1 self.abs_max_v: 5446.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.267036/ 49.822632, val:  60.83%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28816  12.797%\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.352979/ 38.185848, val:  66.67%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0441%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29788  12.678%\n",
      "lif layer 2 self.abs_max_v: 2757.5\n",
      "fc layer 1 self.abs_max_out: 3257.0\n",
      "lif layer 1 self.abs_max_v: 5455.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  5.995065/ 32.624184, val:  75.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30750  12.564%\n",
      "fc layer 1 self.abs_max_out: 3288.0\n",
      "lif layer 1 self.abs_max_v: 5613.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.085946/ 37.869518, val:  72.08%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31708  12.457%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.688370/ 37.611607, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32681  12.364%\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.379912/ 35.135086, val:  68.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33676  12.285%\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.642941/ 37.203865, val:  67.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5019%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34557  12.172%\n",
      "fc layer 1 self.abs_max_out: 3352.0\n",
      "lif layer 1 self.abs_max_v: 5686.5\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.534370/ 46.837410, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35441  12.067%\n",
      "lif layer 2 self.abs_max_v: 2885.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.641044/ 32.392529, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0165%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36321  11.968%\n",
      "fc layer 3 self.abs_max_out: 666.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.616624/ 39.124317, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37174  11.866%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.489104/ 46.775990, val:  69.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38024  11.770%\n",
      "fc layer 3 self.abs_max_out: 679.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.776008/ 35.026394, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38900  11.687%\n",
      "fc layer 1 self.abs_max_out: 3548.0\n",
      "lif layer 1 self.abs_max_v: 5704.5\n",
      "lif layer 1 self.abs_max_v: 6025.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.445024/ 45.553864, val:  62.08%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1022%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39703  11.587%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.387332/ 40.653339, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1661%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40537  11.502%\n",
      "fc layer 2 self.abs_max_out: 1760.0\n",
      "fc layer 2 self.abs_max_out: 1798.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.216361/ 41.902336, val:  71.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41366  11.420%\n",
      "fc layer 2 self.abs_max_out: 1810.0\n",
      "fc layer 1 self.abs_max_out: 3558.0\n",
      "lif layer 1 self.abs_max_v: 6044.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.268590/ 46.889225, val:  66.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9408%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42171  11.336%\n",
      "fc layer 2 self.abs_max_out: 1846.0\n",
      "fc layer 2 self.abs_max_out: 1848.0\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "fc layer 2 self.abs_max_out: 1915.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.591344/ 27.421200, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4393%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43021  11.268%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.890211/ 31.425535, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43798  11.184%\n",
      "fc layer 1 self.abs_max_out: 3589.0\n",
      "lif layer 1 self.abs_max_v: 6048.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.076170/ 43.544189, val:  73.75%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44572  11.104%\n",
      "lif layer 2 self.abs_max_v: 2955.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.742998/ 49.639069, val:  66.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45300  11.017%\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.044265/ 39.146420, val:  73.75%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0431%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46088  10.948%\n",
      "fc layer 1 self.abs_max_out: 3608.0\n",
      "fc layer 3 self.abs_max_out: 699.0\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "fc layer 1 self.abs_max_out: 3743.0\n",
      "lif layer 1 self.abs_max_v: 6068.0\n",
      "lif layer 1 self.abs_max_v: 6351.0\n",
      "lif layer 1 self.abs_max_v: 6381.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.038349/ 48.584259, val:  64.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46831  10.872%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.885507/ 43.314682, val:  72.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47584  10.801%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.527235/ 46.341816, val:  70.83%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0397%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48301  10.725%\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.746253/ 66.407204, val:  61.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49043  10.659%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.932916/ 32.000252, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49783  10.594%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.451475/ 25.440302, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9946%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50466  10.520%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.549962/ 35.005768, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51153  10.450%\n",
      "fc layer 2 self.abs_max_out: 1932.0\n",
      "fc layer 2 self.abs_max_out: 1962.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.577519/ 30.521046, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51854  10.386%\n",
      "lif layer 1 self.abs_max_v: 6382.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.206783/ 33.607693, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4711%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52507  10.314%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.304516/ 33.792080, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53135  10.241%\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.119981/ 47.283184, val:  70.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9690%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53780  10.173%\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "fc layer 2 self.abs_max_out: 2037.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.523180/ 42.324921, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54472  10.116%\n",
      "fc layer 1 self.abs_max_out: 3761.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.452883/ 27.309862, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0659%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55111  10.052%\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 3 self.abs_max_out: 781.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  3.712176/ 34.020145, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55685   9.979%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.262834/ 45.995331, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56306   9.916%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.442864/ 33.864540, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 56959   9.861%\n",
      "fc layer 1 self.abs_max_out: 3774.0\n",
      "lif layer 1 self.abs_max_v: 6477.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.946254/ 43.825104, val:  77.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6462%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57545   9.797%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.960529/ 32.842945, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58139   9.735%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.132415/ 40.026081, val:  77.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58722   9.674%\n",
      "fc layer 1 self.abs_max_out: 3823.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.771773/ 40.150616, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59298   9.614%\n",
      "fc layer 1 self.abs_max_out: 3831.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.457963/ 38.219116, val:  80.00%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 59918   9.563%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.523406/ 37.620804, val:  80.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60490   9.506%\n",
      "fc layer 3 self.abs_max_out: 784.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.697174/ 42.264507, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61055   9.449%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.349484/ 32.852196, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61571   9.387%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.961396/ 35.930794, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62175   9.340%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.068386/ 43.042774, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 62759   9.291%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.516411/ 33.710114, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5520%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63292   9.236%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.556924/ 34.081078, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8298%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8206%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 63859   9.187%\n",
      "fc layer 1 self.abs_max_out: 3871.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.823893/ 33.371090, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6901%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64432   9.141%\n",
      "lif layer 2 self.abs_max_v: 2970.5\n",
      "lif layer 2 self.abs_max_v: 2994.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.716341/ 43.871239, val:  79.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 64990   9.094%\n",
      "fc layer 2 self.abs_max_out: 2097.0\n",
      "lif layer 2 self.abs_max_v: 3129.5\n",
      "lif layer 1 self.abs_max_v: 6480.5\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.485575/ 45.325542, val:  75.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65513   9.043%\n",
      "fc layer 1 self.abs_max_out: 3932.0\n",
      "lif layer 1 self.abs_max_v: 6667.5\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.513396/ 42.914646, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5931%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66027   8.992%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.465799/ 34.902126, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0204%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66551   8.945%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.494252/ 33.496365, val:  88.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0279%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67071   8.897%\n",
      "fc layer 1 self.abs_max_out: 3953.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.512202/ 41.642025, val:  78.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 67604   8.853%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.152767/ 42.369850, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1156%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68123   8.808%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.214145/ 33.989796, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1146%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 68623   8.762%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.231317/ 38.211376, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6138%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69107   8.715%\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.984259/ 34.516468, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 69567   8.666%\n",
      "fc layer 1 self.abs_max_out: 4093.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.348840/ 34.868370, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70072   8.624%\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "fc layer 3 self.abs_max_out: 820.0\n",
      "lif layer 1 self.abs_max_v: 6679.5\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.060696/ 39.050636, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 70543   8.578%\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "lif layer 1 self.abs_max_v: 6736.5\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.313286/ 34.915932, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71040   8.537%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.973222/ 38.098545, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0541%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 71488   8.491%\n",
      "lif layer 1 self.abs_max_v: 6904.5\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.800265/ 40.750443, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 71933   8.446%\n",
      "fc layer 2 self.abs_max_out: 2112.0\n",
      "lif layer 1 self.abs_max_v: 6921.5\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.057988/ 35.502148, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0184%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7966%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72377   8.401%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.204038/ 32.432911, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9403%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 72848   8.361%\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.169592/ 37.604603, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73328   8.322%\n",
      "fc layer 2 self.abs_max_out: 2123.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.273169/ 37.699451, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6401%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 73800   8.284%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.451793/ 45.430511, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2234%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74207   8.239%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.090325/ 37.575920, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0999%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2222%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 74659   8.200%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.845893/ 36.157009, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75069   8.157%\n",
      "fc layer 1 self.abs_max_out: 4117.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.598134/ 42.567593, val:  78.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 75461   8.114%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.714721/ 37.088097, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 75875   8.073%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.904926/ 39.316910, val:  82.50%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 76321   8.037%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.658699/ 37.350380, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0382%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2081%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 76740   7.999%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.848916/ 35.800972, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8106%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 77174   7.963%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.598635/ 45.621223, val:  80.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 77586   7.925%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.860800/ 43.503899, val:  80.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78008   7.889%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.540418/ 44.869919, val:  79.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0594%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 78397   7.851%\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.740848/ 42.300308, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8732%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 78822   7.817%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.435519/ 37.443283, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 79212   7.780%\n",
      "fc layer 1 self.abs_max_out: 4190.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.307934/ 34.726940, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 79578   7.741%\n",
      "fc layer 1 self.abs_max_out: 4204.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.405222/ 35.759693, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 79944   7.704%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.368974/ 39.315529, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 80330   7.669%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.438108/ 51.480366, val:  80.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0640%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 80728   7.635%\n",
      "fc layer 1 self.abs_max_out: 4244.0\n",
      "fc layer 1 self.abs_max_out: 4354.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.687047/ 39.704292, val:  85.00%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8529%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 81117   7.602%\n",
      "lif layer 1 self.abs_max_v: 7068.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.616199/ 33.038456, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1152%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 81490   7.567%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.138412/ 36.916553, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 81848   7.532%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.229628/ 38.661373, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 82201   7.497%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.183105/ 38.300320, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 82519   7.459%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.232556/ 34.137680, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 82848   7.423%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.294395/ 39.683361, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 83196   7.390%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.477195/ 34.277615, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 83571   7.359%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.269819/ 32.449741, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 83917   7.326%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.261549/ 37.918819, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5355%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 84266   7.294%\n",
      "lif layer 2 self.abs_max_v: 3140.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.323297/ 40.227856, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 84633   7.265%\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.052442/ 35.491283, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 84955   7.231%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  1.913712/ 37.026443, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 85257   7.197%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  1.876608/ 38.522289, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 85562   7.164%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.380035/ 40.199329, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0359%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 85932   7.136%\n",
      "fc layer 2 self.abs_max_out: 2181.0\n",
      "fc layer 1 self.abs_max_out: 4394.0\n",
      "lif layer 1 self.abs_max_v: 7090.5\n",
      "lif layer 1 self.abs_max_v: 7500.5\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  1.913592/ 35.991703, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 86243   7.104%\n",
      "fc layer 3 self.abs_max_out: 834.0\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  1.697995/ 32.866199, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1115%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 86536   7.071%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.296036/ 39.189369, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 86878   7.043%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.213336/ 40.037880, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 87234   7.016%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.173076/ 33.814224, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 87574   6.988%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.147434/ 42.170601, val:  83.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 87891   6.959%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.013814/ 33.603642, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2263%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 88221   6.932%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.102940/ 35.958218, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8270%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 88524   6.903%\n",
      "lif layer 2 self.abs_max_v: 3150.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.873981/ 37.363556, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1249%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 88823   6.873%\n",
      "fc layer 2 self.abs_max_out: 2185.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.985056/ 34.853043, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0358%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 89142   6.846%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.675841/ 34.959541, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 89402   6.815%\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.990569/ 43.237892, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 89718   6.788%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.246369/ 35.318542, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6479%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 90065   6.764%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.970968/ 45.047558, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 90393   6.740%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.724846/ 39.317524, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 90689   6.713%\n",
      "fc layer 2 self.abs_max_out: 2237.0\n",
      "lif layer 2 self.abs_max_v: 3186.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.703027/ 35.929604, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 90955   6.684%\n",
      "fc layer 3 self.abs_max_out: 842.0\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.669158/ 35.742020, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 91229   6.656%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.707247/ 40.110397, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 91502   6.629%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.736134/ 36.197327, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 91780   6.602%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.716381/ 33.713558, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1037%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 92056   6.576%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.728712/ 35.258636, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 92342   6.550%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.766798/ 42.488621, val:  85.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 92617   6.524%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.571202/ 50.297585, val:  81.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 92881   6.498%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.483929/ 36.757927, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1358%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 93143   6.472%\n",
      "fc layer 2 self.abs_max_out: 2274.0\n",
      "fc layer 2 self.abs_max_out: 2279.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.875218/ 40.441013, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 93407   6.447%\n",
      "fc layer 1 self.abs_max_out: 4481.0\n",
      "lif layer 1 self.abs_max_v: 7636.5\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.627890/ 37.376091, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 93670   6.421%\n",
      "fc layer 3 self.abs_max_out: 887.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.850226/ 38.770473, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1081%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 93954   6.398%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.582427/ 38.167221, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 94207   6.373%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.449641/ 34.247826, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0794%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 94450   6.347%\n",
      "fc layer 1 self.abs_max_out: 4492.0\n",
      "lif layer 1 self.abs_max_v: 7648.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.455744/ 41.216713, val:  86.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0429%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 94685   6.321%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.607271/ 42.460732, val:  85.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1083%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 94944   6.297%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.594740/ 44.686893, val:  85.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 95181   6.272%\n",
      "lif layer 1 self.abs_max_v: 7668.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.920954/ 39.258785, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 95460   6.250%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.533648/ 36.700989, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3734%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 95710   6.227%\n",
      "fc layer 1 self.abs_max_out: 4506.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.388615/ 41.446342, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5615%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 95956   6.203%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.738715/ 37.737015, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7529%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 96215   6.181%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.239675/ 42.367928, val:  86.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1835%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 96425   6.156%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.511490/ 44.032734, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7958%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2627%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 96673   6.133%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.603824/ 39.353199, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 96926   6.111%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.500633/ 37.773281, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1083%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 97167   6.089%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.508210/ 37.556904, val:  87.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5535%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 97400   6.066%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.410056/ 33.802662, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5149%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 97630   6.044%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.485212/ 39.051388, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 97870   6.022%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.587971/ 37.303951, val:  87.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 98110   6.001%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.575612/ 36.140537, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1417%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 98352   5.980%\n",
      "fc layer 1 self.abs_max_out: 4578.0\n",
      "lif layer 1 self.abs_max_v: 7847.5\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.497025/ 36.341743, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1385%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 98603   5.960%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.764146/ 37.161819, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 98855   5.940%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.428564/ 39.761898, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 99090   5.919%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.215305/ 39.011593, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0567%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7305%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 99287   5.896%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.335076/ 40.686062, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 99516   5.876%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.354641/ 39.952702, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 99731   5.855%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.525786/ 40.357815, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 99959   5.834%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.214646/ 42.826145, val:  85.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6653%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5206%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 100143   5.812%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.045398/ 37.847107, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 100324   5.790%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.238948/ 37.756313, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 100526   5.769%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.274470/ 41.250271, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 100745   5.749%\n",
      "fc layer 3 self.abs_max_out: 899.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.253285/ 39.200096, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 100947   5.728%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.365371/ 39.846169, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1061%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 101183   5.710%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.348610/ 50.802235, val:  84.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 101395   5.691%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.184305/ 44.620354, val:  86.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 101597   5.671%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.335358/ 41.992115, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1138%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 101803   5.651%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.208152/ 40.737534, val:  87.92%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 102007   5.632%\n",
      "fc layer 2 self.abs_max_out: 2291.0\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.322771/ 38.161045, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 102215   5.613%\n",
      "fc layer 1 self.abs_max_out: 4637.0\n",
      "lif layer 1 self.abs_max_v: 8004.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.228426/ 42.513180, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 102410   5.594%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.423114/ 45.268032, val:  86.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 102638   5.577%\n",
      "fc layer 2 self.abs_max_out: 2325.0\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.109129/ 44.082916, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7132%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 102827   5.557%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.176938/ 42.709614, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 103024   5.539%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.312648/ 42.873314, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 103216   5.520%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.146121/ 41.706898, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 103399   5.501%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.134192/ 39.203060, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8773%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 103580   5.482%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.113344/ 43.936008, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8138%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 103771   5.464%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  0.875892/ 38.933826, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 103930   5.444%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.039731/ 41.865913, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 104103   5.425%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.223197/ 48.082092, val:  83.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7072%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 104299   5.408%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.072451/ 36.602959, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 104487   5.390%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.098610/ 37.990215, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 104665   5.372%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.392376/ 40.374622, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7883%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaec3b1517f4452a2132e7c8b63df68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñà‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.39238</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>40.37462</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v46y0e0r' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v46y0e0r</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251212_182043-v46y0e0r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jgdtq7y0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 16465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251212_223935-jgdtq7y0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jgdtq7y0' target=\"_blank\">smooth-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jgdtq7y0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jgdtq7y0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251212_223945_354', 'my_seed': 16465, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 367.0\n",
      "lif layer 1 self.abs_max_v: 367.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 393.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "fc layer 1 self.abs_max_out: 499.0\n",
      "lif layer 1 self.abs_max_v: 515.0\n",
      "lif layer 2 self.abs_max_v: 544.5\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "lif layer 1 self.abs_max_v: 520.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "lif layer 1 self.abs_max_v: 582.0\n",
      "lif layer 1 self.abs_max_v: 597.5\n",
      "fc layer 2 self.abs_max_out: 432.0\n",
      "fc layer 1 self.abs_max_out: 525.0\n",
      "lif layer 2 self.abs_max_v: 561.5\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "fc layer 2 self.abs_max_out: 447.0\n",
      "fc layer 2 self.abs_max_out: 450.0\n",
      "fc layer 2 self.abs_max_out: 482.0\n",
      "lif layer 2 self.abs_max_v: 630.0\n",
      "fc layer 1 self.abs_max_out: 560.0\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "fc layer 1 self.abs_max_out: 598.0\n",
      "lif layer 1 self.abs_max_v: 598.0\n",
      "lif layer 2 self.abs_max_v: 730.5\n",
      "fc layer 1 self.abs_max_out: 645.0\n",
      "lif layer 1 self.abs_max_v: 645.0\n",
      "fc layer 2 self.abs_max_out: 503.0\n",
      "fc layer 2 self.abs_max_out: 534.0\n",
      "lif layer 2 self.abs_max_v: 790.5\n",
      "fc layer 1 self.abs_max_out: 664.0\n",
      "lif layer 1 self.abs_max_v: 664.0\n",
      "fc layer 2 self.abs_max_out: 552.0\n",
      "fc layer 1 self.abs_max_out: 722.0\n",
      "lif layer 1 self.abs_max_v: 722.0\n",
      "fc layer 2 self.abs_max_out: 566.0\n",
      "fc layer 1 self.abs_max_out: 791.0\n",
      "lif layer 1 self.abs_max_v: 791.0\n",
      "fc layer 2 self.abs_max_out: 613.0\n",
      "lif layer 2 self.abs_max_v: 802.5\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "lif layer 2 self.abs_max_v: 821.0\n",
      "lif layer 2 self.abs_max_v: 827.5\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "lif layer 1 self.abs_max_v: 858.5\n",
      "fc layer 1 self.abs_max_out: 818.0\n",
      "lif layer 1 self.abs_max_v: 921.5\n",
      "fc layer 1 self.abs_max_out: 1039.0\n",
      "lif layer 1 self.abs_max_v: 1039.0\n",
      "fc layer 1 self.abs_max_out: 1117.0\n",
      "lif layer 1 self.abs_max_v: 1117.0\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "lif layer 2 self.abs_max_v: 832.5\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 881.0\n",
      "fc layer 2 self.abs_max_out: 718.0\n",
      "lif layer 2 self.abs_max_v: 896.5\n",
      "lif layer 2 self.abs_max_v: 929.5\n",
      "lif layer 2 self.abs_max_v: 970.0\n",
      "lif layer 2 self.abs_max_v: 1064.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 763.0\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "fc layer 2 self.abs_max_out: 876.0\n",
      "fc layer 2 self.abs_max_out: 905.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "fc layer 2 self.abs_max_out: 936.0\n",
      "fc layer 2 self.abs_max_out: 937.0\n",
      "lif layer 2 self.abs_max_v: 1069.0\n",
      "lif layer 1 self.abs_max_v: 1170.5\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "lif layer 1 self.abs_max_v: 1183.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "lif layer 2 self.abs_max_v: 1103.5\n",
      "lif layer 2 self.abs_max_v: 1160.0\n",
      "lif layer 2 self.abs_max_v: 1189.0\n",
      "fc layer 2 self.abs_max_out: 1007.0\n",
      "fc layer 1 self.abs_max_out: 1306.0\n",
      "lif layer 1 self.abs_max_v: 1306.0\n",
      "lif layer 2 self.abs_max_v: 1259.0\n",
      "lif layer 2 self.abs_max_v: 1378.5\n",
      "lif layer 1 self.abs_max_v: 1399.0\n",
      "lif layer 1 self.abs_max_v: 1427.5\n",
      "lif layer 1 self.abs_max_v: 1431.0\n",
      "lif layer 1 self.abs_max_v: 1608.0\n",
      "lif layer 1 self.abs_max_v: 1781.0\n",
      "fc layer 1 self.abs_max_out: 1326.0\n",
      "lif layer 2 self.abs_max_v: 1422.5\n",
      "fc layer 2 self.abs_max_out: 1024.0\n",
      "fc layer 1 self.abs_max_out: 1394.0\n",
      "lif layer 2 self.abs_max_v: 1475.5\n",
      "lif layer 2 self.abs_max_v: 1508.0\n",
      "fc layer 2 self.abs_max_out: 1049.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "fc layer 1 self.abs_max_out: 1395.0\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "lif layer 1 self.abs_max_v: 1863.0\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "lif layer 2 self.abs_max_v: 1546.5\n",
      "lif layer 1 self.abs_max_v: 1999.5\n",
      "fc layer 1 self.abs_max_out: 1410.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 1 self.abs_max_out: 1424.0\n",
      "lif layer 1 self.abs_max_v: 2076.5\n",
      "fc layer 2 self.abs_max_out: 1151.0\n",
      "fc layer 1 self.abs_max_out: 1568.0\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "lif layer 1 self.abs_max_v: 2188.5\n",
      "lif layer 1 self.abs_max_v: 2371.5\n",
      "fc layer 2 self.abs_max_out: 1201.0\n",
      "fc layer 1 self.abs_max_out: 1575.0\n",
      "fc layer 1 self.abs_max_out: 1637.0\n",
      "fc layer 1 self.abs_max_out: 1736.0\n",
      "lif layer 1 self.abs_max_v: 2396.0\n",
      "lif layer 1 self.abs_max_v: 2560.0\n",
      "lif layer 1 self.abs_max_v: 2582.0\n",
      "fc layer 2 self.abs_max_out: 1205.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 15.254165/ 86.884445, val:  35.00%, val_best:  35.00%, tr:  97.45%, tr_best:  97.45%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1319%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2132  21.777%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "lif layer 2 self.abs_max_v: 1553.0\n",
      "lif layer 1 self.abs_max_v: 2654.0\n",
      "lif layer 2 self.abs_max_v: 1561.5\n",
      "lif layer 1 self.abs_max_v: 2691.5\n",
      "lif layer 2 self.abs_max_v: 1584.0\n",
      "lif layer 2 self.abs_max_v: 1593.0\n",
      "lif layer 2 self.abs_max_v: 1639.5\n",
      "lif layer 1 self.abs_max_v: 2854.0\n",
      "lif layer 1 self.abs_max_v: 2982.0\n",
      "lif layer 1 self.abs_max_v: 2989.0\n",
      "lif layer 2 self.abs_max_v: 1662.0\n",
      "lif layer 2 self.abs_max_v: 1692.0\n",
      "lif layer 2 self.abs_max_v: 1702.0\n",
      "fc layer 1 self.abs_max_out: 1779.0\n",
      "lif layer 2 self.abs_max_v: 1721.0\n",
      "fc layer 2 self.abs_max_out: 1223.0\n",
      "lif layer 2 self.abs_max_v: 1834.0\n",
      "fc layer 1 self.abs_max_out: 1902.0\n",
      "lif layer 1 self.abs_max_v: 3227.5\n",
      "fc layer 1 self.abs_max_out: 2017.0\n",
      "lif layer 1 self.abs_max_v: 3631.0\n",
      "fc layer 2 self.abs_max_out: 1253.0\n",
      "fc layer 2 self.abs_max_out: 1326.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.694325/ 73.342903, val:  35.42%, val_best:  35.42%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3726  19.030%\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 2 self.abs_max_out: 1406.0\n",
      "lif layer 2 self.abs_max_v: 1948.0\n",
      "fc layer 1 self.abs_max_out: 2128.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.936610/ 86.687706, val:  43.75%, val_best:  43.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5158  17.562%\n",
      "fc layer 2 self.abs_max_out: 1415.0\n",
      "fc layer 2 self.abs_max_out: 1429.0\n",
      "fc layer 2 self.abs_max_out: 1433.0\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "fc layer 2 self.abs_max_out: 1456.0\n",
      "fc layer 2 self.abs_max_out: 1469.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 3 self.abs_max_out: 530.0\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "fc layer 3 self.abs_max_out: 547.0\n",
      "lif layer 2 self.abs_max_v: 1950.0\n",
      "lif layer 2 self.abs_max_v: 1984.5\n",
      "lif layer 2 self.abs_max_v: 2003.5\n",
      "lif layer 2 self.abs_max_v: 2066.0\n",
      "lif layer 2 self.abs_max_v: 2185.0\n",
      "lif layer 2 self.abs_max_v: 2224.5\n",
      "lif layer 2 self.abs_max_v: 2434.5\n",
      "fc layer 2 self.abs_max_out: 1506.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.344736/ 73.583778, val:  49.58%, val_best:  49.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6515  16.637%\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "fc layer 1 self.abs_max_out: 2267.0\n",
      "lif layer 1 self.abs_max_v: 3714.5\n",
      "lif layer 1 self.abs_max_v: 3829.5\n",
      "fc layer 2 self.abs_max_out: 1602.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 11.106766/ 86.710449, val:  43.75%, val_best:  49.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7863  16.063%\n",
      "fc layer 1 self.abs_max_out: 2326.0\n",
      "lif layer 1 self.abs_max_v: 3839.0\n",
      "lif layer 1 self.abs_max_v: 3957.5\n",
      "fc layer 1 self.abs_max_out: 2347.0\n",
      "lif layer 1 self.abs_max_v: 4303.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.866179/ 87.377335, val:  40.00%, val_best:  49.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9112  15.512%\n",
      "fc layer 3 self.abs_max_out: 621.0\n",
      "fc layer 1 self.abs_max_out: 2381.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.504398/ 65.958939, val:  54.17%, val_best:  54.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1611%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10416  15.199%\n",
      "fc layer 3 self.abs_max_out: 622.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 3 self.abs_max_out: 633.0\n",
      "fc layer 1 self.abs_max_out: 2403.0\n",
      "fc layer 1 self.abs_max_out: 2638.0\n",
      "lif layer 1 self.abs_max_v: 4463.5\n",
      "fc layer 2 self.abs_max_out: 1650.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 10.027547/ 68.653908, val:  51.67%, val_best:  54.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0252%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11649  14.874%\n",
      "fc layer 2 self.abs_max_out: 1669.0\n",
      "lif layer 1 self.abs_max_v: 4505.5\n",
      "lif layer 1 self.abs_max_v: 4517.0\n",
      "lif layer 1 self.abs_max_v: 4671.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.940193/ 51.852940, val:  55.00%, val_best:  55.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12869  14.606%\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 1 self.abs_max_out: 2784.0\n",
      "lif layer 1 self.abs_max_v: 4735.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  9.771530/ 55.926498, val:  65.00%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14104  14.407%\n",
      "fc layer 1 self.abs_max_out: 2952.0\n",
      "lif layer 1 self.abs_max_v: 4770.0\n",
      "lif layer 1 self.abs_max_v: 4865.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  9.592379/ 75.297302, val:  50.00%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4376%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15272  14.181%\n",
      "fc layer 1 self.abs_max_out: 3014.0\n",
      "lif layer 1 self.abs_max_v: 4898.5\n",
      "lif layer 1 self.abs_max_v: 5007.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  9.341363/ 71.526970, val:  51.25%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16438  13.992%\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "fc layer 1 self.abs_max_out: 3032.0\n",
      "lif layer 1 self.abs_max_v: 5040.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  9.060444/ 64.858505, val:  49.17%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17594  13.824%\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "fc layer 3 self.abs_max_out: 754.0\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 1 self.abs_max_out: 3136.0\n",
      "lif layer 1 self.abs_max_v: 5080.5\n",
      "lif layer 1 self.abs_max_v: 5112.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.696209/ 73.739929, val:  52.92%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18704  13.647%\n",
      "lif layer 2 self.abs_max_v: 2436.0\n",
      "fc layer 1 self.abs_max_out: 3299.0\n",
      "lif layer 1 self.abs_max_v: 5380.0\n",
      "lif layer 1 self.abs_max_v: 5458.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  9.061999/ 46.994041, val:  67.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8732%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19806  13.487%\n",
      "lif layer 2 self.abs_max_v: 2437.0\n",
      "lif layer 2 self.abs_max_v: 2523.5\n",
      "lif layer 2 self.abs_max_v: 2559.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  8.805778/ 78.289772, val:  57.08%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 20921  13.356%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  9.186734/ 56.998882, val:  59.58%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22060  13.255%\n",
      "lif layer 1 self.abs_max_v: 5494.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  8.235518/ 69.802422, val:  49.17%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0404%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23087  13.101%\n",
      "lif layer 2 self.abs_max_v: 2581.5\n",
      "lif layer 2 self.abs_max_v: 2646.0\n",
      "lif layer 2 self.abs_max_v: 2692.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  8.330991/ 68.231392, val:  53.33%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0310%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24136  12.976%\n",
      "lif layer 2 self.abs_max_v: 2752.5\n",
      "lif layer 2 self.abs_max_v: 2753.5\n",
      "lif layer 2 self.abs_max_v: 2772.5\n",
      "lif layer 2 self.abs_max_v: 2773.5\n",
      "lif layer 2 self.abs_max_v: 2823.0\n",
      "lif layer 2 self.abs_max_v: 2832.0\n",
      "fc layer 2 self.abs_max_out: 1753.0\n",
      "lif layer 2 self.abs_max_v: 3161.0\n",
      "fc layer 1 self.abs_max_out: 3342.0\n",
      "lif layer 1 self.abs_max_v: 5551.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  8.288735/ 44.409889, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25177  12.859%\n",
      "fc layer 2 self.abs_max_out: 1760.0\n",
      "lif layer 2 self.abs_max_v: 3168.5\n",
      "fc layer 2 self.abs_max_out: 1762.0\n",
      "fc layer 2 self.abs_max_out: 1768.0\n",
      "lif layer 2 self.abs_max_v: 3213.5\n",
      "lif layer 1 self.abs_max_v: 5554.5\n",
      "fc layer 2 self.abs_max_out: 1772.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.839412/ 74.065125, val:  53.75%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1152%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3997%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26190  12.739%\n",
      "lif layer 2 self.abs_max_v: 3283.5\n",
      "fc layer 2 self.abs_max_out: 1782.0\n",
      "fc layer 2 self.abs_max_out: 1798.0\n",
      "lif layer 1 self.abs_max_v: 5559.5\n",
      "fc layer 1 self.abs_max_out: 3459.0\n",
      "lif layer 1 self.abs_max_v: 5608.0\n",
      "lif layer 1 self.abs_max_v: 5786.0\n",
      "lif layer 1 self.abs_max_v: 5787.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  8.086111/ 68.839622, val:  57.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0104%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2033%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27192  12.625%\n",
      "fc layer 2 self.abs_max_out: 1825.0\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "fc layer 2 self.abs_max_out: 1855.0\n",
      "lif layer 2 self.abs_max_v: 3325.5\n",
      "fc layer 2 self.abs_max_out: 1860.0\n",
      "lif layer 2 self.abs_max_v: 3326.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  8.053207/ 79.726662, val:  50.42%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0414%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28281  12.560%\n",
      "lif layer 2 self.abs_max_v: 3342.5\n",
      "fc layer 2 self.abs_max_out: 1868.0\n",
      "lif layer 2 self.abs_max_v: 3388.0\n",
      "lif layer 2 self.abs_max_v: 3392.0\n",
      "fc layer 2 self.abs_max_out: 1895.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.791986/ 65.684685, val:  52.50%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29328  12.482%\n",
      "lif layer 2 self.abs_max_v: 3432.0\n",
      "lif layer 2 self.abs_max_v: 3553.5\n",
      "fc layer 2 self.abs_max_out: 1926.0\n",
      "fc layer 1 self.abs_max_out: 3677.0\n",
      "lif layer 1 self.abs_max_v: 5894.0\n",
      "lif layer 1 self.abs_max_v: 6091.0\n",
      "lif layer 1 self.abs_max_v: 6197.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.857131/ 54.577160, val:  59.58%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30329  12.392%\n",
      "fc layer 2 self.abs_max_out: 1946.0\n",
      "fc layer 1 self.abs_max_out: 3692.0\n",
      "lif layer 1 self.abs_max_v: 6208.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  7.334959/ 64.250740, val:  59.58%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0360%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31308  12.300%\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "lif layer 2 self.abs_max_v: 3559.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  7.053287/ 41.159878, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32248  12.200%\n",
      "fc layer 2 self.abs_max_out: 1993.0\n",
      "lif layer 2 self.abs_max_v: 3605.0\n",
      "lif layer 2 self.abs_max_v: 3608.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  7.365730/ 45.233440, val:  67.50%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33212  12.116%\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.643097/ 63.310055, val:  54.58%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7609%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34108  12.014%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.866224/ 51.758907, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9188%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35014  11.922%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  7.130650/ 41.558498, val:  75.00%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2639%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 35927  11.838%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.580080/ 60.564400, val:  60.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2380%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 36817  11.752%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.550870/ 51.233051, val:  66.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37673  11.661%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.940102/ 37.067783, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4786%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38577  11.590%\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "lif layer 2 self.abs_max_v: 3625.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  6.544927/ 50.100777, val:  69.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39424  11.506%\n",
      "lif layer 2 self.abs_max_v: 3645.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  6.289421/ 39.353592, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40243  11.418%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  6.315322/ 62.158394, val:  64.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0575%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41050  11.333%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  6.347249/ 39.025539, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 41915  11.267%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.697001/ 38.291100, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42712  11.187%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.984833/ 47.283978, val:  72.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43516  11.112%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.808997/ 39.535858, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0530%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44281  11.032%\n",
      "fc layer 2 self.abs_max_out: 2010.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.755473/ 42.509640, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8844%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45048  10.956%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.587461/ 40.605122, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 45808  10.882%\n",
      "lif layer 2 self.abs_max_v: 3681.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.451781/ 33.544014, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46556  10.808%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  6.018523/ 34.385818, val:  82.08%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0987%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47322  10.742%\n",
      "fc layer 1 self.abs_max_out: 3717.0\n",
      "lif layer 1 self.abs_max_v: 6308.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  6.297068/ 42.938450, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48110  10.683%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.486069/ 34.215389, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 48873  10.622%\n",
      "lif layer 2 self.abs_max_v: 3694.0\n",
      "fc layer 2 self.abs_max_out: 2040.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.660165/ 49.198673, val:  76.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49635  10.562%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.646236/ 50.946972, val:  70.00%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50370  10.500%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.346119/ 44.259792, val:  78.75%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0419%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51082  10.436%\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "fc layer 1 self.abs_max_out: 3734.0\n",
      "lif layer 1 self.abs_max_v: 6354.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.207024/ 47.562866, val:  74.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8172%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51799  10.375%\n",
      "fc layer 3 self.abs_max_out: 824.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.916022/ 50.546516, val:  73.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4334%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52452  10.303%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.489125/ 35.504421, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1075%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53085  10.231%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.191487/ 46.354591, val:  75.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53765  10.170%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.600985/ 44.701344, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54417  10.106%\n",
      "fc layer 1 self.abs_max_out: 3745.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.969864/ 45.721806, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55083  10.047%\n",
      "lif layer 2 self.abs_max_v: 3712.5\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.803902/ 58.322834, val:  68.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55745   9.990%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.908418/ 43.690029, val:  76.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56392   9.931%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.933584/ 50.061596, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57047   9.876%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.863084/ 43.200539, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57727   9.828%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.697173/ 47.312733, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4576%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58368   9.774%\n",
      "fc layer 1 self.abs_max_out: 3746.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.566766/ 67.477516, val:  62.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.24 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58996   9.720%\n",
      "lif layer 2 self.abs_max_v: 3733.5\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.520941/ 41.698414, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59598   9.663%\n",
      "fc layer 3 self.abs_max_out: 840.0\n",
      "fc layer 1 self.abs_max_out: 3747.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.412651/ 51.318146, val:  75.42%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0618%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60203   9.608%\n",
      "fc layer 1 self.abs_max_out: 3771.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.289365/ 39.007080, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6910%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60769   9.550%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.246591/ 68.645004, val:  65.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6013%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61337   9.493%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.324224/ 54.778751, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0765%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61930   9.442%\n",
      "fc layer 3 self.abs_max_out: 877.0\n",
      "fc layer 3 self.abs_max_out: 901.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.347381/ 58.425278, val:  70.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0344%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62486   9.386%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.336034/ 46.051601, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63054   9.334%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.291485/ 39.000988, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63630   9.285%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.022333/ 39.442913, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64186   9.234%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.223427/ 40.563480, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64747   9.186%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.924319/ 49.881752, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0342%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7095%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65299   9.137%\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 3 self.abs_max_out: 967.0\n",
      "fc layer 1 self.abs_max_out: 3843.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.349711/ 45.865356, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0519%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65861   9.091%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.731761/ 38.471302, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66379   9.040%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.920622/ 40.215946, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0408%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66934   8.996%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.723959/ 39.549736, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67468   8.950%\n",
      "fc layer 2 self.abs_max_out: 2046.0\n",
      "fc layer 1 self.abs_max_out: 3867.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.766015/ 41.167831, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 67960   8.900%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.920527/ 40.573563, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68487   8.855%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.943782/ 43.331074, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69016   8.812%\n",
      "fc layer 2 self.abs_max_out: 2047.0\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.852987/ 54.455452, val:  70.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69520   8.767%\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.430353/ 37.385494, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70005   8.720%\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "fc layer 1 self.abs_max_out: 3931.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.482605/ 34.156273, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70517   8.678%\n",
      "fc layer 2 self.abs_max_out: 2103.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  4.054485/ 42.866787, val:  82.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71035   8.638%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.429502/ 42.879257, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7679%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1335%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71512   8.594%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.744871/ 43.988701, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72019   8.554%\n",
      "fc layer 1 self.abs_max_out: 3946.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.771629/ 41.259975, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72509   8.513%\n",
      "fc layer 1 self.abs_max_out: 3958.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.681506/ 43.570869, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72992   8.472%\n",
      "fc layer 2 self.abs_max_out: 2106.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.427895/ 46.402740, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73471   8.432%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.351858/ 42.766251, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73937   8.391%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.282220/ 51.383945, val:  75.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0050%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74394   8.351%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.599063/ 59.064438, val:  78.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74871   8.313%\n",
      "fc layer 1 self.abs_max_out: 4040.0\n",
      "lif layer 1 self.abs_max_v: 6415.5\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.729153/ 40.128201, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5690%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75376   8.279%\n",
      "lif layer 1 self.abs_max_v: 6479.5\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.113847/ 46.681099, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75805   8.237%\n",
      "fc layer 2 self.abs_max_out: 2163.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.735131/ 52.550690, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6182%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76297   8.204%\n",
      "lif layer 2 self.abs_max_v: 3738.5\n",
      "fc layer 1 self.abs_max_out: 4041.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.333361/ 60.215977, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76758   8.167%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.111937/ 37.209469, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1239%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77166   8.126%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.826411/ 43.190605, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77569   8.085%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.354297/ 44.292248, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78000   8.048%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.869554/ 49.431595, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78399   8.008%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.153008/ 49.115406, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0498%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78821   7.971%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.182446/ 48.260384, val:  82.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79270   7.938%\n",
      "fc layer 1 self.abs_max_out: 4045.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.208270/ 44.139915, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79687   7.903%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.029081/ 51.576241, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80100   7.867%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.887129/ 41.420902, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0975%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8885%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80489   7.830%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.549437/ 52.424343, val:  75.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6866%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 80872   7.793%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.757656/ 45.555759, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81244   7.756%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.114674/ 47.752079, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4129%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81646   7.722%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  3.350190/ 49.411251, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82083   7.692%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.938299/ 52.833817, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82483   7.659%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.063405/ 42.669121, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82884   7.627%\n",
      "fc layer 1 self.abs_max_out: 4072.0\n",
      "fc layer 2 self.abs_max_out: 2183.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.471169/ 45.136864, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83255   7.593%\n",
      "fc layer 1 self.abs_max_out: 4097.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.710208/ 45.036663, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83653   7.562%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.738862/ 38.282143, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3762%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1814%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84035   7.530%\n",
      "fc layer 1 self.abs_max_out: 4104.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.676670/ 38.261684, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0283%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84439   7.500%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.858943/ 47.213425, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84832   7.470%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.507271/ 38.037735, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85185   7.437%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.584855/ 41.938881, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1168%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85544   7.405%\n",
      "fc layer 2 self.abs_max_out: 2204.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.673157/ 40.123211, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4638%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 85892   7.373%\n",
      "fc layer 1 self.abs_max_out: 4187.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.567398/ 40.535797, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86252   7.342%\n",
      "fc layer 1 self.abs_max_out: 4196.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.513015/ 40.676945, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0575%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86600   7.311%\n",
      "fc layer 2 self.abs_max_out: 2236.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.241829/ 42.146797, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1169%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9794%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 86942   7.279%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.641948/ 74.206238, val:  69.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87317   7.251%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.504033/ 46.416306, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87657   7.221%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.758123/ 42.728901, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 88017   7.192%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.370514/ 48.157162, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1142%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88316   7.160%\n",
      "lif layer 1 self.abs_max_v: 6538.5\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.642013/ 42.684769, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1361%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6594%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88668   7.131%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.523101/ 43.435284, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3467%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 89001   7.102%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.524199/ 53.516453, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1156%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89368   7.076%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.359902/ 51.419987, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8994%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89706   7.048%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.358661/ 54.173817, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1216%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 90037   7.020%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.520891/ 43.545002, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90377   6.994%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.646372/ 41.417110, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90733   6.968%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.352641/ 45.143703, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 91053   6.941%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.424475/ 45.944809, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1399%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91394   6.915%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.631337/ 48.431736, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91743   6.891%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.891139/ 44.369072, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 92050   6.863%\n",
      "fc layer 1 self.abs_max_out: 4214.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.240519/ 51.898293, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1082%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92377   6.838%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.166487/ 46.277405, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92697   6.812%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.151122/ 52.914154, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 93001   6.785%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.248783/ 49.426098, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1687%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93302   6.759%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.069863/ 52.276535, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93609   6.734%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.430592/ 46.739693, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93940   6.710%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.179561/ 45.832668, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 94252   6.686%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.363489/ 46.207352, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94566   6.662%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.943234/ 44.725914, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94841   6.635%\n",
      "fc layer 1 self.abs_max_out: 4281.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.319182/ 49.651543, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1036%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 95156   6.612%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.276804/ 47.808315, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95470   6.589%\n",
      "lif layer 1 self.abs_max_v: 6595.5\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.152461/ 47.776215, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2418%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95765   6.565%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.897669/ 51.563282, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1240%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 96045   6.540%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.465818/ 45.082531, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 96380   6.520%\n",
      "fc layer 3 self.abs_max_out: 971.0\n",
      "fc layer 3 self.abs_max_out: 975.0\n",
      "fc layer 3 self.abs_max_out: 994.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.488924/ 56.197418, val:  80.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96703   6.499%\n",
      "fc layer 3 self.abs_max_out: 1000.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.031321/ 51.913383, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0511%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 97003   6.476%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  2.313744/ 50.898888, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0946%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 97306   6.454%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  2.067813/ 67.344048, val:  72.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97586   6.431%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.014134/ 55.922260, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97877   6.409%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.014796/ 47.366280, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.30 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 98147   6.385%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.782497/ 50.091446, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 98408   6.362%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.844366/ 46.109478, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98665   6.338%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  2.100467/ 53.201321, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1255%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98946   6.317%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  2.014683/ 42.060551, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1205%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 99215   6.295%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.966220/ 45.720081, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 99497   6.274%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  2.004020/ 43.579029, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99797   6.254%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  2.178278/ 46.727200, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0958%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6145%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 100094   6.234%\n",
      "fc layer 1 self.abs_max_out: 4363.0\n",
      "fc layer 3 self.abs_max_out: 1023.0\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  2.081625/ 45.081882, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0378%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 100376   6.214%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  2.135490/ 47.083313, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2033%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 100656   6.194%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  2.064749/ 48.592918, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100946   6.174%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.954657/ 43.050243, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 101210   6.154%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.479303/ 55.173588, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 101430   6.131%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.693876/ 50.583801, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 101676   6.109%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.951992/ 46.707695, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101958   6.090%\n",
      "fc layer 2 self.abs_max_out: 2248.0\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.626063/ 47.619961, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0264%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 102218   6.070%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  2.038393/ 45.870449, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1024%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 102488   6.051%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.822145/ 45.142185, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 102739   6.031%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.829018/ 48.065056, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0816%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102982   6.011%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.619794/ 48.752960, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 103210   5.990%\n",
      "fc layer 2 self.abs_max_out: 2379.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.698968/ 46.776340, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0441%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 103448   5.970%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.665340/ 48.103550, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 103682   5.950%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  2.115048/ 47.281887, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1082%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3045%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103951   5.932%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  2.004144/ 44.683517, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0403%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 104223   5.914%\n",
      "fc layer 3 self.abs_max_out: 1024.0\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.568744/ 46.869438, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3807%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 104467   5.895%\n",
      "fc layer 3 self.abs_max_out: 1035.0\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.758685/ 49.725368, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 104714   5.877%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.690533/ 57.800800, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0519%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9118%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 104951   5.858%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.579495/ 53.601933, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 105185   5.839%\n",
      "fc layer 3 self.abs_max_out: 1040.0\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.475284/ 47.942287, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1158%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 105401   5.820%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.627164/ 46.988873, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 105611   5.800%\n",
      "fc layer 1 self.abs_max_out: 4365.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.351880/ 49.350639, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9616%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 105820   5.780%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.548805/ 51.950615, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0993%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 106033   5.761%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.553965/ 57.680752, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 106254   5.742%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.540462/ 50.966663, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 106478   5.724%\n",
      "fc layer 1 self.abs_max_out: 4418.0\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.628683/ 54.010452, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 106709   5.707%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.701087/ 51.098061, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 106931   5.689%\n",
      "fc layer 3 self.abs_max_out: 1049.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.482242/ 49.627979, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 107144   5.671%\n",
      "lif layer 1 self.abs_max_v: 6648.0\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.484651/ 51.075218, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0335%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 107350   5.652%\n",
      "lif layer 2 self.abs_max_v: 3748.5\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.214389/ 53.924717, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7173%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 107542   5.633%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.642091/ 49.611145, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4599%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 107764   5.616%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.753411/ 50.761940, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 107998   5.600%\n",
      "fc layer 3 self.abs_max_out: 1071.0\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.608425/ 56.793800, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 108211   5.582%\n",
      "fc layer 2 self.abs_max_out: 2398.0\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.317982/ 57.593056, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0275%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 108407   5.564%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.370006/ 63.895058, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1069%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5870%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ea3d26f40d486a9df26c6b04e546cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñà‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.37001</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.82917</td></tr><tr><td>val_loss</td><td>63.89506</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jgdtq7y0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jgdtq7y0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251212_223935-jgdtq7y0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7rxhps9h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 3573\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_025852-7rxhps9h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rxhps9h' target=\"_blank\">logical-sweep-16</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rxhps9h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rxhps9h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251213_025902_651', 'my_seed': 3573, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 266.0\n",
      "lif layer 1 self.abs_max_v: 266.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 177.0\n",
      "lif layer 2 self.abs_max_v: 177.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "lif layer 1 self.abs_max_v: 319.0\n",
      "fc layer 2 self.abs_max_out: 267.0\n",
      "lif layer 2 self.abs_max_v: 283.0\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "fc layer 1 self.abs_max_out: 345.0\n",
      "lif layer 1 self.abs_max_v: 375.5\n",
      "fc layer 2 self.abs_max_out: 314.0\n",
      "lif layer 2 self.abs_max_v: 316.5\n",
      "lif layer 1 self.abs_max_v: 410.0\n",
      "lif layer 2 self.abs_max_v: 364.0\n",
      "lif layer 1 self.abs_max_v: 411.0\n",
      "lif layer 2 self.abs_max_v: 429.5\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "lif layer 1 self.abs_max_v: 442.5\n",
      "fc layer 2 self.abs_max_out: 371.0\n",
      "lif layer 2 self.abs_max_v: 481.0\n",
      "lif layer 2 self.abs_max_v: 575.5\n",
      "fc layer 1 self.abs_max_out: 554.0\n",
      "lif layer 1 self.abs_max_v: 554.0\n",
      "lif layer 2 self.abs_max_v: 635.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 2 self.abs_max_out: 374.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "fc layer 2 self.abs_max_out: 391.0\n",
      "lif layer 2 self.abs_max_v: 688.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "lif layer 1 self.abs_max_v: 583.0\n",
      "fc layer 2 self.abs_max_out: 475.0\n",
      "lif layer 1 self.abs_max_v: 609.0\n",
      "fc layer 2 self.abs_max_out: 518.0\n",
      "lif layer 1 self.abs_max_v: 637.5\n",
      "fc layer 2 self.abs_max_out: 555.0\n",
      "lif layer 2 self.abs_max_v: 839.5\n",
      "fc layer 1 self.abs_max_out: 570.0\n",
      "lif layer 1 self.abs_max_v: 647.0\n",
      "lif layer 2 self.abs_max_v: 925.0\n",
      "fc layer 1 self.abs_max_out: 713.0\n",
      "lif layer 1 self.abs_max_v: 728.0\n",
      "fc layer 1 self.abs_max_out: 733.0\n",
      "lif layer 1 self.abs_max_v: 733.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 2 self.abs_max_out: 557.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 1 self.abs_max_out: 752.0\n",
      "lif layer 1 self.abs_max_v: 752.0\n",
      "lif layer 1 self.abs_max_v: 763.5\n",
      "lif layer 1 self.abs_max_v: 775.0\n",
      "lif layer 1 self.abs_max_v: 779.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "fc layer 1 self.abs_max_out: 835.0\n",
      "lif layer 1 self.abs_max_v: 835.0\n",
      "lif layer 2 self.abs_max_v: 935.5\n",
      "lif layer 1 self.abs_max_v: 868.5\n",
      "lif layer 1 self.abs_max_v: 880.5\n",
      "lif layer 1 self.abs_max_v: 883.5\n",
      "lif layer 1 self.abs_max_v: 888.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "lif layer 1 self.abs_max_v: 908.5\n",
      "lif layer 1 self.abs_max_v: 992.5\n",
      "lif layer 1 self.abs_max_v: 1041.5\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "fc layer 2 self.abs_max_out: 591.0\n",
      "lif layer 1 self.abs_max_v: 1078.5\n",
      "lif layer 1 self.abs_max_v: 1106.0\n",
      "fc layer 2 self.abs_max_out: 595.0\n",
      "lif layer 2 self.abs_max_v: 1003.5\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 2 self.abs_max_out: 634.0\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "lif layer 2 self.abs_max_v: 1037.5\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "lif layer 2 self.abs_max_v: 1199.0\n",
      "fc layer 1 self.abs_max_out: 918.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "fc layer 1 self.abs_max_out: 1036.0\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "fc layer 2 self.abs_max_out: 731.0\n",
      "fc layer 2 self.abs_max_out: 767.0\n",
      "fc layer 2 self.abs_max_out: 844.0\n",
      "lif layer 1 self.abs_max_v: 1118.5\n",
      "fc layer 2 self.abs_max_out: 863.0\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "fc layer 1 self.abs_max_out: 1094.0\n",
      "lif layer 1 self.abs_max_v: 1265.5\n",
      "lif layer 1 self.abs_max_v: 1350.0\n",
      "lif layer 1 self.abs_max_v: 1454.5\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "lif layer 2 self.abs_max_v: 1204.5\n",
      "fc layer 1 self.abs_max_out: 1323.0\n",
      "lif layer 1 self.abs_max_v: 1552.0\n",
      "lif layer 1 self.abs_max_v: 1730.0\n",
      "fc layer 2 self.abs_max_out: 969.0\n",
      "lif layer 2 self.abs_max_v: 1219.0\n",
      "lif layer 1 self.abs_max_v: 1926.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "lif layer 2 self.abs_max_v: 1256.0\n",
      "fc layer 2 self.abs_max_out: 981.0\n",
      "lif layer 2 self.abs_max_v: 1290.0\n",
      "lif layer 2 self.abs_max_v: 1339.0\n",
      "lif layer 2 self.abs_max_v: 1362.5\n",
      "fc layer 2 self.abs_max_out: 988.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "fc layer 2 self.abs_max_out: 1023.0\n",
      "fc layer 2 self.abs_max_out: 1069.0\n",
      "fc layer 2 self.abs_max_out: 1143.0\n",
      "lif layer 1 self.abs_max_v: 1931.0\n",
      "fc layer 1 self.abs_max_out: 1371.0\n",
      "lif layer 1 self.abs_max_v: 1966.0\n",
      "fc layer 1 self.abs_max_out: 1630.0\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "lif layer 2 self.abs_max_v: 1373.0\n",
      "fc layer 2 self.abs_max_out: 1148.0\n",
      "fc layer 2 self.abs_max_out: 1155.0\n",
      "fc layer 2 self.abs_max_out: 1158.0\n",
      "fc layer 3 self.abs_max_out: 367.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "lif layer 1 self.abs_max_v: 1993.5\n",
      "lif layer 1 self.abs_max_v: 2004.5\n",
      "lif layer 1 self.abs_max_v: 2182.5\n",
      "lif layer 1 self.abs_max_v: 2286.5\n",
      "lif layer 2 self.abs_max_v: 1396.5\n",
      "lif layer 2 self.abs_max_v: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1164.0\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "lif layer 2 self.abs_max_v: 1422.0\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "lif layer 2 self.abs_max_v: 1470.5\n",
      "lif layer 1 self.abs_max_v: 2290.0\n",
      "fc layer 1 self.abs_max_out: 1634.0\n",
      "fc layer 1 self.abs_max_out: 1700.0\n",
      "lif layer 1 self.abs_max_v: 2301.0\n",
      "lif layer 1 self.abs_max_v: 2828.5\n",
      "lif layer 1 self.abs_max_v: 2957.5\n",
      "fc layer 2 self.abs_max_out: 1245.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.548556/ 59.974918, val:  42.92%, val_best:  42.92%, tr:  96.53%, tr_best:  96.53%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.7724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2266  23.146%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1285.0\n",
      "lif layer 2 self.abs_max_v: 1518.5\n",
      "lif layer 2 self.abs_max_v: 1543.0\n",
      "fc layer 1 self.abs_max_out: 1739.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 2 self.abs_max_out: 1298.0\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 1 self.abs_max_out: 1850.0\n",
      "fc layer 1 self.abs_max_out: 1913.0\n",
      "lif layer 2 self.abs_max_v: 1601.5\n",
      "lif layer 2 self.abs_max_v: 1612.5\n",
      "lif layer 2 self.abs_max_v: 1671.5\n",
      "fc layer 1 self.abs_max_out: 1918.0\n",
      "fc layer 1 self.abs_max_out: 2069.0\n",
      "lif layer 1 self.abs_max_v: 3439.5\n",
      "lif layer 1 self.abs_max_v: 3595.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.053266/ 51.852768, val:  43.75%, val_best:  43.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1267%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3880  19.816%\n",
      "lif layer 2 self.abs_max_v: 1723.5\n",
      "lif layer 2 self.abs_max_v: 1731.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.653174/ 50.843575, val:  50.00%, val_best:  50.00%, tr:  99.39%, tr_best:  99.49%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5341  18.185%\n",
      "lif layer 2 self.abs_max_v: 1738.0\n",
      "lif layer 2 self.abs_max_v: 1775.0\n",
      "lif layer 2 self.abs_max_v: 1784.0\n",
      "lif layer 2 self.abs_max_v: 1813.5\n",
      "lif layer 2 self.abs_max_v: 1862.5\n",
      "lif layer 2 self.abs_max_v: 1873.0\n",
      "fc layer 1 self.abs_max_out: 2078.0\n",
      "fc layer 1 self.abs_max_out: 2134.0\n",
      "lif layer 1 self.abs_max_v: 3657.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.106146/ 68.256561, val:  50.00%, val_best:  50.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6692  17.089%\n",
      "fc layer 1 self.abs_max_out: 2256.0\n",
      "fc layer 3 self.abs_max_out: 558.0\n",
      "lif layer 1 self.abs_max_v: 3907.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.091757/ 63.107887, val:  44.58%, val_best:  50.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8071  16.488%\n",
      "fc layer 2 self.abs_max_out: 1430.0\n",
      "fc layer 1 self.abs_max_out: 2267.0\n",
      "lif layer 2 self.abs_max_v: 1923.0\n",
      "lif layer 2 self.abs_max_v: 1952.5\n",
      "lif layer 2 self.abs_max_v: 1962.0\n",
      "fc layer 1 self.abs_max_out: 2322.0\n",
      "lif layer 1 self.abs_max_v: 3984.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.573264/ 42.281837, val:  56.25%, val_best:  56.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9356  15.928%\n",
      "fc layer 2 self.abs_max_out: 1487.0\n",
      "lif layer 2 self.abs_max_v: 2071.5\n",
      "fc layer 2 self.abs_max_out: 1529.0\n",
      "lif layer 2 self.abs_max_v: 2150.5\n",
      "lif layer 2 self.abs_max_v: 2286.0\n",
      "fc layer 2 self.abs_max_out: 1534.0\n",
      "fc layer 2 self.abs_max_out: 1685.0\n",
      "fc layer 1 self.abs_max_out: 2376.0\n",
      "lif layer 1 self.abs_max_v: 4137.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.051229/ 73.678543, val:  40.83%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2045%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10665  15.563%\n",
      "lif layer 2 self.abs_max_v: 2427.0\n",
      "lif layer 2 self.abs_max_v: 2463.5\n",
      "fc layer 2 self.abs_max_out: 1705.0\n",
      "fc layer 2 self.abs_max_out: 1726.0\n",
      "fc layer 2 self.abs_max_out: 1840.0\n",
      "fc layer 1 self.abs_max_out: 2451.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.730508/ 52.733559, val:  45.00%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0810%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11900  15.194%\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 1 self.abs_max_out: 2537.0\n",
      "lif layer 1 self.abs_max_v: 4148.0\n",
      "lif layer 1 self.abs_max_v: 4240.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.471816/ 65.070778, val:  50.00%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13156  14.931%\n",
      "fc layer 2 self.abs_max_out: 1893.0\n",
      "fc layer 1 self.abs_max_out: 2773.0\n",
      "lif layer 1 self.abs_max_v: 4559.5\n",
      "lif layer 1 self.abs_max_v: 4770.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.534399/ 63.542526, val:  43.33%, val_best:  56.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8718%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14401  14.710%\n",
      "lif layer 2 self.abs_max_v: 2489.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.165871/ 59.925266, val:  57.50%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0418%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15639  14.522%\n",
      "fc layer 1 self.abs_max_out: 2899.0\n",
      "lif layer 1 self.abs_max_v: 4919.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.891482/ 56.572166, val:  51.25%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16811  14.310%\n",
      "fc layer 3 self.abs_max_out: 582.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.895925/ 53.413399, val:  50.00%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0538%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17991  14.136%\n",
      "fc layer 1 self.abs_max_out: 2928.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.659070/ 46.004814, val:  57.92%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19160  13.979%\n",
      "fc layer 1 self.abs_max_out: 2986.0\n",
      "lif layer 1 self.abs_max_v: 4966.0\n",
      "lif layer 1 self.abs_max_v: 4972.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.493989/ 43.893375, val:  58.75%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20284  13.813%\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 1 self.abs_max_out: 3000.0\n",
      "lif layer 1 self.abs_max_v: 5005.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.221741/ 41.650394, val:  60.83%, val_best:  60.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21368  13.641%\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "fc layer 3 self.abs_max_out: 622.0\n",
      "lif layer 2 self.abs_max_v: 2534.5\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "fc layer 1 self.abs_max_out: 3044.0\n",
      "lif layer 1 self.abs_max_v: 5129.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.602532/ 72.396400, val:  45.00%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22518  13.530%\n",
      "lif layer 2 self.abs_max_v: 2560.0\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "lif layer 1 self.abs_max_v: 5232.5\n",
      "lif layer 1 self.abs_max_v: 5273.0\n",
      "lif layer 1 self.abs_max_v: 5468.5\n",
      "lif layer 1 self.abs_max_v: 5536.5\n",
      "lif layer 1 self.abs_max_v: 5580.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.493126/ 56.697598, val:  53.75%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23654  13.423%\n",
      "lif layer 2 self.abs_max_v: 2711.0\n",
      "lif layer 2 self.abs_max_v: 2878.5\n",
      "fc layer 1 self.abs_max_out: 3074.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.585908/ 45.824757, val:  56.25%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0942%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24744  13.303%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.817535/ 61.743870, val:  54.17%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25791  13.172%\n",
      "fc layer 3 self.abs_max_out: 671.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.687517/ 80.904480, val:  50.00%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26819  13.045%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.416981/ 48.635616, val:  54.17%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9242%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27803  12.909%\n",
      "lif layer 2 self.abs_max_v: 2901.5\n",
      "lif layer 2 self.abs_max_v: 2974.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.671491/ 44.211777, val:  61.67%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1535%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28811  12.795%\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.395494/ 57.663006, val:  57.50%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29810  12.687%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.347922/ 48.224968, val:  65.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30762  12.569%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.141847/ 50.028538, val:  61.67%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31732  12.466%\n",
      "fc layer 1 self.abs_max_out: 3136.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.878417/ 31.016502, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32750  12.390%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.096772/ 42.082275, val:  67.50%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1167%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33741  12.309%\n",
      "fc layer 1 self.abs_max_out: 3198.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.381337/ 34.953102, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34696  12.221%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.225918/ 39.409092, val:  69.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35612  12.125%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.890374/ 38.225761, val:  80.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36570  12.050%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.109283/ 49.636223, val:  60.83%, val_best:  80.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37503  11.971%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.207996/ 32.781120, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38344  11.869%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.317157/ 50.498459, val:  59.17%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 39161  11.765%\n",
      "fc layer 1 self.abs_max_out: 3524.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.661477/ 38.080925, val:  68.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 40026  11.681%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.630236/ 45.330261, val:  67.50%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0987%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40923  11.611%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.063300/ 35.727211, val:  67.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41724  11.519%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.503356/ 34.490013, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1038%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42562  11.441%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.954181/ 39.603371, val:  70.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9731%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43354  11.355%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.734330/ 28.819956, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 44138  11.271%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.975100/ 42.967899, val:  64.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44918  11.191%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.176392/ 33.854908, val:  81.67%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0248%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45719  11.119%\n",
      "fc layer 1 self.abs_max_out: 3640.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.891385/ 28.699545, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1075%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46493  11.044%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.123915/ 35.658485, val:  72.08%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8813%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47289  10.978%\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "lif layer 1 self.abs_max_v: 5671.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.968603/ 31.737017, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 48076  10.913%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.668746/ 40.801144, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48819  10.840%\n",
      "fc layer 1 self.abs_max_out: 3667.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.086240/ 32.899323, val:  80.42%, val_best:  83.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49585  10.776%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.403405/ 44.200176, val:  70.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1327%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 50308  10.706%\n",
      "lif layer 1 self.abs_max_v: 5706.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.811370/ 41.921474, val:  70.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 51059  10.644%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.318693/ 31.595816, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1344%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51734  10.569%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.479949/ 31.227478, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8975%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52445  10.504%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.445428/ 40.685390, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 53157  10.442%\n",
      "fc layer 1 self.abs_max_out: 3698.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.767618/ 39.878944, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1038%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53911  10.390%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  3.901330/ 34.354744, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54567  10.322%\n",
      "fc layer 1 self.abs_max_out: 3957.0\n",
      "lif layer 1 self.abs_max_v: 5715.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.068338/ 45.356476, val:  76.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0978%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 55221  10.256%\n",
      "lif layer 1 self.abs_max_v: 5825.0\n",
      "lif layer 1 self.abs_max_v: 5873.5\n",
      "fc layer 3 self.abs_max_out: 710.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.987374/ 36.911953, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55892  10.195%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.209408/ 48.044041, val:  72.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56570  10.137%\n",
      "lif layer 1 self.abs_max_v: 5891.0\n",
      "lif layer 1 self.abs_max_v: 5956.5\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "lif layer 1 self.abs_max_v: 6033.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.096606/ 31.146280, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 57231  10.079%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  3.928462/ 45.094704, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1011%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57848  10.015%\n",
      "lif layer 2 self.abs_max_v: 3003.5\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.926433/ 37.358791, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0975%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58507   9.960%\n",
      "fc layer 3 self.abs_max_out: 731.0\n",
      "fc layer 3 self.abs_max_out: 735.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.893224/ 39.388039, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 59152   9.905%\n",
      "lif layer 2 self.abs_max_v: 3021.5\n",
      "lif layer 1 self.abs_max_v: 6051.5\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.932723/ 40.188648, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59800   9.852%\n",
      "lif layer 1 self.abs_max_v: 6063.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.872063/ 36.871479, val:  77.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0318%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 60433   9.798%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.005188/ 33.827694, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 61069   9.747%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.787254/ 37.446255, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61692   9.695%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.390924/ 32.553413, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 62272   9.638%\n",
      "fc layer 1 self.abs_max_out: 4027.0\n",
      "lif layer 1 self.abs_max_v: 6294.5\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.626186/ 31.188103, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.75 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62862   9.584%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.795027/ 37.742245, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 63473   9.534%\n",
      "lif layer 1 self.abs_max_v: 6482.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.358915/ 37.052933, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.75 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2415%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 64017   9.477%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.188696/ 36.262985, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5182%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 64561   9.421%\n",
      "fc layer 1 self.abs_max_out: 4034.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.590576/ 35.971531, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0554%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0316%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 65161   9.374%\n",
      "lif layer 2 self.abs_max_v: 3041.0\n",
      "lif layer 2 self.abs_max_v: 3200.5\n",
      "lif layer 2 self.abs_max_v: 3207.5\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.223041/ 37.545795, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65709   9.322%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.489739/ 37.336327, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1003%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 66266   9.272%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.383891/ 36.542942, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66803   9.221%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.323557/ 31.005606, val:  84.17%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0986%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 67381   9.177%\n",
      "fc layer 1 self.abs_max_out: 4210.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.394160/ 30.472357, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6542%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67941   9.131%\n",
      "fc layer 3 self.abs_max_out: 754.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  2.881168/ 29.137680, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 68447   9.080%\n",
      "fc layer 1 self.abs_max_out: 4215.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.402864/ 35.408108, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 69022   9.039%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.331885/ 31.414047, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2318%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 69574   8.996%\n",
      "fc layer 2 self.abs_max_out: 2103.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.166921/ 30.857985, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6298%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 70094   8.950%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.384729/ 32.577858, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0478%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6819%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 70642   8.908%\n",
      "lif layer 1 self.abs_max_v: 6667.5\n",
      "lif layer 1 self.abs_max_v: 6710.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.207999/ 30.543707, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1191%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 71133   8.861%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  2.708917/ 37.326202, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 71606   8.812%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.901644/ 39.059776, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 72114   8.769%\n",
      "fc layer 3 self.abs_max_out: 765.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.110683/ 38.523468, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 72600   8.724%\n",
      "fc layer 3 self.abs_max_out: 775.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.029222/ 42.942120, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 73089   8.681%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.186318/ 51.751106, val:  71.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1014%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 73615   8.643%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.009344/ 33.722027, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 74117   8.603%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.870005/ 31.279577, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 74568   8.558%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.958057/ 33.324535, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 75041   8.517%\n",
      "fc layer 1 self.abs_max_out: 4265.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  2.611251/ 31.249296, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 75473   8.472%\n",
      "fc layer 1 self.abs_max_out: 4275.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.853098/ 32.353203, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75963   8.434%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.603832/ 38.574543, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 76403   8.392%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.623943/ 39.708817, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0986%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8520%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 76844   8.350%\n",
      "lif layer 1 self.abs_max_v: 6819.5\n",
      "lif layer 1 self.abs_max_v: 6820.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.728963/ 45.680550, val:  77.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 77287   8.310%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.809856/ 36.203060, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 77733   8.271%\n",
      "fc layer 1 self.abs_max_out: 4338.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.769518/ 32.982288, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1325%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 78184   8.233%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.648327/ 34.748158, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7949%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 78632   8.196%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.419347/ 38.222393, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 79044   8.156%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.520541/ 36.318172, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0425%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0738%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 79449   8.115%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.740136/ 45.268166, val:  78.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 79906   8.081%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.724692/ 31.615614, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 80333   8.045%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.679961/ 35.381466, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4474%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 80755   8.008%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.423649/ 30.500685, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6855%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 81134   7.969%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.454672/ 36.682804, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 81544   7.933%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.548007/ 49.161625, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 81965   7.898%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.472850/ 37.656181, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7752%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9826%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 82359   7.862%\n",
      "lif layer 1 self.abs_max_v: 6827.5\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.379988/ 33.939877, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 82764   7.828%\n",
      "lif layer 1 self.abs_max_v: 6904.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.599864/ 33.385933, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 83193   7.796%\n",
      "fc layer 3 self.abs_max_out: 784.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.352000/ 33.722080, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 83576   7.761%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.146927/ 33.334881, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1007%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3469%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 83954   7.726%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  1.990526/ 39.038960, val:  87.08%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 84294   7.688%\n",
      "fc layer 2 self.abs_max_out: 2109.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.112156/ 34.412991, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5173%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 84659   7.653%\n",
      "fc layer 3 self.abs_max_out: 794.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.486913/ 35.852428, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 85056   7.621%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.410399/ 39.401440, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7136%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 85440   7.589%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.291999/ 41.861919, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9666%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 85798   7.555%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.484378/ 32.437538, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 86195   7.525%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.078015/ 36.798851, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 86539   7.491%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.254267/ 33.533783, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 86910   7.460%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.145806/ 34.439213, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 87259   7.428%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.149402/ 33.348003, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7259%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 87641   7.398%\n",
      "fc layer 3 self.abs_max_out: 798.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.154090/ 31.356873, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0314%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 88015   7.369%\n",
      "fc layer 3 self.abs_max_out: 803.0\n",
      "fc layer 3 self.abs_max_out: 811.0\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "lif layer 1 self.abs_max_v: 6920.5\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.088140/ 37.685520, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7741%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 88362   7.338%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.365709/ 37.232582, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 88722   7.308%\n",
      "lif layer 2 self.abs_max_v: 3217.5\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  1.942775/ 40.931744, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 89071   7.279%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.277788/ 38.707912, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 89449   7.251%\n",
      "fc layer 3 self.abs_max_out: 851.0\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  1.899355/ 35.088089, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 89785   7.221%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.183557/ 33.243122, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 90120   7.192%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.269345/ 35.091484, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4252%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 90491   7.165%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.853800/ 34.018990, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 90812   7.135%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.140946/ 35.003872, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0942%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9230%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 91169   7.109%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.241755/ 30.359606, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7560%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 91530   7.083%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.992211/ 35.235851, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 91870   7.056%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.979818/ 34.717899, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 92177   7.026%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.901574/ 35.725441, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9474%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 92488   6.998%\n",
      "fc layer 1 self.abs_max_out: 4511.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.196473/ 42.867832, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 92819   6.971%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.729682/ 43.032562, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 93128   6.943%\n",
      "lif layer 1 self.abs_max_v: 6924.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.086121/ 35.788261, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 93459   6.918%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.918747/ 31.665667, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5469%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 93790   6.892%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.805055/ 35.098587, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 94094   6.865%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.470807/ 47.572342, val:  78.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 94380   6.837%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.779078/ 37.594177, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 94667   6.810%\n",
      "lif layer 1 self.abs_max_v: 6936.5\n",
      "fc layer 1 self.abs_max_out: 4540.0\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.972200/ 37.670963, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 94975   6.784%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.789128/ 36.351231, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2889%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 95273   6.758%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.951477/ 42.324310, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 95569   6.732%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.634230/ 40.381451, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 95855   6.706%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.695940/ 38.062614, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1169%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9456%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 96131   6.680%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.757140/ 36.177353, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 96425   6.655%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.694650/ 40.091888, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 96706   6.630%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.687301/ 42.101040, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 96998   6.605%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.631500/ 39.090828, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 97269   6.580%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.751042/ 41.007977, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 97567   6.557%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.571787/ 37.087173, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 97835   6.532%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.703526/ 33.822418, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1054%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 98116   6.508%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.644519/ 36.297447, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0945%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3726%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 98385   6.484%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.490823/ 32.812920, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0409%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 98646   6.459%\n",
      "fc layer 1 self.abs_max_out: 4563.0\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.635869/ 34.847115, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 98920   6.436%\n",
      "lif layer 1 self.abs_max_v: 6949.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.657144/ 35.498840, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 99211   6.414%\n",
      "lif layer 1 self.abs_max_v: 7041.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.446265/ 35.877911, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 99473   6.390%\n",
      "fc layer 1 self.abs_max_out: 4572.0\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.554903/ 40.070747, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5474%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 99747   6.368%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.466133/ 36.880489, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 100008   6.345%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.532210/ 29.991665, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 100270   6.322%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.443908/ 40.371548, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 100544   6.301%\n",
      "lif layer 1 self.abs_max_v: 7122.0\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.467251/ 38.102753, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 100786   6.277%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.771893/ 38.055908, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1170%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 101050   6.256%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.594925/ 38.118401, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 101344   6.236%\n",
      "fc layer 3 self.abs_max_out: 852.0\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.465766/ 34.782619, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0296%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 101582   6.213%\n",
      "lif layer 1 self.abs_max_v: 7272.5\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.707852/ 38.318794, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7962%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 101861   6.193%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.337657/ 37.500839, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6548%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 102098   6.171%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.609336/ 38.325249, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 102359   6.150%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.326828/ 37.829140, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 102575   6.127%\n",
      "fc layer 1 self.abs_max_out: 4593.0\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.403911/ 41.072880, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 102816   6.106%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.373276/ 37.451298, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6463%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 103063   6.085%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.528924/ 33.139694, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4316%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 103310   6.065%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.546883/ 34.222260, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 103578   6.046%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.288380/ 37.536129, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 103800   6.024%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.303541/ 34.491821, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2238%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 104033   6.004%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.403132/ 41.023102, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 104280   5.984%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.427949/ 38.916084, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 104531   5.965%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.454201/ 43.070030, val:  84.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 104770   5.945%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.151473/ 32.126148, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8282%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 104984   5.925%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.534933/ 32.481739, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 105222   5.905%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.533165/ 34.075035, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 105469   5.887%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.081043/ 33.361340, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 105674   5.866%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.251885/ 40.239243, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 105903   5.847%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.236331/ 39.512547, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0677%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 106120   5.828%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.062208/ 33.438961, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0942%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 106301   5.806%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.450166/ 37.298462, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 106540   5.789%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.327512/ 38.476917, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 106758   5.770%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.090153/ 33.116222, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 106967   5.751%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.242377/ 36.906811, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9170%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 107186   5.732%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.138647/ 37.820271, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5754%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 107390   5.713%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.321752/ 39.059185, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8981%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6860%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 107613   5.695%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.043944/ 33.425224, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7145%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 107797   5.676%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.175728/ 34.298897, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1415%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 108003   5.657%\n",
      "fc layer 1 self.abs_max_out: 4597.0\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.096772/ 36.754898, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 108193   5.638%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.459615/ 34.674686, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 108423   5.622%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.265181/ 39.010456, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 108665   5.606%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.087128/ 32.444214, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0517%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 108886   5.589%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.319699/ 37.496746, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1022%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3620e84f44f457e8b5204555426f763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñá‚ñà‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.3197</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>37.49675</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-sweep-16</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rxhps9h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rxhps9h</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_025852-7rxhps9h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t6olsi3t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 25151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_071720-t6olsi3t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/t6olsi3t' target=\"_blank\">hardy-sweep-23</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/t6olsi3t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/t6olsi3t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251213_071730_477', 'my_seed': 25151, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 325.0\n",
      "lif layer 1 self.abs_max_v: 325.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 331.0\n",
      "lif layer 2 self.abs_max_v: 331.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "fc layer 1 self.abs_max_out: 354.0\n",
      "lif layer 1 self.abs_max_v: 414.5\n",
      "lif layer 2 self.abs_max_v: 372.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 1 self.abs_max_out: 413.0\n",
      "lif layer 1 self.abs_max_v: 519.5\n",
      "fc layer 2 self.abs_max_out: 376.0\n",
      "lif layer 2 self.abs_max_v: 490.0\n",
      "fc layer 2 self.abs_max_out: 382.0\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "lif layer 2 self.abs_max_v: 491.0\n",
      "fc layer 2 self.abs_max_out: 413.0\n",
      "lif layer 2 self.abs_max_v: 498.5\n",
      "lif layer 1 self.abs_max_v: 527.0\n",
      "lif layer 1 self.abs_max_v: 544.5\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 2 self.abs_max_out: 460.0\n",
      "lif layer 2 self.abs_max_v: 693.5\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 1 self.abs_max_out: 485.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 2 self.abs_max_out: 498.0\n",
      "fc layer 1 self.abs_max_out: 594.0\n",
      "lif layer 1 self.abs_max_v: 607.5\n",
      "fc layer 1 self.abs_max_out: 753.0\n",
      "lif layer 1 self.abs_max_v: 753.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "lif layer 2 self.abs_max_v: 721.0\n",
      "fc layer 1 self.abs_max_out: 879.0\n",
      "lif layer 1 self.abs_max_v: 879.0\n",
      "lif layer 2 self.abs_max_v: 800.5\n",
      "fc layer 2 self.abs_max_out: 550.0\n",
      "lif layer 2 self.abs_max_v: 855.5\n",
      "lif layer 2 self.abs_max_v: 912.0\n",
      "fc layer 1 self.abs_max_out: 887.0\n",
      "lif layer 1 self.abs_max_v: 887.0\n",
      "fc layer 2 self.abs_max_out: 613.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 1 self.abs_max_out: 890.0\n",
      "lif layer 1 self.abs_max_v: 933.0\n",
      "lif layer 1 self.abs_max_v: 951.5\n",
      "lif layer 2 self.abs_max_v: 977.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "fc layer 1 self.abs_max_out: 911.0\n",
      "lif layer 1 self.abs_max_v: 1025.5\n",
      "fc layer 2 self.abs_max_out: 643.0\n",
      "lif layer 1 self.abs_max_v: 1032.5\n",
      "fc layer 2 self.abs_max_out: 677.0\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "lif layer 2 self.abs_max_v: 980.5\n",
      "lif layer 2 self.abs_max_v: 989.0\n",
      "lif layer 2 self.abs_max_v: 1039.5\n",
      "lif layer 1 self.abs_max_v: 1068.5\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 1 self.abs_max_out: 939.0\n",
      "lif layer 1 self.abs_max_v: 1090.5\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "lif layer 1 self.abs_max_v: 1193.0\n",
      "lif layer 2 self.abs_max_v: 1040.5\n",
      "lif layer 2 self.abs_max_v: 1093.5\n",
      "lif layer 2 self.abs_max_v: 1138.0\n",
      "lif layer 2 self.abs_max_v: 1139.0\n",
      "lif layer 2 self.abs_max_v: 1236.5\n",
      "lif layer 2 self.abs_max_v: 1364.0\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "fc layer 2 self.abs_max_out: 847.0\n",
      "lif layer 1 self.abs_max_v: 1208.5\n",
      "lif layer 1 self.abs_max_v: 1257.5\n",
      "lif layer 1 self.abs_max_v: 1355.5\n",
      "fc layer 1 self.abs_max_out: 975.0\n",
      "fc layer 1 self.abs_max_out: 1122.0\n",
      "fc layer 1 self.abs_max_out: 1129.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "fc layer 2 self.abs_max_out: 959.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 2 self.abs_max_out: 984.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "lif layer 2 self.abs_max_v: 1388.0\n",
      "lif layer 2 self.abs_max_v: 1458.0\n",
      "lif layer 2 self.abs_max_v: 1501.5\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 2 self.abs_max_out: 1011.0\n",
      "lif layer 1 self.abs_max_v: 1414.0\n",
      "fc layer 1 self.abs_max_out: 1143.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "lif layer 1 self.abs_max_v: 1484.5\n",
      "fc layer 1 self.abs_max_out: 1165.0\n",
      "fc layer 1 self.abs_max_out: 1297.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 2 self.abs_max_out: 1172.0\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "lif layer 1 self.abs_max_v: 1532.5\n",
      "lif layer 1 self.abs_max_v: 1691.5\n",
      "lif layer 1 self.abs_max_v: 1715.0\n",
      "fc layer 2 self.abs_max_out: 1220.0\n",
      "fc layer 2 self.abs_max_out: 1281.0\n",
      "lif layer 1 self.abs_max_v: 1818.0\n",
      "fc layer 2 self.abs_max_out: 1290.0\n",
      "fc layer 2 self.abs_max_out: 1378.0\n",
      "fc layer 3 self.abs_max_out: 430.0\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "lif layer 1 self.abs_max_v: 1828.0\n",
      "fc layer 1 self.abs_max_out: 1414.0\n",
      "lif layer 1 self.abs_max_v: 1964.5\n",
      "lif layer 1 self.abs_max_v: 2024.5\n",
      "lif layer 1 self.abs_max_v: 2077.0\n",
      "lif layer 1 self.abs_max_v: 2124.0\n",
      "lif layer 1 self.abs_max_v: 2358.0\n",
      "fc layer 1 self.abs_max_out: 1415.0\n",
      "fc layer 1 self.abs_max_out: 1491.0\n",
      "fc layer 2 self.abs_max_out: 1409.0\n",
      "lif layer 1 self.abs_max_v: 2376.5\n",
      "lif layer 1 self.abs_max_v: 2511.5\n",
      "fc layer 1 self.abs_max_out: 1507.0\n",
      "lif layer 1 self.abs_max_v: 2541.5\n",
      "lif layer 1 self.abs_max_v: 2672.0\n",
      "lif layer 1 self.abs_max_v: 2776.0\n",
      "lif layer 1 self.abs_max_v: 2817.0\n",
      "lif layer 2 self.abs_max_v: 1519.0\n",
      "fc layer 1 self.abs_max_out: 1579.0\n",
      "fc layer 2 self.abs_max_out: 1423.0\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.929493/ 77.898308, val:  31.67%, val_best:  31.67%, tr:  97.14%, tr_best:  97.14%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0939%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7928%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2102  21.471%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1531.5\n",
      "lif layer 2 self.abs_max_v: 1541.5\n",
      "lif layer 2 self.abs_max_v: 1551.0\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "fc layer 2 self.abs_max_out: 1494.0\n",
      "lif layer 2 self.abs_max_v: 1571.5\n",
      "lif layer 2 self.abs_max_v: 1645.0\n",
      "fc layer 3 self.abs_max_out: 509.0\n",
      "lif layer 2 self.abs_max_v: 1657.0\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "fc layer 1 self.abs_max_out: 1738.0\n",
      "fc layer 1 self.abs_max_out: 1833.0\n",
      "lif layer 1 self.abs_max_v: 3044.0\n",
      "lif layer 1 self.abs_max_v: 3208.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.902151/ 70.949646, val:  42.08%, val_best:  42.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3651  18.647%\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "lif layer 2 self.abs_max_v: 1722.0\n",
      "lif layer 2 self.abs_max_v: 1802.0\n",
      "lif layer 2 self.abs_max_v: 1803.5\n",
      "lif layer 2 self.abs_max_v: 1809.5\n",
      "fc layer 1 self.abs_max_out: 1879.0\n",
      "fc layer 1 self.abs_max_out: 1992.0\n",
      "lif layer 1 self.abs_max_v: 3251.5\n",
      "lif layer 1 self.abs_max_v: 3373.0\n",
      "lif layer 1 self.abs_max_v: 3426.0\n",
      "lif layer 1 self.abs_max_v: 3551.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.664371/ 77.957962, val:  41.67%, val_best:  42.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4576%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5085  17.314%\n",
      "fc layer 1 self.abs_max_out: 2051.0\n",
      "lif layer 2 self.abs_max_v: 1826.5\n",
      "lif layer 2 self.abs_max_v: 1835.5\n",
      "lif layer 2 self.abs_max_v: 2004.0\n",
      "fc layer 1 self.abs_max_out: 2128.0\n",
      "lif layer 1 self.abs_max_v: 3848.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.251157/ 66.833122, val:  45.00%, val_best:  45.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6428  16.415%\n",
      "lif layer 2 self.abs_max_v: 2061.5\n",
      "lif layer 2 self.abs_max_v: 2064.5\n",
      "fc layer 1 self.abs_max_out: 2130.0\n",
      "fc layer 1 self.abs_max_out: 2153.0\n",
      "fc layer 1 self.abs_max_out: 2167.0\n",
      "fc layer 1 self.abs_max_out: 2218.0\n",
      "lif layer 1 self.abs_max_v: 4047.5\n",
      "lif layer 2 self.abs_max_v: 2233.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.822432/ 57.112160, val:  51.25%, val_best:  51.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7533%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7784  15.902%\n",
      "fc layer 2 self.abs_max_out: 1623.0\n",
      "fc layer 3 self.abs_max_out: 515.0\n",
      "lif layer 1 self.abs_max_v: 4058.5\n",
      "fc layer 3 self.abs_max_out: 521.0\n",
      "fc layer 2 self.abs_max_out: 1625.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 2 self.abs_max_out: 1634.0\n",
      "fc layer 2 self.abs_max_out: 1676.0\n",
      "fc layer 1 self.abs_max_out: 2239.0\n",
      "lif layer 1 self.abs_max_v: 4091.5\n",
      "lif layer 2 self.abs_max_v: 2269.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 10.424783/ 81.703758, val:  33.75%, val_best:  51.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9083  15.463%\n",
      "fc layer 2 self.abs_max_out: 1677.0\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 2278.0\n",
      "fc layer 2 self.abs_max_out: 1721.0\n",
      "fc layer 1 self.abs_max_out: 2439.0\n",
      "lif layer 1 self.abs_max_v: 4325.0\n",
      "lif layer 2 self.abs_max_v: 2359.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.245807/ 48.445885, val:  61.67%, val_best:  61.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10371  15.134%\n",
      "fc layer 2 self.abs_max_out: 1760.0\n",
      "lif layer 1 self.abs_max_v: 4425.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 10.329859/ 38.027779, val:  60.00%, val_best:  61.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6334%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11613  14.828%\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "fc layer 1 self.abs_max_out: 2531.0\n",
      "fc layer 2 self.abs_max_out: 1795.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 10.006716/ 59.291595, val:  53.33%, val_best:  61.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12841  14.574%\n",
      "fc layer 2 self.abs_max_out: 1812.0\n",
      "fc layer 2 self.abs_max_out: 1821.0\n",
      "fc layer 1 self.abs_max_out: 2704.0\n",
      "lif layer 1 self.abs_max_v: 4665.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  9.666800/ 57.128372, val:  54.17%, val_best:  61.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7759%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14024  14.325%\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  9.999018/ 74.965378, val:  54.17%, val_best:  61.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0031%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15229  14.142%\n",
      "lif layer 2 self.abs_max_v: 2406.0\n",
      "fc layer 1 self.abs_max_out: 2734.0\n",
      "lif layer 1 self.abs_max_v: 4675.0\n",
      "lif layer 1 self.abs_max_v: 4736.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  9.396566/ 36.159855, val:  66.25%, val_best:  66.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9731%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16395  13.956%\n",
      "fc layer 3 self.abs_max_out: 606.0\n",
      "lif layer 2 self.abs_max_v: 2514.5\n",
      "lif layer 2 self.abs_max_v: 2599.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  9.181403/ 62.247662, val:  52.92%, val_best:  66.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0946%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17535  13.778%\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "fc layer 1 self.abs_max_out: 2742.0\n",
      "lif layer 1 self.abs_max_v: 4921.5\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  9.475343/ 46.255508, val:  70.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0219%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4318%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1800%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18688  13.635%\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "fc layer 3 self.abs_max_out: 618.0\n",
      "fc layer 1 self.abs_max_out: 2762.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.540672/ 61.957821, val:  52.92%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19760  13.456%\n",
      "fc layer 1 self.abs_max_out: 2823.0\n",
      "lif layer 1 self.abs_max_v: 5146.0\n",
      "lif layer 1 self.abs_max_v: 5238.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  8.225904/ 73.402512, val:  54.17%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 20824  13.294%\n",
      "fc layer 1 self.abs_max_out: 2855.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.711575/ 58.307438, val:  57.08%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4971%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 21937  13.181%\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  9.116611/ 42.151974, val:  66.25%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23075  13.094%\n",
      "fc layer 1 self.abs_max_out: 3013.0\n",
      "lif layer 1 self.abs_max_v: 5295.5\n",
      "fc layer 2 self.abs_max_out: 1925.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  8.826266/ 51.514626, val:  56.67%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24150  12.983%\n",
      "lif layer 2 self.abs_max_v: 2622.0\n",
      "fc layer 1 self.abs_max_out: 3136.0\n",
      "lif layer 1 self.abs_max_v: 5486.0\n",
      "lif layer 1 self.abs_max_v: 5641.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  8.181164/ 67.672958, val:  58.75%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25187  12.864%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  8.013319/ 76.714088, val:  61.25%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26265  12.775%\n",
      "lif layer 2 self.abs_max_v: 2666.0\n",
      "lif layer 2 self.abs_max_v: 2743.0\n",
      "lif layer 2 self.abs_max_v: 2783.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  8.070082/ 53.790623, val:  63.75%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1268%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27317  12.683%\n",
      "fc layer 3 self.abs_max_out: 633.0\n",
      "fc layer 3 self.abs_max_out: 667.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  8.125355/ 46.550983, val:  67.08%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3104%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28344  12.588%\n",
      "lif layer 2 self.abs_max_v: 2875.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.611603/ 54.840508, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29347  12.490%\n",
      "lif layer 2 self.abs_max_v: 2964.0\n",
      "lif layer 2 self.abs_max_v: 3152.0\n",
      "fc layer 1 self.abs_max_out: 3189.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.788194/ 49.359901, val:  71.67%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30341  12.397%\n",
      "fc layer 2 self.abs_max_out: 1931.0\n",
      "fc layer 1 self.abs_max_out: 3228.0\n",
      "fc layer 1 self.abs_max_out: 3291.0\n",
      "lif layer 1 self.abs_max_v: 5766.0\n",
      "fc layer 3 self.abs_max_out: 678.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  7.934816/ 34.215958, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31334  12.310%\n",
      "fc layer 3 self.abs_max_out: 692.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  7.212012/ 50.560848, val:  68.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32280  12.212%\n",
      "fc layer 1 self.abs_max_out: 3441.0\n",
      "lif layer 1 self.abs_max_v: 5961.0\n",
      "lif layer 1 self.abs_max_v: 6050.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  7.498079/ 41.789322, val:  78.33%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0245%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33213  12.116%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  7.433584/ 35.413334, val:  82.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34171  12.036%\n",
      "fc layer 3 self.abs_max_out: 708.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.902851/ 43.330631, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35105  11.953%\n",
      "fc layer 1 self.abs_max_out: 3642.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  7.442826/ 47.663300, val:  70.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36084  11.890%\n",
      "fc layer 1 self.abs_max_out: 3664.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.504871/ 49.140038, val:  67.50%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8140%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 36921  11.785%\n",
      "fc layer 2 self.abs_max_out: 1946.0\n",
      "lif layer 1 self.abs_max_v: 6079.5\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.606025/ 51.904957, val:  64.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37812  11.704%\n",
      "fc layer 2 self.abs_max_out: 2013.0\n",
      "fc layer 2 self.abs_max_out: 2022.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.009794/ 51.473869, val:  70.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38650  11.611%\n",
      "lif layer 1 self.abs_max_v: 6101.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  7.042334/ 58.742744, val:  62.92%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39516  11.532%\n",
      "fc layer 1 self.abs_max_out: 3732.0\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "lif layer 1 self.abs_max_v: 6159.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  6.907389/ 37.433952, val:  78.33%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.57 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40399  11.463%\n",
      "fc layer 1 self.abs_max_out: 3737.0\n",
      "fc layer 2 self.abs_max_out: 2079.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  6.148339/ 51.844616, val:  75.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0448%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41242  11.386%\n",
      "fc layer 3 self.abs_max_out: 715.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.817232/ 67.631538, val:  60.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6666%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42019  11.295%\n",
      "lif layer 1 self.abs_max_v: 6180.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  6.357944/ 50.731598, val:  63.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42831  11.218%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.947839/ 48.318680, val:  71.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43640  11.144%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  6.181604/ 44.734459, val:  75.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44443  11.072%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  6.123469/ 39.311947, val:  80.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45235  11.001%\n",
      "fc layer 1 self.abs_max_out: 3835.0\n",
      "fc layer 2 self.abs_max_out: 2101.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  6.447180/ 36.537350, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46055  10.940%\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "fc layer 2 self.abs_max_out: 2172.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.981520/ 39.926010, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46847  10.875%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.874551/ 54.755466, val:  69.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6246%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47634  10.812%\n",
      "lif layer 2 self.abs_max_v: 3165.0\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.764351/ 37.354591, val:  80.42%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48370  10.741%\n",
      "lif layer 2 self.abs_max_v: 3232.0\n",
      "lif layer 2 self.abs_max_v: 3338.5\n",
      "lif layer 2 self.abs_max_v: 3418.5\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.757317/ 41.966732, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49142  10.680%\n",
      "fc layer 3 self.abs_max_out: 756.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.678367/ 38.209206, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49876  10.614%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.479254/ 36.764427, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50580  10.544%\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.669642/ 46.058197, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51302  10.480%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.986951/ 40.753223, val:  82.08%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6505%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51958  10.406%\n",
      "fc layer 1 self.abs_max_out: 3902.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.242787/ 54.609283, val:  67.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52645  10.341%\n",
      "fc layer 1 self.abs_max_out: 3918.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.961089/ 42.916431, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1134%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53308  10.274%\n",
      "lif layer 2 self.abs_max_v: 3419.0\n",
      "lif layer 2 self.abs_max_v: 3528.0\n",
      "lif layer 2 self.abs_max_v: 3623.5\n",
      "fc layer 1 self.abs_max_out: 3940.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.961404/ 45.246380, val:  77.08%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53975  10.210%\n",
      "fc layer 2 self.abs_max_out: 2180.0\n",
      "lif layer 2 self.abs_max_v: 3692.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  5.005947/ 40.854687, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54625  10.145%\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.917462/ 35.703117, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1276%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55247  10.077%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.893166/ 54.738735, val:  70.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55863  10.011%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  5.228412/ 32.578930, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56554   9.960%\n",
      "fc layer 2 self.abs_max_out: 2227.0\n",
      "fc layer 2 self.abs_max_out: 2229.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.599457/ 59.753174, val:  65.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2932%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57162   9.896%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  5.158238/ 42.355896, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57802   9.840%\n",
      "fc layer 3 self.abs_max_out: 789.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  5.019986/ 47.111763, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8120%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58470   9.791%\n",
      "lif layer 1 self.abs_max_v: 6298.5\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.227328/ 58.809429, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59011   9.722%\n",
      "fc layer 2 self.abs_max_out: 2264.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  5.156719/ 37.721489, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59643   9.670%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.456946/ 48.206379, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60231   9.613%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.521428/ 39.354286, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0599%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60852   9.563%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.455638/ 37.660091, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6058%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61421   9.506%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.594411/ 42.726620, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0298%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62001   9.452%\n",
      "fc layer 2 self.abs_max_out: 2280.0\n",
      "fc layer 2 self.abs_max_out: 2284.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.861516/ 43.297413, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1022%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62616   9.406%\n",
      "fc layer 1 self.abs_max_out: 3975.0\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.833525/ 45.427921, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9355%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63234   9.361%\n",
      "lif layer 1 self.abs_max_v: 6628.5\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.252496/ 39.986156, val:  79.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63794   9.309%\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 832.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.388906/ 54.649300, val:  72.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64355   9.259%\n",
      "fc layer 2 self.abs_max_out: 2287.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.864244/ 45.894398, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64966   9.217%\n",
      "fc layer 1 self.abs_max_out: 4032.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  4.005564/ 45.118343, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65516   9.167%\n",
      "lif layer 1 self.abs_max_v: 6646.0\n",
      "fc layer 2 self.abs_max_out: 2399.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.123163/ 40.313980, val:  84.17%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9880%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66053   9.118%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.969889/ 41.123924, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66573   9.067%\n",
      "lif layer 2 self.abs_max_v: 3860.5\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.677316/ 48.913563, val:  77.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67063   9.013%\n",
      "fc layer 1 self.abs_max_out: 4058.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.832294/ 37.322922, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67600   8.968%\n",
      "fc layer 1 self.abs_max_out: 4069.0\n",
      "fc layer 3 self.abs_max_out: 840.0\n",
      "fc layer 3 self.abs_max_out: 847.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.041842/ 39.657169, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68132   8.922%\n",
      "fc layer 3 self.abs_max_out: 875.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.553117/ 39.451290, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6599%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68608   8.871%\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.632703/ 51.144871, val:  76.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0685%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69122   8.826%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  4.140123/ 66.595551, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69652   8.783%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.583461/ 39.741554, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70145   8.738%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.758028/ 42.430832, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70636   8.693%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.560332/ 40.504627, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0331%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8314%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71132   8.650%\n",
      "fc layer 2 self.abs_max_out: 2440.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.697683/ 40.206585, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1146%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5787%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71612   8.606%\n",
      "lif layer 1 self.abs_max_v: 6878.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.629862/ 42.634556, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72094   8.563%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.193794/ 44.292484, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72544   8.517%\n",
      "fc layer 1 self.abs_max_out: 4103.0\n",
      "lif layer 1 self.abs_max_v: 6889.0\n",
      "lif layer 1 self.abs_max_v: 7113.0\n",
      "fc layer 1 self.abs_max_out: 4153.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.611482/ 41.518177, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4251%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73036   8.478%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.590747/ 40.147793, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73525   8.438%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.377705/ 41.918365, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1060%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73969   8.395%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.053455/ 41.056458, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8641%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0120%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74405   8.352%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.194326/ 42.780079, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1245%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74854   8.311%\n",
      "fc layer 1 self.abs_max_out: 4245.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.917505/ 43.955120, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75343   8.275%\n",
      "lif layer 1 self.abs_max_v: 7119.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.353362/ 38.172363, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0541%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8319%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75782   8.235%\n",
      "fc layer 1 self.abs_max_out: 4292.0\n",
      "lif layer 1 self.abs_max_v: 7344.5\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.127039/ 41.696716, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9640%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6917%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76220   8.195%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.085218/ 45.071125, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76652   8.156%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.388183/ 46.486561, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0391%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5417%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77099   8.119%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.196545/ 46.832554, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77541   8.082%\n",
      "fc layer 2 self.abs_max_out: 2477.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.426608/ 48.401768, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 77975   8.045%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.179131/ 43.717846, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78421   8.010%\n",
      "fc layer 2 self.abs_max_out: 2481.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.985082/ 44.276192, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78840   7.973%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.765930/ 36.325920, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79244   7.936%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.709496/ 42.884144, val:  85.83%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79624   7.896%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.744975/ 44.982975, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80003   7.858%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.216559/ 52.834072, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7802%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80445   7.826%\n",
      "fc layer 3 self.abs_max_out: 940.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.925175/ 48.229019, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 80842   7.790%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.965022/ 46.774006, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4620%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81241   7.755%\n",
      "lif layer 1 self.abs_max_v: 7384.5\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.687661/ 49.467613, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81618   7.719%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.749492/ 37.492390, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3354%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 81999   7.684%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.852729/ 41.342415, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82408   7.652%\n",
      "fc layer 2 self.abs_max_out: 2498.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.117438/ 43.087227, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82824   7.622%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.768366/ 49.836151, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5297%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83172   7.585%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.896302/ 45.101913, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83568   7.554%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  3.017106/ 36.767117, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3165%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 83946   7.522%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.868118/ 46.003002, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84325   7.490%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.679865/ 52.806992, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0989%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84698   7.458%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.844454/ 50.498928, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1388%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85085   7.428%\n",
      "lif layer 1 self.abs_max_v: 7450.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.315275/ 53.450142, val:  77.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3962%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85404   7.393%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.370273/ 52.938759, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 85727   7.358%\n",
      "fc layer 1 self.abs_max_out: 4298.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.645714/ 45.970325, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86091   7.328%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.543939/ 42.127819, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86448   7.298%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.957350/ 45.215046, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0284%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 86844   7.271%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.735693/ 41.477142, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87217   7.243%\n",
      "fc layer 2 self.abs_max_out: 2523.0\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.776191/ 47.052280, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87601   7.216%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.238141/ 47.254532, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 87932   7.185%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.674885/ 43.655334, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88269   7.156%\n",
      "fc layer 2 self.abs_max_out: 2571.0\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.445978/ 47.160694, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3152%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88609   7.127%\n",
      "fc layer 1 self.abs_max_out: 4322.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.543615/ 63.593422, val:  71.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3802%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 88948   7.098%\n",
      "lif layer 1 self.abs_max_v: 7546.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.253333/ 46.397991, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89264   7.068%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.067904/ 47.228924, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89569   7.038%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.412845/ 54.051792, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0387%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 89896   7.009%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.560600/ 45.954094, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90232   6.982%\n",
      "fc layer 3 self.abs_max_out: 949.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.242257/ 51.112404, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90539   6.953%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.221037/ 51.897167, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9280%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 90848   6.925%\n",
      "fc layer 2 self.abs_max_out: 2573.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.198586/ 39.692638, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0506%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5474%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91165   6.898%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.098071/ 47.799583, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91460   6.869%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.954519/ 42.662292, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5762%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2004%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 91731   6.839%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.892016/ 43.239941, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92013   6.811%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.146214/ 47.719463, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92332   6.785%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.307349/ 42.828846, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 92638   6.759%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.167281/ 40.127052, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 92934   6.732%\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.337180/ 42.207603, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4031%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93238   6.707%\n",
      "fc layer 1 self.abs_max_out: 4409.0\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.519350/ 46.591061, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93555   6.683%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.259694/ 43.116913, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 93859   6.658%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.012557/ 55.109974, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94147   6.632%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.865106/ 39.514530, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94422   6.606%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.594604/ 42.842842, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 94761   6.585%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.143196/ 49.367519, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95062   6.561%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.696299/ 40.297947, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2814%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95312   6.534%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.775291/ 44.874767, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1014%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 95578   6.509%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.907768/ 46.161362, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 95846   6.484%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.701185/ 48.742493, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4494%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96092   6.457%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.226400/ 56.735950, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1134%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 96392   6.435%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.722846/ 42.279213, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 96631   6.409%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.576294/ 48.064220, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 96863   6.383%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.506966/ 53.143375, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97097   6.358%\n",
      "fc layer 2 self.abs_max_out: 2598.0\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.007596/ 51.662148, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1057%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7239%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 97360   6.334%\n",
      "fc layer 3 self.abs_max_out: 964.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.025018/ 48.595215, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1675%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 97639   6.312%\n",
      "fc layer 3 self.abs_max_out: 989.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.525940/ 45.382343, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1030%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 97871   6.287%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.931080/ 51.215206, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98149   6.266%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.830437/ 45.455566, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 98393   6.242%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.783292/ 43.537792, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 98663   6.221%\n",
      "lif layer 1 self.abs_max_v: 7627.5\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.769222/ 44.305988, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0298%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 98919   6.199%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  2.283504/ 45.021152, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 99213   6.179%\n",
      "fc layer 2 self.abs_max_out: 2653.0\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.702690/ 45.520374, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1034%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 99467   6.158%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.755131/ 39.236523, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 99706   6.135%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.628492/ 48.050037, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2887%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 99930   6.112%\n",
      "fc layer 2 self.abs_max_out: 2677.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.622245/ 45.195393, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2010%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 100161   6.090%\n",
      "fc layer 1 self.abs_max_out: 4473.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.739342/ 43.264484, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0526%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 100407   6.069%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.537930/ 48.183601, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 100638   6.047%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.531866/ 49.859787, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3161%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 100861   6.025%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.506575/ 46.342556, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1992%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 101082   6.003%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.942248/ 48.402233, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 101345   5.984%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.747339/ 45.351479, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 101590   5.964%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.894890/ 49.468639, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0280%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 101864   5.946%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.766511/ 43.579197, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1013%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 102112   5.926%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.690238/ 50.258278, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1382%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 102368   5.908%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.970047/ 45.908867, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1295%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 102632   5.890%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.488253/ 48.331150, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 102849   5.869%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.920392/ 53.919960, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0416%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1668%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 103110   5.851%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.735901/ 43.187378, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 103349   5.832%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.793828/ 52.273907, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9113%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 103605   5.815%\n",
      "fc layer 1 self.abs_max_out: 4485.0\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.562897/ 61.025890, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 103830   5.795%\n",
      "fc layer 2 self.abs_max_out: 2688.0\n",
      "fc layer 3 self.abs_max_out: 1023.0\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.473092/ 40.755463, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1048%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104040   5.776%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.614261/ 57.213440, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 104272   5.757%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.517910/ 50.704876, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 104503   5.739%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.397908/ 49.783142, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 104719   5.720%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.435126/ 45.421219, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 104942   5.702%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.402283/ 48.392361, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 105144   5.683%\n",
      "fc layer 1 self.abs_max_out: 4493.0\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.574047/ 48.786331, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 105384   5.666%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.389481/ 53.668037, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 105594   5.647%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.451773/ 47.362820, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 105792   5.628%\n",
      "fc layer 1 self.abs_max_out: 4572.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.527350/ 53.148506, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1287%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106015   5.611%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.398431/ 47.488037, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0333%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1209%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 106215   5.592%\n",
      "fc layer 3 self.abs_max_out: 1026.0\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.329688/ 56.212208, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 106405   5.574%\n",
      "fc layer 1 self.abs_max_out: 4594.0\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.302242/ 52.487213, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2141%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 106584   5.555%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.829106/ 51.136398, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 106820   5.539%\n",
      "fc layer 1 self.abs_max_out: 4657.0\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.384712/ 45.922981, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1617%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107008   5.520%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.588656/ 46.335564, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2257%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 107207   5.503%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.373293/ 49.680256, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376dd6b05bca493ea243f89538ff202a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÅ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñà‚ñÅ‚ñá‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.37329</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>49.68026</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hardy-sweep-23</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/t6olsi3t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/t6olsi3t</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_071720-t6olsi3t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kdxigs0c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 16828\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_113518-kdxigs0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kdxigs0c' target=\"_blank\">hopeful-sweep-29</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kdxigs0c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kdxigs0c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251213_113528_515', 'my_seed': 16828, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 216.0\n",
      "lif layer 1 self.abs_max_v: 216.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 162.0\n",
      "lif layer 2 self.abs_max_v: 162.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 26.0\n",
      "lif layer 1 self.abs_max_v: 220.5\n",
      "fc layer 2 self.abs_max_out: 328.0\n",
      "lif layer 2 self.abs_max_v: 380.5\n",
      "fc layer 3 self.abs_max_out: 103.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 284.5\n",
      "fc layer 1 self.abs_max_out: 258.0\n",
      "lif layer 1 self.abs_max_v: 299.5\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "lif layer 1 self.abs_max_v: 311.5\n",
      "fc layer 2 self.abs_max_out: 436.0\n",
      "lif layer 2 self.abs_max_v: 558.5\n",
      "fc layer 3 self.abs_max_out: 144.0\n",
      "fc layer 1 self.abs_max_out: 268.0\n",
      "lif layer 1 self.abs_max_v: 335.0\n",
      "fc layer 1 self.abs_max_out: 312.0\n",
      "lif layer 1 self.abs_max_v: 363.5\n",
      "lif layer 2 self.abs_max_v: 636.5\n",
      "fc layer 1 self.abs_max_out: 360.0\n",
      "lif layer 2 self.abs_max_v: 670.0\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "lif layer 1 self.abs_max_v: 419.0\n",
      "fc layer 1 self.abs_max_out: 444.0\n",
      "lif layer 1 self.abs_max_v: 444.0\n",
      "lif layer 2 self.abs_max_v: 708.0\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "fc layer 1 self.abs_max_out: 484.0\n",
      "lif layer 1 self.abs_max_v: 484.0\n",
      "fc layer 2 self.abs_max_out: 507.0\n",
      "lif layer 1 self.abs_max_v: 523.5\n",
      "fc layer 1 self.abs_max_out: 569.0\n",
      "lif layer 1 self.abs_max_v: 569.0\n",
      "fc layer 3 self.abs_max_out: 234.0\n",
      "fc layer 2 self.abs_max_out: 515.0\n",
      "fc layer 2 self.abs_max_out: 553.0\n",
      "lif layer 2 self.abs_max_v: 810.5\n",
      "fc layer 1 self.abs_max_out: 626.0\n",
      "lif layer 1 self.abs_max_v: 626.0\n",
      "lif layer 2 self.abs_max_v: 915.5\n",
      "lif layer 2 self.abs_max_v: 983.0\n",
      "fc layer 2 self.abs_max_out: 577.0\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "fc layer 2 self.abs_max_out: 610.0\n",
      "lif layer 1 self.abs_max_v: 695.0\n",
      "fc layer 2 self.abs_max_out: 675.0\n",
      "lif layer 1 self.abs_max_v: 749.5\n",
      "fc layer 1 self.abs_max_out: 647.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "fc layer 2 self.abs_max_out: 700.0\n",
      "fc layer 1 self.abs_max_out: 683.0\n",
      "fc layer 1 self.abs_max_out: 776.0\n",
      "lif layer 1 self.abs_max_v: 776.0\n",
      "lif layer 2 self.abs_max_v: 1010.5\n",
      "lif layer 1 self.abs_max_v: 778.0\n",
      "lif layer 2 self.abs_max_v: 1024.0\n",
      "lif layer 2 self.abs_max_v: 1054.5\n",
      "lif layer 1 self.abs_max_v: 853.0\n",
      "lif layer 1 self.abs_max_v: 918.5\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 2 self.abs_max_out: 768.0\n",
      "lif layer 2 self.abs_max_v: 1188.5\n",
      "lif layer 2 self.abs_max_v: 1263.5\n",
      "lif layer 2 self.abs_max_v: 1364.0\n",
      "fc layer 1 self.abs_max_out: 882.0\n",
      "fc layer 2 self.abs_max_out: 782.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "lif layer 1 self.abs_max_v: 998.0\n",
      "fc layer 2 self.abs_max_out: 829.0\n",
      "fc layer 1 self.abs_max_out: 916.0\n",
      "fc layer 1 self.abs_max_out: 1062.0\n",
      "lif layer 1 self.abs_max_v: 1062.0\n",
      "lif layer 1 self.abs_max_v: 1110.0\n",
      "lif layer 1 self.abs_max_v: 1130.0\n",
      "fc layer 1 self.abs_max_out: 1110.0\n",
      "fc layer 2 self.abs_max_out: 853.0\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 1 self.abs_max_out: 1145.0\n",
      "lif layer 1 self.abs_max_v: 1145.0\n",
      "lif layer 1 self.abs_max_v: 1200.0\n",
      "lif layer 1 self.abs_max_v: 1209.5\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "lif layer 1 self.abs_max_v: 1231.5\n",
      "lif layer 1 self.abs_max_v: 1257.0\n",
      "fc layer 2 self.abs_max_out: 868.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 1 self.abs_max_out: 1176.0\n",
      "lif layer 1 self.abs_max_v: 1318.0\n",
      "fc layer 2 self.abs_max_out: 879.0\n",
      "fc layer 1 self.abs_max_out: 1213.0\n",
      "fc layer 2 self.abs_max_out: 888.0\n",
      "fc layer 1 self.abs_max_out: 1253.0\n",
      "fc layer 1 self.abs_max_out: 1330.0\n",
      "lif layer 1 self.abs_max_v: 1330.0\n",
      "fc layer 2 self.abs_max_out: 987.0\n",
      "lif layer 1 self.abs_max_v: 1443.0\n",
      "lif layer 2 self.abs_max_v: 1368.5\n",
      "fc layer 1 self.abs_max_out: 1331.0\n",
      "fc layer 2 self.abs_max_out: 1007.0\n",
      "fc layer 2 self.abs_max_out: 1030.0\n",
      "fc layer 2 self.abs_max_out: 1087.0\n",
      "lif layer 2 self.abs_max_v: 1396.0\n",
      "lif layer 1 self.abs_max_v: 1515.0\n",
      "lif layer 1 self.abs_max_v: 1519.5\n",
      "lif layer 1 self.abs_max_v: 1530.0\n",
      "lif layer 2 self.abs_max_v: 1396.5\n",
      "lif layer 1 self.abs_max_v: 1683.0\n",
      "lif layer 1 self.abs_max_v: 1684.0\n",
      "lif layer 1 self.abs_max_v: 1741.5\n",
      "fc layer 1 self.abs_max_out: 1370.0\n",
      "lif layer 2 self.abs_max_v: 1406.0\n",
      "lif layer 2 self.abs_max_v: 1532.0\n",
      "fc layer 1 self.abs_max_out: 1380.0\n",
      "lif layer 2 self.abs_max_v: 1649.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "fc layer 2 self.abs_max_out: 1160.0\n",
      "fc layer 1 self.abs_max_out: 1402.0\n",
      "fc layer 1 self.abs_max_out: 1448.0\n",
      "lif layer 1 self.abs_max_v: 1773.0\n",
      "fc layer 1 self.abs_max_out: 1508.0\n",
      "fc layer 1 self.abs_max_out: 1518.0\n",
      "lif layer 1 self.abs_max_v: 1796.0\n",
      "lif layer 1 self.abs_max_v: 1979.0\n",
      "fc layer 1 self.abs_max_out: 1621.0\n",
      "lif layer 1 self.abs_max_v: 2041.0\n",
      "lif layer 1 self.abs_max_v: 2145.5\n",
      "lif layer 1 self.abs_max_v: 2214.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "lif layer 1 self.abs_max_v: 2290.0\n",
      "lif layer 2 self.abs_max_v: 1661.5\n",
      "lif layer 1 self.abs_max_v: 2471.5\n",
      "lif layer 1 self.abs_max_v: 2540.0\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 1 self.abs_max_out: 1757.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 15.086932/ 47.891777, val:  47.08%, val_best:  47.08%, tr:  96.53%, tr_best:  96.53%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2184  22.308%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "lif layer 2 self.abs_max_v: 1663.5\n",
      "lif layer 1 self.abs_max_v: 2563.0\n",
      "fc layer 1 self.abs_max_out: 1810.0\n",
      "lif layer 1 self.abs_max_v: 2751.0\n",
      "fc layer 2 self.abs_max_out: 1176.0\n",
      "fc layer 2 self.abs_max_out: 1188.0\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "fc layer 2 self.abs_max_out: 1301.0\n",
      "lif layer 1 self.abs_max_v: 2815.5\n",
      "lif layer 1 self.abs_max_v: 2839.0\n",
      "fc layer 1 self.abs_max_out: 1887.0\n",
      "lif layer 2 self.abs_max_v: 1668.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.612823/ 69.363396, val:  45.83%, val_best:  47.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3795  19.382%\n",
      "lif layer 2 self.abs_max_v: 1699.0\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "fc layer 2 self.abs_max_out: 1311.0\n",
      "fc layer 1 self.abs_max_out: 1902.0\n",
      "fc layer 1 self.abs_max_out: 1918.0\n",
      "fc layer 3 self.abs_max_out: 523.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "fc layer 1 self.abs_max_out: 1968.0\n",
      "fc layer 3 self.abs_max_out: 540.0\n",
      "fc layer 1 self.abs_max_out: 2008.0\n",
      "fc layer 2 self.abs_max_out: 1377.0\n",
      "lif layer 1 self.abs_max_v: 3158.0\n",
      "lif layer 1 self.abs_max_v: 3199.0\n",
      "lif layer 1 self.abs_max_v: 3249.5\n",
      "fc layer 1 self.abs_max_out: 2156.0\n",
      "lif layer 1 self.abs_max_v: 3286.0\n",
      "lif layer 1 self.abs_max_v: 3329.0\n",
      "fc layer 2 self.abs_max_out: 1404.0\n",
      "fc layer 2 self.abs_max_out: 1428.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.729633/ 74.669991, val:  45.83%, val_best:  47.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1157%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5282  17.984%\n",
      "fc layer 3 self.abs_max_out: 552.0\n",
      "fc layer 3 self.abs_max_out: 554.0\n",
      "lif layer 2 self.abs_max_v: 1801.0\n",
      "fc layer 3 self.abs_max_out: 556.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "fc layer 1 self.abs_max_out: 2192.0\n",
      "lif layer 1 self.abs_max_v: 3669.0\n",
      "lif layer 1 self.abs_max_v: 3757.5\n",
      "fc layer 1 self.abs_max_out: 2202.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.129909/ 46.235401, val:  50.42%, val_best:  50.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0801%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6702  17.114%\n",
      "fc layer 1 self.abs_max_out: 2212.0\n",
      "lif layer 2 self.abs_max_v: 1822.5\n",
      "lif layer 2 self.abs_max_v: 1936.5\n",
      "fc layer 1 self.abs_max_out: 2249.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "lif layer 2 self.abs_max_v: 1942.0\n",
      "lif layer 2 self.abs_max_v: 1971.5\n",
      "fc layer 1 self.abs_max_out: 2283.0\n",
      "lif layer 1 self.abs_max_v: 3765.0\n",
      "lif layer 1 self.abs_max_v: 3870.5\n",
      "lif layer 1 self.abs_max_v: 4062.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.457802/ 46.952602, val:  52.08%, val_best:  52.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1233%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8055  16.456%\n",
      "lif layer 2 self.abs_max_v: 2044.5\n",
      "lif layer 2 self.abs_max_v: 2058.0\n",
      "lif layer 2 self.abs_max_v: 2282.5\n",
      "fc layer 1 self.abs_max_out: 2357.0\n",
      "fc layer 1 self.abs_max_out: 2485.0\n",
      "lif layer 1 self.abs_max_v: 4127.5\n",
      "lif layer 1 self.abs_max_v: 4139.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.754283/119.778305, val:  32.08%, val_best:  52.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9396  15.996%\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "fc layer 2 self.abs_max_out: 1495.0\n",
      "lif layer 2 self.abs_max_v: 2296.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.491715/ 61.295574, val:  49.17%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10747  15.682%\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "fc layer 1 self.abs_max_out: 2643.0\n",
      "lif layer 1 self.abs_max_v: 4288.0\n",
      "lif layer 1 self.abs_max_v: 4409.0\n",
      "lif layer 1 self.abs_max_v: 4553.5\n",
      "fc layer 2 self.abs_max_out: 1496.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.132434/ 59.865704, val:  43.33%, val_best:  52.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12021  15.349%\n",
      "fc layer 2 self.abs_max_out: 1559.0\n",
      "lif layer 2 self.abs_max_v: 2311.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.976361/ 64.728142, val:  48.33%, val_best:  52.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13273  15.064%\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "fc layer 3 self.abs_max_out: 684.0\n",
      "fc layer 1 self.abs_max_out: 2644.0\n",
      "fc layer 1 self.abs_max_out: 2792.0\n",
      "lif layer 1 self.abs_max_v: 4608.5\n",
      "lif layer 1 self.abs_max_v: 4638.5\n",
      "lif layer 1 self.abs_max_v: 4672.5\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.354087/ 66.483047, val:  49.58%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1217%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14462  14.772%\n",
      "lif layer 1 self.abs_max_v: 4696.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  9.140446/ 59.321404, val:  50.83%, val_best:  52.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15729  14.606%\n",
      "fc layer 2 self.abs_max_out: 1571.0\n",
      "lif layer 2 self.abs_max_v: 2377.0\n",
      "lif layer 2 self.abs_max_v: 2470.5\n",
      "lif layer 2 self.abs_max_v: 2500.0\n",
      "lif layer 2 self.abs_max_v: 2557.0\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.845818/ 53.405258, val:  56.25%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5248%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16985  14.458%\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "lif layer 1 self.abs_max_v: 4710.5\n",
      "fc layer 2 self.abs_max_out: 1598.0\n",
      "fc layer 1 self.abs_max_out: 2818.0\n",
      "fc layer 1 self.abs_max_out: 2887.0\n",
      "lif layer 1 self.abs_max_v: 4763.5\n",
      "fc layer 2 self.abs_max_out: 1656.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.406521/ 67.581345, val:  43.75%, val_best:  56.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18169  14.276%\n",
      "fc layer 1 self.abs_max_out: 2972.0\n",
      "lif layer 1 self.abs_max_v: 4897.0\n",
      "lif layer 1 self.abs_max_v: 4996.5\n",
      "fc layer 1 self.abs_max_out: 3035.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.297507/ 50.124714, val:  58.33%, val_best:  58.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6889%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19372  14.134%\n",
      "lif layer 2 self.abs_max_v: 2613.5\n",
      "lif layer 1 self.abs_max_v: 5010.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.388729/ 78.286880, val:  47.50%, val_best:  58.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20576  14.012%\n",
      "fc layer 2 self.abs_max_out: 1688.0\n",
      "fc layer 2 self.abs_max_out: 1693.0\n",
      "lif layer 2 self.abs_max_v: 2614.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  8.141431/ 54.902405, val:  56.67%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0542%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3104%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21721  13.867%\n",
      "fc layer 2 self.abs_max_out: 1735.0\n",
      "fc layer 1 self.abs_max_out: 3061.0\n",
      "fc layer 1 self.abs_max_out: 3080.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.137313/ 48.128040, val:  59.58%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0554%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22893  13.755%\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "fc layer 2 self.abs_max_out: 1749.0\n",
      "fc layer 1 self.abs_max_out: 3081.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  8.070776/ 54.284302, val:  56.25%, val_best:  59.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24042  13.643%\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "fc layer 2 self.abs_max_out: 1794.0\n",
      "lif layer 1 self.abs_max_v: 5053.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.904547/ 42.725101, val:  65.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8294%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 25166  13.529%\n",
      "lif layer 1 self.abs_max_v: 5094.5\n",
      "lif layer 1 self.abs_max_v: 5117.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.631243/ 48.518372, val:  64.58%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0407%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26237  13.400%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.132668/ 64.106667, val:  52.50%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1165%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27308  13.283%\n",
      "fc layer 2 self.abs_max_out: 1807.0\n",
      "fc layer 2 self.abs_max_out: 1821.0\n",
      "fc layer 2 self.abs_max_out: 1924.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.601170/ 35.737041, val:  75.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9234%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28352  13.164%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.915591/ 36.802624, val:  67.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29364  13.041%\n",
      "fc layer 2 self.abs_max_out: 1932.0\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.281185/ 44.387852, val:  69.58%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0261%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30421  12.947%\n",
      "fc layer 1 self.abs_max_out: 3227.0\n",
      "lif layer 1 self.abs_max_v: 5144.5\n",
      "lif layer 1 self.abs_max_v: 5150.5\n",
      "lif layer 1 self.abs_max_v: 5381.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.483415/ 41.635941, val:  64.58%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1062%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31484  12.864%\n",
      "lif layer 2 self.abs_max_v: 2627.0\n",
      "lif layer 2 self.abs_max_v: 2658.0\n",
      "lif layer 2 self.abs_max_v: 2660.5\n",
      "lif layer 2 self.abs_max_v: 2866.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.504663/ 37.821133, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6742%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32439  12.744%\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "fc layer 3 self.abs_max_out: 690.0\n",
      "fc layer 1 self.abs_max_out: 3257.0\n",
      "lif layer 1 self.abs_max_v: 5474.5\n",
      "lif layer 1 self.abs_max_v: 5713.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.970337/ 48.720802, val:  65.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5252%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33478  12.665%\n",
      "fc layer 3 self.abs_max_out: 692.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.951156/ 61.649403, val:  55.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34459  12.571%\n",
      "fc layer 1 self.abs_max_out: 3259.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  7.001592/ 56.025375, val:  62.92%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1144%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35438  12.482%\n",
      "fc layer 3 self.abs_max_out: 703.0\n",
      "fc layer 2 self.abs_max_out: 2021.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.298100/ 48.958862, val:  67.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2309%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9784%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 36356  12.379%\n",
      "fc layer 3 self.abs_max_out: 729.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.735803/ 33.776077, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8355%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 37293  12.288%\n",
      "fc layer 1 self.abs_max_out: 3304.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.201850/ 46.539810, val:  64.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 38190  12.190%\n",
      "fc layer 1 self.abs_max_out: 3495.0\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.539061/ 36.807121, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1026%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 39100  12.103%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.666405/ 33.974884, val:  75.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1063%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 40003  12.018%\n",
      "fc layer 2 self.abs_max_out: 2104.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  6.153147/ 44.237492, val:  72.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 40883  11.931%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.660809/ 55.338829, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 41703  11.833%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.805532/ 36.517849, val:  77.92%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 42537  11.743%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.686696/ 42.421906, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 43347  11.652%\n",
      "fc layer 3 self.abs_max_out: 749.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  6.344688/ 47.409660, val:  68.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 44219  11.581%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.828114/ 36.454147, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 45018  11.496%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.865686/ 40.195568, val:  75.83%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 45868  11.427%\n",
      "fc layer 1 self.abs_max_out: 3519.0\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "fc layer 3 self.abs_max_out: 797.0\n",
      "lif layer 1 self.abs_max_v: 5778.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.902070/ 53.785408, val:  65.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7968%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 46681  11.353%\n",
      "fc layer 1 self.abs_max_out: 3585.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.524448/ 36.340260, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 47477  11.278%\n",
      "lif layer 2 self.abs_max_v: 2923.0\n",
      "lif layer 2 self.abs_max_v: 2983.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.496631/ 39.126045, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 48259  11.203%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.089907/ 38.921570, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 49032  11.130%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.541648/ 38.114922, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 49799  11.058%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.975978/ 40.352360, val:  71.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8207%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 50486  10.972%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.198503/ 36.262630, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 51223  10.900%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.137150/ 43.894562, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1448%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 51940  10.827%\n",
      "fc layer 1 self.abs_max_out: 3768.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.006666/ 54.454983, val:  71.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 52678  10.762%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.358614/ 39.455784, val:  74.17%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 53452  10.706%\n",
      "fc layer 2 self.abs_max_out: 2147.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.154109/ 29.675690, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0747%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 54203  10.647%\n",
      "lif layer 1 self.abs_max_v: 5786.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  5.176775/ 65.435051, val:  65.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 54952  10.591%\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.489239/ 38.960117, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4161%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 55682  10.533%\n",
      "lif layer 2 self.abs_max_v: 3043.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  5.108912/ 41.215073, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1360%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 56392  10.473%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.649179/ 37.993599, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 57075  10.411%\n",
      "lif layer 1 self.abs_max_v: 5927.5\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.840883/ 48.076488, val:  75.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 57770  10.352%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.968060/ 44.754971, val:  74.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3968%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 58466  10.297%\n",
      "lif layer 1 self.abs_max_v: 6202.5\n",
      "lif layer 1 self.abs_max_v: 6447.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.701001/ 34.437355, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 59134  10.238%\n",
      "fc layer 1 self.abs_max_out: 3844.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.718137/ 42.958229, val:  79.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 59783  10.178%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.527087/ 56.964382, val:  70.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3762%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 60437  10.120%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.664667/ 42.335873, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 61107  10.067%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.606105/ 37.014477, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0414%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 61769  10.015%\n",
      "fc layer 1 self.abs_max_out: 3908.0\n",
      "fc layer 1 self.abs_max_out: 3997.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.576895/ 39.859131, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 62441   9.966%\n",
      "lif layer 2 self.abs_max_v: 3152.0\n",
      "lif layer 2 self.abs_max_v: 3246.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.493604/ 52.822952, val:  75.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6104%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 63073   9.912%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.468796/ 51.569096, val:  74.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 63703   9.859%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.115984/ 38.196453, val:  85.00%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0401%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9709%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 64296   9.802%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.375290/ 66.471718, val:  69.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3870%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 64912   9.751%\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.238841/ 38.955894, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0320%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 65548   9.703%\n",
      "lif layer 2 self.abs_max_v: 3314.5\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.228628/ 49.113129, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0881%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 66139   9.651%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.662858/ 49.584618, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 66784   9.608%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.082643/ 36.647770, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9773%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 67360   9.556%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.981433/ 45.948307, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0338%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 67941   9.507%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.732414/ 34.893738, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 68558   9.463%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  4.327233/ 36.789944, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1640%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 69148   9.418%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.462376/ 45.746021, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 69664   9.363%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.752479/ 42.995354, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 70204   9.313%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.633535/ 38.432198, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0795%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 70729   9.262%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  4.016111/ 36.373730, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 71307   9.220%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.833411/ 50.069683, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 71826   9.171%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.754256/ 34.420372, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 72379   9.127%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.599287/ 49.982090, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0390%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 72898   9.081%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.997176/ 33.912426, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 73434   9.037%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.671048/ 48.767899, val:  75.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1171%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 73949   8.992%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  4.140243/ 52.272953, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1014%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 74518   8.955%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.538629/ 43.961208, val:  79.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4474%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5579%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 75045   8.913%\n",
      "lif layer 2 self.abs_max_v: 3390.5\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.787481/ 37.967396, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1256%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 75595   8.875%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.703116/ 43.061428, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 76119   8.835%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.475899/ 44.078800, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0396%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7777%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 76626   8.794%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.315024/ 33.197422, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0308%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 77114   8.752%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.508689/ 33.709137, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5024%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 77635   8.714%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.719460/ 39.871174, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 78131   8.675%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.487311/ 44.913727, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0959%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6726%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 78650   8.638%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.296660/ 39.088806, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6810%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 79107   8.596%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.250351/ 41.943180, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1121%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1921%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 79540   8.552%\n",
      "fc layer 2 self.abs_max_out: 2177.0\n",
      "fc layer 1 self.abs_max_out: 4083.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.160467/ 36.523129, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0279%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 79990   8.511%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.291432/ 35.238895, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3668%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0036%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 80459   8.473%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.343820/ 38.964443, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 80940   8.436%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.476295/ 38.148460, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 81440   8.403%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.203210/ 49.492607, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 81916   8.367%\n",
      "fc layer 1 self.abs_max_out: 4100.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.084932/ 38.255440, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2585%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 82356   8.329%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.941722/ 43.740417, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5675%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 82819   8.294%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.808658/ 39.595585, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 83231   8.254%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.052298/ 42.168541, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 83675   8.218%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.922455/ 35.804867, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0421%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 84092   8.181%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  3.438469/ 45.423599, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 84544   8.147%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.133734/ 45.624538, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 85007   8.115%\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "fc layer 3 self.abs_max_out: 862.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.253002/ 43.439137, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 85463   8.083%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  3.365673/ 42.889797, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6448%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 85912   8.051%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.837823/ 36.224590, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 86327   8.016%\n",
      "fc layer 2 self.abs_max_out: 2234.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.961700/ 50.205685, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 86766   7.984%\n",
      "fc layer 3 self.abs_max_out: 866.0\n",
      "fc layer 3 self.abs_max_out: 883.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.751090/ 46.088120, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6378%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 87167   7.950%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  3.097913/ 40.683510, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1206%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 87599   7.918%\n",
      "fc layer 1 self.abs_max_out: 4165.0\n",
      "fc layer 2 self.abs_max_out: 2252.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.822884/ 47.429031, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 87989   7.884%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.081229/ 50.706383, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 88421   7.854%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  3.065729/ 41.400681, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 88840   7.823%\n",
      "fc layer 2 self.abs_max_out: 2263.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.527609/ 37.757614, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1139%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 89225   7.790%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.430355/ 38.966557, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 89602   7.756%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  3.113761/ 43.128513, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8449%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 90039   7.729%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.529354/ 42.218544, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 90432   7.698%\n",
      "fc layer 3 self.abs_max_out: 889.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.767054/ 50.641041, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 90822   7.667%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.529827/ 41.939831, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 91195   7.635%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.889230/ 39.777637, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7138%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 91577   7.605%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.482111/ 49.420967, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 91949   7.574%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.599283/ 38.448479, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 92325   7.544%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.530434/ 35.875908, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 92686   7.514%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.806979/ 34.212833, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0421%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 93056   7.484%\n",
      "fc layer 1 self.abs_max_out: 4180.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.818281/ 37.771919, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 93475   7.459%\n",
      "fc layer 1 self.abs_max_out: 4184.0\n",
      "fc layer 1 self.abs_max_out: 4190.0\n",
      "fc layer 2 self.abs_max_out: 2269.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.350218/ 35.995625, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 93832   7.430%\n",
      "fc layer 1 self.abs_max_out: 4195.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.341103/ 42.892109, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 94186   7.400%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.574167/ 41.957100, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1249%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 94539   7.372%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.522242/ 32.949299, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3154%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 94899   7.344%\n",
      "fc layer 3 self.abs_max_out: 902.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.421103/ 40.248711, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0765%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 95254   7.316%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.368771/ 36.744732, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0503%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 95573   7.285%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.384879/ 39.755512, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 95912   7.257%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.456139/ 45.678539, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6504%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 96274   7.231%\n",
      "fc layer 3 self.abs_max_out: 918.0\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.116834/ 52.680233, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6901%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 96613   7.203%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.119235/ 46.261684, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4417%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 96939   7.175%\n",
      "fc layer 1 self.abs_max_out: 4230.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.489272/ 53.141060, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 97286   7.149%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.081194/ 35.584820, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 97611   7.122%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.954737/ 40.165184, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2990%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 97910   7.093%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.226479/ 39.457569, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 98218   7.065%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.049670/ 39.697575, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4742%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 98514   7.037%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.185469/ 50.605312, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1072%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 98826   7.010%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.760838/ 43.483089, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4711%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 99199   6.988%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.012612/ 37.946842, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 99509   6.962%\n",
      "fc layer 1 self.abs_max_out: 4312.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.257049/ 42.230312, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 99852   6.938%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.315243/ 41.499863, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 100179   6.914%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.121281/ 34.133022, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 100477   6.888%\n",
      "fc layer 1 self.abs_max_out: 4318.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.549192/ 40.007431, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 100829   6.866%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.121844/ 45.412994, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 101153   6.843%\n",
      "fc layer 1 self.abs_max_out: 4324.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.334218/ 40.604187, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 101465   6.819%\n",
      "fc layer 1 self.abs_max_out: 4327.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.043977/ 50.450558, val:  83.33%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 101775   6.795%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  2.219055/ 44.369385, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1787%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 102101   6.772%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.987792/ 37.210033, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2958%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 102394   6.748%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.394248/ 43.009197, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 102734   6.727%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.919328/ 46.915211, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 103025   6.703%\n",
      "fc layer 2 self.abs_max_out: 2336.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.224816/ 46.763092, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1037%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 103350   6.681%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  2.080801/ 42.282047, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 103650   6.659%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.884597/ 44.689911, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0419%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 103931   6.635%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.944432/ 43.730350, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 104204   6.611%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  2.266425/ 38.156406, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 104520   6.590%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  2.045414/ 43.602566, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 104816   6.568%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.889069/ 49.394772, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 105113   6.547%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.776420/ 52.691944, val:  82.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 105365   6.523%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.648792/ 46.542732, val:  84.58%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 105617   6.499%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.752696/ 39.404251, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 105898   6.477%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.823694/ 42.698345, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 106154   6.454%\n",
      "lif layer 1 self.abs_max_v: 6502.0\n",
      "lif layer 1 self.abs_max_v: 6512.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.928470/ 40.494125, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7524%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 106437   6.433%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  2.139615/ 41.832630, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 106736   6.413%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.763485/ 42.425243, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 107001   6.392%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.698879/ 49.920788, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 107253   6.369%\n",
      "fc layer 3 self.abs_max_out: 938.0\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.828586/ 47.563896, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 107526   6.349%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.621189/ 38.504524, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4686%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 107783   6.327%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.791189/ 41.783028, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 108053   6.307%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.847604/ 39.283836, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3933%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 108311   6.286%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.595898/ 38.397217, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 108545   6.264%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  2.019816/ 42.294136, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3887%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8702%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 108822   6.245%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.621964/ 45.094475, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 109077   6.224%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.432661/ 48.191536, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 109298   6.202%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.649552/ 47.030544, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8558%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 109535   6.181%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.738569/ 41.182446, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 109783   6.161%\n",
      "fc layer 1 self.abs_max_out: 4351.0\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.560794/ 46.219326, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1092%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 110009   6.140%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.811628/ 46.879814, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0594%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2972%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 110278   6.122%\n",
      "fc layer 1 self.abs_max_out: 4352.0\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.341353/ 42.174519, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 110473   6.100%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.397652/ 43.544014, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 110679   6.078%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.726415/ 45.975597, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 110908   6.058%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.649038/ 50.560005, val:  84.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 111156   6.039%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.833420/ 39.723053, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 111427   6.022%\n",
      "fc layer 1 self.abs_max_out: 4363.0\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.346798/ 44.474636, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 111630   6.001%\n",
      "fc layer 2 self.abs_max_out: 2355.0\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.573088/ 42.016136, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6937%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 111857   5.982%\n",
      "fc layer 1 self.abs_max_out: 4380.0\n",
      "fc layer 2 self.abs_max_out: 2410.0\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.574126/ 44.880283, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0487%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 112108   5.964%\n",
      "fc layer 3 self.abs_max_out: 957.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.302440/ 42.451313, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0307%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 112304   5.944%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.717143/ 42.505325, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7248%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 112537   5.925%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.537084/ 48.350876, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5609%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 112782   5.908%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.492427/ 43.737076, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1222%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 113008   5.889%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.862440/ 43.177040, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1159%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 113269   5.873%\n",
      "fc layer 1 self.abs_max_out: 4420.0\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.419866/ 44.658062, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1075%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 113477   5.854%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.619527/ 37.407795, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 113693   5.836%\n",
      "fc layer 1 self.abs_max_out: 4462.0\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.533632/ 41.687569, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0177%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eece2606204241559e58c4545651cf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.53363</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>41.68757</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-sweep-29</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kdxigs0c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kdxigs0c</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_113518-kdxigs0c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vf0pb3md with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 32151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_155356-vf0pb3md</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vf0pb3md' target=\"_blank\">twilight-sweep-36</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vf0pb3md' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vf0pb3md</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251213_155405_812', 'my_seed': 32151, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 208.0\n",
      "lif layer 1 self.abs_max_v: 208.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 103.0\n",
      "lif layer 2 self.abs_max_v: 103.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 221.5\n",
      "fc layer 2 self.abs_max_out: 178.0\n",
      "lif layer 2 self.abs_max_v: 191.5\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "lif layer 1 self.abs_max_v: 253.0\n",
      "fc layer 2 self.abs_max_out: 257.0\n",
      "lif layer 2 self.abs_max_v: 286.5\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "fc layer 3 self.abs_max_out: 87.0\n",
      "lif layer 1 self.abs_max_v: 314.5\n",
      "fc layer 1 self.abs_max_out: 223.0\n",
      "lif layer 1 self.abs_max_v: 324.0\n",
      "lif layer 2 self.abs_max_v: 361.0\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "lif layer 1 self.abs_max_v: 354.0\n",
      "fc layer 2 self.abs_max_out: 267.0\n",
      "fc layer 1 self.abs_max_out: 246.0\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "fc layer 1 self.abs_max_out: 252.0\n",
      "fc layer 1 self.abs_max_out: 260.0\n",
      "fc layer 2 self.abs_max_out: 317.0\n",
      "lif layer 2 self.abs_max_v: 408.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "fc layer 1 self.abs_max_out: 337.0\n",
      "fc layer 2 self.abs_max_out: 329.0\n",
      "lif layer 2 self.abs_max_v: 476.5\n",
      "fc layer 2 self.abs_max_out: 355.0\n",
      "lif layer 1 self.abs_max_v: 377.5\n",
      "lif layer 1 self.abs_max_v: 466.0\n",
      "fc layer 1 self.abs_max_out: 397.0\n",
      "lif layer 1 self.abs_max_v: 630.0\n",
      "fc layer 2 self.abs_max_out: 381.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "lif layer 2 self.abs_max_v: 548.5\n",
      "fc layer 1 self.abs_max_out: 448.0\n",
      "fc layer 2 self.abs_max_out: 415.0\n",
      "fc layer 2 self.abs_max_out: 470.0\n",
      "lif layer 2 self.abs_max_v: 555.0\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "lif layer 2 self.abs_max_v: 588.5\n",
      "lif layer 2 self.abs_max_v: 670.5\n",
      "lif layer 2 self.abs_max_v: 799.5\n",
      "fc layer 2 self.abs_max_out: 526.0\n",
      "fc layer 1 self.abs_max_out: 481.0\n",
      "fc layer 2 self.abs_max_out: 692.0\n",
      "fc layer 1 self.abs_max_out: 486.0\n",
      "fc layer 3 self.abs_max_out: 199.0\n",
      "fc layer 1 self.abs_max_out: 526.0\n",
      "fc layer 1 self.abs_max_out: 565.0\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "fc layer 1 self.abs_max_out: 607.0\n",
      "fc layer 1 self.abs_max_out: 679.0\n",
      "lif layer 1 self.abs_max_v: 679.0\n",
      "fc layer 1 self.abs_max_out: 753.0\n",
      "lif layer 1 self.abs_max_v: 753.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "lif layer 2 self.abs_max_v: 867.5\n",
      "lif layer 2 self.abs_max_v: 1009.0\n",
      "fc layer 2 self.abs_max_out: 711.0\n",
      "fc layer 1 self.abs_max_out: 764.0\n",
      "lif layer 1 self.abs_max_v: 764.0\n",
      "fc layer 1 self.abs_max_out: 875.0\n",
      "lif layer 1 self.abs_max_v: 875.0\n",
      "lif layer 1 self.abs_max_v: 995.5\n",
      "lif layer 1 self.abs_max_v: 1100.0\n",
      "fc layer 2 self.abs_max_out: 722.0\n",
      "fc layer 1 self.abs_max_out: 928.0\n",
      "fc layer 1 self.abs_max_out: 963.0\n",
      "lif layer 1 self.abs_max_v: 1112.5\n",
      "fc layer 1 self.abs_max_out: 999.0\n",
      "lif layer 2 self.abs_max_v: 1056.0\n",
      "fc layer 2 self.abs_max_out: 723.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 2 self.abs_max_out: 730.0\n",
      "fc layer 1 self.abs_max_out: 1003.0\n",
      "fc layer 2 self.abs_max_out: 770.0\n",
      "fc layer 2 self.abs_max_out: 803.0\n",
      "lif layer 1 self.abs_max_v: 1193.5\n",
      "lif layer 1 self.abs_max_v: 1215.0\n",
      "lif layer 2 self.abs_max_v: 1074.5\n",
      "fc layer 2 self.abs_max_out: 812.0\n",
      "lif layer 1 self.abs_max_v: 1277.0\n",
      "lif layer 1 self.abs_max_v: 1335.5\n",
      "fc layer 1 self.abs_max_out: 1020.0\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "fc layer 1 self.abs_max_out: 1044.0\n",
      "fc layer 1 self.abs_max_out: 1102.0\n",
      "lif layer 1 self.abs_max_v: 1344.5\n",
      "lif layer 2 self.abs_max_v: 1148.0\n",
      "fc layer 2 self.abs_max_out: 828.0\n",
      "fc layer 2 self.abs_max_out: 841.0\n",
      "lif layer 1 self.abs_max_v: 1573.5\n",
      "lif layer 1 self.abs_max_v: 1855.0\n",
      "fc layer 2 self.abs_max_out: 863.0\n",
      "fc layer 2 self.abs_max_out: 883.0\n",
      "fc layer 2 self.abs_max_out: 919.0\n",
      "fc layer 1 self.abs_max_out: 1195.0\n",
      "fc layer 2 self.abs_max_out: 941.0\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "fc layer 2 self.abs_max_out: 955.0\n",
      "lif layer 2 self.abs_max_v: 1246.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "fc layer 1 self.abs_max_out: 1225.0\n",
      "fc layer 1 self.abs_max_out: 1311.0\n",
      "lif layer 1 self.abs_max_v: 1857.0\n",
      "fc layer 2 self.abs_max_out: 1015.0\n",
      "lif layer 2 self.abs_max_v: 1279.0\n",
      "fc layer 2 self.abs_max_out: 1048.0\n",
      "lif layer 2 self.abs_max_v: 1474.0\n",
      "lif layer 2 self.abs_max_v: 1538.0\n",
      "fc layer 1 self.abs_max_out: 1441.0\n",
      "fc layer 3 self.abs_max_out: 371.0\n",
      "fc layer 1 self.abs_max_out: 1599.0\n",
      "fc layer 3 self.abs_max_out: 385.0\n",
      "fc layer 3 self.abs_max_out: 387.0\n",
      "lif layer 1 self.abs_max_v: 1931.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 3 self.abs_max_out: 425.0\n",
      "lif layer 1 self.abs_max_v: 1983.0\n",
      "lif layer 1 self.abs_max_v: 2069.5\n",
      "lif layer 1 self.abs_max_v: 2170.0\n",
      "lif layer 1 self.abs_max_v: 2238.0\n",
      "lif layer 1 self.abs_max_v: 2352.0\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "lif layer 1 self.abs_max_v: 2370.0\n",
      "lif layer 1 self.abs_max_v: 2372.0\n",
      "fc layer 1 self.abs_max_out: 1607.0\n",
      "lif layer 1 self.abs_max_v: 2439.0\n",
      "lif layer 1 self.abs_max_v: 2450.0\n",
      "lif layer 1 self.abs_max_v: 2458.5\n",
      "lif layer 1 self.abs_max_v: 2477.0\n",
      "lif layer 1 self.abs_max_v: 2783.5\n",
      "lif layer 1 self.abs_max_v: 2943.0\n",
      "lif layer 1 self.abs_max_v: 2992.5\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "lif layer 2 self.abs_max_v: 1546.0\n",
      "lif layer 2 self.abs_max_v: 1567.0\n",
      "lif layer 2 self.abs_max_v: 1608.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 15.620570/ 95.834381, val:  31.67%, val_best:  31.67%, tr:  97.14%, tr_best:  97.14%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1067%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2181  22.278%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1055.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 2 self.abs_max_out: 1119.0\n",
      "fc layer 1 self.abs_max_out: 1619.0\n",
      "fc layer 2 self.abs_max_out: 1196.0\n",
      "fc layer 1 self.abs_max_out: 1630.0\n",
      "fc layer 1 self.abs_max_out: 1671.0\n",
      "lif layer 2 self.abs_max_v: 1652.5\n",
      "fc layer 2 self.abs_max_out: 1236.0\n",
      "lif layer 2 self.abs_max_v: 1668.5\n",
      "lif layer 2 self.abs_max_v: 1696.5\n",
      "fc layer 1 self.abs_max_out: 1780.0\n",
      "fc layer 1 self.abs_max_out: 1817.0\n",
      "lif layer 2 self.abs_max_v: 1740.5\n",
      "fc layer 1 self.abs_max_out: 1819.0\n",
      "fc layer 1 self.abs_max_out: 1920.0\n",
      "lif layer 1 self.abs_max_v: 3129.5\n",
      "lif layer 1 self.abs_max_v: 3311.5\n",
      "fc layer 2 self.abs_max_out: 1304.0\n",
      "lif layer 2 self.abs_max_v: 1747.0\n",
      "lif layer 2 self.abs_max_v: 1770.5\n",
      "lif layer 2 self.abs_max_v: 1771.5\n",
      "lif layer 2 self.abs_max_v: 1799.5\n",
      "lif layer 2 self.abs_max_v: 1813.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.821865/ 93.237785, val:  33.33%, val_best:  33.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3812  19.469%\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "lif layer 1 self.abs_max_v: 3388.0\n",
      "lif layer 2 self.abs_max_v: 1844.5\n",
      "lif layer 2 self.abs_max_v: 1883.5\n",
      "lif layer 2 self.abs_max_v: 1961.5\n",
      "fc layer 1 self.abs_max_out: 1996.0\n",
      "fc layer 1 self.abs_max_out: 2067.0\n",
      "lif layer 1 self.abs_max_v: 3421.5\n",
      "lif layer 1 self.abs_max_v: 3571.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.832909/ 45.020157, val:  47.92%, val_best:  47.92%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2319%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5312  18.086%\n",
      "lif layer 2 self.abs_max_v: 1979.0\n",
      "fc layer 2 self.abs_max_out: 1335.0\n",
      "fc layer 3 self.abs_max_out: 495.0\n",
      "fc layer 1 self.abs_max_out: 2079.0\n",
      "fc layer 1 self.abs_max_out: 2095.0\n",
      "fc layer 1 self.abs_max_out: 2361.0\n",
      "lif layer 1 self.abs_max_v: 3837.5\n",
      "lif layer 1 self.abs_max_v: 3896.0\n",
      "lif layer 2 self.abs_max_v: 1980.0\n",
      "lif layer 2 self.abs_max_v: 1982.0\n",
      "lif layer 2 self.abs_max_v: 2004.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.325822/ 39.257980, val:  56.67%, val_best:  56.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6724  17.171%\n",
      "lif layer 2 self.abs_max_v: 2102.5\n",
      "lif layer 2 self.abs_max_v: 2113.5\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 1 self.abs_max_out: 2390.0\n",
      "lif layer 1 self.abs_max_v: 3933.0\n",
      "fc layer 1 self.abs_max_out: 2428.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.086242/ 52.179539, val:  52.50%, val_best:  56.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8103  16.554%\n",
      "fc layer 2 self.abs_max_out: 1376.0\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "fc layer 2 self.abs_max_out: 1443.0\n",
      "fc layer 1 self.abs_max_out: 2488.0\n",
      "lif layer 1 self.abs_max_v: 3989.5\n",
      "lif layer 1 self.abs_max_v: 4012.0\n",
      "fc layer 1 self.abs_max_out: 2537.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.685696/ 79.585266, val:  41.25%, val_best:  56.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9390  15.986%\n",
      "fc layer 2 self.abs_max_out: 1508.0\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 2 self.abs_max_out: 1513.0\n",
      "fc layer 1 self.abs_max_out: 2621.0\n",
      "lif layer 1 self.abs_max_v: 4226.0\n",
      "lif layer 1 self.abs_max_v: 4354.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.980299/ 62.148674, val:  56.67%, val_best:  56.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10691  15.600%\n",
      "fc layer 1 self.abs_max_out: 2797.0\n",
      "fc layer 3 self.abs_max_out: 613.0\n",
      "lif layer 2 self.abs_max_v: 2124.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.448504/ 85.895966, val:  48.75%, val_best:  56.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11903  15.198%\n",
      "lif layer 1 self.abs_max_v: 4382.0\n",
      "lif layer 1 self.abs_max_v: 4406.0\n",
      "fc layer 2 self.abs_max_out: 1548.0\n",
      "lif layer 2 self.abs_max_v: 2155.0\n",
      "fc layer 2 self.abs_max_out: 1566.0\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "fc layer 2 self.abs_max_out: 1668.0\n",
      "lif layer 2 self.abs_max_v: 2216.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.436085/ 58.572617, val:  53.75%, val_best:  56.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13140  14.913%\n",
      "lif layer 2 self.abs_max_v: 2217.0\n",
      "lif layer 2 self.abs_max_v: 2243.5\n",
      "lif layer 1 self.abs_max_v: 4450.5\n",
      "lif layer 1 self.abs_max_v: 4526.0\n",
      "lif layer 2 self.abs_max_v: 2251.0\n",
      "lif layer 2 self.abs_max_v: 2282.5\n",
      "lif layer 2 self.abs_max_v: 2384.5\n",
      "lif layer 2 self.abs_max_v: 2398.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  9.740460/ 35.565063, val:  61.25%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14353  14.661%\n",
      "fc layer 2 self.abs_max_out: 1718.0\n",
      "fc layer 2 self.abs_max_out: 1748.0\n",
      "fc layer 2 self.abs_max_out: 1760.0\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 1 self.abs_max_out: 2806.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  9.168242/ 53.006413, val:  57.08%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2435%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15586  14.473%\n",
      "lif layer 2 self.abs_max_v: 2414.0\n",
      "lif layer 2 self.abs_max_v: 2428.5\n",
      "lif layer 1 self.abs_max_v: 4754.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.882859/ 46.280918, val:  57.92%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16755  14.262%\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "fc layer 2 self.abs_max_out: 1790.0\n",
      "fc layer 1 self.abs_max_out: 2860.0\n",
      "lif layer 1 self.abs_max_v: 4774.5\n",
      "lif layer 1 self.abs_max_v: 5005.5\n",
      "lif layer 1 self.abs_max_v: 5216.0\n",
      "fc layer 1 self.abs_max_out: 2888.0\n",
      "lif layer 1 self.abs_max_v: 5496.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  9.021309/ 98.999039, val:  46.67%, val_best:  61.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1147%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17927  14.086%\n",
      "lif layer 2 self.abs_max_v: 2473.0\n",
      "lif layer 2 self.abs_max_v: 2540.5\n",
      "fc layer 3 self.abs_max_out: 673.0\n",
      "lif layer 2 self.abs_max_v: 2566.5\n",
      "fc layer 1 self.abs_max_out: 3090.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.792482/ 74.903816, val:  46.25%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19079  13.920%\n",
      "lif layer 2 self.abs_max_v: 2577.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.806227/ 55.996269, val:  59.17%, val_best:  61.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20225  13.773%\n",
      "fc layer 2 self.abs_max_out: 1796.0\n",
      "fc layer 2 self.abs_max_out: 1833.0\n",
      "fc layer 3 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 2590.5\n",
      "fc layer 2 self.abs_max_out: 1841.0\n",
      "fc layer 2 self.abs_max_out: 1880.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  8.440533/ 49.956345, val:  60.42%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21327  13.615%\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "fc layer 3 self.abs_max_out: 748.0\n",
      "fc layer 3 self.abs_max_out: 769.0\n",
      "lif layer 2 self.abs_max_v: 2638.5\n",
      "lif layer 2 self.abs_max_v: 2808.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.353859/ 58.982769, val:  57.08%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22459  13.495%\n",
      "fc layer 1 self.abs_max_out: 3229.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.770539/ 53.770546, val:  56.67%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23510  13.341%\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  8.377547/ 50.545296, val:  61.67%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24592  13.221%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  8.401810/ 40.229679, val:  75.42%, val_best:  75.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1869%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25686  13.118%\n",
      "fc layer 1 self.abs_max_out: 3338.0\n",
      "lif layer 1 self.abs_max_v: 5552.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.655803/ 57.400620, val:  58.75%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4889%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26736  13.005%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  8.058790/ 70.708794, val:  60.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27772  12.894%\n",
      "lif layer 1 self.abs_max_v: 5664.0\n",
      "lif layer 1 self.abs_max_v: 5749.0\n",
      "lif layer 1 self.abs_max_v: 5786.5\n",
      "fc layer 2 self.abs_max_out: 1883.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.356505/ 44.879200, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28774  12.779%\n",
      "lif layer 1 self.abs_max_v: 5843.5\n",
      "lif layer 1 self.abs_max_v: 5916.0\n",
      "lif layer 1 self.abs_max_v: 5954.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.489587/ 45.363235, val:  67.92%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6328%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29733  12.654%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.407270/ 76.947571, val:  52.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1001%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5754%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30714  12.549%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  7.262412/ 36.479309, val:  72.08%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31644  12.432%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.978816/ 57.640297, val:  63.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1057%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3939%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32573  12.323%\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  7.221222/ 57.993771, val:  56.67%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0599%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33523  12.229%\n",
      "lif layer 2 self.abs_max_v: 2897.5\n",
      "lif layer 2 self.abs_max_v: 2956.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.922637/ 33.969742, val:  71.25%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0541%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34434  12.128%\n",
      "fc layer 1 self.abs_max_out: 3359.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.479686/ 40.159801, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35349  12.036%\n",
      "fc layer 2 self.abs_max_out: 1996.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.450670/ 44.924210, val:  67.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0401%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36196  11.927%\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.495925/ 57.555504, val:  57.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37058  11.829%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.415778/ 45.331882, val:  68.75%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37907  11.733%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.102703/ 43.701141, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5225%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38744  11.640%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  7.132735/ 33.606876, val:  78.33%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39657  11.574%\n",
      "fc layer 1 self.abs_max_out: 3419.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  6.187788/ 39.350777, val:  76.67%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40494  11.490%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  6.030828/ 34.371704, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5704%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41323  11.408%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  6.131804/ 50.483917, val:  67.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42168  11.335%\n",
      "lif layer 2 self.abs_max_v: 3017.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  6.102066/ 43.150349, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42980  11.257%\n",
      "fc layer 2 self.abs_max_out: 2027.0\n",
      "lif layer 1 self.abs_max_v: 6050.5\n",
      "lif layer 1 self.abs_max_v: 6110.5\n",
      "lif layer 1 self.abs_max_v: 6158.5\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.771233/ 37.638554, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43792  11.183%\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "lif layer 1 self.abs_max_v: 6339.5\n",
      "lif layer 1 self.abs_max_v: 6437.0\n",
      "lif layer 1 self.abs_max_v: 6514.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  6.250777/ 46.940830, val:  74.17%, val_best:  81.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44606  11.113%\n",
      "lif layer 2 self.abs_max_v: 3170.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.459339/ 42.761425, val:  76.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45360  11.032%\n",
      "lif layer 2 self.abs_max_v: 3222.0\n",
      "lif layer 2 self.abs_max_v: 3341.0\n",
      "lif layer 2 self.abs_max_v: 3347.0\n",
      "lif layer 2 self.abs_max_v: 3469.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.963755/ 41.333172, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46165  10.966%\n",
      "fc layer 1 self.abs_max_out: 3451.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  6.064377/ 32.867821, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46979  10.906%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.446784/ 32.025612, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47740  10.836%\n",
      "fc layer 1 self.abs_max_out: 3460.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.453180/ 38.554829, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48470  10.763%\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "fc layer 1 self.abs_max_out: 3510.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.118396/ 46.593155, val:  74.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49165  10.685%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.071518/ 47.799629, val:  70.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49851  10.608%\n",
      "fc layer 2 self.abs_max_out: 2158.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.431007/ 63.968044, val:  67.08%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5175%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50593  10.547%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.390923/ 33.009094, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2706%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51305  10.481%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.797467/ 55.350628, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51955  10.406%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.949998/ 41.795879, val:  82.08%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52622  10.337%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  5.380390/ 40.086212, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7960%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53318  10.276%\n",
      "fc layer 3 self.abs_max_out: 781.0\n",
      "fc layer 1 self.abs_max_out: 3523.0\n",
      "fc layer 1 self.abs_max_out: 3558.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.926353/ 42.240261, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53995  10.214%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.943181/ 36.299957, val:  84.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54635  10.147%\n",
      "fc layer 2 self.abs_max_out: 2163.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.844335/ 48.740231, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2594%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55314  10.089%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.669663/ 41.974247, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55976  10.031%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  5.044225/ 55.490021, val:  72.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1161%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56662   9.979%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.909616/ 37.911678, val:  84.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1003%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57325   9.925%\n",
      "fc layer 2 self.abs_max_out: 2188.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.623309/ 52.307903, val:  74.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1921%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57955   9.866%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.684980/ 42.713322, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58592   9.811%\n",
      "fc layer 1 self.abs_max_out: 3708.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.485254/ 44.062031, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59224   9.757%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.349362/ 35.721214, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7693%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59844   9.703%\n",
      "fc layer 1 self.abs_max_out: 3726.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.171745/ 40.827408, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60427   9.644%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.235596/ 42.215702, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1190%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9890%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61009   9.587%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.383987/ 52.923531, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61612   9.535%\n",
      "fc layer 3 self.abs_max_out: 806.0\n",
      "fc layer 1 self.abs_max_out: 3730.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.996100/ 38.010494, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62254   9.491%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.973891/ 38.313274, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62824   9.437%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.716513/ 56.809895, val:  70.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63355   9.379%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.444673/ 34.928413, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0305%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63951   9.332%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.157036/ 42.152367, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0342%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64521   9.282%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.775839/ 47.829544, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65049   9.228%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.785483/ 44.687584, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65588   9.177%\n",
      "lif layer 2 self.abs_max_v: 3508.5\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.131844/ 38.388638, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6796%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66140   9.130%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.838264/ 38.037827, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66670   9.080%\n",
      "fc layer 2 self.abs_max_out: 2237.0\n",
      "fc layer 3 self.abs_max_out: 833.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.524383/ 46.470245, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7734%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67165   9.027%\n",
      "fc layer 2 self.abs_max_out: 2304.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  4.116457/ 48.873997, val:  79.58%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67741   8.986%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.494920/ 39.264854, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68222   8.934%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.708139/ 46.549614, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0790%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68753   8.890%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  4.231214/ 46.810795, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0532%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69303   8.849%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.894160/ 46.742565, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69837   8.807%\n",
      "lif layer 2 self.abs_max_v: 3523.5\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.595532/ 43.828075, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70321   8.760%\n",
      "lif layer 2 self.abs_max_v: 3602.0\n",
      "lif layer 1 self.abs_max_v: 6552.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.757920/ 43.460793, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70829   8.717%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.309920/ 42.234501, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5686%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71273   8.667%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.154065/ 38.389091, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3154%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71732   8.620%\n",
      "fc layer 2 self.abs_max_out: 2307.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.813141/ 44.595459, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72249   8.581%\n",
      "fc layer 3 self.abs_max_out: 851.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.632618/ 41.047009, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0351%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72755   8.542%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.955170/ 38.387108, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1253%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0163%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73258   8.503%\n",
      "fc layer 1 self.abs_max_out: 3734.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.331754/ 41.601051, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73745   8.464%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.425803/ 41.434113, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0333%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74230   8.425%\n",
      "fc layer 1 self.abs_max_out: 3785.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.352475/ 39.686447, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0575%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74720   8.387%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.217292/ 40.410347, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75159   8.345%\n",
      "fc layer 1 self.abs_max_out: 3796.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.155215/ 40.076466, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0082%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75615   8.305%\n",
      "lif layer 1 self.abs_max_v: 6674.5\n",
      "fc layer 1 self.abs_max_out: 3958.0\n",
      "lif layer 1 self.abs_max_v: 6696.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.424607/ 52.085133, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 76072   8.266%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.923531/ 39.782417, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0388%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76473   8.222%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.289143/ 43.515976, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76931   8.186%\n",
      "lif layer 2 self.abs_max_v: 3627.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.907599/ 55.089188, val:  78.33%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77350   8.145%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.816701/ 47.408146, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77752   8.104%\n",
      "lif layer 2 self.abs_max_v: 3707.0\n",
      "lif layer 1 self.abs_max_v: 6864.5\n",
      "fc layer 3 self.abs_max_out: 854.0\n",
      "lif layer 2 self.abs_max_v: 3794.5\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.076986/ 48.200420, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0765%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78178   8.066%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.743848/ 57.112740, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78569   8.025%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.079051/ 44.347149, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78981   7.988%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.942011/ 44.385098, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79378   7.949%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.832120/ 45.451218, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79770   7.911%\n",
      "fc layer 3 self.abs_max_out: 858.0\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.128192/ 47.384853, val:  82.08%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1275%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1972%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80194   7.876%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.745115/ 42.398655, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80592   7.840%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.860367/ 43.757607, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 80973   7.803%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.012069/ 47.116562, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81386   7.769%\n",
      "fc layer 3 self.abs_max_out: 876.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.648125/ 52.247101, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81757   7.732%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.739228/ 45.546860, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82121   7.696%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.833384/ 47.964397, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0549%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82537   7.664%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.664464/ 49.194366, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1128%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82906   7.629%\n",
      "fc layer 2 self.abs_max_out: 2362.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.366894/ 41.660522, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1011%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83250   7.592%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.579178/ 48.458233, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83632   7.560%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.608438/ 53.139751, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9266%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 83994   7.526%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.380968/ 45.612381, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84334   7.491%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.450114/ 39.187527, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84662   7.455%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.062893/ 53.903732, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7449%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85050   7.425%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.524927/ 59.049576, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3230%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85390   7.392%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  3.023198/ 48.955906, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 85787   7.364%\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "fc layer 2 self.abs_max_out: 2408.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.509507/ 48.990833, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86147   7.333%\n",
      "fc layer 3 self.abs_max_out: 892.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.628935/ 52.451553, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1303%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86494   7.302%\n",
      "fc layer 1 self.abs_max_out: 3978.0\n",
      "fc layer 3 self.abs_max_out: 918.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.591444/ 49.894356, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 86851   7.272%\n",
      "lif layer 1 self.abs_max_v: 6902.5\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.665578/ 47.810089, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87218   7.243%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.533368/ 51.201538, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87548   7.212%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.530861/ 51.245247, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 87898   7.183%\n",
      "fc layer 3 self.abs_max_out: 936.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.406960/ 50.405243, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88231   7.153%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.432168/ 47.091000, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88579   7.124%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.243921/ 49.724724, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 88908   7.095%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.429591/ 47.314766, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0747%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89251   7.067%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.108938/ 40.586597, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1135%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89569   7.038%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.331178/ 40.722591, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 89889   7.009%\n",
      "fc layer 2 self.abs_max_out: 2455.0\n",
      "fc layer 1 self.abs_max_out: 3997.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.293722/ 47.901272, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90224   6.982%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.204669/ 43.936043, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9191%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90535   6.953%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.343761/ 43.842667, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 90848   6.925%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.489372/ 55.866821, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5154%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91184   6.899%\n",
      "fc layer 1 self.abs_max_out: 4001.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.372821/ 43.335114, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0281%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1206%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91503   6.872%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.448874/ 45.384331, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 91837   6.847%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.541072/ 46.734989, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92191   6.824%\n",
      "lif layer 1 self.abs_max_v: 7003.5\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.145503/ 43.963539, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92496   6.797%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.027125/ 43.297459, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 92787   6.770%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.126369/ 52.204269, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0356%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93073   6.743%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.507085/ 50.342308, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93403   6.719%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.018810/ 45.879902, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93683   6.692%\n",
      "lif layer 1 self.abs_max_v: 7086.0\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.022621/ 49.612560, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 93972   6.666%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.978706/ 63.198040, val:  77.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5009%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94251   6.639%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.201787/ 45.496319, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3547%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94566   6.616%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.905006/ 53.650513, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9052%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 94841   6.590%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.460581/ 53.376957, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1157%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95166   6.568%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.908384/ 47.625263, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95458   6.544%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.204092/ 41.744389, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 95766   6.521%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.965623/ 61.010815, val:  77.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1257%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 96052   6.498%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.105945/ 45.668427, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1073%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96360   6.475%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.047500/ 44.458778, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 96638   6.452%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.758853/ 43.441628, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 96908   6.428%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.559783/ 49.609322, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7966%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5050%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97143   6.402%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.718853/ 43.352169, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97383   6.376%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.166688/ 54.758793, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0549%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2035%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 97691   6.356%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.071094/ 41.237099, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 97974   6.334%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.711528/ 56.386082, val:  78.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98224   6.310%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.780999/ 43.362827, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6582%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98473   6.287%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.646256/ 43.790333, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 98727   6.264%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.901598/ 52.727692, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 98994   6.242%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.803330/ 49.337311, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8702%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99251   6.220%\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.827254/ 47.162170, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 99507   6.198%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.839917/ 51.146088, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 99763   6.176%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.824645/ 44.693840, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1300%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 100020   6.155%\n",
      "lif layer 1 self.abs_max_v: 7188.0\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.496477/ 47.645283, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100236   6.131%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.914455/ 46.832191, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 100494   6.110%\n",
      "fc layer 1 self.abs_max_out: 4035.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.667267/ 42.809731, val:  88.33%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 100717   6.087%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.732308/ 48.650932, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1569%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 100972   6.067%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.626916/ 52.706966, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101225   6.047%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.612321/ 45.716640, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 101466   6.026%\n",
      "lif layer 1 self.abs_max_v: 7292.5\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.528139/ 44.319614, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 101682   6.004%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.454950/ 46.648323, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8554%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 101905   5.982%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.884770/ 46.009861, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102177   5.964%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.811839/ 43.710293, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0491%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 102408   5.943%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.395052/ 42.000572, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1390%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 102621   5.922%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.861208/ 44.514244, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 102880   5.904%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.516831/ 46.181744, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1206%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103102   5.883%\n",
      "fc layer 3 self.abs_max_out: 961.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.764641/ 46.660980, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 103365   5.866%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.380057/ 40.378864, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 103572   5.845%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.713555/ 42.501144, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 103806   5.826%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.672821/ 46.642548, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 104034   5.807%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.340083/ 47.479626, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5731%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104225   5.786%\n",
      "fc layer 3 self.abs_max_out: 994.0\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.535753/ 49.106693, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8556%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 104459   5.768%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.652825/ 47.703266, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0076%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 104686   5.749%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.599118/ 49.854099, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 104905   5.730%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.450161/ 50.327705, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 105123   5.712%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.540311/ 45.369175, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 105361   5.694%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.212989/ 47.325409, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8111%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 105554   5.675%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.276067/ 48.003349, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 105752   5.656%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.197336/ 40.199631, val:  93.33%, val_best:  93.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 105941   5.636%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.564853/ 50.044556, val:  87.50%, val_best:  93.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106165   5.619%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.450062/ 49.750660, val:  85.83%, val_best:  93.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 106380   5.601%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.540966/ 45.613091, val:  89.17%, val_best:  93.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 106576   5.583%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.530895/ 51.218128, val:  85.42%, val_best:  93.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 106784   5.565%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.534344/ 50.913040, val:  87.08%, val_best:  93.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0388%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 107006   5.548%\n",
      "fc layer 3 self.abs_max_out: 1008.0\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.580670/ 48.396023, val:  85.83%, val_best:  93.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107222   5.531%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.430994/ 45.724434, val:  89.17%, val_best:  93.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 107431   5.514%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.473409/ 43.869217, val:  90.00%, val_best:  93.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9668%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d022d14fce6943698662e0aa6f6665ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.47341</td></tr><tr><td>val_acc_best</td><td>0.93333</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>43.86922</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-sweep-36</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vf0pb3md' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vf0pb3md</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_155356-vf0pb3md/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dk80dt78 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 15482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_201130-dk80dt78</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dk80dt78' target=\"_blank\">fearless-sweep-43</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dk80dt78' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dk80dt78</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251213_201139_796', 'my_seed': 15482, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 223.0\n",
      "lif layer 1 self.abs_max_v: 223.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "lif layer 2 self.abs_max_v: 95.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 227.0\n",
      "lif layer 1 self.abs_max_v: 336.0\n",
      "fc layer 2 self.abs_max_out: 166.0\n",
      "lif layer 2 self.abs_max_v: 187.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "fc layer 2 self.abs_max_out: 170.0\n",
      "lif layer 2 self.abs_max_v: 207.0\n",
      "fc layer 3 self.abs_max_out: 60.0\n",
      "fc layer 1 self.abs_max_out: 235.0\n",
      "lif layer 1 self.abs_max_v: 361.0\n",
      "lif layer 2 self.abs_max_v: 213.0\n",
      "lif layer 1 self.abs_max_v: 367.5\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "lif layer 2 self.abs_max_v: 275.5\n",
      "fc layer 3 self.abs_max_out: 75.0\n",
      "fc layer 2 self.abs_max_out: 279.0\n",
      "lif layer 2 self.abs_max_v: 324.5\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 1 self.abs_max_out: 253.0\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "fc layer 1 self.abs_max_out: 284.0\n",
      "lif layer 2 self.abs_max_v: 329.5\n",
      "fc layer 2 self.abs_max_out: 288.0\n",
      "fc layer 1 self.abs_max_out: 305.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 2 self.abs_max_out: 386.0\n",
      "lif layer 2 self.abs_max_v: 437.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "lif layer 1 self.abs_max_v: 393.0\n",
      "fc layer 1 self.abs_max_out: 400.0\n",
      "lif layer 1 self.abs_max_v: 400.0\n",
      "lif layer 1 self.abs_max_v: 455.0\n",
      "fc layer 2 self.abs_max_out: 423.0\n",
      "lif layer 2 self.abs_max_v: 585.0\n",
      "lif layer 1 self.abs_max_v: 515.5\n",
      "lif layer 2 self.abs_max_v: 628.5\n",
      "fc layer 3 self.abs_max_out: 182.0\n",
      "fc layer 1 self.abs_max_out: 483.0\n",
      "fc layer 1 self.abs_max_out: 525.0\n",
      "lif layer 1 self.abs_max_v: 525.0\n",
      "fc layer 1 self.abs_max_out: 548.0\n",
      "lif layer 1 self.abs_max_v: 548.0\n",
      "fc layer 1 self.abs_max_out: 553.0\n",
      "lif layer 1 self.abs_max_v: 564.5\n",
      "fc layer 2 self.abs_max_out: 475.0\n",
      "lif layer 2 self.abs_max_v: 660.5\n",
      "lif layer 2 self.abs_max_v: 682.0\n",
      "lif layer 1 self.abs_max_v: 568.0\n",
      "lif layer 2 self.abs_max_v: 748.0\n",
      "lif layer 1 self.abs_max_v: 686.5\n",
      "fc layer 1 self.abs_max_out: 610.0\n",
      "lif layer 1 self.abs_max_v: 691.5\n",
      "fc layer 2 self.abs_max_out: 544.0\n",
      "lif layer 1 self.abs_max_v: 706.5\n",
      "lif layer 1 self.abs_max_v: 734.0\n",
      "fc layer 1 self.abs_max_out: 621.0\n",
      "fc layer 1 self.abs_max_out: 640.0\n",
      "fc layer 1 self.abs_max_out: 716.0\n",
      "fc layer 2 self.abs_max_out: 545.0\n",
      "lif layer 1 self.abs_max_v: 763.0\n",
      "fc layer 3 self.abs_max_out: 184.0\n",
      "fc layer 2 self.abs_max_out: 549.0\n",
      "fc layer 3 self.abs_max_out: 197.0\n",
      "fc layer 2 self.abs_max_out: 560.0\n",
      "fc layer 2 self.abs_max_out: 647.0\n",
      "fc layer 2 self.abs_max_out: 677.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "fc layer 1 self.abs_max_out: 786.0\n",
      "lif layer 1 self.abs_max_v: 786.0\n",
      "lif layer 2 self.abs_max_v: 748.5\n",
      "lif layer 2 self.abs_max_v: 802.5\n",
      "lif layer 2 self.abs_max_v: 865.5\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "fc layer 3 self.abs_max_out: 222.0\n",
      "fc layer 2 self.abs_max_out: 692.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 2 self.abs_max_out: 697.0\n",
      "lif layer 1 self.abs_max_v: 794.5\n",
      "lif layer 2 self.abs_max_v: 880.5\n",
      "lif layer 1 self.abs_max_v: 834.5\n",
      "fc layer 2 self.abs_max_out: 711.0\n",
      "lif layer 1 self.abs_max_v: 874.0\n",
      "lif layer 1 self.abs_max_v: 952.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 1 self.abs_max_out: 860.0\n",
      "fc layer 1 self.abs_max_out: 862.0\n",
      "fc layer 1 self.abs_max_out: 898.0\n",
      "lif layer 2 self.abs_max_v: 905.5\n",
      "lif layer 2 self.abs_max_v: 907.0\n",
      "lif layer 2 self.abs_max_v: 980.5\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "lif layer 1 self.abs_max_v: 1014.0\n",
      "fc layer 2 self.abs_max_out: 811.0\n",
      "lif layer 2 self.abs_max_v: 992.0\n",
      "lif layer 1 self.abs_max_v: 1137.0\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 1186.5\n",
      "lif layer 1 self.abs_max_v: 1202.5\n",
      "lif layer 1 self.abs_max_v: 1210.5\n",
      "lif layer 1 self.abs_max_v: 1218.5\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "fc layer 2 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 1289.5\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "lif layer 2 self.abs_max_v: 1002.5\n",
      "lif layer 1 self.abs_max_v: 1300.0\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "fc layer 1 self.abs_max_out: 907.0\n",
      "lif layer 2 self.abs_max_v: 1024.0\n",
      "fc layer 1 self.abs_max_out: 915.0\n",
      "lif layer 2 self.abs_max_v: 1032.0\n",
      "lif layer 2 self.abs_max_v: 1082.0\n",
      "lif layer 2 self.abs_max_v: 1185.0\n",
      "lif layer 1 self.abs_max_v: 1392.0\n",
      "fc layer 1 self.abs_max_out: 943.0\n",
      "fc layer 1 self.abs_max_out: 1010.0\n",
      "fc layer 1 self.abs_max_out: 1018.0\n",
      "lif layer 1 self.abs_max_v: 1420.0\n",
      "fc layer 1 self.abs_max_out: 1052.0\n",
      "fc layer 1 self.abs_max_out: 1140.0\n",
      "fc layer 1 self.abs_max_out: 1272.0\n",
      "lif layer 1 self.abs_max_v: 1472.5\n",
      "lif layer 1 self.abs_max_v: 1476.5\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "lif layer 1 self.abs_max_v: 1513.5\n",
      "fc layer 3 self.abs_max_out: 319.0\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "lif layer 1 self.abs_max_v: 1645.0\n",
      "lif layer 1 self.abs_max_v: 1729.5\n",
      "lif layer 1 self.abs_max_v: 1753.0\n",
      "lif layer 1 self.abs_max_v: 1772.5\n",
      "lif layer 2 self.abs_max_v: 1200.0\n",
      "lif layer 2 self.abs_max_v: 1271.0\n",
      "lif layer 1 self.abs_max_v: 1814.5\n",
      "lif layer 1 self.abs_max_v: 1931.5\n",
      "lif layer 2 self.abs_max_v: 1271.5\n",
      "fc layer 2 self.abs_max_out: 1038.0\n",
      "lif layer 2 self.abs_max_v: 1428.0\n",
      "lif layer 2 self.abs_max_v: 1617.0\n",
      "lif layer 2 self.abs_max_v: 1649.5\n",
      "lif layer 2 self.abs_max_v: 1655.0\n",
      "lif layer 1 self.abs_max_v: 1977.0\n",
      "fc layer 1 self.abs_max_out: 1281.0\n",
      "fc layer 1 self.abs_max_out: 1356.0\n",
      "lif layer 1 self.abs_max_v: 2184.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 2 self.abs_max_out: 1061.0\n",
      "fc layer 2 self.abs_max_out: 1065.0\n",
      "fc layer 2 self.abs_max_out: 1093.0\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "fc layer 2 self.abs_max_out: 1131.0\n",
      "fc layer 1 self.abs_max_out: 1397.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 1 self.abs_max_out: 1458.0\n",
      "lif layer 1 self.abs_max_v: 2296.5\n",
      "lif layer 1 self.abs_max_v: 2414.5\n",
      "lif layer 1 self.abs_max_v: 2421.5\n",
      "lif layer 2 self.abs_max_v: 1681.0\n",
      "lif layer 2 self.abs_max_v: 1702.5\n",
      "fc layer 1 self.abs_max_out: 1523.0\n",
      "fc layer 1 self.abs_max_out: 1541.0\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "fc layer 1 self.abs_max_out: 1578.0\n",
      "fc layer 2 self.abs_max_out: 1151.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "fc layer 1 self.abs_max_out: 1699.0\n",
      "lif layer 1 self.abs_max_v: 2422.0\n",
      "fc layer 3 self.abs_max_out: 373.0\n",
      "lif layer 2 self.abs_max_v: 1710.0\n",
      "fc layer 2 self.abs_max_out: 1182.0\n",
      "fc layer 2 self.abs_max_out: 1238.0\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "lif layer 2 self.abs_max_v: 1751.0\n",
      "lif layer 2 self.abs_max_v: 1765.0\n",
      "lif layer 1 self.abs_max_v: 2486.5\n",
      "lif layer 1 self.abs_max_v: 2517.5\n",
      "lif layer 1 self.abs_max_v: 2520.0\n",
      "lif layer 1 self.abs_max_v: 2583.5\n",
      "lif layer 2 self.abs_max_v: 1911.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 13.948576/ 83.087433, val:  38.75%, val_best:  38.75%, tr:  97.45%, tr_best:  97.45%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2170  22.165%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1254.0\n",
      "fc layer 1 self.abs_max_out: 1802.0\n",
      "fc layer 1 self.abs_max_out: 1837.0\n",
      "fc layer 2 self.abs_max_out: 1256.0\n",
      "fc layer 1 self.abs_max_out: 1867.0\n",
      "fc layer 1 self.abs_max_out: 1913.0\n",
      "fc layer 2 self.abs_max_out: 1261.0\n",
      "fc layer 3 self.abs_max_out: 404.0\n",
      "fc layer 3 self.abs_max_out: 427.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "lif layer 1 self.abs_max_v: 2896.5\n",
      "lif layer 1 self.abs_max_v: 2995.5\n",
      "fc layer 1 self.abs_max_out: 1949.0\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.629782/ 64.350075, val:  37.92%, val_best:  38.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3716  18.979%\n",
      "fc layer 1 self.abs_max_out: 2010.0\n",
      "lif layer 1 self.abs_max_v: 3071.5\n",
      "fc layer 2 self.abs_max_out: 1301.0\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "lif layer 1 self.abs_max_v: 3132.5\n",
      "lif layer 1 self.abs_max_v: 3311.5\n",
      "fc layer 1 self.abs_max_out: 2066.0\n",
      "fc layer 1 self.abs_max_out: 2193.0\n",
      "fc layer 1 self.abs_max_out: 2284.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.268719/ 47.126022, val:  45.00%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7631%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5178  17.630%\n",
      "fc layer 3 self.abs_max_out: 444.0\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "lif layer 1 self.abs_max_v: 3406.0\n",
      "lif layer 1 self.abs_max_v: 3536.0\n",
      "fc layer 1 self.abs_max_out: 2296.0\n",
      "fc layer 1 self.abs_max_out: 2396.0\n",
      "lif layer 1 self.abs_max_v: 3572.5\n",
      "fc layer 3 self.abs_max_out: 459.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.565159/ 50.909454, val:  49.17%, val_best:  49.17%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6533  16.683%\n",
      "fc layer 2 self.abs_max_out: 1444.0\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "lif layer 1 self.abs_max_v: 3634.5\n",
      "lif layer 1 self.abs_max_v: 3901.5\n",
      "fc layer 1 self.abs_max_out: 2420.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.552057/ 78.614792, val:  42.50%, val_best:  49.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0420%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7863  16.063%\n",
      "fc layer 3 self.abs_max_out: 496.0\n",
      "fc layer 2 self.abs_max_out: 1525.0\n",
      "fc layer 1 self.abs_max_out: 2464.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.777359/ 64.651825, val:  47.50%, val_best:  49.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0483%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9221%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9180  15.628%\n",
      "fc layer 1 self.abs_max_out: 2477.0\n",
      "lif layer 2 self.abs_max_v: 1915.0\n",
      "lif layer 2 self.abs_max_v: 1948.0\n",
      "lif layer 2 self.abs_max_v: 1999.0\n",
      "lif layer 2 self.abs_max_v: 2030.5\n",
      "lif layer 1 self.abs_max_v: 4152.0\n",
      "fc layer 1 self.abs_max_out: 2528.0\n",
      "lif layer 1 self.abs_max_v: 4218.5\n",
      "lif layer 2 self.abs_max_v: 2038.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.743197/ 55.793434, val:  49.58%, val_best:  49.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10420  15.205%\n",
      "fc layer 1 self.abs_max_out: 2626.0\n",
      "fc layer 1 self.abs_max_out: 2719.0\n",
      "lif layer 2 self.abs_max_v: 2057.5\n",
      "lif layer 1 self.abs_max_v: 4371.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.402646/ 78.118469, val:  41.25%, val_best:  49.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4163%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11624  14.842%\n",
      "lif layer 2 self.abs_max_v: 2081.0\n",
      "lif layer 2 self.abs_max_v: 2162.5\n",
      "lif layer 2 self.abs_max_v: 2178.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.904994/ 46.305882, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12865  14.601%\n",
      "lif layer 2 self.abs_max_v: 2211.0\n",
      "lif layer 2 self.abs_max_v: 2229.0\n",
      "lif layer 2 self.abs_max_v: 2238.0\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "lif layer 2 self.abs_max_v: 2257.5\n",
      "fc layer 2 self.abs_max_out: 1553.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.572154/ 50.466259, val:  52.08%, val_best:  57.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0338%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14055  14.356%\n",
      "fc layer 1 self.abs_max_out: 2760.0\n",
      "lif layer 2 self.abs_max_v: 2343.0\n",
      "lif layer 2 self.abs_max_v: 2465.0\n",
      "lif layer 2 self.abs_max_v: 2500.0\n",
      "lif layer 2 self.abs_max_v: 2678.5\n",
      "lif layer 1 self.abs_max_v: 4478.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.598845/ 75.973534, val:  45.83%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15277  14.186%\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "lif layer 1 self.abs_max_v: 4705.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.206125/ 49.968201, val:  51.25%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16452  14.004%\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "fc layer 1 self.abs_max_out: 2792.0\n",
      "fc layer 1 self.abs_max_out: 2793.0\n",
      "lif layer 1 self.abs_max_v: 5120.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.448545/ 43.327286, val:  64.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17634  13.856%\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "lif layer 2 self.abs_max_v: 2680.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.255094/ 45.115635, val:  59.17%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18793  13.712%\n",
      "lif layer 2 self.abs_max_v: 2742.5\n",
      "lif layer 2 self.abs_max_v: 2768.5\n",
      "fc layer 2 self.abs_max_out: 1592.0\n",
      "lif layer 2 self.abs_max_v: 2873.5\n",
      "lif layer 2 self.abs_max_v: 2917.5\n",
      "fc layer 2 self.abs_max_out: 1639.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.249116/ 64.718819, val:  48.75%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19973  13.601%\n",
      "fc layer 1 self.abs_max_out: 2930.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.799304/ 42.647030, val:  62.08%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21082  13.459%\n",
      "fc layer 1 self.abs_max_out: 3009.0\n",
      "fc layer 2 self.abs_max_out: 1648.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.433908/ 38.954327, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22161  13.316%\n",
      "fc layer 2 self.abs_max_out: 1650.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.136031/ 41.659889, val:  65.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23243  13.190%\n",
      "fc layer 1 self.abs_max_out: 3015.0\n",
      "lif layer 1 self.abs_max_v: 5198.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.340201/ 53.975945, val:  56.67%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24289  13.058%\n",
      "fc layer 1 self.abs_max_out: 3024.0\n",
      "fc layer 2 self.abs_max_out: 1666.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.058606/ 40.541527, val:  62.92%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25345  12.944%\n",
      "fc layer 1 self.abs_max_out: 3029.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.199578/ 40.410831, val:  65.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4270%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26377  12.830%\n",
      "fc layer 1 self.abs_max_out: 3196.0\n",
      "fc layer 2 self.abs_max_out: 1701.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.863130/ 58.258568, val:  55.00%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27383  12.714%\n",
      "fc layer 1 self.abs_max_out: 3275.0\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "fc layer 2 self.abs_max_out: 1702.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.233307/ 46.506248, val:  71.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1233%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9663%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28422  12.622%\n",
      "lif layer 1 self.abs_max_v: 5476.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.409050/ 40.479782, val:  67.50%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29354  12.493%\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.074965/ 36.094067, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8594%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30274  12.369%\n",
      "fc layer 3 self.abs_max_out: 622.0\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "lif layer 1 self.abs_max_v: 5710.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.599004/ 65.199387, val:  54.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5546%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31271  12.285%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.547706/ 37.422401, val:  71.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32230  12.193%\n",
      "fc layer 2 self.abs_max_out: 1714.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.475863/ 47.078377, val:  67.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33181  12.105%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.096395/ 46.158092, val:  61.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34067  11.999%\n",
      "fc layer 1 self.abs_max_out: 3324.0\n",
      "fc layer 2 self.abs_max_out: 1759.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.940993/ 48.254009, val:  61.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1196%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 34975  11.908%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.547965/ 40.871376, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0269%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 35823  11.804%\n",
      "fc layer 1 self.abs_max_out: 3350.0\n",
      "fc layer 3 self.abs_max_out: 633.0\n",
      "fc layer 2 self.abs_max_out: 1782.0\n",
      "lif layer 1 self.abs_max_v: 5791.5\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.533649/ 35.584290, val:  70.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 36749  11.730%\n",
      "fc layer 2 self.abs_max_out: 1793.0\n",
      "fc layer 1 self.abs_max_out: 3358.0\n",
      "lif layer 1 self.abs_max_v: 5909.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.932657/ 42.253948, val:  68.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37620  11.645%\n",
      "fc layer 1 self.abs_max_out: 3380.0\n",
      "lif layer 1 self.abs_max_v: 6023.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.665152/ 52.058186, val:  56.25%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38476  11.559%\n",
      "fc layer 1 self.abs_max_out: 3395.0\n",
      "fc layer 2 self.abs_max_out: 1881.0\n",
      "fc layer 1 self.abs_max_out: 3405.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.794091/ 41.932583, val:  72.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0959%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39332  11.479%\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.578305/ 39.377846, val:  71.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40131  11.387%\n",
      "fc layer 3 self.abs_max_out: 671.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.057448/ 43.162258, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4754%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 40887  11.288%\n",
      "fc layer 1 self.abs_max_out: 3454.0\n",
      "lif layer 1 self.abs_max_v: 6103.5\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.269554/ 37.647575, val:  76.25%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 41684  11.205%\n",
      "fc layer 1 self.abs_max_out: 3456.0\n",
      "fc layer 1 self.abs_max_out: 3541.0\n",
      "lif layer 1 self.abs_max_v: 6218.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.225228/ 31.703039, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42446  11.117%\n",
      "fc layer 2 self.abs_max_out: 1887.0\n",
      "fc layer 2 self.abs_max_out: 1926.0\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "lif layer 1 self.abs_max_v: 6449.5\n",
      "fc layer 1 self.abs_max_out: 3656.0\n",
      "fc layer 1 self.abs_max_out: 3670.0\n",
      "lif layer 1 self.abs_max_v: 6512.5\n",
      "lif layer 1 self.abs_max_v: 6559.5\n",
      "lif layer 1 self.abs_max_v: 6599.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.111797/ 30.810362, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0530%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43230  11.039%\n",
      "fc layer 1 self.abs_max_out: 3709.0\n",
      "lif layer 1 self.abs_max_v: 6716.0\n",
      "lif layer 1 self.abs_max_v: 6763.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.172827/ 39.253620, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1860%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44002  10.962%\n",
      "fc layer 2 self.abs_max_out: 1929.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.388029/ 46.609798, val:  69.17%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 44783  10.891%\n",
      "fc layer 2 self.abs_max_out: 1962.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.373846/ 31.798515, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7646%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 45561  10.823%\n",
      "fc layer 3 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 3017.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.209671/ 56.526287, val:  62.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46338  10.757%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.170876/ 49.169003, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47090  10.689%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.152761/ 44.121216, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 47838  10.623%\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.654290/ 35.716553, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6077%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 48571  10.556%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.854393/ 44.888386, val:  76.25%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0688%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0562%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49296  10.490%\n",
      "fc layer 2 self.abs_max_out: 1998.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.765680/ 39.973850, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 49988  10.420%\n",
      "fc layer 3 self.abs_max_out: 692.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.807390/ 36.920448, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1345%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 50698  10.357%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.890708/ 38.235779, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51399  10.294%\n",
      "fc layer 1 self.abs_max_out: 3753.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.122583/ 32.045738, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52136  10.241%\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "fc layer 2 self.abs_max_out: 2003.0\n",
      "lif layer 2 self.abs_max_v: 3075.5\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.283711/ 41.541241, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 52809  10.178%\n",
      "fc layer 2 self.abs_max_out: 2007.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.737764/ 36.471336, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0386%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53482  10.117%\n",
      "fc layer 1 self.abs_max_out: 3798.0\n",
      "fc layer 3 self.abs_max_out: 735.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.730730/ 34.237022, val:  81.25%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54154  10.057%\n",
      "fc layer 3 self.abs_max_out: 766.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.990490/ 36.269714, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 54757   9.988%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.147545/ 51.174950, val:  72.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55364   9.921%\n",
      "fc layer 3 self.abs_max_out: 803.0\n",
      "fc layer 1 self.abs_max_out: 3885.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.183804/ 32.011932, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 55973   9.858%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.821938/ 50.186836, val:  72.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 56622   9.803%\n",
      "lif layer 2 self.abs_max_v: 3121.5\n",
      "lif layer 2 self.abs_max_v: 3144.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.158475/ 58.702202, val:  62.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57208   9.739%\n",
      "fc layer 1 self.abs_max_out: 3908.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.565491/ 46.336037, val:  77.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 57863   9.689%\n",
      "fc layer 1 self.abs_max_out: 3982.0\n",
      "fc layer 3 self.abs_max_out: 822.0\n",
      "fc layer 3 self.abs_max_out: 847.0\n",
      "lif layer 2 self.abs_max_v: 3210.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.057617/ 47.714298, val:  75.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1205%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58482   9.635%\n",
      "lif layer 2 self.abs_max_v: 3274.5\n",
      "fc layer 2 self.abs_max_out: 2017.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.062243/ 40.089123, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59085   9.580%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.309681/ 37.268276, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7711%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 59692   9.527%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.250147/ 33.231697, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60292   9.475%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.188824/ 40.494408, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 60908   9.426%\n",
      "lif layer 2 self.abs_max_v: 3289.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.977509/ 37.042988, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61473   9.372%\n",
      "lif layer 2 self.abs_max_v: 3298.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.614047/ 47.373615, val:  75.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 61989   9.312%\n",
      "lif layer 2 self.abs_max_v: 3311.5\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.790507/ 39.551407, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 62524   9.256%\n",
      "lif layer 2 self.abs_max_v: 3385.5\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.072444/ 52.052193, val:  77.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63055   9.201%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.591854/ 41.720905, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 63609   9.151%\n",
      "lif layer 1 self.abs_max_v: 6829.5\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.073977/ 51.096127, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0207%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64177   9.105%\n",
      "fc layer 1 self.abs_max_out: 4017.0\n",
      "lif layer 2 self.abs_max_v: 3418.5\n",
      "lif layer 1 self.abs_max_v: 6855.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.780265/ 55.504719, val:  73.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 64734   9.058%\n",
      "lif layer 1 self.abs_max_v: 6864.5\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.488558/ 48.183231, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1736%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65243   9.006%\n",
      "fc layer 1 self.abs_max_out: 4047.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.587787/ 38.270271, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9415%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 65755   8.955%\n",
      "fc layer 1 self.abs_max_out: 4139.0\n",
      "lif layer 1 self.abs_max_v: 6912.5\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.733255/ 59.764835, val:  70.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66279   8.908%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.963307/ 36.291939, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5503%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 66822   8.864%\n",
      "lif layer 1 self.abs_max_v: 7034.5\n",
      "lif layer 1 self.abs_max_v: 7232.0\n",
      "lif layer 1 self.abs_max_v: 7391.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.853907/ 49.355404, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 67332   8.817%\n",
      "fc layer 2 self.abs_max_out: 2048.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.761760/ 50.777477, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 67847   8.772%\n",
      "fc layer 3 self.abs_max_out: 855.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.768562/ 36.691154, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 68352   8.727%\n",
      "fc layer 2 self.abs_max_out: 2076.0\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.236998/ 34.102615, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 68837   8.681%\n",
      "fc layer 2 self.abs_max_out: 2110.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.562522/ 48.514271, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0368%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 69329   8.636%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.317586/ 34.600838, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 69809   8.591%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.408975/ 42.953754, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 70284   8.547%\n",
      "fc layer 1 self.abs_max_out: 4235.0\n",
      "fc layer 3 self.abs_max_out: 856.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.175167/ 39.023296, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0173%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7024%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 70761   8.503%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.151989/ 44.076599, val:  75.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 71232   8.460%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.118231/ 41.533180, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 71696   8.418%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.452033/ 34.083683, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72174   8.378%\n",
      "fc layer 1 self.abs_max_out: 4265.0\n",
      "fc layer 3 self.abs_max_out: 871.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.122725/ 37.740463, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 72620   8.335%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.077755/ 40.965981, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0978%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73064   8.292%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.010174/ 50.346275, val:  80.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0790%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1298%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 73516   8.252%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.329826/ 40.894989, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 73971   8.213%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.138846/ 40.800125, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 74410   8.173%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.967356/ 40.384808, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 74863   8.135%\n",
      "fc layer 2 self.abs_max_out: 2146.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.031251/ 38.891315, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 75324   8.099%\n",
      "fc layer 2 self.abs_max_out: 2209.0\n",
      "fc layer 1 self.abs_max_out: 4298.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.868423/ 35.913517, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 75727   8.057%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.585808/ 62.340717, val:  76.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 76110   8.015%\n",
      "fc layer 1 self.abs_max_out: 4317.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.709276/ 41.679428, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 76519   7.976%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.735589/ 46.682129, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0975%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 76915   7.936%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.902956/ 51.442684, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 77324   7.898%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.225612/ 42.048859, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 77753   7.863%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.700792/ 45.274979, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0747%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4467%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 78167   7.828%\n",
      "lif layer 2 self.abs_max_v: 3491.5\n",
      "lif layer 2 self.abs_max_v: 3593.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.710979/ 37.166195, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 78576   7.792%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.565962/ 42.515530, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0404%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 78959   7.755%\n",
      "fc layer 1 self.abs_max_out: 4351.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.718363/ 41.822845, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 79350   7.719%\n",
      "fc layer 1 self.abs_max_out: 4362.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.966858/ 44.838108, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 79771   7.687%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.817971/ 40.299454, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 80187   7.655%\n",
      "fc layer 3 self.abs_max_out: 886.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.680811/ 40.655254, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0241%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9474%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 80589   7.622%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.560651/ 46.365829, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0380%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 80962   7.587%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.604506/ 51.564274, val:  78.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 81341   7.553%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.769160/ 42.621189, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 81737   7.522%\n",
      "fc layer 1 self.abs_max_out: 4363.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.879342/ 56.103649, val:  75.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 82162   7.493%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.453722/ 36.941647, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 82506   7.458%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.096320/ 40.353439, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 82846   7.423%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.360686/ 49.831841, val:  81.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0830%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 83206   7.391%\n",
      "fc layer 2 self.abs_max_out: 2253.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.314515/ 46.622322, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 83554   7.357%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.231270/ 51.174171, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 83905   7.325%\n",
      "fc layer 1 self.abs_max_out: 4382.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.512682/ 46.073200, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0281%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 84258   7.294%\n",
      "fc layer 2 self.abs_max_out: 2257.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.373404/ 43.707020, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 84609   7.263%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.316682/ 44.542629, val:  84.17%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 84921   7.229%\n",
      "fc layer 2 self.abs_max_out: 2321.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.288748/ 48.270008, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6835%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 85269   7.198%\n",
      "fc layer 3 self.abs_max_out: 892.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.368064/ 40.829964, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 85611   7.168%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.365954/ 38.979057, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0371%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 85958   7.138%\n",
      "fc layer 1 self.abs_max_out: 4435.0\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.180656/ 52.743542, val:  80.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1196%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 86264   7.106%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.343700/ 38.104279, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 86614   7.078%\n",
      "fc layer 3 self.abs_max_out: 908.0\n",
      "fc layer 1 self.abs_max_out: 4488.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.295791/ 42.503120, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4888%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 86944   7.048%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.418979/ 42.939754, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 87275   7.019%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.374253/ 38.353714, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3807%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 87619   6.992%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.149066/ 44.016148, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 87952   6.964%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.086329/ 38.647461, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 88273   6.936%\n",
      "fc layer 1 self.abs_max_out: 4496.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.180138/ 43.285194, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4182%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 88591   6.908%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.031091/ 40.804840, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 88909   6.880%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.355133/ 36.432156, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 89244   6.854%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.428407/ 36.086369, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 89570   6.828%\n",
      "fc layer 1 self.abs_max_out: 4515.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.096412/ 36.955658, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 89884   6.801%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.001885/ 40.223774, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 90180   6.773%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.880328/ 49.881966, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 90475   6.746%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.881639/ 45.459496, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 90756   6.718%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.884682/ 47.645008, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0538%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 91063   6.692%\n",
      "fc layer 1 self.abs_max_out: 4640.0\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.000333/ 44.104103, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5754%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 91364   6.666%\n",
      "fc layer 3 self.abs_max_out: 912.0\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.179265/ 38.571735, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 91672   6.641%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.027934/ 48.184990, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 91979   6.616%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.006154/ 43.299133, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4942%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 92246   6.589%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.005662/ 40.726086, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1295%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 92524   6.563%\n",
      "fc layer 3 self.abs_max_out: 920.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.189021/ 46.215637, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 92820   6.539%\n",
      "fc layer 3 self.abs_max_out: 967.0\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.710609/ 50.866524, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 93093   6.513%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.038661/ 48.317471, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0521%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 93392   6.489%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.168461/ 48.805061, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 93708   6.467%\n",
      "fc layer 2 self.abs_max_out: 2336.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.718711/ 44.531902, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1412%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 93971   6.442%\n",
      "fc layer 1 self.abs_max_out: 4734.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.803298/ 41.890507, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 94222   6.416%\n",
      "fc layer 2 self.abs_max_out: 2346.0\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.022420/ 41.360641, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 94512   6.393%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.686482/ 40.323250, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 94756   6.368%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.963320/ 45.027760, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 95060   6.346%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.599544/ 40.958294, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1446%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4890%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 95301   6.321%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.530778/ 42.325752, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0975%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 95533   6.296%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.745265/ 49.142765, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0483%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6830%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 95786   6.272%\n",
      "fc layer 2 self.abs_max_out: 2349.0\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.779175/ 41.414833, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 96067   6.250%\n",
      "fc layer 2 self.abs_max_out: 2354.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.646391/ 41.717484, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 96301   6.226%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.757046/ 46.752899, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 96583   6.205%\n",
      "fc layer 2 self.abs_max_out: 2391.0\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.838819/ 52.671520, val:  83.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2787%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 96867   6.184%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.792544/ 46.721088, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1226%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 97111   6.161%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.690126/ 47.368774, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 97379   6.140%\n",
      "fc layer 2 self.abs_max_out: 2392.0\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.717804/ 41.361507, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 97611   6.117%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.561197/ 42.216759, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 97842   6.094%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.592734/ 51.680115, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 98092   6.072%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  2.053405/ 43.965908, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 98366   6.053%\n",
      "fc layer 2 self.abs_max_out: 2397.0\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.572702/ 48.440990, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1230%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 98605   6.031%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.493228/ 56.066616, val:  81.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 98826   6.009%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.604425/ 48.521538, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 99060   5.987%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.473626/ 42.337414, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0618%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6024%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 99302   5.967%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.697614/ 39.120136, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0315%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 99563   5.947%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.319758/ 42.749908, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 99785   5.926%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.328783/ 41.780918, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3126%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9713%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 99979   5.903%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.409070/ 52.479336, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4081%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 100200   5.882%\n",
      "fc layer 2 self.abs_max_out: 2407.0\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.444037/ 47.610466, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 100422   5.861%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.530221/ 51.652405, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3785%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 100660   5.842%\n",
      "fc layer 3 self.abs_max_out: 971.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.442984/ 49.847153, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 100881   5.822%\n",
      "lif layer 1 self.abs_max_v: 7512.5\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.406781/ 51.226387, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 101094   5.801%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.398290/ 47.367035, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 101327   5.782%\n",
      "lif layer 2 self.abs_max_v: 3632.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.565261/ 49.791931, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 101550   5.763%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.533450/ 41.946743, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 101785   5.744%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.554095/ 47.925770, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0331%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 102022   5.726%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.535534/ 48.404354, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 102249   5.707%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.502229/ 42.739105, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0487%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 102466   5.688%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.328407/ 45.417786, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1334%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 102664   5.668%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.133063/ 47.129585, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 102865   5.649%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.315827/ 46.532478, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0237%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 103056   5.629%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.293618/ 53.464741, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 103255   5.610%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.528014/ 42.687874, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 103476   5.592%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.395198/ 47.609818, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 103692   5.575%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.377776/ 48.728794, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 103895   5.556%\n",
      "lif layer 2 self.abs_max_v: 3653.0\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.325102/ 41.304729, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0399%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 104094   5.538%\n",
      "fc layer 3 self.abs_max_out: 997.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.329647/ 45.684635, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 104303   5.520%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.311259/ 44.898430, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 104492   5.502%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.244299/ 51.483952, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.44 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 104691   5.484%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.077796/ 43.525040, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0986%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 104868   5.465%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.248017/ 47.382004, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 105064   5.448%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.464131/ 46.811607, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.30 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 105267   5.431%\n",
      "fc layer 3 self.abs_max_out: 1005.0\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.563449/ 44.345779, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0975%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 105486   5.415%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.330432/ 43.273777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0248%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74991fd11aa4d4cafb1720af266c282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÉ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.33043</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>43.27378</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-sweep-43</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dk80dt78' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dk80dt78</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_201130-dk80dt78/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pt3szsvb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 40878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_002747-pt3szsvb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pt3szsvb' target=\"_blank\">lively-sweep-50</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pt3szsvb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pt3szsvb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_002757_720', 'my_seed': 40878, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 135.0\n",
      "lif layer 1 self.abs_max_v: 135.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 65.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 155.0\n",
      "lif layer 1 self.abs_max_v: 192.5\n",
      "fc layer 2 self.abs_max_out: 221.0\n",
      "lif layer 2 self.abs_max_v: 221.0\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "lif layer 1 self.abs_max_v: 211.5\n",
      "fc layer 2 self.abs_max_out: 231.0\n",
      "lif layer 2 self.abs_max_v: 239.5\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "lif layer 1 self.abs_max_v: 215.0\n",
      "lif layer 1 self.abs_max_v: 222.0\n",
      "lif layer 2 self.abs_max_v: 249.5\n",
      "fc layer 1 self.abs_max_out: 213.0\n",
      "fc layer 1 self.abs_max_out: 293.0\n",
      "lif layer 1 self.abs_max_v: 326.5\n",
      "fc layer 2 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 314.5\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "lif layer 2 self.abs_max_v: 389.0\n",
      "lif layer 1 self.abs_max_v: 362.5\n",
      "fc layer 2 self.abs_max_out: 327.0\n",
      "fc layer 1 self.abs_max_out: 351.0\n",
      "lif layer 2 self.abs_max_v: 476.5\n",
      "fc layer 1 self.abs_max_out: 424.0\n",
      "lif layer 1 self.abs_max_v: 424.0\n",
      "lif layer 2 self.abs_max_v: 493.0\n",
      "fc layer 1 self.abs_max_out: 440.0\n",
      "lif layer 1 self.abs_max_v: 440.0\n",
      "lif layer 2 self.abs_max_v: 529.5\n",
      "fc layer 2 self.abs_max_out: 357.0\n",
      "lif layer 1 self.abs_max_v: 479.5\n",
      "fc layer 2 self.abs_max_out: 406.0\n",
      "lif layer 1 self.abs_max_v: 501.0\n",
      "fc layer 2 self.abs_max_out: 425.0\n",
      "lif layer 1 self.abs_max_v: 592.5\n",
      "lif layer 2 self.abs_max_v: 538.0\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "lif layer 1 self.abs_max_v: 637.0\n",
      "lif layer 2 self.abs_max_v: 588.0\n",
      "fc layer 1 self.abs_max_out: 454.0\n",
      "fc layer 2 self.abs_max_out: 544.0\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 1 self.abs_max_out: 457.0\n",
      "fc layer 1 self.abs_max_out: 488.0\n",
      "fc layer 1 self.abs_max_out: 559.0\n",
      "fc layer 1 self.abs_max_out: 578.0\n",
      "lif layer 2 self.abs_max_v: 606.0\n",
      "lif layer 1 self.abs_max_v: 684.5\n",
      "lif layer 2 self.abs_max_v: 644.5\n",
      "fc layer 1 self.abs_max_out: 593.0\n",
      "lif layer 1 self.abs_max_v: 690.5\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "fc layer 2 self.abs_max_out: 572.0\n",
      "fc layer 1 self.abs_max_out: 637.0\n",
      "lif layer 1 self.abs_max_v: 753.0\n",
      "lif layer 2 self.abs_max_v: 664.0\n",
      "fc layer 1 self.abs_max_out: 793.0\n",
      "lif layer 1 self.abs_max_v: 793.0\n",
      "fc layer 1 self.abs_max_out: 807.0\n",
      "lif layer 1 self.abs_max_v: 807.0\n",
      "lif layer 2 self.abs_max_v: 686.0\n",
      "lif layer 2 self.abs_max_v: 704.0\n",
      "lif layer 2 self.abs_max_v: 770.0\n",
      "lif layer 2 self.abs_max_v: 815.0\n",
      "lif layer 2 self.abs_max_v: 843.5\n",
      "lif layer 2 self.abs_max_v: 971.0\n",
      "fc layer 2 self.abs_max_out: 701.0\n",
      "fc layer 1 self.abs_max_out: 824.0\n",
      "lif layer 1 self.abs_max_v: 824.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "lif layer 1 self.abs_max_v: 853.5\n",
      "lif layer 1 self.abs_max_v: 901.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 1 self.abs_max_out: 931.0\n",
      "lif layer 1 self.abs_max_v: 931.0\n",
      "lif layer 1 self.abs_max_v: 1025.5\n",
      "fc layer 1 self.abs_max_out: 948.0\n",
      "lif layer 1 self.abs_max_v: 1056.0\n",
      "fc layer 1 self.abs_max_out: 1153.0\n",
      "lif layer 1 self.abs_max_v: 1153.0\n",
      "fc layer 2 self.abs_max_out: 753.0\n",
      "fc layer 2 self.abs_max_out: 774.0\n",
      "lif layer 2 self.abs_max_v: 987.5\n",
      "lif layer 2 self.abs_max_v: 1002.0\n",
      "lif layer 2 self.abs_max_v: 1003.0\n",
      "lif layer 2 self.abs_max_v: 1105.5\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "lif layer 2 self.abs_max_v: 1136.0\n",
      "lif layer 2 self.abs_max_v: 1180.0\n",
      "fc layer 3 self.abs_max_out: 293.0\n",
      "fc layer 2 self.abs_max_out: 852.0\n",
      "fc layer 2 self.abs_max_out: 971.0\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "fc layer 3 self.abs_max_out: 363.0\n",
      "lif layer 1 self.abs_max_v: 1195.5\n",
      "lif layer 1 self.abs_max_v: 1321.5\n",
      "lif layer 1 self.abs_max_v: 1374.5\n",
      "lif layer 1 self.abs_max_v: 1575.5\n",
      "lif layer 2 self.abs_max_v: 1229.5\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "fc layer 1 self.abs_max_out: 1160.0\n",
      "lif layer 2 self.abs_max_v: 1255.0\n",
      "lif layer 2 self.abs_max_v: 1257.5\n",
      "lif layer 2 self.abs_max_v: 1279.0\n",
      "lif layer 2 self.abs_max_v: 1408.5\n",
      "lif layer 2 self.abs_max_v: 1421.5\n",
      "lif layer 2 self.abs_max_v: 1435.0\n",
      "lif layer 1 self.abs_max_v: 1636.0\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "fc layer 1 self.abs_max_out: 1171.0\n",
      "fc layer 1 self.abs_max_out: 1254.0\n",
      "fc layer 1 self.abs_max_out: 1255.0\n",
      "lif layer 1 self.abs_max_v: 1880.0\n",
      "lif layer 1 self.abs_max_v: 1893.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "fc layer 2 self.abs_max_out: 1070.0\n",
      "fc layer 2 self.abs_max_out: 1114.0\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "fc layer 2 self.abs_max_out: 1163.0\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "lif layer 1 self.abs_max_v: 1978.0\n",
      "fc layer 1 self.abs_max_out: 1284.0\n",
      "lif layer 1 self.abs_max_v: 2036.0\n",
      "lif layer 1 self.abs_max_v: 2112.0\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "fc layer 3 self.abs_max_out: 385.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 1 self.abs_max_out: 1285.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 1 self.abs_max_out: 1286.0\n",
      "fc layer 1 self.abs_max_out: 1291.0\n",
      "fc layer 1 self.abs_max_out: 1497.0\n",
      "lif layer 1 self.abs_max_v: 2199.5\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "fc layer 3 self.abs_max_out: 459.0\n",
      "lif layer 1 self.abs_max_v: 2220.5\n",
      "lif layer 1 self.abs_max_v: 2247.0\n",
      "lif layer 2 self.abs_max_v: 1473.5\n",
      "fc layer 1 self.abs_max_out: 1515.0\n",
      "fc layer 1 self.abs_max_out: 1557.0\n",
      "lif layer 1 self.abs_max_v: 2546.0\n",
      "lif layer 1 self.abs_max_v: 2751.0\n",
      "lif layer 2 self.abs_max_v: 1521.5\n",
      "fc layer 1 self.abs_max_out: 1607.0\n",
      "fc layer 1 self.abs_max_out: 1679.0\n",
      "fc layer 1 self.abs_max_out: 1745.0\n",
      "lif layer 1 self.abs_max_v: 2862.0\n",
      "fc layer 1 self.abs_max_out: 1765.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.766675/ 91.291939, val:  37.92%, val_best:  37.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1120%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2110  21.553%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "lif layer 2 self.abs_max_v: 1522.5\n",
      "lif layer 2 self.abs_max_v: 1573.5\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 1580.0\n",
      "lif layer 2 self.abs_max_v: 1613.0\n",
      "fc layer 2 self.abs_max_out: 1315.0\n",
      "fc layer 1 self.abs_max_out: 1791.0\n",
      "lif layer 1 self.abs_max_v: 2913.5\n",
      "lif layer 1 self.abs_max_v: 2988.0\n",
      "fc layer 1 self.abs_max_out: 1833.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.990665/ 72.884529, val:  42.08%, val_best:  42.08%, tr:  98.98%, tr_best:  98.98%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3711  18.953%\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "lif layer 2 self.abs_max_v: 1616.0\n",
      "fc layer 3 self.abs_max_out: 537.0\n",
      "lif layer 2 self.abs_max_v: 1626.5\n",
      "lif layer 2 self.abs_max_v: 1690.5\n",
      "lif layer 2 self.abs_max_v: 1699.5\n",
      "lif layer 2 self.abs_max_v: 1725.0\n",
      "lif layer 2 self.abs_max_v: 1733.0\n",
      "lif layer 2 self.abs_max_v: 1783.5\n",
      "fc layer 1 self.abs_max_out: 1921.0\n",
      "fc layer 1 self.abs_max_out: 2189.0\n",
      "lif layer 1 self.abs_max_v: 3508.0\n",
      "lif layer 1 self.abs_max_v: 3612.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.553601/ 77.376610, val:  34.58%, val_best:  42.08%, tr:  99.18%, tr_best:  99.18%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5157  17.559%\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "fc layer 2 self.abs_max_out: 1324.0\n",
      "fc layer 2 self.abs_max_out: 1330.0\n",
      "fc layer 1 self.abs_max_out: 2190.0\n",
      "lif layer 1 self.abs_max_v: 3659.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.053969/ 99.831146, val:  31.25%, val_best:  42.08%, tr:  99.18%, tr_best:  99.18%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6548  16.721%\n",
      "lif layer 2 self.abs_max_v: 1922.5\n",
      "lif layer 1 self.abs_max_v: 3805.5\n",
      "fc layer 2 self.abs_max_out: 1359.0\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 1 self.abs_max_out: 2221.0\n",
      "fc layer 1 self.abs_max_out: 2532.0\n",
      "lif layer 1 self.abs_max_v: 4033.0\n",
      "lif layer 1 self.abs_max_v: 4091.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.595886/ 55.275288, val:  58.75%, val_best:  58.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0528%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7869  16.076%\n",
      "lif layer 2 self.abs_max_v: 1947.5\n",
      "fc layer 2 self.abs_max_out: 1427.0\n",
      "fc layer 2 self.abs_max_out: 1453.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.254453/ 34.440556, val:  60.00%, val_best:  60.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9194  15.652%\n",
      "fc layer 2 self.abs_max_out: 1500.0\n",
      "lif layer 2 self.abs_max_v: 2006.5\n",
      "lif layer 2 self.abs_max_v: 2012.5\n",
      "lif layer 2 self.abs_max_v: 2076.5\n",
      "lif layer 2 self.abs_max_v: 2140.5\n",
      "fc layer 1 self.abs_max_out: 2588.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.763543/ 73.548737, val:  45.83%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10466  15.272%\n",
      "fc layer 2 self.abs_max_out: 1573.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "lif layer 2 self.abs_max_v: 2196.5\n",
      "fc layer 1 self.abs_max_out: 2724.0\n",
      "lif layer 1 self.abs_max_v: 4419.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.523442/ 62.591267, val:  46.25%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11725  14.971%\n",
      "fc layer 1 self.abs_max_out: 2753.0\n",
      "lif layer 1 self.abs_max_v: 4477.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.259853/ 57.511887, val:  49.58%, val_best:  60.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12948  14.695%\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.161057/ 69.315086, val:  47.08%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14179  14.483%\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "fc layer 2 self.abs_max_out: 1604.0\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "lif layer 2 self.abs_max_v: 2226.0\n",
      "fc layer 2 self.abs_max_out: 1626.0\n",
      "fc layer 1 self.abs_max_out: 2820.0\n",
      "lif layer 1 self.abs_max_v: 4607.0\n",
      "fc layer 2 self.abs_max_out: 1647.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.488906/ 58.321655, val:  56.25%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6609%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15397  14.298%\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "fc layer 1 self.abs_max_out: 2965.0\n",
      "lif layer 1 self.abs_max_v: 4864.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.906329/ 56.361458, val:  51.67%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16585  14.117%\n",
      "fc layer 3 self.abs_max_out: 607.0\n",
      "lif layer 2 self.abs_max_v: 2266.5\n",
      "fc layer 1 self.abs_max_out: 3099.0\n",
      "lif layer 1 self.abs_max_v: 5069.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.197464/ 35.283043, val:  56.25%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1016%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17782  13.972%\n",
      "lif layer 2 self.abs_max_v: 2393.0\n",
      "fc layer 2 self.abs_max_out: 1672.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.193580/ 43.053600, val:  57.92%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18869  13.767%\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.733296/ 74.113968, val:  38.33%, val_best:  60.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20011  13.627%\n",
      "fc layer 2 self.abs_max_out: 1733.0\n",
      "fc layer 2 self.abs_max_out: 1797.0\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.202248/ 35.511169, val:  63.33%, val_best:  63.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21102  13.472%\n",
      "fc layer 1 self.abs_max_out: 3173.0\n",
      "lif layer 1 self.abs_max_v: 5244.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.383856/ 61.241940, val:  49.58%, val_best:  63.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22219  13.350%\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "fc layer 1 self.abs_max_out: 3195.0\n",
      "lif layer 1 self.abs_max_v: 5249.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.648012/ 40.116226, val:  67.08%, val_best:  67.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23338  13.244%\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "lif layer 2 self.abs_max_v: 2475.0\n",
      "lif layer 2 self.abs_max_v: 2555.0\n",
      "lif layer 2 self.abs_max_v: 2601.5\n",
      "lif layer 2 self.abs_max_v: 2610.0\n",
      "fc layer 1 self.abs_max_out: 3219.0\n",
      "lif layer 1 self.abs_max_v: 5321.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.901533/ 55.828987, val:  50.83%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0631%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24424  13.130%\n",
      "lif layer 2 self.abs_max_v: 2796.0\n",
      "lif layer 2 self.abs_max_v: 2823.0\n",
      "lif layer 2 self.abs_max_v: 2866.5\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 1 self.abs_max_out: 3286.0\n",
      "lif layer 1 self.abs_max_v: 5403.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.090334/ 56.177807, val:  51.67%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0526%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25474  13.010%\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.492860/ 66.514046, val:  45.83%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26524  12.901%\n",
      "lif layer 2 self.abs_max_v: 2900.0\n",
      "lif layer 2 self.abs_max_v: 2948.0\n",
      "lif layer 2 self.abs_max_v: 2963.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.379688/ 54.293293, val:  59.58%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1259%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27571  12.801%\n",
      "lif layer 2 self.abs_max_v: 2967.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.057129/ 53.052494, val:  60.00%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28593  12.698%\n",
      "lif layer 2 self.abs_max_v: 3008.5\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "lif layer 2 self.abs_max_v: 3019.5\n",
      "fc layer 1 self.abs_max_out: 3335.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.075200/ 36.415462, val:  68.33%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29549  12.576%\n",
      "lif layer 2 self.abs_max_v: 3027.0\n",
      "lif layer 2 self.abs_max_v: 3063.5\n",
      "lif layer 2 self.abs_max_v: 3129.5\n",
      "fc layer 1 self.abs_max_out: 3406.0\n",
      "lif layer 1 self.abs_max_v: 5512.0\n",
      "lif layer 1 self.abs_max_v: 5517.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.496564/ 34.392384, val:  69.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0503%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30511  12.466%\n",
      "fc layer 3 self.abs_max_out: 691.0\n",
      "fc layer 1 self.abs_max_out: 3528.0\n",
      "lif layer 1 self.abs_max_v: 5741.0\n",
      "lif layer 1 self.abs_max_v: 5751.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.696607/ 57.039673, val:  61.67%, val_best:  69.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31501  12.376%\n",
      "fc layer 2 self.abs_max_out: 1823.0\n",
      "lif layer 2 self.abs_max_v: 3170.5\n",
      "fc layer 1 self.abs_max_out: 3645.0\n",
      "lif layer 1 self.abs_max_v: 5960.5\n",
      "lif layer 1 self.abs_max_v: 5975.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.988536/ 49.388798, val:  66.67%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32488  12.291%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  5.905398/ 50.990574, val:  69.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0211%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9316%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33436  12.198%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.160294/ 44.630951, val:  65.83%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34341  12.096%\n",
      "fc layer 2 self.abs_max_out: 1896.0\n",
      "fc layer 1 self.abs_max_out: 3664.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.520339/ 32.634033, val:  77.92%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35315  12.024%\n",
      "lif layer 2 self.abs_max_v: 3195.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.180174/ 32.632198, val:  77.50%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8371%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36231  11.938%\n",
      "lif layer 2 self.abs_max_v: 3249.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.614895/ 47.245319, val:  65.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37067  11.832%\n",
      "lif layer 2 self.abs_max_v: 3311.0\n",
      "fc layer 1 self.abs_max_out: 3809.0\n",
      "lif layer 1 self.abs_max_v: 6173.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.668755/ 35.175919, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37930  11.740%\n",
      "lif layer 2 self.abs_max_v: 3363.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.904261/ 42.034561, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2220%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38804  11.658%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.607951/ 38.830151, val:  82.50%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39640  11.569%\n",
      "fc layer 1 self.abs_max_out: 3889.0\n",
      "lif layer 1 self.abs_max_v: 6298.5\n",
      "lif layer 1 self.abs_max_v: 6334.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.750937/ 43.291859, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40491  11.489%\n",
      "lif layer 2 self.abs_max_v: 3373.5\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.957106/ 45.155582, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41330  11.410%\n",
      "lif layer 2 self.abs_max_v: 3388.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  6.070820/ 38.177547, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42174  11.336%\n",
      "fc layer 1 self.abs_max_out: 3915.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  6.103569/ 51.400021, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0423%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43062  11.278%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.392930/ 47.795792, val:  72.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1985%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43841  11.195%\n",
      "fc layer 2 self.abs_max_out: 1911.0\n",
      "lif layer 2 self.abs_max_v: 3437.5\n",
      "lif layer 2 self.abs_max_v: 3456.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.804580/ 42.099064, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44654  11.125%\n",
      "fc layer 2 self.abs_max_out: 1913.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.931950/ 37.185043, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45467  11.058%\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "fc layer 1 self.abs_max_out: 4039.0\n",
      "lif layer 1 self.abs_max_v: 6523.0\n",
      "lif layer 1 self.abs_max_v: 6652.5\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.692776/ 42.426815, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3245%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46290  10.996%\n",
      "lif layer 2 self.abs_max_v: 3474.0\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.357014/ 46.977386, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47060  10.925%\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.176354/ 45.275627, val:  71.25%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47819  10.854%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.038660/ 44.838585, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48582  10.788%\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "fc layer 2 self.abs_max_out: 1985.0\n",
      "fc layer 2 self.abs_max_out: 2031.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.105735/ 48.931259, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49303  10.715%\n",
      "fc layer 2 self.abs_max_out: 2047.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.705683/ 38.353962, val:  81.25%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 50016  10.644%\n",
      "fc layer 2 self.abs_max_out: 2051.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.391344/ 37.174736, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50679  10.565%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.995878/ 57.370979, val:  67.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51433  10.507%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.050362/ 47.626816, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52150  10.445%\n",
      "fc layer 2 self.abs_max_out: 2103.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.712203/ 42.002804, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52862  10.384%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.456800/ 41.699718, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7462%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3355%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53514  10.314%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.045444/ 36.373886, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54219  10.256%\n",
      "lif layer 1 self.abs_max_v: 6713.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.518216/ 37.017475, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9403%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54868  10.190%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.669202/ 37.503487, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0191%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55552  10.133%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.196251/ 46.585041, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5726%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56172  10.066%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.965468/ 51.659988, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56754   9.995%\n",
      "fc layer 3 self.abs_max_out: 753.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.305955/ 43.136200, val:  77.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7531%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57393   9.936%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.090477/ 38.345310, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2754%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58002   9.874%\n",
      "fc layer 1 self.abs_max_out: 4047.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.141082/ 36.935745, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58612   9.815%\n",
      "fc layer 1 self.abs_max_out: 4100.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.334446/ 51.418205, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59230   9.758%\n",
      "fc layer 3 self.abs_max_out: 784.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.246494/ 39.360916, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59869   9.707%\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.325217/ 38.870945, val:  84.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60508   9.657%\n",
      "fc layer 2 self.abs_max_out: 2112.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.206036/ 32.109180, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0441%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61118   9.604%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.854260/ 34.176876, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1196%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2365%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61696   9.548%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.539301/ 53.986748, val:  71.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62223   9.486%\n",
      "fc layer 2 self.abs_max_out: 2127.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.731253/ 40.336369, val:  80.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62775   9.430%\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.068496/ 35.506531, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7393%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63340   9.377%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.725079/ 39.584728, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63883   9.322%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.971548/ 42.285648, val:  81.25%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64461   9.274%\n",
      "fc layer 2 self.abs_max_out: 2144.0\n",
      "fc layer 1 self.abs_max_out: 4109.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.806949/ 42.403629, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1052%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65011   9.223%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.330355/ 48.457943, val:  80.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1106%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1844%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65516   9.167%\n",
      "fc layer 3 self.abs_max_out: 824.0\n",
      "fc layer 2 self.abs_max_out: 2164.0\n",
      "fc layer 2 self.abs_max_out: 2168.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.034594/ 48.843330, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1151%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66087   9.122%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.838593/ 38.641731, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66628   9.074%\n",
      "fc layer 3 self.abs_max_out: 830.0\n",
      "fc layer 2 self.abs_max_out: 2202.0\n",
      "fc layer 2 self.abs_max_out: 2209.0\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "fc layer 2 self.abs_max_out: 2227.0\n",
      "fc layer 2 self.abs_max_out: 2251.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.659600/ 46.927052, val:  81.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67147   9.025%\n",
      "fc layer 2 self.abs_max_out: 2338.0\n",
      "fc layer 2 self.abs_max_out: 2347.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  4.069293/ 38.816090, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1170%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67732   8.985%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.373269/ 41.984802, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68245   8.937%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.384573/ 49.046959, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68752   8.889%\n",
      "fc layer 1 self.abs_max_out: 4139.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.657856/ 58.593758, val:  68.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69287   8.847%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.676435/ 41.890739, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2126%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0800%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69791   8.801%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.558693/ 35.031986, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0405%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3762%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70295   8.756%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.151908/ 44.301559, val:  84.17%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70778   8.710%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.755013/ 41.896618, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0895%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71291   8.669%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.370308/ 40.588898, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71770   8.625%\n",
      "fc layer 1 self.abs_max_out: 4234.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.480624/ 40.152603, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72247   8.581%\n",
      "fc layer 1 self.abs_max_out: 4236.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.251268/ 59.249489, val:  79.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72719   8.538%\n",
      "fc layer 1 self.abs_max_out: 4255.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.455663/ 37.505108, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73187   8.495%\n",
      "fc layer 2 self.abs_max_out: 2354.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.351025/ 42.972839, val:  84.17%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5037%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73659   8.454%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.252300/ 36.446175, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5242%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74118   8.412%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.349926/ 43.071659, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4318%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74607   8.374%\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "fc layer 2 self.abs_max_out: 2372.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.137465/ 48.569489, val:  75.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75045   8.332%\n",
      "fc layer 2 self.abs_max_out: 2395.0\n",
      "fc layer 2 self.abs_max_out: 2497.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.239430/ 37.891312, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75499   8.292%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.520820/ 39.965382, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75973   8.256%\n",
      "fc layer 1 self.abs_max_out: 4290.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.097631/ 51.925552, val:  79.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1217%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76414   8.216%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.211658/ 48.209652, val:  81.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76874   8.179%\n",
      "fc layer 3 self.abs_max_out: 886.0\n",
      "fc layer 2 self.abs_max_out: 2513.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.202217/ 56.514652, val:  77.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0135%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77338   8.144%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.388752/ 47.937729, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.62 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77802   8.109%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.611837/ 44.376682, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78193   8.068%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.542479/ 38.427246, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78556   8.024%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.302204/ 42.856224, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 79016   7.991%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.017251/ 48.362556, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79447   7.956%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.293843/ 37.849319, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8008%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79893   7.923%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.040928/ 41.071987, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80286   7.885%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.031663/ 51.492321, val:  77.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1310%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80705   7.851%\n",
      "fc layer 3 self.abs_max_out: 903.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.727154/ 40.920341, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8851%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 81104   7.815%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.939573/ 40.382118, val:  87.92%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3582%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81496   7.780%\n",
      "fc layer 1 self.abs_max_out: 4329.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.830179/ 41.668972, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1095%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81884   7.744%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.508137/ 38.606453, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0687%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82251   7.708%\n",
      "fc layer 1 self.abs_max_out: 4332.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.268504/ 41.523987, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82586   7.669%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.596312/ 46.909412, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82970   7.635%\n",
      "lif layer 1 self.abs_max_v: 6716.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  3.034666/ 47.149445, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83364   7.603%\n",
      "lif layer 1 self.abs_max_v: 6853.5\n",
      "lif layer 1 self.abs_max_v: 6875.5\n",
      "lif layer 1 self.abs_max_v: 6913.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.408433/ 52.079247, val:  80.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1322%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83749   7.570%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.749719/ 45.230503, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84138   7.539%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.646490/ 50.865318, val:  80.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1399%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84511   7.506%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.446519/ 44.575775, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0923%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84854   7.472%\n",
      "fc layer 3 self.abs_max_out: 916.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.537360/ 43.834419, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1068%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85230   7.441%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.415529/ 49.577663, val:  80.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85606   7.410%\n",
      "fc layer 1 self.abs_max_out: 4448.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.641355/ 43.569118, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 85961   7.379%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.534503/ 45.287876, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86310   7.347%\n",
      "fc layer 2 self.abs_max_out: 2516.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.604454/ 43.981369, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7784%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86707   7.320%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.210868/ 42.270401, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2031%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 87054   7.289%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.632738/ 48.590931, val:  81.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2802%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87429   7.261%\n",
      "fc layer 1 self.abs_max_out: 4478.0\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.357908/ 45.731262, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3990%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87767   7.230%\n",
      "fc layer 3 self.abs_max_out: 926.0\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.362905/ 50.195118, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 88117   7.201%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.156707/ 46.302933, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6931%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9565%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88437   7.169%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.438926/ 42.552364, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88792   7.141%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.495114/ 43.277195, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0790%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 89150   7.114%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.264429/ 54.728638, val:  78.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89482   7.085%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.252712/ 50.576256, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89849   7.060%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.434689/ 48.942497, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 90169   7.031%\n",
      "fc layer 1 self.abs_max_out: 4489.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.119481/ 51.630913, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90482   7.002%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.371886/ 48.546482, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1304%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90804   6.974%\n",
      "fc layer 1 self.abs_max_out: 4558.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.098030/ 44.165127, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8761%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 91127   6.946%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.279734/ 54.614796, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7069%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2173%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91447   6.919%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.165122/ 40.635330, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1193%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91737   6.890%\n",
      "fc layer 2 self.abs_max_out: 2567.0\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.139871/ 48.709789, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 92088   6.866%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.270881/ 43.703655, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5677%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92412   6.840%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.124672/ 46.984047, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7246%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92725   6.814%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.114188/ 55.223042, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1106%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 93019   6.787%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.124498/ 51.586243, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93310   6.760%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.071644/ 41.547272, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93600   6.733%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.240027/ 46.463589, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93912   6.708%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.111260/ 54.903702, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 94231   6.684%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.111317/ 46.166794, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94525   6.659%\n",
      "fc layer 1 self.abs_max_out: 4578.0\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.809000/ 42.534527, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94805   6.633%\n",
      "fc layer 3 self.abs_max_out: 954.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.745267/ 46.270916, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 95060   6.605%\n",
      "fc layer 1 self.abs_max_out: 4735.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.956721/ 45.522964, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95320   6.579%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.070920/ 46.707748, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8270%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95621   6.555%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.978875/ 46.392490, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 95892   6.530%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.086776/ 40.111244, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4297%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 96204   6.508%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.077103/ 45.637341, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96511   6.486%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.910707/ 45.427135, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9280%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 96783   6.461%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.887548/ 53.321537, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 97091   6.440%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.759386/ 59.547810, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97347   6.415%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.199840/ 42.186268, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97642   6.393%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.113670/ 45.007729, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 97946   6.372%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.926096/ 49.104786, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 98229   6.350%\n",
      "fc layer 3 self.abs_max_out: 979.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.750004/ 50.716743, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8711%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5230%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98483   6.327%\n",
      "fc layer 3 self.abs_max_out: 985.0\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.661107/ 52.950249, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98714   6.302%\n",
      "fc layer 1 self.abs_max_out: 4757.0\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  2.028397/ 45.450771, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3081%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 98996   6.281%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.464886/ 51.448727, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 99233   6.257%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.755017/ 45.152180, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99498   6.235%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.667317/ 40.428608, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 99750   6.213%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.633989/ 56.593796, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 99982   6.189%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.854635/ 44.562401, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0497%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 100237   6.168%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.854702/ 47.439980, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0505%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100498   6.147%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.687297/ 46.448891, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6646%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 100737   6.125%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  2.056392/ 49.778038, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 101016   6.105%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.594928/ 54.710709, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 101265   6.085%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.466053/ 49.626785, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101483   6.062%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.366872/ 46.998749, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1007%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 101684   6.039%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.463794/ 48.870007, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 101920   6.018%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.623076/ 48.982346, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 102155   5.997%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.552108/ 46.979694, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102402   5.977%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.732738/ 54.541428, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 102647   5.957%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.587844/ 55.424263, val:  81.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0397%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0586%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 102891   5.938%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.611064/ 47.832317, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1124%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 103123   5.918%\n",
      "fc layer 3 self.abs_max_out: 1023.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.405135/ 45.812210, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0575%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1451%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103330   5.896%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.563168/ 52.579124, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 103550   5.876%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.542986/ 53.911583, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 103767   5.856%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.352107/ 49.295403, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 103986   5.836%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.635018/ 47.430367, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 104204   5.816%\n",
      "fc layer 2 self.abs_max_out: 2593.0\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.475634/ 60.394650, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7887%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104434   5.798%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.606876/ 43.990719, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 104685   5.780%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.572839/ 51.821011, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0497%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 104894   5.760%\n",
      "fc layer 2 self.abs_max_out: 2627.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.331367/ 50.438908, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1016%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 105108   5.741%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.524261/ 51.085014, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5636%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 105324   5.723%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.300407/ 50.751911, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.00 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 105518   5.703%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.475230/ 46.790058, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 105714   5.683%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.388144/ 42.975384, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 105905   5.664%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.682369/ 49.459923, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7659%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 106133   5.646%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.178516/ 49.671566, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106334   5.628%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.296023/ 43.712074, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 106546   5.610%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.263254/ 44.440182, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 106752   5.592%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.655594/ 48.937572, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 106971   5.575%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.146905/ 50.271496, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5762%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 107158   5.556%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.368039/ 43.867725, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107356   5.538%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.550973/ 49.631672, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 107578   5.522%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.416856/ 44.260006, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb1201303a04c36a3c6f1e26ff6bae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.41686</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>44.26001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-50</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pt3szsvb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pt3szsvb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_002747-pt3szsvb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e654aki8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 40853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_043839-e654aki8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e654aki8' target=\"_blank\">cerulean-sweep-57</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e654aki8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e654aki8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_043848_350', 'my_seed': 40853, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 208.0\n",
      "lif layer 1 self.abs_max_v: 208.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 152.0\n",
      "lif layer 2 self.abs_max_v: 152.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 15.0\n",
      "lif layer 1 self.abs_max_v: 233.0\n",
      "lif layer 2 self.abs_max_v: 166.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 1 self.abs_max_out: 215.0\n",
      "lif layer 1 self.abs_max_v: 280.5\n",
      "fc layer 2 self.abs_max_out: 176.0\n",
      "lif layer 2 self.abs_max_v: 208.5\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "fc layer 1 self.abs_max_out: 228.0\n",
      "lif layer 1 self.abs_max_v: 289.5\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 2 self.abs_max_out: 280.0\n",
      "lif layer 2 self.abs_max_v: 349.0\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "lif layer 1 self.abs_max_v: 311.5\n",
      "fc layer 1 self.abs_max_out: 266.0\n",
      "lif layer 1 self.abs_max_v: 366.5\n",
      "fc layer 2 self.abs_max_out: 309.0\n",
      "lif layer 2 self.abs_max_v: 445.0\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "fc layer 2 self.abs_max_out: 315.0\n",
      "lif layer 2 self.abs_max_v: 487.5\n",
      "fc layer 1 self.abs_max_out: 267.0\n",
      "lif layer 2 self.abs_max_v: 523.5\n",
      "fc layer 1 self.abs_max_out: 378.0\n",
      "lif layer 1 self.abs_max_v: 378.0\n",
      "lif layer 1 self.abs_max_v: 458.0\n",
      "fc layer 2 self.abs_max_out: 323.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 1 self.abs_max_out: 380.0\n",
      "lif layer 1 self.abs_max_v: 576.0\n",
      "fc layer 2 self.abs_max_out: 392.0\n",
      "lif layer 2 self.abs_max_v: 581.5\n",
      "fc layer 1 self.abs_max_out: 391.0\n",
      "lif layer 1 self.abs_max_v: 604.0\n",
      "fc layer 2 self.abs_max_out: 447.0\n",
      "lif layer 2 self.abs_max_v: 738.0\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 1 self.abs_max_out: 418.0\n",
      "fc layer 2 self.abs_max_out: 491.0\n",
      "lif layer 2 self.abs_max_v: 752.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 1 self.abs_max_out: 429.0\n",
      "fc layer 1 self.abs_max_out: 460.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 1 self.abs_max_out: 507.0\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "fc layer 1 self.abs_max_out: 595.0\n",
      "lif layer 1 self.abs_max_v: 628.5\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "lif layer 2 self.abs_max_v: 753.0\n",
      "lif layer 2 self.abs_max_v: 765.5\n",
      "lif layer 2 self.abs_max_v: 766.0\n",
      "lif layer 2 self.abs_max_v: 772.0\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "lif layer 2 self.abs_max_v: 867.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "fc layer 1 self.abs_max_out: 669.0\n",
      "lif layer 1 self.abs_max_v: 669.0\n",
      "fc layer 1 self.abs_max_out: 685.0\n",
      "lif layer 1 self.abs_max_v: 685.0\n",
      "fc layer 1 self.abs_max_out: 773.0\n",
      "lif layer 1 self.abs_max_v: 773.0\n",
      "fc layer 2 self.abs_max_out: 530.0\n",
      "fc layer 1 self.abs_max_out: 800.0\n",
      "lif layer 1 self.abs_max_v: 800.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 2 self.abs_max_out: 538.0\n",
      "lif layer 2 self.abs_max_v: 941.5\n",
      "fc layer 2 self.abs_max_out: 618.0\n",
      "lif layer 2 self.abs_max_v: 1028.0\n",
      "lif layer 2 self.abs_max_v: 1066.0\n",
      "fc layer 2 self.abs_max_out: 641.0\n",
      "fc layer 1 self.abs_max_out: 894.0\n",
      "lif layer 1 self.abs_max_v: 894.0\n",
      "lif layer 1 self.abs_max_v: 1001.5\n",
      "fc layer 2 self.abs_max_out: 704.0\n",
      "fc layer 3 self.abs_max_out: 234.0\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "fc layer 2 self.abs_max_out: 719.0\n",
      "fc layer 1 self.abs_max_out: 925.0\n",
      "fc layer 2 self.abs_max_out: 783.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "lif layer 1 self.abs_max_v: 1011.5\n",
      "lif layer 1 self.abs_max_v: 1026.5\n",
      "lif layer 1 self.abs_max_v: 1066.5\n",
      "lif layer 1 self.abs_max_v: 1109.5\n",
      "lif layer 1 self.abs_max_v: 1167.5\n",
      "fc layer 2 self.abs_max_out: 792.0\n",
      "fc layer 2 self.abs_max_out: 825.0\n",
      "fc layer 1 self.abs_max_out: 966.0\n",
      "lif layer 2 self.abs_max_v: 1107.5\n",
      "lif layer 1 self.abs_max_v: 1197.5\n",
      "lif layer 2 self.abs_max_v: 1116.0\n",
      "lif layer 2 self.abs_max_v: 1175.0\n",
      "lif layer 2 self.abs_max_v: 1264.5\n",
      "fc layer 1 self.abs_max_out: 1024.0\n",
      "lif layer 1 self.abs_max_v: 1296.5\n",
      "lif layer 2 self.abs_max_v: 1388.0\n",
      "fc layer 1 self.abs_max_out: 1090.0\n",
      "lif layer 1 self.abs_max_v: 1301.0\n",
      "lif layer 1 self.abs_max_v: 1448.5\n",
      "fc layer 2 self.abs_max_out: 833.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "fc layer 2 self.abs_max_out: 968.0\n",
      "fc layer 1 self.abs_max_out: 1151.0\n",
      "fc layer 2 self.abs_max_out: 969.0\n",
      "fc layer 1 self.abs_max_out: 1173.0\n",
      "fc layer 1 self.abs_max_out: 1228.0\n",
      "lif layer 1 self.abs_max_v: 1454.0\n",
      "lif layer 2 self.abs_max_v: 1472.0\n",
      "lif layer 1 self.abs_max_v: 1493.0\n",
      "lif layer 1 self.abs_max_v: 1516.5\n",
      "lif layer 1 self.abs_max_v: 1541.5\n",
      "lif layer 1 self.abs_max_v: 1575.5\n",
      "lif layer 1 self.abs_max_v: 1699.0\n",
      "lif layer 1 self.abs_max_v: 1745.5\n",
      "lif layer 1 self.abs_max_v: 1760.0\n",
      "lif layer 1 self.abs_max_v: 1800.0\n",
      "lif layer 1 self.abs_max_v: 1814.0\n",
      "lif layer 1 self.abs_max_v: 1827.5\n",
      "fc layer 2 self.abs_max_out: 1018.0\n",
      "fc layer 1 self.abs_max_out: 1323.0\n",
      "lif layer 1 self.abs_max_v: 1913.0\n",
      "lif layer 1 self.abs_max_v: 2002.5\n",
      "fc layer 2 self.abs_max_out: 1072.0\n",
      "fc layer 1 self.abs_max_out: 1326.0\n",
      "fc layer 1 self.abs_max_out: 1353.0\n",
      "fc layer 1 self.abs_max_out: 1400.0\n",
      "fc layer 1 self.abs_max_out: 1433.0\n",
      "fc layer 1 self.abs_max_out: 1530.0\n",
      "lif layer 1 self.abs_max_v: 2168.0\n",
      "fc layer 2 self.abs_max_out: 1100.0\n",
      "fc layer 1 self.abs_max_out: 1617.0\n",
      "lif layer 2 self.abs_max_v: 1499.0\n",
      "lif layer 2 self.abs_max_v: 1501.0\n",
      "lif layer 2 self.abs_max_v: 1509.0\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 3 self.abs_max_out: 390.0\n",
      "lif layer 1 self.abs_max_v: 2281.5\n",
      "lif layer 1 self.abs_max_v: 2318.0\n",
      "lif layer 1 self.abs_max_v: 2492.5\n",
      "lif layer 2 self.abs_max_v: 1567.0\n",
      "lif layer 2 self.abs_max_v: 1594.5\n",
      "lif layer 2 self.abs_max_v: 1602.5\n",
      "fc layer 2 self.abs_max_out: 1110.0\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.527447/104.984032, val:  23.75%, val_best:  23.75%, tr:  96.53%, tr_best:  96.53%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2228  22.758%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "fc layer 3 self.abs_max_out: 435.0\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "fc layer 2 self.abs_max_out: 1163.0\n",
      "lif layer 2 self.abs_max_v: 1636.5\n",
      "fc layer 2 self.abs_max_out: 1230.0\n",
      "fc layer 2 self.abs_max_out: 1236.0\n",
      "fc layer 1 self.abs_max_out: 1685.0\n",
      "fc layer 1 self.abs_max_out: 1798.0\n",
      "fc layer 1 self.abs_max_out: 1941.0\n",
      "lif layer 2 self.abs_max_v: 1730.5\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "lif layer 2 self.abs_max_v: 1821.5\n",
      "lif layer 1 self.abs_max_v: 2497.5\n",
      "lif layer 1 self.abs_max_v: 2785.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.104851/ 52.788040, val:  47.08%, val_best:  47.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3749  19.147%\n",
      "fc layer 1 self.abs_max_out: 2044.0\n",
      "fc layer 2 self.abs_max_out: 1292.0\n",
      "lif layer 2 self.abs_max_v: 1864.5\n",
      "lif layer 1 self.abs_max_v: 2800.5\n",
      "lif layer 1 self.abs_max_v: 2966.0\n",
      "lif layer 1 self.abs_max_v: 3129.0\n",
      "fc layer 2 self.abs_max_out: 1308.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.579481/ 51.663219, val:  47.92%, val_best:  47.92%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5184  17.651%\n",
      "lif layer 2 self.abs_max_v: 1949.0\n",
      "lif layer 2 self.abs_max_v: 1972.5\n",
      "fc layer 2 self.abs_max_out: 1370.0\n",
      "fc layer 2 self.abs_max_out: 1373.0\n",
      "fc layer 2 self.abs_max_out: 1491.0\n",
      "lif layer 1 self.abs_max_v: 3208.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.830493/ 66.709122, val:  43.75%, val_best:  47.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6638  16.951%\n",
      "lif layer 2 self.abs_max_v: 1975.0\n",
      "lif layer 2 self.abs_max_v: 1989.0\n",
      "lif layer 2 self.abs_max_v: 1992.5\n",
      "fc layer 2 self.abs_max_out: 1492.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "lif layer 2 self.abs_max_v: 2030.5\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "lif layer 2 self.abs_max_v: 2107.5\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "lif layer 1 self.abs_max_v: 3331.0\n",
      "lif layer 1 self.abs_max_v: 3446.5\n",
      "fc layer 1 self.abs_max_out: 2088.0\n",
      "lif layer 1 self.abs_max_v: 3585.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.766934/ 52.257229, val:  49.58%, val_best:  49.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7984  16.311%\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 1 self.abs_max_out: 2151.0\n",
      "lif layer 2 self.abs_max_v: 2120.0\n",
      "lif layer 1 self.abs_max_v: 3702.0\n",
      "lif layer 1 self.abs_max_v: 3769.5\n",
      "lif layer 1 self.abs_max_v: 3909.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.051158/ 31.197912, val:  61.25%, val_best:  61.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9677%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9298  15.829%\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "fc layer 1 self.abs_max_out: 2175.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.999743/ 56.623848, val:  50.00%, val_best:  61.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10607  15.478%\n",
      "fc layer 1 self.abs_max_out: 2190.0\n",
      "lif layer 2 self.abs_max_v: 2139.5\n",
      "lif layer 2 self.abs_max_v: 2153.0\n",
      "lif layer 2 self.abs_max_v: 2184.5\n",
      "lif layer 2 self.abs_max_v: 2186.0\n",
      "fc layer 1 self.abs_max_out: 2205.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.082405/ 61.457169, val:  54.58%, val_best:  61.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11818  15.089%\n",
      "lif layer 2 self.abs_max_v: 2218.0\n",
      "fc layer 1 self.abs_max_out: 2225.0\n",
      "fc layer 1 self.abs_max_out: 2229.0\n",
      "lif layer 1 self.abs_max_v: 4008.0\n",
      "fc layer 1 self.abs_max_out: 2278.0\n",
      "fc layer 1 self.abs_max_out: 2434.0\n",
      "lif layer 1 self.abs_max_v: 4159.5\n",
      "lif layer 1 self.abs_max_v: 4248.5\n",
      "lif layer 1 self.abs_max_v: 4419.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.990576/ 66.923691, val:  47.50%, val_best:  61.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13002  14.757%\n",
      "lif layer 2 self.abs_max_v: 2234.0\n",
      "lif layer 2 self.abs_max_v: 2252.0\n",
      "fc layer 3 self.abs_max_out: 570.0\n",
      "lif layer 2 self.abs_max_v: 2381.0\n",
      "fc layer 3 self.abs_max_out: 575.0\n",
      "fc layer 1 self.abs_max_out: 2441.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.992887/ 45.510422, val:  55.42%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8335%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14219  14.524%\n",
      "lif layer 2 self.abs_max_v: 2437.0\n",
      "lif layer 2 self.abs_max_v: 2509.5\n",
      "fc layer 1 self.abs_max_out: 2447.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.416383/ 45.471230, val:  59.58%, val_best:  61.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0030%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15416  14.315%\n",
      "fc layer 1 self.abs_max_out: 2624.0\n",
      "fc layer 2 self.abs_max_out: 1535.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.247348/ 43.395935, val:  55.42%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16631  14.156%\n",
      "lif layer 2 self.abs_max_v: 2553.5\n",
      "fc layer 1 self.abs_max_out: 2672.0\n",
      "lif layer 2 self.abs_max_v: 2575.0\n",
      "fc layer 1 self.abs_max_out: 2846.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.396829/ 62.485386, val:  53.33%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17722  13.925%\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "lif layer 2 self.abs_max_v: 2647.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.634541/ 53.982517, val:  53.33%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18867  13.766%\n",
      "lif layer 1 self.abs_max_v: 4702.5\n",
      "fc layer 1 self.abs_max_out: 2887.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.730381/ 58.450516, val:  56.25%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20067  13.665%\n",
      "fc layer 2 self.abs_max_out: 1538.0\n",
      "fc layer 2 self.abs_max_out: 1542.0\n",
      "lif layer 1 self.abs_max_v: 4739.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.276570/ 52.425316, val:  57.08%, val_best:  61.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21165  13.512%\n",
      "fc layer 2 self.abs_max_out: 1584.0\n",
      "lif layer 2 self.abs_max_v: 2674.0\n",
      "lif layer 2 self.abs_max_v: 2745.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.371917/ 40.680733, val:  62.50%, val_best:  62.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0366%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22278  13.386%\n",
      "fc layer 2 self.abs_max_out: 1592.0\n",
      "fc layer 1 self.abs_max_out: 2944.0\n",
      "lif layer 1 self.abs_max_v: 4878.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.175066/ 61.191078, val:  51.67%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23356  13.254%\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.505633/ 45.631191, val:  61.67%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1396%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24392  13.113%\n",
      "lif layer 2 self.abs_max_v: 2867.0\n",
      "fc layer 2 self.abs_max_out: 1597.0\n",
      "fc layer 1 self.abs_max_out: 2975.0\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "lif layer 2 self.abs_max_v: 2945.5\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "lif layer 1 self.abs_max_v: 4925.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.788021/ 50.552593, val:  62.08%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1046%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6777%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25416  12.981%\n",
      "fc layer 2 self.abs_max_out: 1673.0\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "fc layer 2 self.abs_max_out: 1678.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.917593/ 47.534164, val:  65.42%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26401  12.842%\n",
      "fc layer 2 self.abs_max_out: 1713.0\n",
      "fc layer 1 self.abs_max_out: 3035.0\n",
      "fc layer 1 self.abs_max_out: 3058.0\n",
      "lif layer 1 self.abs_max_v: 5097.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.876722/ 46.057293, val:  65.00%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27433  12.737%\n",
      "fc layer 1 self.abs_max_out: 3113.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.000627/ 31.678322, val:  70.42%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0046%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28442  12.631%\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.703397/ 75.467255, val:  50.00%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2828%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29459  12.538%\n",
      "fc layer 2 self.abs_max_out: 1726.0\n",
      "fc layer 3 self.abs_max_out: 637.0\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "lif layer 1 self.abs_max_v: 5117.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.075410/ 51.442421, val:  66.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30416  12.427%\n",
      "fc layer 1 self.abs_max_out: 3239.0\n",
      "fc layer 3 self.abs_max_out: 678.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.218029/ 40.658073, val:  65.83%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6483%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31352  12.317%\n",
      "fc layer 2 self.abs_max_out: 1737.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.160457/ 41.983440, val:  65.00%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32290  12.216%\n",
      "fc layer 3 self.abs_max_out: 744.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  5.689749/ 41.797642, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0752%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33188  12.107%\n",
      "fc layer 1 self.abs_max_out: 3248.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.985482/ 37.965649, val:  72.50%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34093  12.008%\n",
      "fc layer 2 self.abs_max_out: 1862.0\n",
      "fc layer 1 self.abs_max_out: 3324.0\n",
      "lif layer 1 self.abs_max_v: 5268.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.688291/ 33.974491, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 34982  11.911%\n",
      "lif layer 2 self.abs_max_v: 2946.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.684693/ 38.415802, val:  68.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3819%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 35855  11.814%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.798127/ 42.504387, val:  72.08%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 36747  11.730%\n",
      "fc layer 1 self.abs_max_out: 3332.0\n",
      "fc layer 1 self.abs_max_out: 3369.0\n",
      "lif layer 2 self.abs_max_v: 2960.5\n",
      "lif layer 1 self.abs_max_v: 5313.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.516165/ 56.707439, val:  61.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.06 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37596  11.637%\n",
      "lif layer 1 self.abs_max_v: 5761.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.777668/ 31.043636, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.1084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38457  11.554%\n",
      "fc layer 1 self.abs_max_out: 3380.0\n",
      "fc layer 1 self.abs_max_out: 3502.0\n",
      "lif layer 2 self.abs_max_v: 2979.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.022921/ 36.668388, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39220  11.446%\n",
      "fc layer 1 self.abs_max_out: 3648.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.397151/ 36.579456, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40042  11.361%\n",
      "lif layer 2 self.abs_max_v: 3032.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.780620/ 47.615585, val:  68.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 40909  11.294%\n",
      "fc layer 2 self.abs_max_out: 1913.0\n",
      "lif layer 1 self.abs_max_v: 5887.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.897440/ 34.964687, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 41699  11.209%\n",
      "fc layer 1 self.abs_max_out: 3728.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.557716/ 33.192547, val:  78.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42421  11.110%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.167071/ 38.495457, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43190  11.029%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.980912/ 34.038204, val:  73.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 43953  10.950%\n",
      "fc layer 2 self.abs_max_out: 1929.0\n",
      "fc layer 2 self.abs_max_out: 1934.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.419022/ 35.030331, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 44748  10.883%\n",
      "fc layer 2 self.abs_max_out: 1946.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.095451/ 65.710526, val:  59.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 45531  10.816%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.713974/ 30.327538, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46268  10.741%\n",
      "fc layer 2 self.abs_max_out: 1971.0\n",
      "fc layer 1 self.abs_max_out: 3744.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.748671/ 35.208771, val:  78.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47014  10.672%\n",
      "fc layer 1 self.abs_max_out: 3765.0\n",
      "lif layer 2 self.abs_max_v: 3151.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.552456/ 30.896845, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0418%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4709%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 47740  10.601%\n",
      "fc layer 1 self.abs_max_out: 3806.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.705387/ 30.718950, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 48472  10.534%\n",
      "fc layer 2 self.abs_max_out: 1974.0\n",
      "fc layer 2 self.abs_max_out: 2000.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.253360/ 57.977901, val:  64.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49191  10.468%\n",
      "fc layer 1 self.abs_max_out: 3814.0\n",
      "fc layer 2 self.abs_max_out: 2037.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.610669/ 27.864996, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8928%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 49897  10.401%\n",
      "fc layer 1 self.abs_max_out: 3847.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.444270/ 41.216232, val:  77.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 50595  10.336%\n",
      "lif layer 2 self.abs_max_v: 3174.5\n",
      "lif layer 2 self.abs_max_v: 3244.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.534782/ 52.508575, val:  64.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51289  10.272%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.270908/ 42.137775, val:  72.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 51944  10.204%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.306751/ 35.100040, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 52627  10.143%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.185207/ 31.678757, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53261  10.075%\n",
      "fc layer 2 self.abs_max_out: 2082.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.950100/ 49.892708, val:  72.50%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 53918  10.014%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.061538/ 33.621571, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4284%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 54568   9.953%\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "fc layer 1 self.abs_max_out: 3877.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.034861/ 28.028131, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55177   9.888%\n",
      "fc layer 1 self.abs_max_out: 3928.0\n",
      "fc layer 2 self.abs_max_out: 2155.0\n",
      "fc layer 3 self.abs_max_out: 752.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.279620/ 52.623814, val:  66.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.00 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 55845   9.835%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.277722/ 49.549316, val:  75.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 56474   9.777%\n",
      "lif layer 2 self.abs_max_v: 3296.5\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.410814/ 32.703438, val:  81.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57144   9.728%\n",
      "fc layer 1 self.abs_max_out: 3988.0\n",
      "lif layer 2 self.abs_max_v: 3317.5\n",
      "lif layer 2 self.abs_max_v: 3489.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.843429/ 41.750946, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 57760   9.672%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.891328/ 37.532288, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1314%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58385   9.619%\n",
      "fc layer 1 self.abs_max_out: 4105.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.699228/ 52.088249, val:  66.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5013%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 58972   9.561%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.159871/ 43.993397, val:  78.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 59607   9.513%\n",
      "fc layer 1 self.abs_max_out: 4119.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.536019/ 40.803040, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1079%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5449%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60160   9.454%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.996291/ 29.857058, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6264%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 60742   9.401%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.171879/ 35.677681, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61276   9.342%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.837588/ 34.432743, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 61845   9.290%\n",
      "fc layer 1 self.abs_max_out: 4170.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.754501/ 33.385620, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1028%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 62407   9.239%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.620730/ 45.935535, val:  74.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 62961   9.187%\n",
      "fc layer 1 self.abs_max_out: 4186.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.519658/ 38.271347, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 63507   9.137%\n",
      "lif layer 1 self.abs_max_v: 6035.5\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.445512/ 40.175026, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64051   9.087%\n",
      "lif layer 1 self.abs_max_v: 6433.5\n",
      "fc layer 2 self.abs_max_out: 2195.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.787205/ 44.787327, val:  71.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1007%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 64613   9.041%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.311395/ 32.347836, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3163%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65116   8.988%\n",
      "fc layer 3 self.abs_max_out: 763.0\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.335509/ 46.299282, val:  74.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 65649   8.941%\n",
      "fc layer 1 self.abs_max_out: 4199.0\n",
      "fc layer 2 self.abs_max_out: 2240.0\n",
      "fc layer 2 self.abs_max_out: 2347.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.113425/ 41.897758, val:  82.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66152   8.891%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.243810/ 38.060757, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 66674   8.845%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.497979/ 38.381748, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0349%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 67214   8.802%\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.293987/ 34.313477, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 67712   8.755%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.190589/ 34.620564, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9122%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 68219   8.710%\n",
      "fc layer 1 self.abs_max_out: 4251.0\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.143694/ 40.701607, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5686%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 68717   8.666%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.437211/ 39.712177, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0594%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 69224   8.623%\n",
      "fc layer 3 self.abs_max_out: 769.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.322706/ 29.590431, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7524%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 69753   8.584%\n",
      "fc layer 3 self.abs_max_out: 800.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.984738/ 38.693310, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 70241   8.541%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.127736/ 34.614635, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7616%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 70728   8.499%\n",
      "lif layer 2 self.abs_max_v: 3546.0\n",
      "lif layer 2 self.abs_max_v: 3597.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.614802/ 45.498123, val:  72.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4786%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 71159   8.452%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.116356/ 37.902348, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 71659   8.413%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  2.855685/ 39.719398, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72107   8.370%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.865187/ 33.100708, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 72543   8.326%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.615078/ 49.055725, val:  75.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1228%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 72971   8.282%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  2.902572/ 38.625618, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 73448   8.244%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.990841/ 32.030811, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 73933   8.209%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.808298/ 39.095421, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4479%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 74372   8.169%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.703992/ 42.858387, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 74823   8.131%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.781287/ 35.199402, val:  84.17%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7949%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 75267   8.093%\n",
      "fc layer 1 self.abs_max_out: 4337.0\n",
      "lif layer 2 self.abs_max_v: 3611.5\n",
      "lif layer 1 self.abs_max_v: 6434.0\n",
      "lif layer 1 self.abs_max_v: 6502.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.355938/ 31.930086, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 75673   8.052%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.631680/ 31.055553, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 76085   8.012%\n",
      "fc layer 2 self.abs_max_out: 2427.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.992824/ 33.678440, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 76531   7.977%\n",
      "lif layer 2 self.abs_max_v: 3614.5\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.513800/ 30.846636, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 76945   7.939%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.402094/ 36.015343, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0526%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 77365   7.902%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.687014/ 35.638126, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0153%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 77780   7.866%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.740650/ 38.139713, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1009%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 78199   7.831%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.826875/ 35.844791, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 78650   7.800%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.244670/ 43.827087, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 79026   7.762%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.915684/ 33.518536, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3484%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 79452   7.729%\n",
      "lif layer 1 self.abs_max_v: 6664.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.494013/ 39.153507, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7731%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 79850   7.695%\n",
      "lif layer 1 self.abs_max_v: 6664.5\n",
      "lif layer 1 self.abs_max_v: 6986.5\n",
      "fc layer 2 self.abs_max_out: 2464.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.311831/ 32.196243, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 80234   7.659%\n",
      "fc layer 1 self.abs_max_out: 4372.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.570407/ 44.603031, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8968%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 80621   7.625%\n",
      "fc layer 1 self.abs_max_out: 4414.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.468292/ 33.990105, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 81006   7.591%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.364105/ 36.501541, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 81401   7.559%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.064163/ 31.423313, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 81748   7.523%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.326924/ 33.965347, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 82122   7.490%\n",
      "fc layer 3 self.abs_max_out: 807.0\n",
      "fc layer 1 self.abs_max_out: 4415.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.078769/ 37.402248, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 82474   7.455%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.135574/ 37.542233, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 82836   7.422%\n",
      "fc layer 1 self.abs_max_out: 4481.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.282842/ 32.943787, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1171%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1111%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 83176   7.388%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.157854/ 38.506596, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 83516   7.354%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.277246/ 36.391003, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1075%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9284%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 83857   7.321%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  1.950014/ 41.456932, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 84193   7.288%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.270039/ 34.281387, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 84527   7.255%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.133085/ 33.474796, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0503%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 84876   7.225%\n",
      "lif layer 1 self.abs_max_v: 7036.5\n",
      "fc layer 2 self.abs_max_out: 2468.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.093352/ 31.625896, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 85201   7.192%\n",
      "fc layer 1 self.abs_max_out: 4583.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  1.969983/ 37.947140, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 85538   7.162%\n",
      "fc layer 2 self.abs_max_out: 2514.0\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  1.975711/ 48.526917, val:  76.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8080%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 85863   7.130%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  1.963839/ 43.392807, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 86182   7.099%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  1.778109/ 32.999729, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0543%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6732%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 86476   7.066%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.406157/ 34.845531, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2350%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 86854   7.041%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  1.988428/ 41.058289, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 87202   7.014%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.250071/ 33.927212, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 87563   6.988%\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.136331/ 33.417278, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0318%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 87913   6.961%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.283550/ 36.345070, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1124%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6383%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 88248   6.934%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  1.999597/ 32.853874, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7546%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 88581   6.907%\n",
      "fc layer 3 self.abs_max_out: 812.0\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.823786/ 45.457909, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1139%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9464%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 88895   6.879%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.021006/ 37.696960, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 89203   6.851%\n",
      "lif layer 1 self.abs_max_v: 7052.5\n",
      "fc layer 3 self.abs_max_out: 876.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.959015/ 37.333191, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 89501   6.822%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.793797/ 44.320442, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 89796   6.794%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.975197/ 32.210690, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3140%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 90092   6.767%\n",
      "lif layer 1 self.abs_max_v: 7054.5\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.055036/ 47.149773, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 90416   6.741%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.039760/ 43.013672, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1021%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 90749   6.717%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.692947/ 32.600925, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 91036   6.690%\n",
      "lif layer 2 self.abs_max_v: 3723.5\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.715078/ 37.238426, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8992%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 91328   6.663%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.860817/ 36.964233, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1185%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3707%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 91603   6.636%\n",
      "lif layer 2 self.abs_max_v: 3823.5\n",
      "lif layer 2 self.abs_max_v: 3897.0\n",
      "lif layer 2 self.abs_max_v: 3978.5\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.634162/ 34.420174, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 91893   6.610%\n",
      "fc layer 2 self.abs_max_out: 2538.0\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.628667/ 39.735863, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9462%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 92163   6.583%\n",
      "fc layer 3 self.abs_max_out: 883.0\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.611510/ 38.556477, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0710%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 92437   6.557%\n",
      "lif layer 1 self.abs_max_v: 7134.5\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.736776/ 43.996456, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 92715   6.531%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.571237/ 41.202549, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0388%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4335%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 92983   6.505%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.755137/ 61.823475, val:  72.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 93264   6.481%\n",
      "lif layer 1 self.abs_max_v: 7235.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.717841/ 47.774822, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1146%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5761%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 93560   6.457%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.686197/ 37.069042, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 93837   6.433%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.809470/ 34.007843, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5940%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 94127   6.410%\n",
      "lif layer 1 self.abs_max_v: 7357.5\n",
      "fc layer 1 self.abs_max_out: 4589.0\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.835341/ 43.851757, val:  80.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 94421   6.387%\n",
      "fc layer 1 self.abs_max_out: 4618.0\n",
      "lif layer 1 self.abs_max_v: 7532.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.651382/ 36.113724, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4339%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 94690   6.363%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.748321/ 42.126381, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 94989   6.342%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.999190/ 36.813408, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 95293   6.321%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.460111/ 42.947155, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 95556   6.297%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.820683/ 39.516312, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0344%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 95838   6.275%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.860031/ 40.865875, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 96120   6.254%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.640598/ 39.317322, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 96370   6.230%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.371867/ 38.403561, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4690%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 96606   6.206%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.510891/ 41.580307, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9019%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 96871   6.184%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.374679/ 46.519547, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0384%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 97110   6.161%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.391013/ 36.365368, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9206%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 97378   6.140%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.474102/ 40.815071, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1209%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1083%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 97630   6.118%\n",
      "fc layer 1 self.abs_max_out: 4654.0\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.104809/ 36.530056, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0221%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 97822   6.093%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.521355/ 37.893421, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1080%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 98075   6.071%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.461684/ 38.099590, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9390%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 98316   6.050%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.438697/ 37.407890, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1052%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 98553   6.028%\n",
      "fc layer 1 self.abs_max_out: 4680.0\n",
      "lif layer 1 self.abs_max_v: 7600.5\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.244555/ 38.538536, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 98783   6.006%\n",
      "fc layer 2 self.abs_max_out: 2649.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.315934/ 40.481529, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 99005   5.984%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.174239/ 38.859764, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 99218   5.962%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.510633/ 36.441181, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0494%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4844%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 99472   5.942%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.536460/ 37.456028, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 99715   5.922%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.418428/ 50.555363, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8761%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 99975   5.903%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.594848/ 46.945477, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 100235   5.884%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.171134/ 44.125854, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 100443   5.863%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.275589/ 43.682411, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0442%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4463%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 100657   5.842%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.499831/ 44.096901, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7440%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 100896   5.823%\n",
      "fc layer 3 self.abs_max_out: 892.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.449166/ 36.171894, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 101123   5.803%\n",
      "fc layer 3 self.abs_max_out: 908.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.520303/ 39.132042, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 101357   5.784%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.313557/ 37.583725, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4869%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4659%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 101595   5.765%\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "fc layer 3 self.abs_max_out: 921.0\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.548093/ 37.908943, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 101838   5.747%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.255623/ 38.363049, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 102054   5.728%\n",
      "fc layer 3 self.abs_max_out: 938.0\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.528742/ 52.844727, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 102287   5.709%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.205262/ 37.042614, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 102501   5.690%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.608531/ 40.867287, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.05 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 102753   5.673%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.292315/ 41.572956, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1046%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5816%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 102959   5.654%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.186705/ 36.396687, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 103170   5.635%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.164837/ 39.559830, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5687%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 103379   5.617%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.144376/ 36.792912, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4546%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 103581   5.598%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.023567/ 36.940491, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7090%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 103748   5.578%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.242855/ 36.319157, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 103947   5.559%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.099563/ 35.068417, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 104140   5.540%\n",
      "fc layer 2 self.abs_max_out: 2654.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.254809/ 40.356766, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1701%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 104342   5.522%\n",
      "fc layer 1 self.abs_max_out: 4780.0\n",
      "lif layer 1 self.abs_max_v: 7765.0\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.267333/ 39.260647, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0301%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 104529   5.504%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.100549/ 37.689827, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 104714   5.485%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.159711/ 42.454742, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 104899   5.467%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  0.879132/ 39.885750, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 105051   5.447%\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.107606/ 34.733807, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0987%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 105240   5.429%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.345815/ 38.171501, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0958%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 105458   5.413%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.088468/ 39.746208, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc67589296f4ba585484423c09e79d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÅ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.08847</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>39.74621</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-sweep-57</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e654aki8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e654aki8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_043839-e654aki8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iephv5yz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 12088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_084444-iephv5yz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iephv5yz' target=\"_blank\">super-sweep-65</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iephv5yz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iephv5yz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_084452_631', 'my_seed': 12088, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 272.0\n",
      "lif layer 1 self.abs_max_v: 272.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 187.0\n",
      "lif layer 2 self.abs_max_v: 187.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 1 self.abs_max_out: 394.0\n",
      "lif layer 1 self.abs_max_v: 450.0\n",
      "fc layer 2 self.abs_max_out: 321.0\n",
      "lif layer 2 self.abs_max_v: 338.0\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "lif layer 1 self.abs_max_v: 502.5\n",
      "fc layer 2 self.abs_max_out: 430.0\n",
      "lif layer 2 self.abs_max_v: 447.5\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "fc layer 1 self.abs_max_out: 409.0\n",
      "lif layer 1 self.abs_max_v: 599.0\n",
      "fc layer 1 self.abs_max_out: 496.0\n",
      "lif layer 1 self.abs_max_v: 637.5\n",
      "lif layer 2 self.abs_max_v: 509.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 1 self.abs_max_out: 648.0\n",
      "lif layer 1 self.abs_max_v: 698.0\n",
      "lif layer 2 self.abs_max_v: 621.5\n",
      "lif layer 2 self.abs_max_v: 698.0\n",
      "fc layer 1 self.abs_max_out: 741.0\n",
      "lif layer 1 self.abs_max_v: 741.0\n",
      "fc layer 2 self.abs_max_out: 528.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "lif layer 1 self.abs_max_v: 746.0\n",
      "fc layer 1 self.abs_max_out: 753.0\n",
      "lif layer 1 self.abs_max_v: 753.0\n",
      "lif layer 2 self.abs_max_v: 756.5\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "lif layer 2 self.abs_max_v: 780.0\n",
      "fc layer 1 self.abs_max_out: 827.0\n",
      "lif layer 1 self.abs_max_v: 827.0\n",
      "lif layer 2 self.abs_max_v: 816.0\n",
      "lif layer 2 self.abs_max_v: 823.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "lif layer 1 self.abs_max_v: 927.0\n",
      "lif layer 1 self.abs_max_v: 931.5\n",
      "lif layer 2 self.abs_max_v: 825.0\n",
      "lif layer 1 self.abs_max_v: 1017.5\n",
      "lif layer 2 self.abs_max_v: 912.5\n",
      "lif layer 2 self.abs_max_v: 1016.5\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 1026.0\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 1094.5\n",
      "lif layer 1 self.abs_max_v: 1111.5\n",
      "fc layer 1 self.abs_max_out: 880.0\n",
      "fc layer 2 self.abs_max_out: 701.0\n",
      "fc layer 1 self.abs_max_out: 960.0\n",
      "fc layer 2 self.abs_max_out: 717.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 2 self.abs_max_out: 761.0\n",
      "fc layer 2 self.abs_max_out: 765.0\n",
      "fc layer 2 self.abs_max_out: 813.0\n",
      "lif layer 2 self.abs_max_v: 1028.0\n",
      "lif layer 2 self.abs_max_v: 1038.0\n",
      "lif layer 2 self.abs_max_v: 1082.0\n",
      "fc layer 2 self.abs_max_out: 857.0\n",
      "lif layer 1 self.abs_max_v: 1205.0\n",
      "lif layer 1 self.abs_max_v: 1240.5\n",
      "lif layer 1 self.abs_max_v: 1241.5\n",
      "lif layer 1 self.abs_max_v: 1308.0\n",
      "fc layer 2 self.abs_max_out: 870.0\n",
      "fc layer 2 self.abs_max_out: 903.0\n",
      "fc layer 1 self.abs_max_out: 1115.0\n",
      "lif layer 1 self.abs_max_v: 1323.0\n",
      "lif layer 2 self.abs_max_v: 1098.5\n",
      "fc layer 1 self.abs_max_out: 1274.0\n",
      "lif layer 1 self.abs_max_v: 1327.0\n",
      "lif layer 1 self.abs_max_v: 1451.0\n",
      "lif layer 1 self.abs_max_v: 1489.5\n",
      "lif layer 1 self.abs_max_v: 1534.5\n",
      "lif layer 1 self.abs_max_v: 1540.0\n",
      "lif layer 1 self.abs_max_v: 1559.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 2 self.abs_max_out: 904.0\n",
      "fc layer 2 self.abs_max_out: 925.0\n",
      "lif layer 1 self.abs_max_v: 1689.0\n",
      "lif layer 1 self.abs_max_v: 1753.5\n",
      "lif layer 1 self.abs_max_v: 1775.0\n",
      "lif layer 2 self.abs_max_v: 1113.5\n",
      "fc layer 1 self.abs_max_out: 1304.0\n",
      "lif layer 1 self.abs_max_v: 1792.5\n",
      "lif layer 1 self.abs_max_v: 1795.5\n",
      "lif layer 1 self.abs_max_v: 1840.0\n",
      "lif layer 2 self.abs_max_v: 1119.5\n",
      "lif layer 2 self.abs_max_v: 1223.0\n",
      "fc layer 1 self.abs_max_out: 1405.0\n",
      "lif layer 1 self.abs_max_v: 1864.5\n",
      "fc layer 2 self.abs_max_out: 964.0\n",
      "fc layer 2 self.abs_max_out: 971.0\n",
      "lif layer 1 self.abs_max_v: 1940.0\n",
      "lif layer 1 self.abs_max_v: 2051.0\n",
      "lif layer 1 self.abs_max_v: 2071.5\n",
      "lif layer 1 self.abs_max_v: 2111.0\n",
      "lif layer 1 self.abs_max_v: 2168.5\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "lif layer 2 self.abs_max_v: 1227.0\n",
      "fc layer 2 self.abs_max_out: 977.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "lif layer 2 self.abs_max_v: 1246.0\n",
      "fc layer 1 self.abs_max_out: 1469.0\n",
      "fc layer 1 self.abs_max_out: 1770.0\n",
      "fc layer 1 self.abs_max_out: 1810.0\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "lif layer 2 self.abs_max_v: 1250.5\n",
      "lif layer 2 self.abs_max_v: 1270.0\n",
      "fc layer 2 self.abs_max_out: 982.0\n",
      "lif layer 2 self.abs_max_v: 1298.0\n",
      "fc layer 2 self.abs_max_out: 1065.0\n",
      "lif layer 2 self.abs_max_v: 1300.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 2 self.abs_max_out: 1082.0\n",
      "lif layer 1 self.abs_max_v: 2222.5\n",
      "lif layer 1 self.abs_max_v: 2348.5\n",
      "lif layer 1 self.abs_max_v: 2439.0\n",
      "lif layer 1 self.abs_max_v: 2507.0\n",
      "lif layer 1 self.abs_max_v: 2622.5\n",
      "lif layer 1 self.abs_max_v: 2676.0\n",
      "lif layer 1 self.abs_max_v: 2719.0\n",
      "lif layer 2 self.abs_max_v: 1301.0\n",
      "fc layer 2 self.abs_max_out: 1133.0\n",
      "lif layer 2 self.abs_max_v: 1309.0\n",
      "lif layer 2 self.abs_max_v: 1374.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.766003/ 54.571140, val:  40.83%, val_best:  40.83%, tr:  96.94%, tr_best:  96.94%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2217  22.646%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "lif layer 2 self.abs_max_v: 1387.5\n",
      "lif layer 2 self.abs_max_v: 1402.5\n",
      "lif layer 2 self.abs_max_v: 1467.5\n",
      "lif layer 2 self.abs_max_v: 1486.5\n",
      "lif layer 2 self.abs_max_v: 1491.5\n",
      "lif layer 2 self.abs_max_v: 1494.0\n",
      "fc layer 1 self.abs_max_out: 1853.0\n",
      "fc layer 1 self.abs_max_out: 1891.0\n",
      "lif layer 2 self.abs_max_v: 1544.0\n",
      "fc layer 2 self.abs_max_out: 1218.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "fc layer 3 self.abs_max_out: 405.0\n",
      "fc layer 2 self.abs_max_out: 1252.0\n",
      "lif layer 1 self.abs_max_v: 2842.5\n",
      "lif layer 1 self.abs_max_v: 2983.5\n",
      "lif layer 1 self.abs_max_v: 3253.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.189373/ 54.248798, val:  44.58%, val_best:  44.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3847  19.648%\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "lif layer 2 self.abs_max_v: 1622.5\n",
      "fc layer 1 self.abs_max_out: 1927.0\n",
      "fc layer 2 self.abs_max_out: 1317.0\n",
      "fc layer 2 self.abs_max_out: 1320.0\n",
      "lif layer 2 self.abs_max_v: 1757.0\n",
      "fc layer 2 self.abs_max_out: 1322.0\n",
      "fc layer 1 self.abs_max_out: 2109.0\n",
      "lif layer 1 self.abs_max_v: 3383.5\n",
      "lif layer 1 self.abs_max_v: 3488.0\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.189410/ 54.335327, val:  48.33%, val_best:  48.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0498%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4256%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5377  18.308%\n",
      "lif layer 1 self.abs_max_v: 3516.0\n",
      "fc layer 1 self.abs_max_out: 2130.0\n",
      "fc layer 1 self.abs_max_out: 2391.0\n",
      "lif layer 1 self.abs_max_v: 3889.5\n",
      "lif layer 1 self.abs_max_v: 4065.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.458661/ 51.586658, val:  48.75%, val_best:  48.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6805  17.377%\n",
      "lif layer 2 self.abs_max_v: 1876.5\n",
      "lif layer 2 self.abs_max_v: 1885.0\n",
      "fc layer 1 self.abs_max_out: 2607.0\n",
      "lif layer 1 self.abs_max_v: 4252.0\n",
      "lif layer 1 self.abs_max_v: 4413.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.288205/ 68.725967, val:  39.58%, val_best:  48.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8175  16.701%\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "lif layer 2 self.abs_max_v: 1901.0\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 1 self.abs_max_out: 2630.0\n",
      "lif layer 1 self.abs_max_v: 4461.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.770047/ 47.529583, val:  54.58%, val_best:  54.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4922%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9515  16.199%\n",
      "lif layer 2 self.abs_max_v: 1936.5\n",
      "lif layer 2 self.abs_max_v: 1963.0\n",
      "lif layer 2 self.abs_max_v: 1970.0\n",
      "lif layer 2 self.abs_max_v: 1972.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  7.994887/ 48.627960, val:  53.75%, val_best:  54.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10762  15.704%\n",
      "lif layer 2 self.abs_max_v: 2015.5\n",
      "lif layer 2 self.abs_max_v: 2029.0\n",
      "lif layer 2 self.abs_max_v: 2128.0\n",
      "fc layer 3 self.abs_max_out: 512.0\n",
      "fc layer 1 self.abs_max_out: 2817.0\n",
      "lif layer 1 self.abs_max_v: 4600.5\n",
      "lif layer 1 self.abs_max_v: 4763.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.046520/ 50.857346, val:  55.42%, val_best:  55.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7890%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12019  15.346%\n",
      "lif layer 2 self.abs_max_v: 2134.5\n",
      "lif layer 2 self.abs_max_v: 2167.0\n",
      "fc layer 1 self.abs_max_out: 2851.0\n",
      "lif layer 1 self.abs_max_v: 4798.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.139903/ 64.695229, val:  42.92%, val_best:  55.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13307  15.103%\n",
      "lif layer 2 self.abs_max_v: 2322.0\n",
      "fc layer 3 self.abs_max_out: 545.0\n",
      "fc layer 1 self.abs_max_out: 2923.0\n",
      "lif layer 1 self.abs_max_v: 4876.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.251976/ 50.935780, val:  47.92%, val_best:  55.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14566  14.878%\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.174423/ 53.384079, val:  56.25%, val_best:  56.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15827  14.697%\n",
      "fc layer 2 self.abs_max_out: 1481.0\n",
      "fc layer 1 self.abs_max_out: 3196.0\n",
      "lif layer 1 self.abs_max_v: 5221.5\n",
      "lif layer 1 self.abs_max_v: 5362.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.814199/ 45.597195, val:  59.58%, val_best:  59.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17066  14.527%\n",
      "fc layer 2 self.abs_max_out: 1495.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.559772/ 47.661720, val:  61.25%, val_best:  61.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7242%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18281  14.364%\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "fc layer 2 self.abs_max_out: 1520.0\n",
      "lif layer 2 self.abs_max_v: 2437.0\n",
      "fc layer 1 self.abs_max_out: 3242.0\n",
      "lif layer 1 self.abs_max_v: 5410.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.361224/ 60.155476, val:  51.67%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19453  14.193%\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "fc layer 2 self.abs_max_out: 1535.0\n",
      "fc layer 2 self.abs_max_out: 1562.0\n",
      "lif layer 2 self.abs_max_v: 2511.5\n",
      "fc layer 1 self.abs_max_out: 3246.0\n",
      "lif layer 1 self.abs_max_v: 5441.5\n",
      "lif layer 1 self.abs_max_v: 5542.5\n",
      "lif layer 1 self.abs_max_v: 5544.5\n",
      "lif layer 1 self.abs_max_v: 5633.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.154511/ 54.673374, val:  47.92%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20631  14.049%\n",
      "fc layer 2 self.abs_max_out: 1570.0\n",
      "fc layer 1 self.abs_max_out: 3266.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.631142/ 63.802738, val:  50.00%, val_best:  61.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8284%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21819  13.929%\n",
      "fc layer 1 self.abs_max_out: 3421.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.415254/ 44.054630, val:  60.42%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22992  13.815%\n",
      "fc layer 1 self.abs_max_out: 3521.0\n",
      "lif layer 1 self.abs_max_v: 5693.5\n",
      "lif layer 1 self.abs_max_v: 5751.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  6.932001/ 62.774921, val:  54.17%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24128  13.692%\n",
      "lif layer 2 self.abs_max_v: 2542.5\n",
      "fc layer 2 self.abs_max_out: 1604.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.650453/ 37.298397, val:  63.33%, val_best:  63.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 25277  13.589%\n",
      "lif layer 2 self.abs_max_v: 2558.0\n",
      "lif layer 2 self.abs_max_v: 2645.0\n",
      "fc layer 2 self.abs_max_out: 1642.0\n",
      "fc layer 1 self.abs_max_out: 3602.0\n",
      "lif layer 1 self.abs_max_v: 5885.5\n",
      "lif layer 1 self.abs_max_v: 6025.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.988354/ 30.587412, val:  74.17%, val_best:  74.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26393  13.480%\n",
      "fc layer 2 self.abs_max_out: 1664.0\n",
      "lif layer 2 self.abs_max_v: 2664.0\n",
      "lif layer 2 self.abs_max_v: 2699.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.721681/ 42.334301, val:  63.33%, val_best:  74.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27458  13.356%\n",
      "fc layer 2 self.abs_max_out: 1707.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.582479/ 55.205677, val:  61.25%, val_best:  74.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28524  13.244%\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "lif layer 2 self.abs_max_v: 2801.5\n",
      "lif layer 2 self.abs_max_v: 2812.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.781205/ 39.500069, val:  63.75%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29578  13.136%\n",
      "fc layer 2 self.abs_max_out: 1806.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.160464/ 37.760429, val:  71.25%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30563  13.008%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.630631/ 41.832817, val:  65.83%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5238%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31602  12.912%\n",
      "fc layer 3 self.abs_max_out: 599.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.498588/ 47.573235, val:  60.00%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0709%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32591  12.804%\n",
      "lif layer 2 self.abs_max_v: 2832.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.462862/ 38.377522, val:  70.42%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33524  12.683%\n",
      "fc layer 1 self.abs_max_out: 3689.0\n",
      "lif layer 1 self.abs_max_v: 6246.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.099038/ 40.106312, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0481%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34472  12.576%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.639133/ 62.273018, val:  57.50%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8794%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35345  12.449%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.785283/ 48.146729, val:  65.00%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 36232  12.336%\n",
      "fc layer 1 self.abs_max_out: 3717.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.667094/ 37.154552, val:  67.50%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7396%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 37132  12.235%\n",
      "fc layer 1 self.abs_max_out: 3735.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.756544/ 42.167480, val:  64.58%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 38041  12.143%\n",
      "fc layer 1 self.abs_max_out: 3830.0\n",
      "lif layer 1 self.abs_max_v: 6415.5\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.060844/ 41.997738, val:  70.42%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0987%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38885  12.036%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.403737/ 39.956867, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 39740  11.939%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.406119/ 46.555099, val:  65.42%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.26 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 40612  11.852%\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "fc layer 3 self.abs_max_out: 637.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.150304/ 40.001801, val:  67.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6077%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 41426  11.754%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  4.995096/ 32.434631, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 42226  11.657%\n",
      "lif layer 1 self.abs_max_v: 6524.5\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.973867/ 30.020958, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7354%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 43007  11.560%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.760745/ 38.027557, val:  77.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.88 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6086%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43776  11.465%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.076473/ 40.712685, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2768%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 44563  11.380%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.890044/ 35.611370, val:  73.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 45327  11.293%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.016734/ 31.862030, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 46104  11.213%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.935237/ 32.569504, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2290%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46895  11.140%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.994304/ 50.935543, val:  61.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47666  11.066%\n",
      "fc layer 1 self.abs_max_out: 3837.0\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.767612/ 43.272953, val:  72.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 48443  10.996%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.436660/ 36.319111, val:  80.83%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 49167  10.918%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.395169/ 38.208820, val:  74.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3803%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49845  10.833%\n",
      "lif layer 2 self.abs_max_v: 2849.5\n",
      "fc layer 2 self.abs_max_out: 1826.0\n",
      "fc layer 2 self.abs_max_out: 1849.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.468044/ 33.814819, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4437%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 50568  10.761%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.613497/ 32.237862, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9291%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 51282  10.690%\n",
      "fc layer 1 self.abs_max_out: 3846.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.186275/ 40.962677, val:  74.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51941  10.611%\n",
      "fc layer 2 self.abs_max_out: 1854.0\n",
      "fc layer 1 self.abs_max_out: 3883.0\n",
      "lif layer 1 self.abs_max_v: 6732.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.576634/ 36.629169, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0507%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52651  10.545%\n",
      "fc layer 2 self.abs_max_out: 1934.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.727920/ 39.113503, val:  74.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 53369  10.483%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.349117/ 29.925219, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 54016  10.410%\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  3.955066/ 37.702484, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54651  10.338%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.003355/ 40.391953, val:  78.33%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2133%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 55262  10.263%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.193747/ 37.206451, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0252%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55884  10.193%\n",
      "fc layer 2 self.abs_max_out: 2026.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  3.984736/ 34.683899, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56515  10.128%\n",
      "fc layer 3 self.abs_max_out: 660.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "fc layer 3 self.abs_max_out: 708.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.988053/ 40.586617, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 57101  10.056%\n",
      "lif layer 2 self.abs_max_v: 2851.5\n",
      "lif layer 2 self.abs_max_v: 2904.0\n",
      "lif layer 2 self.abs_max_v: 3016.5\n",
      "lif layer 2 self.abs_max_v: 3090.0\n",
      "lif layer 2 self.abs_max_v: 3261.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.000330/ 34.513195, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9141%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57719   9.993%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.745613/ 48.118393, val:  72.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58307   9.926%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.798908/ 38.844868, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5220%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58891   9.861%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.127241/ 32.666790, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59505   9.803%\n",
      "fc layer 2 self.abs_max_out: 2094.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.430137/ 39.751858, val:  80.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 60141   9.751%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.894948/ 32.901203, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4392%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60750   9.696%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.936158/ 32.400681, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8143%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61323   9.637%\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "fc layer 1 self.abs_max_out: 4078.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.903288/ 38.274426, val:  77.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1155%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8417%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61934   9.585%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.004426/ 41.215385, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62536   9.534%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.956327/ 44.549629, val:  75.00%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 63124   9.482%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.680922/ 38.679615, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6385%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63680   9.427%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.936018/ 32.060516, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 64263   9.377%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.654348/ 36.798157, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64786   9.321%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.654813/ 32.133392, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65326   9.268%\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.884007/ 38.455040, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65929   9.225%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.361068/ 31.417955, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.53 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.1036%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66460   9.174%\n",
      "fc layer 1 self.abs_max_out: 4127.0\n",
      "fc layer 2 self.abs_max_out: 2212.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.677679/ 43.223366, val:  77.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 67018   9.127%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.789655/ 29.683283, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67578   9.083%\n",
      "fc layer 1 self.abs_max_out: 4304.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.090306/ 33.874039, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 68087   9.032%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.350645/ 33.807312, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68622   8.986%\n",
      "lif layer 1 self.abs_max_v: 6915.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.363916/ 41.422848, val:  79.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 69151   8.941%\n",
      "fc layer 1 self.abs_max_out: 4343.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.234397/ 37.727638, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69642   8.892%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  2.996636/ 38.137020, val:  80.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 70118   8.842%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.966181/ 35.702156, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1173%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70608   8.795%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.171998/ 47.296066, val:  76.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8718%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 71091   8.749%\n",
      "fc layer 1 self.abs_max_out: 4369.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.149698/ 44.135384, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71572   8.703%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.030164/ 34.379086, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7038%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 72059   8.659%\n",
      "fc layer 1 self.abs_max_out: 4378.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.382764/ 33.024197, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72552   8.617%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.936870/ 37.697235, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 73032   8.575%\n",
      "fc layer 1 self.abs_max_out: 4466.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.254976/ 38.537270, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73518   8.534%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.843897/ 35.247688, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73954   8.488%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.862081/ 47.515999, val:  77.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74403   8.444%\n",
      "fc layer 3 self.abs_max_out: 734.0\n",
      "lif layer 1 self.abs_max_v: 6975.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.134788/ 36.680069, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74885   8.406%\n",
      "fc layer 2 self.abs_max_out: 2270.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.271072/ 35.617786, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1155%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75377   8.369%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.920264/ 33.342442, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75832   8.329%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.670843/ 37.470665, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0491%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 76225   8.283%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.669382/ 47.703457, val:  80.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76650   8.241%\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.722837/ 34.974392, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0329%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 77083   8.202%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.238281/ 33.796661, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77560   8.167%\n",
      "fc layer 2 self.abs_max_out: 2303.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.885711/ 35.750160, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77992   8.129%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.608944/ 38.289555, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1041%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78379   8.087%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.965803/ 37.792973, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78841   8.053%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.822848/ 35.722198, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 79253   8.015%\n",
      "fc layer 1 self.abs_max_out: 4488.0\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.455058/ 37.015362, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79644   7.976%\n",
      "lif layer 1 self.abs_max_v: 7001.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.927997/ 35.572845, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8988%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 80092   7.943%\n",
      "lif layer 1 self.abs_max_v: 7155.0\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.788615/ 38.159580, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80524   7.909%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.624597/ 40.126362, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.00 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80922   7.872%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.960362/ 33.826443, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 81346   7.839%\n",
      "fc layer 3 self.abs_max_out: 770.0\n",
      "fc layer 3 self.abs_max_out: 775.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.496878/ 34.933647, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81736   7.803%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.425273/ 39.514824, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 82126   7.767%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.389831/ 38.100449, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1354%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82510   7.732%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.675490/ 36.033096, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0353%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82895   7.698%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.381230/ 54.037014, val:  79.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 83287   7.664%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.382410/ 35.732491, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0058%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9932%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83662   7.630%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.467669/ 44.168972, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 84043   7.597%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.568224/ 43.234089, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84449   7.567%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.652103/ 36.336208, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84853   7.537%\n",
      "fc layer 3 self.abs_max_out: 784.0\n",
      "fc layer 3 self.abs_max_out: 796.0\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.413863/ 43.831478, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1535%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 85235   7.505%\n",
      "fc layer 1 self.abs_max_out: 4628.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.472290/ 38.005302, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0497%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85638   7.476%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.137441/ 46.695881, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85975   7.442%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.029942/ 36.774418, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 86310   7.409%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.145807/ 40.219280, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86619   7.373%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.187491/ 37.666805, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1741%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86961   7.341%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.295582/ 38.714699, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0999%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 87296   7.309%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.293147/ 40.551743, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0549%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87666   7.280%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.335724/ 37.448578, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4339%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9252%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 88043   7.253%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.192982/ 38.263020, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0895%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 88408   7.224%\n",
      "lif layer 1 self.abs_max_v: 7165.5\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.553469/ 38.817383, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0470%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88801   7.199%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  1.842595/ 36.743294, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 89109   7.167%\n",
      "lif layer 1 self.abs_max_v: 7247.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.114282/ 36.836903, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 89439   7.137%\n",
      "lif layer 1 self.abs_max_v: 7409.5\n",
      "lif layer 1 self.abs_max_v: 7489.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.092967/ 41.024124, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0551%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89767   7.108%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.030869/ 42.344528, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 90085   7.078%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.048916/ 40.362270, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 90431   7.051%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.027285/ 41.383503, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0964%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90767   7.024%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.020468/ 39.087517, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 91071   6.994%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.301272/ 37.832684, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 91417   6.969%\n",
      "fc layer 3 self.abs_max_out: 820.0\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.117362/ 38.314320, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91730   6.941%\n",
      "fc layer 1 self.abs_max_out: 4666.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.721066/ 42.416466, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1103%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 92014   6.911%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.082226/ 37.709087, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 92326   6.884%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.812444/ 39.550232, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92631   6.856%\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.115285/ 49.470818, val:  76.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92933   6.829%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.183590/ 37.021343, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 93267   6.805%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.992895/ 37.210888, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93593   6.780%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.931338/ 43.683510, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9850%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93899   6.754%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.813320/ 49.841297, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.00 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 94193   6.728%\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.908027/ 42.905815, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 94494   6.703%\n",
      "fc layer 1 self.abs_max_out: 4761.0\n",
      "fc layer 3 self.abs_max_out: 848.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.325637/ 37.648815, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94819   6.680%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.995002/ 40.336685, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4650%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 95142   6.656%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.919405/ 38.386147, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 95443   6.632%\n",
      "fc layer 3 self.abs_max_out: 851.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.820332/ 43.184475, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95735   6.607%\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.075188/ 41.557838, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 96067   6.586%\n",
      "fc layer 2 self.abs_max_out: 2438.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.766723/ 42.995220, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 96351   6.561%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.865880/ 45.967457, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 96642   6.537%\n",
      "fc layer 1 self.abs_max_out: 4770.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.768206/ 49.058128, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9700%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0002%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96922   6.513%\n",
      "fc layer 1 self.abs_max_out: 4775.0\n",
      "fc layer 2 self.abs_max_out: 2471.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.028950/ 47.502090, val:  85.42%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 97223   6.491%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.867381/ 47.946156, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 97528   6.469%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.600195/ 44.086800, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.26 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97786   6.444%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.797488/ 41.683788, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 98048   6.420%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.934617/ 44.867672, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 98337   6.398%\n",
      "fc layer 1 self.abs_max_out: 4824.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.842601/ 42.022072, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 98629   6.376%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.539160/ 39.095032, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1949%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98878   6.352%\n",
      "lif layer 2 self.abs_max_v: 3335.5\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.625696/ 42.040100, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 99146   6.330%\n",
      "fc layer 2 self.abs_max_out: 2479.0\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.629986/ 40.325310, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1361%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 99411   6.307%\n",
      "fc layer 2 self.abs_max_out: 2512.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.829987/ 50.515022, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 99683   6.285%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.703623/ 40.620193, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99954   6.264%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.753305/ 41.949310, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1423%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 100213   6.242%\n",
      "fc layer 2 self.abs_max_out: 2533.0\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.586119/ 42.782394, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 100469   6.220%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.459033/ 42.851803, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 100702   6.197%\n",
      "fc layer 3 self.abs_max_out: 874.0\n",
      "fc layer 3 self.abs_max_out: 897.0\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.489714/ 41.216194, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0358%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100935   6.174%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.611456/ 46.710175, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 101181   6.152%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.495013/ 43.885319, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9704%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 101409   6.129%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.683239/ 43.221283, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 101667   6.109%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.544857/ 39.857925, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3033%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101891   6.086%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.570675/ 43.350159, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 102140   6.066%\n",
      "fc layer 3 self.abs_max_out: 914.0\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "fc layer 3 self.abs_max_out: 926.0\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.371395/ 38.688068, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 102356   6.043%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.410022/ 39.196037, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1972%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 102595   6.023%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.393661/ 39.701035, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102813   6.001%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.320789/ 41.259167, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 103031   5.980%\n",
      "fc layer 1 self.abs_max_out: 4887.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.676562/ 43.059494, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3255%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 103278   5.960%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.683965/ 42.895226, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 103525   5.941%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.551427/ 40.806793, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0475%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2080%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103767   5.921%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.248735/ 40.797802, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0710%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 103973   5.900%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.668730/ 39.631073, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 104222   5.882%\n",
      "fc layer 1 self.abs_max_out: 4961.0\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.625303/ 41.984707, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0532%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9449%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 104478   5.864%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.313319/ 37.644833, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1044%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9038%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 104693   5.844%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.041394/ 41.805832, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104889   5.823%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.266535/ 43.572025, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 105112   5.804%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.328588/ 40.644802, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1081%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 105320   5.784%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.226398/ 45.153561, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 105516   5.764%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.280753/ 39.725399, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0946%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 105721   5.744%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.466862/ 46.894817, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 105950   5.726%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.375911/ 43.794590, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 106170   5.708%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.262312/ 41.283539, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0391%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 106384   5.689%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.344983/ 42.441135, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 106598   5.671%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.192313/ 40.389866, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106805   5.653%\n",
      "fc layer 2 self.abs_max_out: 2581.0\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.369994/ 41.468403, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3686%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 107015   5.635%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.343100/ 40.104069, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 107227   5.617%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.220120/ 42.242069, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 107442   5.599%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.166477/ 39.032043, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1207%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5565%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 107642   5.581%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.396699/ 46.960373, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8385%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107852   5.564%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.454230/ 40.924385, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 108092   5.548%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.384330/ 42.219761, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1068%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7677%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02192c4c834943cca38a4681e233f314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñá‚ñà‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.38433</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>42.21976</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-sweep-65</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iephv5yz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iephv5yz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_084444-iephv5yz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8eguv8au with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 39201\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_125049-8eguv8au</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eguv8au' target=\"_blank\">fearless-sweep-72</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eguv8au' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eguv8au</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_125058_268', 'my_seed': 39201, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 354.0\n",
      "lif layer 1 self.abs_max_v: 354.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 333.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 96.0\n",
      "fc layer 1 self.abs_max_out: 358.0\n",
      "lif layer 1 self.abs_max_v: 432.5\n",
      "lif layer 2 self.abs_max_v: 403.5\n",
      "fc layer 3 self.abs_max_out: 161.0\n",
      "fc layer 1 self.abs_max_out: 512.0\n",
      "lif layer 1 self.abs_max_v: 512.0\n",
      "lif layer 1 self.abs_max_v: 599.0\n",
      "lif layer 2 self.abs_max_v: 439.5\n",
      "fc layer 2 self.abs_max_out: 501.0\n",
      "lif layer 2 self.abs_max_v: 721.0\n",
      "lif layer 1 self.abs_max_v: 613.0\n",
      "fc layer 1 self.abs_max_out: 522.0\n",
      "fc layer 1 self.abs_max_out: 594.0\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "lif layer 2 self.abs_max_v: 812.5\n",
      "fc layer 2 self.abs_max_out: 503.0\n",
      "lif layer 2 self.abs_max_v: 883.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 2 self.abs_max_out: 582.0\n",
      "lif layer 2 self.abs_max_v: 961.0\n",
      "fc layer 2 self.abs_max_out: 583.0\n",
      "fc layer 1 self.abs_max_out: 599.0\n",
      "fc layer 1 self.abs_max_out: 603.0\n",
      "lif layer 1 self.abs_max_v: 623.5\n",
      "fc layer 1 self.abs_max_out: 723.0\n",
      "lif layer 1 self.abs_max_v: 723.0\n",
      "lif layer 2 self.abs_max_v: 984.5\n",
      "lif layer 1 self.abs_max_v: 766.5\n",
      "lif layer 1 self.abs_max_v: 788.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "lif layer 1 self.abs_max_v: 872.0\n",
      "fc layer 1 self.abs_max_out: 758.0\n",
      "fc layer 1 self.abs_max_out: 906.0\n",
      "lif layer 1 self.abs_max_v: 906.0\n",
      "fc layer 1 self.abs_max_out: 999.0\n",
      "lif layer 1 self.abs_max_v: 1054.0\n",
      "fc layer 2 self.abs_max_out: 626.0\n",
      "lif layer 1 self.abs_max_v: 1057.5\n",
      "lif layer 1 self.abs_max_v: 1097.0\n",
      "fc layer 3 self.abs_max_out: 283.0\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "fc layer 2 self.abs_max_out: 645.0\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "lif layer 1 self.abs_max_v: 1152.5\n",
      "lif layer 1 self.abs_max_v: 1184.0\n",
      "lif layer 1 self.abs_max_v: 1243.5\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "lif layer 2 self.abs_max_v: 1008.5\n",
      "fc layer 1 self.abs_max_out: 1026.0\n",
      "fc layer 2 self.abs_max_out: 721.0\n",
      "fc layer 2 self.abs_max_out: 810.0\n",
      "fc layer 3 self.abs_max_out: 288.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "lif layer 1 self.abs_max_v: 1278.0\n",
      "lif layer 2 self.abs_max_v: 1035.0\n",
      "lif layer 2 self.abs_max_v: 1035.5\n",
      "lif layer 2 self.abs_max_v: 1036.5\n",
      "lif layer 1 self.abs_max_v: 1308.5\n",
      "lif layer 1 self.abs_max_v: 1346.5\n",
      "lif layer 1 self.abs_max_v: 1505.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "fc layer 1 self.abs_max_out: 1053.0\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "fc layer 1 self.abs_max_out: 1054.0\n",
      "lif layer 2 self.abs_max_v: 1052.5\n",
      "lif layer 2 self.abs_max_v: 1117.5\n",
      "lif layer 2 self.abs_max_v: 1133.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "fc layer 1 self.abs_max_out: 1055.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 1 self.abs_max_out: 1092.0\n",
      "lif layer 2 self.abs_max_v: 1135.5\n",
      "lif layer 2 self.abs_max_v: 1162.5\n",
      "fc layer 1 self.abs_max_out: 1121.0\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "fc layer 1 self.abs_max_out: 1252.0\n",
      "lif layer 1 self.abs_max_v: 1510.0\n",
      "lif layer 1 self.abs_max_v: 1557.5\n",
      "lif layer 1 self.abs_max_v: 1611.5\n",
      "lif layer 1 self.abs_max_v: 1860.0\n",
      "lif layer 1 self.abs_max_v: 1887.0\n",
      "lif layer 1 self.abs_max_v: 1893.5\n",
      "lif layer 1 self.abs_max_v: 1982.5\n",
      "lif layer 1 self.abs_max_v: 2168.5\n",
      "lif layer 2 self.abs_max_v: 1183.0\n",
      "fc layer 1 self.abs_max_out: 1260.0\n",
      "fc layer 1 self.abs_max_out: 1403.0\n",
      "fc layer 2 self.abs_max_out: 899.0\n",
      "lif layer 2 self.abs_max_v: 1192.0\n",
      "lif layer 2 self.abs_max_v: 1217.5\n",
      "fc layer 2 self.abs_max_out: 905.0\n",
      "fc layer 1 self.abs_max_out: 1416.0\n",
      "fc layer 2 self.abs_max_out: 907.0\n",
      "fc layer 2 self.abs_max_out: 910.0\n",
      "fc layer 2 self.abs_max_out: 950.0\n",
      "fc layer 2 self.abs_max_out: 971.0\n",
      "fc layer 2 self.abs_max_out: 1002.0\n",
      "lif layer 2 self.abs_max_v: 1218.5\n",
      "fc layer 3 self.abs_max_out: 354.0\n",
      "fc layer 2 self.abs_max_out: 1012.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "fc layer 2 self.abs_max_out: 1023.0\n",
      "lif layer 2 self.abs_max_v: 1233.0\n",
      "lif layer 2 self.abs_max_v: 1264.5\n",
      "lif layer 2 self.abs_max_v: 1279.5\n",
      "fc layer 2 self.abs_max_out: 1043.0\n",
      "lif layer 2 self.abs_max_v: 1294.5\n",
      "lif layer 2 self.abs_max_v: 1295.5\n",
      "lif layer 2 self.abs_max_v: 1312.5\n",
      "lif layer 2 self.abs_max_v: 1344.5\n",
      "lif layer 2 self.abs_max_v: 1352.5\n",
      "lif layer 2 self.abs_max_v: 1400.5\n",
      "fc layer 1 self.abs_max_out: 1449.0\n",
      "lif layer 2 self.abs_max_v: 1403.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 1 self.abs_max_out: 1508.0\n",
      "fc layer 2 self.abs_max_out: 1053.0\n",
      "fc layer 1 self.abs_max_out: 1656.0\n",
      "lif layer 2 self.abs_max_v: 1485.0\n",
      "fc layer 2 self.abs_max_out: 1070.0\n",
      "lif layer 1 self.abs_max_v: 2416.0\n",
      "fc layer 2 self.abs_max_out: 1082.0\n",
      "fc layer 2 self.abs_max_out: 1126.0\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "lif layer 2 self.abs_max_v: 1518.0\n",
      "lif layer 1 self.abs_max_v: 2416.5\n",
      "lif layer 1 self.abs_max_v: 2523.5\n",
      "lif layer 2 self.abs_max_v: 1602.5\n",
      "lif layer 2 self.abs_max_v: 1662.5\n",
      "lif layer 1 self.abs_max_v: 2629.5\n",
      "lif layer 1 self.abs_max_v: 2687.0\n",
      "lif layer 1 self.abs_max_v: 2839.0\n",
      "lif layer 1 self.abs_max_v: 2992.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 15.678172/ 93.433220, val:  37.50%, val_best:  37.50%, tr:  97.04%, tr_best:  97.04%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0448%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2213  22.605%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 1 self.abs_max_out: 1821.0\n",
      "fc layer 2 self.abs_max_out: 1309.0\n",
      "fc layer 2 self.abs_max_out: 1402.0\n",
      "fc layer 2 self.abs_max_out: 1404.0\n",
      "lif layer 2 self.abs_max_v: 1730.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 1 self.abs_max_out: 1874.0\n",
      "lif layer 1 self.abs_max_v: 3111.0\n",
      "lif layer 1 self.abs_max_v: 3209.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 11.640830/ 51.558590, val:  52.92%, val_best:  52.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1253%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3771  19.259%\n",
      "lif layer 2 self.abs_max_v: 1766.0\n",
      "lif layer 2 self.abs_max_v: 1791.5\n",
      "lif layer 2 self.abs_max_v: 1844.0\n",
      "lif layer 2 self.abs_max_v: 1907.0\n",
      "lif layer 2 self.abs_max_v: 1927.5\n",
      "lif layer 2 self.abs_max_v: 2002.5\n",
      "fc layer 1 self.abs_max_out: 1908.0\n",
      "lif layer 2 self.abs_max_v: 2018.0\n",
      "lif layer 2 self.abs_max_v: 2063.0\n",
      "lif layer 2 self.abs_max_v: 2069.5\n",
      "lif layer 2 self.abs_max_v: 2140.5\n",
      "fc layer 1 self.abs_max_out: 1960.0\n",
      "fc layer 1 self.abs_max_out: 2113.0\n",
      "lif layer 1 self.abs_max_v: 3501.5\n",
      "lif layer 1 self.abs_max_v: 3567.0\n",
      "lif layer 1 self.abs_max_v: 3598.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 11.183852/ 87.802673, val:  30.42%, val_best:  52.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.06 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5291%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3636%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5246  17.862%\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "fc layer 2 self.abs_max_out: 1405.0\n",
      "fc layer 1 self.abs_max_out: 2184.0\n",
      "fc layer 1 self.abs_max_out: 2385.0\n",
      "lif layer 1 self.abs_max_v: 3934.0\n",
      "lif layer 1 self.abs_max_v: 4074.0\n",
      "fc layer 2 self.abs_max_out: 1421.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 11.342298/ 76.560745, val:  43.33%, val_best:  52.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6651  16.984%\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.065078/ 61.530994, val:  50.00%, val_best:  52.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2778%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8992%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7938  16.217%\n",
      "lif layer 2 self.abs_max_v: 2156.0\n",
      "lif layer 2 self.abs_max_v: 2198.0\n",
      "lif layer 2 self.abs_max_v: 2222.5\n",
      "lif layer 2 self.abs_max_v: 2272.5\n",
      "lif layer 2 self.abs_max_v: 2299.5\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "fc layer 1 self.abs_max_out: 2621.0\n",
      "lif layer 1 self.abs_max_v: 4284.5\n",
      "lif layer 1 self.abs_max_v: 4334.5\n",
      "lif layer 1 self.abs_max_v: 4361.5\n",
      "lif layer 1 self.abs_max_v: 4461.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.939399/ 93.930374, val:  41.25%, val_best:  52.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1298%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5805%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9225  15.705%\n",
      "fc layer 1 self.abs_max_out: 2681.0\n",
      "lif layer 2 self.abs_max_v: 2328.0\n",
      "lif layer 2 self.abs_max_v: 2381.5\n",
      "lif layer 2 self.abs_max_v: 2417.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.313144/ 51.790104, val:  52.92%, val_best:  52.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7257%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10506  15.331%\n",
      "fc layer 2 self.abs_max_out: 1426.0\n",
      "lif layer 2 self.abs_max_v: 2436.5\n",
      "lif layer 2 self.abs_max_v: 2593.0\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "lif layer 2 self.abs_max_v: 2615.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.705568/ 55.286041, val:  51.67%, val_best:  52.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8376%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11777  15.037%\n",
      "fc layer 2 self.abs_max_out: 1483.0\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "fc layer 3 self.abs_max_out: 624.0\n",
      "fc layer 2 self.abs_max_out: 1503.0\n",
      "lif layer 2 self.abs_max_v: 2732.5\n",
      "fc layer 2 self.abs_max_out: 1537.0\n",
      "fc layer 3 self.abs_max_out: 646.0\n",
      "fc layer 1 self.abs_max_out: 2720.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.319216/ 61.665092, val:  55.42%, val_best:  55.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0418%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13033  14.792%\n",
      "lif layer 1 self.abs_max_v: 4490.5\n",
      "fc layer 1 self.abs_max_out: 2888.0\n",
      "lif layer 1 self.abs_max_v: 4666.5\n",
      "lif layer 1 self.abs_max_v: 4723.5\n",
      "fc layer 2 self.abs_max_out: 1541.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  9.345210/ 70.619865, val:  51.25%, val_best:  55.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14263  14.569%\n",
      "lif layer 2 self.abs_max_v: 2764.0\n",
      "fc layer 2 self.abs_max_out: 1573.0\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "lif layer 2 self.abs_max_v: 2790.5\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 2 self.abs_max_out: 1664.0\n",
      "fc layer 1 self.abs_max_out: 2895.0\n",
      "lif layer 1 self.abs_max_v: 4749.0\n",
      "lif layer 2 self.abs_max_v: 2812.0\n",
      "lif layer 2 self.abs_max_v: 2884.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.944450/ 43.333469, val:  61.25%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15414  14.313%\n",
      "lif layer 2 self.abs_max_v: 2899.5\n",
      "lif layer 2 self.abs_max_v: 2920.0\n",
      "lif layer 2 self.abs_max_v: 2928.0\n",
      "lif layer 2 self.abs_max_v: 3022.0\n",
      "lif layer 1 self.abs_max_v: 4859.0\n",
      "fc layer 2 self.abs_max_out: 1674.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  9.308393/ 77.085144, val:  52.92%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0366%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16568  14.103%\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "fc layer 1 self.abs_max_out: 2912.0\n",
      "lif layer 1 self.abs_max_v: 4881.0\n",
      "lif layer 1 self.abs_max_v: 4968.5\n",
      "lif layer 1 self.abs_max_v: 4975.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.302165/ 55.764675, val:  52.50%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5711%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17652  13.870%\n",
      "fc layer 2 self.abs_max_out: 1700.0\n",
      "fc layer 2 self.abs_max_out: 1707.0\n",
      "fc layer 1 self.abs_max_out: 2979.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.689201/ 68.570068, val:  47.08%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18759  13.687%\n",
      "fc layer 2 self.abs_max_out: 1710.0\n",
      "fc layer 2 self.abs_max_out: 1728.0\n",
      "fc layer 2 self.abs_max_out: 1766.0\n",
      "fc layer 1 self.abs_max_out: 3055.0\n",
      "lif layer 1 self.abs_max_v: 5080.5\n",
      "lif layer 1 self.abs_max_v: 5123.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.632840/ 61.035912, val:  54.17%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19929  13.571%\n",
      "fc layer 3 self.abs_max_out: 718.0\n",
      "fc layer 2 self.abs_max_out: 1774.0\n",
      "fc layer 1 self.abs_max_out: 3162.0\n",
      "lif layer 1 self.abs_max_v: 5177.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  8.283218/ 50.486832, val:  59.17%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.15 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21085  13.461%\n",
      "lif layer 2 self.abs_max_v: 3052.5\n",
      "lif layer 1 self.abs_max_v: 5204.5\n",
      "fc layer 2 self.abs_max_out: 1824.0\n",
      "lif layer 1 self.abs_max_v: 5283.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.303680/ 67.118416, val:  50.42%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22189  13.332%\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  8.447145/ 48.762836, val:  60.42%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23332  13.240%\n",
      "fc layer 1 self.abs_max_out: 3343.0\n",
      "lif layer 1 self.abs_max_v: 5631.0\n",
      "lif layer 1 self.abs_max_v: 5834.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  8.084458/ 51.334061, val:  55.83%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0361%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6195%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24417  13.127%\n",
      "lif layer 2 self.abs_max_v: 3147.0\n",
      "fc layer 2 self.abs_max_out: 1828.0\n",
      "lif layer 2 self.abs_max_v: 3248.0\n",
      "fc layer 2 self.abs_max_out: 1854.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.470856/ 43.466778, val:  65.42%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.26 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3182%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1835%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25457  13.002%\n",
      "fc layer 2 self.abs_max_out: 1889.0\n",
      "lif layer 2 self.abs_max_v: 3280.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  8.001274/ 54.642376, val:  63.75%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26545  12.912%\n",
      "fc layer 2 self.abs_max_out: 1898.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.304963/ 41.423241, val:  71.67%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27528  12.781%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.664844/ 40.927113, val:  63.75%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2885%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28564  12.686%\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.393538/ 52.153721, val:  67.08%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29553  12.578%\n",
      "fc layer 2 self.abs_max_out: 1909.0\n",
      "fc layer 2 self.abs_max_out: 1985.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.391636/ 47.822025, val:  67.92%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2997%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30562  12.487%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  7.023499/ 45.086102, val:  66.67%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31566  12.401%\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.894634/ 57.279343, val:  50.83%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32530  12.307%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.674216/ 38.271313, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33448  12.202%\n",
      "lif layer 2 self.abs_max_v: 3314.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  7.324839/ 43.148602, val:  72.08%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34414  12.121%\n",
      "fc layer 2 self.abs_max_out: 1992.0\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "fc layer 1 self.abs_max_out: 3403.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.787704/ 51.062084, val:  61.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35367  12.042%\n",
      "lif layer 2 self.abs_max_v: 3314.5\n",
      "lif layer 2 self.abs_max_v: 3318.5\n",
      "lif layer 2 self.abs_max_v: 3334.0\n",
      "lif layer 2 self.abs_max_v: 3377.0\n",
      "lif layer 2 self.abs_max_v: 3466.5\n",
      "lif layer 2 self.abs_max_v: 3483.0\n",
      "fc layer 3 self.abs_max_out: 777.0\n",
      "fc layer 2 self.abs_max_out: 1997.0\n",
      "lif layer 2 self.abs_max_v: 3605.0\n",
      "lif layer 1 self.abs_max_v: 6000.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.960766/ 57.300442, val:  63.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8663%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36292  11.958%\n",
      "fc layer 2 self.abs_max_out: 2018.0\n",
      "fc layer 1 self.abs_max_out: 3471.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.525933/ 47.594612, val:  74.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37179  11.868%\n",
      "fc layer 2 self.abs_max_out: 2029.0\n",
      "fc layer 1 self.abs_max_out: 3493.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.124609/ 34.829556, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38006  11.764%\n",
      "lif layer 1 self.abs_max_v: 6025.0\n",
      "fc layer 1 self.abs_max_out: 3511.0\n",
      "lif layer 1 self.abs_max_v: 6220.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.228792/ 36.256294, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38863  11.675%\n",
      "fc layer 2 self.abs_max_out: 2043.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  6.185121/ 49.239292, val:  72.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39710  11.589%\n",
      "fc layer 2 self.abs_max_out: 2104.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  6.445844/ 56.395050, val:  69.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4060%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40565  11.510%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  6.433644/ 40.491337, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41432  11.438%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  6.325777/ 45.773247, val:  72.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42297  11.370%\n",
      "fc layer 3 self.abs_max_out: 779.0\n",
      "fc layer 3 self.abs_max_out: 815.0\n",
      "fc layer 1 self.abs_max_out: 3523.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.920267/ 35.736698, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43116  11.293%\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.757920/ 46.578083, val:  74.17%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43884  11.206%\n",
      "fc layer 1 self.abs_max_out: 3563.0\n",
      "lif layer 1 self.abs_max_v: 6312.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  6.235671/ 48.341175, val:  66.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.85 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44713  11.140%\n",
      "fc layer 2 self.abs_max_out: 2182.0\n",
      "fc layer 1 self.abs_max_out: 3604.0\n",
      "fc layer 1 self.abs_max_out: 3665.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.753006/ 39.862324, val:  80.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 68.66 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1300%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45487  11.063%\n",
      "fc layer 1 self.abs_max_out: 3676.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.592391/ 40.681805, val:  77.50%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46242  10.985%\n",
      "fc layer 1 self.abs_max_out: 3681.0\n",
      "lif layer 1 self.abs_max_v: 6502.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.468072/ 52.581059, val:  67.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0364%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46997  10.910%\n",
      "fc layer 2 self.abs_max_out: 2190.0\n",
      "fc layer 2 self.abs_max_out: 2245.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.994380/ 54.766216, val:  74.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47704  10.828%\n",
      "fc layer 2 self.abs_max_out: 2251.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.922660/ 39.113617, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48452  10.759%\n",
      "fc layer 3 self.abs_max_out: 869.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.738965/ 30.630960, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49209  10.695%\n",
      "lif layer 1 self.abs_max_v: 6510.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.729026/ 37.587200, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1073%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49985  10.637%\n",
      "fc layer 1 self.abs_max_out: 3765.0\n",
      "lif layer 1 self.abs_max_v: 6532.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.246212/ 44.274452, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50707  10.570%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.401502/ 56.564381, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51425  10.506%\n",
      "fc layer 1 self.abs_max_out: 3798.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.026886/ 39.853115, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4752%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52106  10.436%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.193318/ 46.927040, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52812  10.374%\n",
      "fc layer 1 self.abs_max_out: 3845.0\n",
      "lif layer 1 self.abs_max_v: 6632.5\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.899484/ 48.767300, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53460  10.303%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.346787/ 49.938396, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1807%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54158  10.244%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  5.248541/ 44.078285, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54858  10.188%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.944852/ 37.627235, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6060%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55543  10.131%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.659050/ 42.628956, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56189  10.069%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.823208/ 36.027866, val:  82.08%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56802  10.004%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.486012/ 47.351864, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1048%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57408   9.939%\n",
      "fc layer 1 self.abs_max_out: 3910.0\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "lif layer 1 self.abs_max_v: 6645.5\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.828194/ 48.869804, val:  72.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3704%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58083   9.888%\n",
      "fc layer 2 self.abs_max_out: 2255.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.305042/ 44.841789, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0470%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58678   9.826%\n",
      "fc layer 2 self.abs_max_out: 2262.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.433887/ 67.342621, val:  64.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8805%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59273   9.765%\n",
      "fc layer 1 self.abs_max_out: 4004.0\n",
      "lif layer 1 self.abs_max_v: 6945.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.705870/ 43.222321, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.30 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0964%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59870   9.707%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.643797/ 50.502090, val:  73.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60490   9.654%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.283247/ 39.351791, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61078   9.598%\n",
      "fc layer 2 self.abs_max_out: 2273.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.413893/ 36.917919, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1230%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3592%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61679   9.546%\n",
      "fc layer 2 self.abs_max_out: 2289.0\n",
      "fc layer 1 self.abs_max_out: 4018.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.207153/ 45.829094, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0365%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62264   9.492%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.192013/ 41.095882, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62825   9.437%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.046082/ 45.293655, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63381   9.383%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.177518/ 34.561707, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63953   9.332%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.475049/ 36.189178, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5928%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64543   9.286%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.899703/ 40.514156, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.05 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6069%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3163%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65098   9.235%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  4.202903/ 39.495838, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65648   9.186%\n",
      "fc layer 1 self.abs_max_out: 4121.0\n",
      "lif layer 1 self.abs_max_v: 7124.0\n",
      "lif layer 1 self.abs_max_v: 7196.5\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.814627/ 41.904552, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2131%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66207   9.139%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.888411/ 35.869652, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5230%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66731   9.088%\n",
      "fc layer 2 self.abs_max_out: 2290.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  4.054035/ 37.437004, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7408%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67296   9.045%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.772510/ 39.036335, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67822   8.997%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.258673/ 48.454384, val:  77.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68389   8.956%\n",
      "fc layer 2 self.abs_max_out: 2301.0\n",
      "fc layer 2 self.abs_max_out: 2432.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.649734/ 34.245464, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0331%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68903   8.909%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.832581/ 44.014050, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69450   8.867%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.570676/ 39.391029, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69960   8.822%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.894797/ 43.440544, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0581%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70501   8.782%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.332190/ 41.913513, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0416%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8319%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70976   8.735%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.443336/ 41.284153, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0567%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71477   8.692%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.681293/ 49.797348, val:  76.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71978   8.650%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.952021/ 39.274338, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8196%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4435%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72503   8.611%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.011392/ 34.518417, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72954   8.565%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.584756/ 53.222050, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73450   8.526%\n",
      "fc layer 3 self.abs_max_out: 893.0\n",
      "fc layer 3 self.abs_max_out: 896.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.703523/ 38.747498, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73934   8.485%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.122983/ 41.027138, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74379   8.442%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.550376/ 45.014690, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74865   8.403%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.168830/ 49.272572, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7060%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75312   8.362%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.244248/ 40.856102, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75760   8.321%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.171589/ 40.372623, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1021%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 76209   8.281%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.859504/ 50.358643, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76611   8.237%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.265037/ 37.711620, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9446%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 77066   8.200%\n",
      "fc layer 2 self.abs_max_out: 2446.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.517522/ 39.312511, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0958%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77524   8.164%\n",
      "fc layer 3 self.abs_max_out: 937.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.049965/ 39.689369, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77974   8.127%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.156753/ 39.738625, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1787%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78423   8.091%\n",
      "fc layer 1 self.abs_max_out: 4137.0\n",
      "lif layer 1 self.abs_max_v: 7288.5\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.029361/ 44.786793, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0320%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2118%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78851   8.054%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.744536/ 38.115711, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3994%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 79264   8.016%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.922851/ 38.661060, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79682   7.980%\n",
      "fc layer 2 self.abs_max_out: 2457.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.913692/ 49.058128, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 80094   7.943%\n",
      "fc layer 3 self.abs_max_out: 950.0\n",
      "fc layer 1 self.abs_max_out: 4258.0\n",
      "lif layer 1 self.abs_max_v: 7495.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.055748/ 44.234955, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80531   7.909%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.885804/ 42.877911, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80939   7.874%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.837527/ 33.320847, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 81340   7.838%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.998054/ 44.505657, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81765   7.806%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.031970/ 38.065792, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 82166   7.771%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.708221/ 47.796104, val:  76.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82564   7.737%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  3.215903/ 43.186741, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 83007   7.708%\n",
      "fc layer 1 self.abs_max_out: 4284.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.843659/ 43.073124, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 83424   7.677%\n",
      "fc layer 2 self.abs_max_out: 2466.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.939172/ 35.453754, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9483%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83832   7.646%\n",
      "fc layer 2 self.abs_max_out: 2474.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.657573/ 46.820183, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 84226   7.614%\n",
      "fc layer 1 self.abs_max_out: 4305.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.547611/ 44.909740, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8937%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84591   7.579%\n",
      "fc layer 3 self.abs_max_out: 959.0\n",
      "fc layer 2 self.abs_max_out: 2484.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.430111/ 43.338806, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84965   7.547%\n",
      "fc layer 3 self.abs_max_out: 971.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.726082/ 48.964634, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 85318   7.513%\n",
      "fc layer 2 self.abs_max_out: 2503.0\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.672133/ 40.217625, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85683   7.480%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.536326/ 40.902828, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2309%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 86054   7.449%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.422819/ 45.050205, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5173%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 86421   7.418%\n",
      "fc layer 2 self.abs_max_out: 2531.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.433509/ 43.572262, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86765   7.386%\n",
      "fc layer 3 self.abs_max_out: 997.0\n",
      "fc layer 3 self.abs_max_out: 1030.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.343631/ 42.084843, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1017%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 87108   7.353%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.453379/ 58.831669, val:  75.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 87454   7.322%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.335025/ 48.546940, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2505%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87796   7.291%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.698036/ 43.892128, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 88151   7.261%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.377571/ 38.170547, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0166%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 88485   7.231%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.425526/ 42.680237, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88848   7.203%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.317896/ 38.989124, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 89153   7.171%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.139875/ 43.239223, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 89462   7.139%\n",
      "fc layer 1 self.abs_max_out: 4365.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.194465/ 39.617897, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89774   7.109%\n",
      "fc layer 1 self.abs_max_out: 4403.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.305367/ 35.303822, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 90108   7.080%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.097623/ 44.251045, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2480%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 90419   7.050%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.406698/ 55.906883, val:  77.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3663%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90745   7.022%\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.023069/ 43.943054, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 91054   6.993%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.517628/ 43.148365, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 91358   6.964%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.981678/ 43.570854, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91656   6.935%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.379313/ 39.869671, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91997   6.910%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.124001/ 51.801266, val:  80.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6251%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 92314   6.883%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.374192/ 44.483223, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92638   6.857%\n",
      "fc layer 1 self.abs_max_out: 4427.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.168252/ 43.138527, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92948   6.830%\n",
      "fc layer 1 self.abs_max_out: 4481.0\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.933076/ 40.045406, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 93247   6.803%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.536147/ 43.368942, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93589   6.780%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.314089/ 41.126186, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93905   6.755%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.097858/ 42.948555, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 94209   6.729%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.051147/ 46.087143, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0205%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 94519   6.705%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.192101/ 40.973354, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9850%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94837   6.681%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.308231/ 42.712704, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1058%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 95160   6.658%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.516290/ 41.846516, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 95479   6.634%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.933764/ 48.676785, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95766   6.609%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.877949/ 40.764561, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 96045   6.584%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.664596/ 45.111179, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6113%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 96301   6.558%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.992446/ 41.067791, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 96590   6.534%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.091671/ 40.411686, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96877   6.510%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.049653/ 40.533943, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 97157   6.486%\n",
      "fc layer 2 self.abs_max_out: 2557.0\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.800800/ 38.873970, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7802%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 97422   6.462%\n",
      "fc layer 2 self.abs_max_out: 2563.0\n",
      "lif layer 1 self.abs_max_v: 7586.0\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.971457/ 40.159767, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97679   6.437%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.151408/ 43.488125, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97981   6.416%\n",
      "lif layer 1 self.abs_max_v: 7723.5\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.985345/ 46.075653, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 98288   6.395%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.868487/ 44.898659, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 98559   6.372%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.693098/ 41.370811, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5929%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98807   6.348%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.961978/ 39.263138, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0494%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4113%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 99072   6.325%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.755202/ 46.424461, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9659%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 99317   6.301%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.501304/ 40.018116, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 99564   6.278%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.741817/ 39.538818, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99821   6.255%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.734372/ 46.468323, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 100077   6.233%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.739717/ 52.905792, val:  81.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 100345   6.212%\n",
      "fc layer 2 self.abs_max_out: 2615.0\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.822911/ 43.530209, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1672%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 100617   6.191%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.576087/ 41.831387, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100858   6.169%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.881158/ 44.171978, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 101146   6.150%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.802077/ 42.883476, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 101391   6.128%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.721155/ 40.732052, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5380%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 101636   6.107%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.773771/ 35.428955, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101890   6.086%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.594386/ 37.825695, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 102115   6.064%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.384402/ 42.319016, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 102339   6.042%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.544796/ 43.147530, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0196%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 102583   6.022%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.599690/ 41.480583, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0513%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102817   6.001%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.448730/ 38.768448, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7488%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 103041   5.980%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.588446/ 46.976364, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0231%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 103266   5.959%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.853072/ 42.470310, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 103535   5.941%\n",
      "fc layer 3 self.abs_max_out: 1079.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.882164/ 40.960609, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1080%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103777   5.922%\n",
      "fc layer 1 self.abs_max_out: 4484.0\n",
      "fc layer 1 self.abs_max_out: 4565.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.785378/ 41.869804, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 104038   5.904%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.397916/ 41.794197, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5298%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 104253   5.883%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.520075/ 45.879436, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8268%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 104487   5.864%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.565336/ 41.183582, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 104705   5.844%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.573468/ 43.661400, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0275%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104921   5.825%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.370280/ 45.530029, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8576%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 105117   5.804%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.410852/ 43.269848, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 105326   5.784%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.366444/ 45.873280, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1158%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 105539   5.765%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.605714/ 41.639038, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0332%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 105763   5.746%\n",
      "fc layer 1 self.abs_max_out: 4597.0\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.393979/ 44.865589, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 105988   5.728%\n",
      "fc layer 3 self.abs_max_out: 1094.0\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.412642/ 46.838501, val:  85.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 106197   5.709%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.417731/ 45.786217, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5928%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 106394   5.690%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.613065/ 46.691273, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 106609   5.672%\n",
      "fc layer 1 self.abs_max_out: 4610.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.305859/ 40.427937, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106802   5.652%\n",
      "fc layer 3 self.abs_max_out: 1106.0\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.467782/ 46.285618, val:  85.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5063%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 107022   5.635%\n",
      "fc layer 1 self.abs_max_out: 4614.0\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.383086/ 46.760742, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0275%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 107234   5.617%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.514166/ 39.660797, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1235%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0810%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 107457   5.600%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.466873/ 38.973614, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 107665   5.582%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.250681/ 42.775112, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107847   5.564%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.252321/ 43.109619, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 108032   5.545%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.305276/ 48.640911, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1194%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13304d06e374f47b77c1ccb7f1dabda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.30528</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>48.64091</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-sweep-72</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eguv8au' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eguv8au</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_125049-8eguv8au/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pmdxlz5g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 10320\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_170039-pmdxlz5g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pmdxlz5g' target=\"_blank\">toasty-sweep-76</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pmdxlz5g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pmdxlz5g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251214_170048_730', 'my_seed': 10320, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 370.0\n",
      "lif layer 1 self.abs_max_v: 370.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 318.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "lif layer 1 self.abs_max_v: 514.0\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 385.0\n",
      "fc layer 2 self.abs_max_out: 401.0\n",
      "lif layer 2 self.abs_max_v: 474.0\n",
      "fc layer 1 self.abs_max_out: 385.0\n",
      "lif layer 1 self.abs_max_v: 532.5\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "fc layer 1 self.abs_max_out: 482.0\n",
      "lif layer 1 self.abs_max_v: 559.5\n",
      "lif layer 2 self.abs_max_v: 529.0\n",
      "fc layer 1 self.abs_max_out: 584.0\n",
      "lif layer 1 self.abs_max_v: 584.0\n",
      "fc layer 2 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 591.5\n",
      "fc layer 1 self.abs_max_out: 752.0\n",
      "lif layer 1 self.abs_max_v: 752.0\n",
      "fc layer 3 self.abs_max_out: 162.0\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "fc layer 2 self.abs_max_out: 442.0\n",
      "fc layer 2 self.abs_max_out: 477.0\n",
      "lif layer 2 self.abs_max_v: 613.5\n",
      "fc layer 2 self.abs_max_out: 522.0\n",
      "lif layer 2 self.abs_max_v: 755.5\n",
      "fc layer 1 self.abs_max_out: 791.0\n",
      "lif layer 1 self.abs_max_v: 791.0\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "fc layer 2 self.abs_max_out: 554.0\n",
      "lif layer 1 self.abs_max_v: 859.0\n",
      "lif layer 2 self.abs_max_v: 790.0\n",
      "lif layer 2 self.abs_max_v: 818.0\n",
      "lif layer 2 self.abs_max_v: 889.0\n",
      "fc layer 2 self.abs_max_out: 577.0\n",
      "lif layer 2 self.abs_max_v: 1021.5\n",
      "fc layer 2 self.abs_max_out: 590.0\n",
      "fc layer 2 self.abs_max_out: 598.0\n",
      "fc layer 2 self.abs_max_out: 624.0\n",
      "lif layer 2 self.abs_max_v: 1028.0\n",
      "fc layer 2 self.abs_max_out: 675.0\n",
      "lif layer 2 self.abs_max_v: 1068.5\n",
      "lif layer 1 self.abs_max_v: 880.0\n",
      "lif layer 1 self.abs_max_v: 946.0\n",
      "fc layer 1 self.abs_max_out: 845.0\n",
      "fc layer 2 self.abs_max_out: 728.0\n",
      "fc layer 2 self.abs_max_out: 739.0\n",
      "fc layer 1 self.abs_max_out: 1076.0\n",
      "lif layer 1 self.abs_max_v: 1076.0\n",
      "fc layer 2 self.abs_max_out: 742.0\n",
      "lif layer 1 self.abs_max_v: 1091.5\n",
      "lif layer 1 self.abs_max_v: 1127.0\n",
      "fc layer 2 self.abs_max_out: 786.0\n",
      "fc layer 2 self.abs_max_out: 793.0\n",
      "lif layer 2 self.abs_max_v: 1097.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "fc layer 2 self.abs_max_out: 845.0\n",
      "fc layer 2 self.abs_max_out: 868.0\n",
      "lif layer 2 self.abs_max_v: 1106.5\n",
      "lif layer 1 self.abs_max_v: 1215.0\n",
      "lif layer 1 self.abs_max_v: 1387.5\n",
      "lif layer 2 self.abs_max_v: 1141.5\n",
      "lif layer 2 self.abs_max_v: 1148.0\n",
      "fc layer 1 self.abs_max_out: 1167.0\n",
      "lif layer 2 self.abs_max_v: 1188.0\n",
      "lif layer 2 self.abs_max_v: 1194.0\n",
      "lif layer 2 self.abs_max_v: 1254.0\n",
      "fc layer 2 self.abs_max_out: 905.0\n",
      "fc layer 2 self.abs_max_out: 936.0\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "fc layer 2 self.abs_max_out: 1001.0\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "lif layer 2 self.abs_max_v: 1304.0\n",
      "lif layer 1 self.abs_max_v: 1464.0\n",
      "lif layer 2 self.abs_max_v: 1318.0\n",
      "lif layer 2 self.abs_max_v: 1369.5\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "fc layer 1 self.abs_max_out: 1189.0\n",
      "fc layer 1 self.abs_max_out: 1386.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "lif layer 2 self.abs_max_v: 1423.0\n",
      "lif layer 1 self.abs_max_v: 1565.0\n",
      "lif layer 1 self.abs_max_v: 1610.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "lif layer 1 self.abs_max_v: 1658.5\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "fc layer 1 self.abs_max_out: 1656.0\n",
      "lif layer 1 self.abs_max_v: 1709.5\n",
      "lif layer 2 self.abs_max_v: 1470.0\n",
      "fc layer 3 self.abs_max_out: 385.0\n",
      "lif layer 1 self.abs_max_v: 1788.0\n",
      "lif layer 1 self.abs_max_v: 1874.5\n",
      "lif layer 1 self.abs_max_v: 1961.5\n",
      "lif layer 1 self.abs_max_v: 2044.0\n",
      "fc layer 2 self.abs_max_out: 1174.0\n",
      "lif layer 2 self.abs_max_v: 1505.5\n",
      "lif layer 2 self.abs_max_v: 1532.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "lif layer 2 self.abs_max_v: 1537.5\n",
      "lif layer 2 self.abs_max_v: 1571.0\n",
      "lif layer 1 self.abs_max_v: 2114.5\n",
      "lif layer 1 self.abs_max_v: 2242.5\n",
      "lif layer 2 self.abs_max_v: 1586.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 15.404350/ 86.837852, val:  39.58%, val_best:  39.58%, tr:  96.83%, tr_best:  96.83%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2178  22.247%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1604.5\n",
      "lif layer 1 self.abs_max_v: 2295.0\n",
      "lif layer 2 self.abs_max_v: 1686.0\n",
      "lif layer 2 self.abs_max_v: 1750.0\n",
      "lif layer 1 self.abs_max_v: 2456.0\n",
      "lif layer 2 self.abs_max_v: 1862.5\n",
      "lif layer 2 self.abs_max_v: 1921.5\n",
      "fc layer 1 self.abs_max_out: 1671.0\n",
      "fc layer 2 self.abs_max_out: 1197.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "fc layer 3 self.abs_max_out: 406.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "lif layer 2 self.abs_max_v: 1962.5\n",
      "lif layer 2 self.abs_max_v: 1966.0\n",
      "fc layer 1 self.abs_max_out: 1695.0\n",
      "fc layer 1 self.abs_max_out: 1723.0\n",
      "fc layer 1 self.abs_max_out: 1867.0\n",
      "lif layer 1 self.abs_max_v: 2750.5\n",
      "lif layer 1 self.abs_max_v: 2863.5\n",
      "fc layer 2 self.abs_max_out: 1243.0\n",
      "fc layer 2 self.abs_max_out: 1244.0\n",
      "fc layer 2 self.abs_max_out: 1266.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.888577/ 66.892044, val:  39.17%, val_best:  39.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3723  19.014%\n",
      "fc layer 2 self.abs_max_out: 1364.0\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "fc layer 3 self.abs_max_out: 459.0\n",
      "fc layer 1 self.abs_max_out: 1874.0\n",
      "lif layer 1 self.abs_max_v: 2978.0\n",
      "lif layer 1 self.abs_max_v: 3161.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.191771/ 39.916977, val:  59.17%, val_best:  59.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5160  17.569%\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 1 self.abs_max_out: 1891.0\n",
      "fc layer 1 self.abs_max_out: 2023.0\n",
      "fc layer 1 self.abs_max_out: 2032.0\n",
      "lif layer 1 self.abs_max_v: 3167.5\n",
      "lif layer 1 self.abs_max_v: 3440.0\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "fc layer 1 self.abs_max_out: 2059.0\n",
      "fc layer 1 self.abs_max_out: 2075.0\n",
      "lif layer 1 self.abs_max_v: 3566.5\n",
      "lif layer 1 self.abs_max_v: 3717.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.621689/ 76.704369, val:  38.33%, val_best:  59.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6638%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6492  16.578%\n",
      "fc layer 1 self.abs_max_out: 2102.0\n",
      "fc layer 2 self.abs_max_out: 1393.0\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "fc layer 2 self.abs_max_out: 1546.0\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 1 self.abs_max_out: 2148.0\n",
      "fc layer 3 self.abs_max_out: 534.0\n",
      "fc layer 1 self.abs_max_out: 2312.0\n",
      "lif layer 1 self.abs_max_v: 3799.5\n",
      "lif layer 1 self.abs_max_v: 3871.5\n",
      "lif layer 1 self.abs_max_v: 4105.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.436942/ 52.322628, val:  55.42%, val_best:  59.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7802  15.939%\n",
      "lif layer 1 self.abs_max_v: 4172.0\n",
      "lif layer 2 self.abs_max_v: 1987.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.086404/ 60.409611, val:  48.33%, val_best:  59.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0551%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9058  15.420%\n",
      "lif layer 2 self.abs_max_v: 2002.5\n",
      "fc layer 1 self.abs_max_out: 2436.0\n",
      "lif layer 1 self.abs_max_v: 4226.0\n",
      "lif layer 2 self.abs_max_v: 2046.0\n",
      "fc layer 2 self.abs_max_out: 1554.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.261457/ 73.433426, val:  40.42%, val_best:  59.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10352  15.106%\n",
      "fc layer 2 self.abs_max_out: 1575.0\n",
      "fc layer 2 self.abs_max_out: 1620.0\n",
      "lif layer 1 self.abs_max_v: 4249.0\n",
      "lif layer 2 self.abs_max_v: 2130.5\n",
      "fc layer 1 self.abs_max_out: 2520.0\n",
      "lif layer 1 self.abs_max_v: 4284.5\n",
      "lif layer 2 self.abs_max_v: 2140.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.685362/ 67.643234, val:  44.58%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11559  14.759%\n",
      "lif layer 2 self.abs_max_v: 2228.0\n",
      "lif layer 2 self.abs_max_v: 2249.0\n",
      "lif layer 2 self.abs_max_v: 2321.5\n",
      "lif layer 2 self.abs_max_v: 2368.5\n",
      "lif layer 2 self.abs_max_v: 2482.5\n",
      "lif layer 2 self.abs_max_v: 2534.0\n",
      "lif layer 1 self.abs_max_v: 4495.5\n",
      "lif layer 1 self.abs_max_v: 4505.0\n",
      "fc layer 3 self.abs_max_out: 546.0\n",
      "fc layer 1 self.abs_max_out: 2666.0\n",
      "lif layer 1 self.abs_max_v: 4709.5\n",
      "fc layer 2 self.abs_max_out: 1687.0\n",
      "fc layer 2 self.abs_max_out: 1715.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.468754/ 48.183510, val:  54.58%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12728  14.446%\n",
      "lif layer 2 self.abs_max_v: 2581.0\n",
      "lif layer 2 self.abs_max_v: 2595.0\n",
      "lif layer 2 self.abs_max_v: 2685.5\n",
      "lif layer 2 self.abs_max_v: 2688.5\n",
      "lif layer 2 self.abs_max_v: 2742.0\n",
      "lif layer 2 self.abs_max_v: 2941.5\n",
      "fc layer 1 self.abs_max_out: 2786.0\n",
      "lif layer 1 self.abs_max_v: 5028.0\n",
      "lif layer 2 self.abs_max_v: 2989.0\n",
      "lif layer 2 self.abs_max_v: 3044.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.527446/ 51.777939, val:  60.00%, val_best:  60.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 13921  14.220%\n",
      "fc layer 3 self.abs_max_out: 578.0\n",
      "fc layer 1 self.abs_max_out: 2911.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.258585/ 74.364471, val:  41.25%, val_best:  60.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15077  14.000%\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 1 self.abs_max_out: 2969.0\n",
      "lif layer 1 self.abs_max_v: 5072.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.667280/ 42.918201, val:  60.42%, val_best:  60.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16181  13.773%\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.981323/ 40.644360, val:  61.67%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4063%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17338  13.623%\n",
      "fc layer 2 self.abs_max_out: 1732.0\n",
      "fc layer 1 self.abs_max_out: 2978.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.409098/ 48.692444, val:  58.75%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0408%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18430  13.447%\n",
      "lif layer 1 self.abs_max_v: 5088.0\n",
      "lif layer 1 self.abs_max_v: 5158.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.157373/ 47.393616, val:  56.67%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19577  13.331%\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.569563/ 66.354042, val:  51.25%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 20682  13.204%\n",
      "fc layer 1 self.abs_max_out: 2985.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.574021/ 44.616024, val:  53.75%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 21763  13.076%\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "fc layer 1 self.abs_max_out: 3002.0\n",
      "lif layer 1 self.abs_max_v: 5208.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.036847/ 35.742908, val:  67.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3988%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 22795  12.936%\n",
      "fc layer 2 self.abs_max_out: 1824.0\n",
      "fc layer 1 self.abs_max_out: 3056.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.094540/ 56.810356, val:  49.58%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 23863  12.829%\n",
      "fc layer 1 self.abs_max_out: 3061.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.012412/ 95.644455, val:  40.83%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 24930  12.732%\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "lif layer 1 self.abs_max_v: 5309.5\n",
      "lif layer 2 self.abs_max_v: 3146.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.793763/ 36.983135, val:  68.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3063%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 25978  12.636%\n",
      "lif layer 2 self.abs_max_v: 3217.0\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "fc layer 1 self.abs_max_out: 3090.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.008482/ 62.788792, val:  54.17%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27019  12.545%\n",
      "lif layer 1 self.abs_max_v: 5324.0\n",
      "fc layer 1 self.abs_max_out: 3106.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.469304/ 46.262787, val:  66.67%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5599%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 27995  12.433%\n",
      "lif layer 2 self.abs_max_v: 3223.0\n",
      "lif layer 2 self.abs_max_v: 3275.0\n",
      "fc layer 3 self.abs_max_out: 661.0\n",
      "fc layer 1 self.abs_max_out: 3150.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.475558/ 36.278080, val:  72.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1154%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 28984  12.336%\n",
      "fc layer 3 self.abs_max_out: 680.0\n",
      "fc layer 1 self.abs_max_out: 3179.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.762019/ 37.960831, val:  70.83%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0958%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30006  12.260%\n",
      "lif layer 2 self.abs_max_v: 3301.0\n",
      "lif layer 1 self.abs_max_v: 5484.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.054984/ 60.077076, val:  59.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3004%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 30939  12.155%\n",
      "lif layer 2 self.abs_max_v: 3322.0\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.389485/ 50.280369, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0494%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5170%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 31902  12.069%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.562157/ 61.131336, val:  61.25%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 32845  11.982%\n",
      "fc layer 1 self.abs_max_out: 3182.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.055847/ 30.365238, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 33760  11.891%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.866338/ 44.200550, val:  72.08%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 34611  11.784%\n",
      "fc layer 2 self.abs_max_out: 1858.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.289314/ 44.114071, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9229%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 35542  11.711%\n",
      "fc layer 2 self.abs_max_out: 1873.0\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.048096/ 53.172993, val:  63.33%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1031%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 36461  11.638%\n",
      "fc layer 3 self.abs_max_out: 694.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.101809/ 53.412304, val:  65.00%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37346  11.560%\n",
      "fc layer 2 self.abs_max_out: 1900.0\n",
      "fc layer 3 self.abs_max_out: 763.0\n",
      "fc layer 1 self.abs_max_out: 3203.0\n",
      "lif layer 1 self.abs_max_v: 5522.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.934655/ 56.045033, val:  57.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38254  11.493%\n",
      "fc layer 1 self.abs_max_out: 3303.0\n",
      "lif layer 1 self.abs_max_v: 5701.5\n",
      "lif layer 1 self.abs_max_v: 5827.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.566434/ 55.137020, val:  59.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0478%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39090  11.408%\n",
      "fc layer 2 self.abs_max_out: 2078.0\n",
      "lif layer 2 self.abs_max_v: 3454.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.793143/ 43.808022, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0382%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 39988  11.346%\n",
      "fc layer 1 self.abs_max_out: 3391.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.827296/ 43.636227, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0248%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 40866  11.282%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.614186/ 56.952362, val:  63.33%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3052%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1611%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 41725  11.216%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.524767/ 36.424160, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1181%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42539  11.141%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.055475/ 35.420761, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43332  11.065%\n",
      "fc layer 1 self.abs_max_out: 3487.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.992826/ 41.305939, val:  73.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.30 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1034%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44100  10.987%\n",
      "fc layer 2 self.abs_max_out: 2130.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.027956/ 38.746204, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 44857  10.909%\n",
      "fc layer 1 self.abs_max_out: 3495.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.099663/ 46.146278, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 45635  10.840%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.701483/ 32.416599, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4136%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46379  10.767%\n",
      "fc layer 1 self.abs_max_out: 3579.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.866252/ 40.708164, val:  72.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4585%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47136  10.699%\n",
      "lif layer 1 self.abs_max_v: 5927.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.344782/ 47.674355, val:  67.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3615%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 47921  10.641%\n",
      "lif layer 2 self.abs_max_v: 3594.0\n",
      "fc layer 1 self.abs_max_out: 3598.0\n",
      "lif layer 1 self.abs_max_v: 6049.5\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.185938/ 48.103012, val:  68.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 48677  10.579%\n",
      "fc layer 1 self.abs_max_out: 3671.0\n",
      "fc layer 1 self.abs_max_out: 3730.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.546405/ 38.220234, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49399  10.512%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.475466/ 34.086544, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1679%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50099  10.444%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.713930/ 35.856728, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 50816  10.381%\n",
      "lif layer 1 self.abs_max_v: 6162.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.474737/ 31.129095, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51502  10.315%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.225540/ 35.549755, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52139  10.242%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.118340/ 35.227142, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0306%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9742%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 52834  10.183%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.479141/ 39.329510, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0141%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53513  10.122%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.633813/ 36.025196, val:  82.92%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54216  10.069%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.873539/ 44.453114, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2263%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 54812   9.998%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.428408/ 42.084373, val:  75.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4154%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55468   9.940%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.910613/ 39.618206, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1252%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9704%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56103   9.880%\n",
      "fc layer 1 self.abs_max_out: 3735.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.070345/ 47.965210, val:  75.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 56701   9.816%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.879976/ 39.477512, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1592%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57293   9.754%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.059436/ 33.092682, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 57934   9.701%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.644472/ 31.486637, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1299%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58533   9.643%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.723143/ 34.692772, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59135   9.588%\n",
      "fc layer 1 self.abs_max_out: 3745.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.891898/ 36.685680, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 59725   9.532%\n",
      "fc layer 1 self.abs_max_out: 3820.0\n",
      "fc layer 2 self.abs_max_out: 2144.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.038565/ 41.673882, val:  78.33%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60349   9.484%\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.544321/ 41.928928, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 60909   9.427%\n",
      "fc layer 2 self.abs_max_out: 2193.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.933601/ 42.910015, val:  74.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7268%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61492   9.375%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.625835/ 34.464687, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0141%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62082   9.326%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.336517/ 32.611149, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 62636   9.272%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.597477/ 35.861839, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5691%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63181   9.219%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.352039/ 33.936455, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7369%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 63734   9.169%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.694489/ 35.525990, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64284   9.120%\n",
      "fc layer 1 self.abs_max_out: 3882.0\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.654517/ 40.680851, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 64842   9.073%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.474660/ 39.983711, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8844%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65388   9.026%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.227204/ 40.742737, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3547%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 65897   8.975%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.340693/ 40.331795, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66417   8.927%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.111042/ 35.368423, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 66929   8.879%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.150159/ 35.950886, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 67433   8.831%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.025330/ 32.155170, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2892%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 67930   8.783%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.081272/ 35.225807, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0654%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 68421   8.736%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.079237/ 36.988064, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7887%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0370%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 68917   8.691%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.991285/ 32.944954, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 69404   8.645%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  2.974914/ 45.667801, val:  77.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 69860   8.597%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.274393/ 38.395561, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2499%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3302%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 70375   8.558%\n",
      "fc layer 1 self.abs_max_out: 3884.0\n",
      "lif layer 1 self.abs_max_v: 6201.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.001382/ 32.130989, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 70876   8.517%\n",
      "fc layer 1 self.abs_max_out: 3887.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.946939/ 32.622334, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 71330   8.472%\n",
      "fc layer 2 self.abs_max_out: 2256.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.683949/ 37.492790, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 71747   8.424%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.047848/ 39.351368, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72241   8.385%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.583876/ 38.849388, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 72657   8.339%\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "fc layer 3 self.abs_max_out: 778.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.792816/ 47.244892, val:  77.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0206%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73089   8.295%\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "fc layer 2 self.abs_max_out: 2290.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.138629/ 33.420288, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2939%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 73563   8.257%\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.877564/ 44.538155, val:  81.67%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74030   8.219%\n",
      "lif layer 1 self.abs_max_v: 6362.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.449956/ 53.095600, val:  74.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 74436   8.176%\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "fc layer 1 self.abs_max_out: 3904.0\n",
      "fc layer 2 self.abs_max_out: 2299.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.843307/ 45.633190, val:  77.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 74863   8.135%\n",
      "fc layer 1 self.abs_max_out: 3984.0\n",
      "fc layer 2 self.abs_max_out: 2331.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.799484/ 36.331795, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 75315   8.098%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.674828/ 43.339775, val:  75.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 75745   8.059%\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.776206/ 31.749201, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9620%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8828%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 76174   8.021%\n",
      "fc layer 2 self.abs_max_out: 2395.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.658770/ 33.768047, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 76566   7.980%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.525629/ 41.351627, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 76993   7.944%\n",
      "fc layer 3 self.abs_max_out: 885.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.978985/ 30.253605, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 77423   7.908%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.927152/ 32.256065, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 77881   7.876%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.680123/ 33.119041, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0567%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 78314   7.843%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.717454/ 42.216679, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1238%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 78747   7.809%\n",
      "fc layer 1 self.abs_max_out: 4055.0\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.410251/ 34.015659, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 79128   7.772%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.382612/ 33.950943, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 79515   7.735%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.419951/ 33.771267, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 79876   7.697%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.147281/ 37.963615, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 80230   7.659%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.484174/ 33.648735, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2282%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 80589   7.622%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.051999/ 41.816345, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1342%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 80920   7.583%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.106849/ 36.168045, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3942%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 81281   7.548%\n",
      "fc layer 2 self.abs_max_out: 2409.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.450544/ 34.052898, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1796%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 81654   7.514%\n",
      "fc layer 1 self.abs_max_out: 4152.0\n",
      "fc layer 1 self.abs_max_out: 4169.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.016434/ 37.118164, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 81974   7.476%\n",
      "fc layer 1 self.abs_max_out: 4223.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.350229/ 35.605251, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 82349   7.444%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.799735/ 36.775730, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 82776   7.417%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.317833/ 37.566914, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 83132   7.384%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.234107/ 37.519455, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 83484   7.351%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.261324/ 37.088005, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 83838   7.319%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.236900/ 44.870880, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 84203   7.289%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.280417/ 35.269215, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5653%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 84565   7.259%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.249590/ 36.702793, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 84922   7.229%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.144874/ 41.405777, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 85261   7.198%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  1.926680/ 38.439411, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 85585   7.166%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.270940/ 35.192574, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 85925   7.136%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.519978/ 37.835232, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1040%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4537%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 86294   7.108%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  1.837560/ 39.045223, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 86606   7.077%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.199027/ 41.730301, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 86954   7.049%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  1.838505/ 37.912464, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1624%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 87282   7.020%\n",
      "fc layer 3 self.abs_max_out: 888.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.013535/ 39.099747, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2383%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5302%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 87615   6.992%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  1.966565/ 36.645218, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1016%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 87922   6.962%\n",
      "lif layer 1 self.abs_max_v: 6481.0\n",
      "lif layer 1 self.abs_max_v: 6491.5\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "fc layer 3 self.abs_max_out: 903.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.128800/ 35.409420, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 88248   6.934%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  1.764106/ 46.504169, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3264%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 88537   6.904%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.968463/ 39.829060, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6154%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2002%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 88828   6.874%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.749897/ 38.376354, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 89106   6.843%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.769094/ 44.463371, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 89404   6.815%\n",
      "fc layer 1 self.abs_max_out: 4229.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.928349/ 37.022079, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1087%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 89718   6.788%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.814668/ 41.500301, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0985%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 89986   6.759%\n",
      "fc layer 1 self.abs_max_out: 4298.0\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.665935/ 35.995911, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0365%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 90269   6.730%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.160552/ 46.410797, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 90585   6.705%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.867756/ 41.945236, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1942%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 90881   6.678%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.865750/ 33.166111, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 91194   6.654%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.853418/ 30.233414, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 91466   6.626%\n",
      "lif layer 2 self.abs_max_v: 3595.5\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.506995/ 36.411556, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3063%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 91727   6.598%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.844658/ 33.571083, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 92033   6.574%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.778469/ 40.045681, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 92339   6.550%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.800555/ 36.988972, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1030%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2649%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 92628   6.525%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.460641/ 37.740761, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 92895   6.499%\n",
      "fc layer 1 self.abs_max_out: 4304.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.576315/ 33.590870, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 93172   6.474%\n",
      "fc layer 3 self.abs_max_out: 905.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.591584/ 37.573288, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0484%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 93414   6.447%\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.768920/ 42.361782, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 93697   6.423%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.551149/ 34.346127, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2640%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 93949   6.398%\n",
      "lif layer 2 self.abs_max_v: 3776.5\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.821987/ 38.216522, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 94233   6.374%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.580160/ 37.543530, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7757%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 94505   6.351%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.663352/ 46.273495, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 94785   6.328%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.608284/ 38.460381, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 95036   6.304%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.541337/ 36.318027, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 95303   6.280%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.649994/ 37.189796, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 95575   6.258%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.738467/ 39.392048, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 95831   6.235%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.650464/ 37.661530, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 96088   6.212%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.446432/ 42.392872, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4163%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 96324   6.188%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.528103/ 37.088367, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 96592   6.166%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.353051/ 42.389095, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 96820   6.143%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.222897/ 36.998371, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0378%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 97029   6.118%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.416350/ 34.474625, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0482%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 97260   6.095%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.468940/ 43.574135, val:  81.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1130%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9462%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 97487   6.072%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.178406/ 44.501431, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 97711   6.049%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.204849/ 41.978130, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0999%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2972%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 97921   6.025%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.344071/ 45.626667, val:  84.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7524%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 98149   6.003%\n",
      "lif layer 2 self.abs_max_v: 3805.5\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.606359/ 36.171303, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 98410   5.983%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.494872/ 37.069565, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 98653   5.963%\n",
      "fc layer 1 self.abs_max_out: 4313.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.400105/ 36.984047, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2328%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 98901   5.942%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.175186/ 32.818401, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0492%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 99103   5.920%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.343792/ 40.492695, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3157%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 99347   5.900%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.352581/ 39.149849, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 99572   5.879%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.295864/ 42.262074, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0503%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5823%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 99794   5.858%\n",
      "lif layer 1 self.abs_max_v: 6525.5\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.284401/ 48.625881, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1196%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 100016   5.838%\n",
      "lif layer 1 self.abs_max_v: 6635.5\n",
      "fc layer 1 self.abs_max_out: 4343.0\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.305937/ 45.642170, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0491%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 100224   5.817%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.275025/ 42.142418, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 100427   5.796%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.011412/ 38.008984, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1624%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 100615   5.774%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.313948/ 41.100040, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 100834   5.754%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.381918/ 40.784359, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8981%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 101068   5.735%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.376941/ 37.205349, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 101281   5.716%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.142957/ 41.482933, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 101490   5.696%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.266596/ 44.173733, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4385%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 101705   5.677%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [128.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [4.0*2]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [4.0*2]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [128.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"init_scaling_0\": {\"values\": [10000+ 9]},\n",
    "        \"init_scaling_1\": {\"values\": [10000+ 9]},\n",
    "        \"init_scaling_2\": {\"values\": [10000+ 8]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"3\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'gsnqjucp'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
