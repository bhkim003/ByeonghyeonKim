{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ground-truth spike로 AE train data 꾸리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# os.chdir(\"./../data/\")\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "\n",
    "spike_tot = [\"BH_Spike_e1n005.npy\", \"BH_Spike_e1n010.npy\", \"BH_Spike_e1n015.npy\", \"BH_Spike_e1n020.npy\",\n",
    "            \"BH_Spike_e2n005.npy\", \"BH_Spike_e2n010.npy\", \"BH_Spike_e2n015.npy\", \"BH_Spike_e2n020.npy\",\n",
    "            \"BH_Spike_d1n005.npy\", \"BH_Spike_d1n010.npy\", \"BH_Spike_d1n015.npy\", \"BH_Spike_d1n020.npy\",\n",
    "            \"BH_Spike_d2n005.npy\", \"BH_Spike_d2n010.npy\", \"BH_Spike_d2n015.npy\", \"BH_Spike_d2n020.npy\"]\n",
    "\n",
    "label_tot = [\"BH_Label_e1n005.npy\", \"BH_Label_e1n010.npy\", \"BH_Label_e1n015.npy\", \"BH_Label_e1n020.npy\",\n",
    "            \"BH_Label_e2n005.npy\", \"BH_Label_e2n010.npy\", \"BH_Label_e2n015.npy\", \"BH_Label_e2n020.npy\",\n",
    "            \"BH_Label_d1n005.npy\", \"BH_Label_d1n010.npy\", \"BH_Label_d1n015.npy\", \"BH_Label_d1n020.npy\",\n",
    "            \"BH_Label_d2n005.npy\", \"BH_Label_d2n010.npy\", \"BH_Label_d2n015.npy\", \"BH_Label_d2n020.npy\"]\n",
    "\n",
    "\n",
    "dataset_num = 16\n",
    "training_num = 2400\n",
    "spike_length = 50\n",
    "\n",
    "spike_train = [] # 스파이크 데이터를 저장할 배열\n",
    "spike_test = [] # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    ans_times = mat1['spike_times'][0][0][0]\n",
    "    ans_cluster = mat1['spike_class'][0][0][0]\n",
    "\n",
    "    spike_this_dataset = []\n",
    "    labal_this_dataset = []\n",
    "\n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "\n",
    "    spike_group = np.zeros((len(ans_times), spike_length))\n",
    "\n",
    "    train_spike_count = 0\n",
    "    for i in range(len(ans_times)):\n",
    "        max_slope_index = ans_times[i] + np.argmax(slope[ans_times[i] : ans_times[i] + 25])\n",
    "        now_spike = raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]\n",
    "        spike_this_dataset.append(now_spike)\n",
    "        labal_this_dataset.append(ans_cluster[i])\n",
    "        if train_spike_count < training_num:\n",
    "            train_spike_count += 1\n",
    "            spike_train.append(now_spike)\n",
    "        else:\n",
    "            spike_test.append(now_spike)\n",
    "\n",
    "    spike_this_dataset = np.array(spike_this_dataset)\n",
    "    labal_this_dataset = np.array(labal_this_dataset)\n",
    "\n",
    "    np.save(my_path_ground_BH + spike_tot[ds], spike_this_dataset)\n",
    "    np.save(my_path_ground_BH + label_tot[ds], labal_this_dataset)\n",
    "\n",
    "spike_train = np.array(spike_train)\n",
    "spike_test = np.array(spike_test)\n",
    "np.random.shuffle(spike_train)\n",
    "np.random.shuffle(spike_test)\n",
    "\n",
    "torch.save(torch.tensor(spike_train, dtype=torch.float32), my_path_ground_BH + 'BH_training_dataset_gt_detect.pt')\n",
    "torch.save(torch.tensor(spike_test, dtype=torch.float32), my_path_ground_BH + 'BH_test_dataset_gt_detect.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detected spike로 AE train data와 acc 측정을 위한 스파이크 꾸리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "# os.chdir(\"./../data/\")\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "thr_tot = np.array([0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7])\n",
    "\n",
    "dataset_num = 16\n",
    "training_num = 2400\n",
    "spike_length = 50\n",
    "wait_term = 20\n",
    "\n",
    "spike_train = [] # 스파이크 데이터를 저장할 배열\n",
    "spike_test = [] # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "    past_tr_num = len(spike_train)\n",
    "    past_te_num = len(spike_test)\n",
    "    train_spike_count = 0\n",
    "\n",
    "    # 데이터 파일 불러오기\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    thr = thr_tot[ds]  # 스파이크 탐지 임계값 설정 \n",
    "    \n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "    # 스파이크 탐지\n",
    "    wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "    for i in range(len(raw)-2):\n",
    "        wait += 1\n",
    "        if(wait_term < wait):\n",
    "            if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "                max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "                if train_spike_count < training_num:\n",
    "                    train_spike_count += 1\n",
    "                    spike_train.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "                else:\n",
    "                    spike_test.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length])\n",
    "                wait = 0  # 대기 시간 초기화 # 다시 wait_term만큼 기다려라\n",
    "\n",
    "                # if train_spike_count == 1: # 그림으로 보기\n",
    "                #     plt.plot(raw[max_slope_index - 10 : max_slope_index + 40])\n",
    "                    \n",
    "                #     plt.title(f\"align, max_slope_index={max_slope_index}\")\n",
    "                #     plt.xticks(range(50), labels=range(50))  # x축 눈금 설정\n",
    "                #     plt.yticks(np.arange(min(raw[max_slope_index - 10 : max_slope_index + 40]), \n",
    "                #                         max(raw[max_slope_index - 10 : max_slope_index + 40]) + 1, \n",
    "                #                         step=1))  # y축 눈금 설정, step=1로 매 x마다 보이게 함\n",
    "                    \n",
    "                #     plt.grid(True, which='both', linestyle='--', linewidth=0.5)  # 그래프에 그리드 추가\n",
    "                #     plt.show()\n",
    "    print(\"spike_train_size\", len(spike_train)-past_tr_num)\n",
    "    print(\"spike_test_size\", len(spike_test)-past_te_num)\n",
    "    print(\"spike_total\", len(spike_train)-past_tr_num+len(spike_test)-past_te_num)\n",
    "spike_train = np.array(spike_train)\n",
    "spike_test = np.array(spike_test)\n",
    "np.random.shuffle(spike_train)\n",
    "np.random.shuffle(spike_test)\n",
    "\n",
    "torch.save(torch.tensor(spike_train, dtype=torch.float32), my_path_ground_BH + 'BH_training_dataset_real_detect.pt')\n",
    "torch.save(torch.tensor(spike_test, dtype=torch.float32), my_path_ground_BH + 'BH_test_dataset_real_detect.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 논문대로 템플릿 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from scipy import io\n",
    "\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "# 데이터 파일 목록과 템플릿 파일 목록 설정\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "thr_tem = np.array([0.9, 0.9, 0.9, 0.9,   1.0, 1.0, 1.0, 1.0,   0.7, 0.7, 0.7, 0.9,   0.9, 0.9, 0.9, 0.9])\n",
    "thr_new_cluster = np.array([50, 50, 50, 50,   50, 50, 50, 50,   50, 30, 30, 50,   50, 50, 50, 50])\n",
    "thr_merge_cluster = np.array([110, 110, 110, 110,   110, 110, 110, 110,   110, 110, 110, 110,   110, 110, 110, 110])\n",
    "wait_term = np.array([20, 20, 20, 20,   20, 20, 20, 20,   20, 20, 20, 20,   20, 20, 20, 20])\n",
    "\n",
    "# thr_tot = np.array([0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7])\n",
    "\n",
    "dataset_num = 16\n",
    "spike_needs = 100 # 템플릿을 만들기 위해 필요한 스파이크 수\n",
    "spike_length = 50\n",
    "num_cluster = 4  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "cluster_delete_thr = 9 # spike_needs/10 # 이 숫자 초과인 클러스터만 생존\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "\n",
    "    # 데이터 파일 불러오기\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    thr = thr_tem[ds]  # 스파이크 탐지 임계값 설정 \n",
    "    spike = []  # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "    # 스파이크 탐지\n",
    "    wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "    spike_count = 0\n",
    "    for i in range(len(raw)-2):\n",
    "        wait += 1\n",
    "        if(wait_term[ds] < wait):\n",
    "            if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "                spike_count += 1\n",
    "                max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "                spike.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "                wait = 0  # 대기 시간 초기화 # 다시 wait_term[ds]만큼 기다려라\n",
    "                if spike_count == spike_needs:\n",
    "                    break\n",
    "    spike = np.array(spike)\n",
    "\n",
    "    Cluster = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "    cluster_num = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    \n",
    "    sm_distance = np.zeros(num_cluster)\n",
    "    mm_distance = np.zeros((num_cluster, num_cluster))\n",
    "\n",
    "    # 훈련 사이클 시작\n",
    "    current_cluster_num = 0\n",
    "    for spike_index in range(spike_needs):\n",
    "        spike_n = spike[spike_index, :]  # 현재 스파이크\n",
    "        \n",
    "        if(spike_index == 0):\n",
    "            Cluster[0, :] = spike_n  # 첫 번째 스파이크는 첫 번째 클러스터에 배정\n",
    "            cluster_num[0] = 1  # 클러스터 데이터 수 증가\n",
    "            current_cluster_num += 1\n",
    "        else:\n",
    "            # 각 클러스터와의 거리 계산\n",
    "\n",
    "            sm_smallest = 1000000000000\n",
    "            sm_smallest_index = 0\n",
    "            for i in range(num_cluster): # 0, 1, 2 까지는 기존 클러스터와 지금 스파이크와의 거리\n",
    "                if cluster_num[i] > 0:\n",
    "                    sm_distance[i] = np.sum(abs(Cluster[i, 5:25] - spike_n[5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - spike_n[0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - spike_n[25:50])) * 2\n",
    "                    if sm_smallest > sm_distance[i]:\n",
    "                        sm_smallest = sm_distance[i]\n",
    "                        sm_smallest_index = i\n",
    "\n",
    "            mm_smallest = 1000000000000\n",
    "            mm_smallest_index_i = 0\n",
    "            mm_smallest_index_j = 0\n",
    "            for i in range(num_cluster):\n",
    "                for j in range(i+1, num_cluster):\n",
    "                    if cluster_num[i] > 0 and cluster_num[j] > 0:\n",
    "                        mer_thr = 1.5 if spike_index < 30 else 2.5\n",
    "                        mm_distance[i, j] = np.sum(abs(Cluster[i, 5:25] - Cluster[j, 5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - Cluster[j, 0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - Cluster[j, 25:50])) * 2            \n",
    "                        mm_distance[i, j] = mm_distance[i, j] * mer_thr\n",
    "                        if mm_smallest > mm_distance[i, j]:\n",
    "                            mm_smallest = mm_distance[i, j]\n",
    "                            mm_smallest_index_i = i\n",
    "                            mm_smallest_index_j = j\n",
    "\n",
    "            if current_cluster_num < num_cluster:\n",
    "                # print('sm_smallest', sm_smallest)\n",
    "                if sm_smallest > thr_new_cluster[ds]:\n",
    "                    Cluster[current_cluster_num, :] = spike_n\n",
    "                    cluster_num[current_cluster_num] = 1\n",
    "                    current_cluster_num += 1\n",
    "                else:\n",
    "                    Cluster[sm_smallest_index, :] = (Cluster[sm_smallest_index, :] * 15 + spike_n) / 16\n",
    "                    cluster_num[sm_smallest_index] += 1\n",
    "            \n",
    "            else:\n",
    "                if sm_smallest < mm_smallest:\n",
    "                    Cluster[sm_smallest_index, :] = (Cluster[sm_smallest_index, :] * 15 + spike_n) / 16\n",
    "                    cluster_num[sm_smallest_index] += 1\n",
    "                else: #merge\n",
    "                    Cluster[mm_smallest_index_i, :] = (Cluster[mm_smallest_index_i, :] + Cluster[mm_smallest_index_j, :]) / 2\n",
    "                    cluster_num[mm_smallest_index_i] += cluster_num[mm_smallest_index_j]\n",
    "                    Cluster[mm_smallest_index_j, :] = spike_n\n",
    "                    cluster_num[mm_smallest_index_j] = 1\n",
    "\n",
    "    # print('before delete under 11', cluster_num)\n",
    "    # cluster_num이 11이하면 해당 클러스터 없애기\n",
    "    Cluster_temp = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "    cluster_num_temp = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    final_index = 0\n",
    "    for i in range(num_cluster):\n",
    "        if cluster_num[i] > cluster_delete_thr:\n",
    "            Cluster_temp[final_index, :] = Cluster[i, :]\n",
    "            cluster_num_temp[final_index] = cluster_num[i]\n",
    "            final_index += 1\n",
    "    current_cluster_num = final_index\n",
    "    # print(cluster_num_temp)\n",
    "    Cluster = Cluster_temp\n",
    "    cluster_num = cluster_num_temp\n",
    "\n",
    "\n",
    "    # \n",
    "    no_more_merge = False\n",
    "    while (no_more_merge == False):\n",
    "        no_more_merge = True\n",
    "        final_mm_merge_index_i = 0\n",
    "        final_mm_merge_index_j = 0\n",
    "        for i in range(current_cluster_num):\n",
    "            for j in range(i+1, current_cluster_num):\n",
    "                if cluster_num[i] > 0 and cluster_num[j] > 0:\n",
    "                    mer_thr = 2.5\n",
    "                    final_mm_distance = np.sum(abs(Cluster[i, 5:25] - Cluster[j, 5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - Cluster[j, 0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - Cluster[j, 25:50])) * 2            \n",
    "                    # print('final_mm_distance', final_mm_distance)\n",
    "                    # print(cluster_num)\n",
    "                    final_mm_distance = final_mm_distance * mer_thr\n",
    "                    if final_mm_distance < thr_merge_cluster[ds]:\n",
    "                        final_mm_merge_index_i = i\n",
    "                        final_mm_merge_index_j = j\n",
    "                        no_more_merge = False\n",
    "                        break\n",
    "            if no_more_merge == False:\n",
    "                break\n",
    "        \n",
    "        if no_more_merge == False:\n",
    "            Cluster[final_mm_merge_index_i, :] = (Cluster[final_mm_merge_index_i, :] + Cluster[final_mm_merge_index_j, :]) / 2\n",
    "            cluster_num[final_mm_merge_index_i] += cluster_num[final_mm_merge_index_j]\n",
    "            cluster_num[final_mm_merge_index_j] = 0\n",
    "            current_cluster_num -= 1\n",
    "\n",
    "            \n",
    "            # 앞으로 다시 땡기기\n",
    "            Cluster_temp = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "            cluster_num_temp = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "            final_index = 0\n",
    "            for i in range(num_cluster):\n",
    "                if cluster_num[i] > cluster_delete_thr:\n",
    "                    Cluster_temp[final_index, :] = Cluster[i, :]\n",
    "                    cluster_num_temp[final_index] = cluster_num[i]\n",
    "                    final_index += 1\n",
    "            current_cluster_num = final_index\n",
    "            Cluster = Cluster_temp\n",
    "            cluster_num = cluster_num_temp\n",
    "\n",
    "\n",
    "\n",
    "    # # Cluster plot\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # colors = ['b', 'g', 'r', 'k']  # 클러스터별 색상 지정\n",
    "    # x_axis = np.arange(spike_length)  # x축 값 (스파이크 길이)\n",
    "    # # print(cluster_num)\n",
    "    # for i in range(num_cluster):\n",
    "    #     plt.plot(x_axis, Cluster[i, :], label=f'Cluster {i+1}', color=colors[i % len(colors)])\n",
    "\n",
    "    # plt.title(f'Cluster Templates for {filename[ds]}, nums{cluster_num}')\n",
    "    # plt.xlabel('Sample Index')\n",
    "    # plt.ylabel('Amplitude')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    # 클러스터 템플릿을 파일로 저장\n",
    "    np.save(my_path_ground_BH + template[ds], Cluster)\n",
    "    print(Cluster)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssp.train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from scipy import io\n",
    "import itertools\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "\n",
    "spike_tot = [\"BH_Spike_e1n005.npy\", \"BH_Spike_e1n010.npy\", \"BH_Spike_e1n015.npy\", \"BH_Spike_e1n020.npy\",\n",
    "            \"BH_Spike_e2n005.npy\", \"BH_Spike_e2n010.npy\", \"BH_Spike_e2n015.npy\", \"BH_Spike_e2n020.npy\",\n",
    "            \"BH_Spike_d1n005.npy\", \"BH_Spike_d1n010.npy\", \"BH_Spike_d1n015.npy\", \"BH_Spike_d1n020.npy\",\n",
    "            \"BH_Spike_d2n005.npy\", \"BH_Spike_d2n010.npy\", \"BH_Spike_d2n015.npy\", \"BH_Spike_d2n020.npy\"]\n",
    "\n",
    "label_tot = [\"BH_Label_e1n005.npy\", \"BH_Label_e1n010.npy\", \"BH_Label_e1n015.npy\", \"BH_Label_e1n020.npy\",\n",
    "            \"BH_Label_e2n005.npy\", \"BH_Label_e2n010.npy\", \"BH_Label_e2n015.npy\", \"BH_Label_e2n020.npy\",\n",
    "            \"BH_Label_d1n005.npy\", \"BH_Label_d1n010.npy\", \"BH_Label_d1n015.npy\", \"BH_Label_d1n020.npy\",\n",
    "            \"BH_Label_d2n005.npy\", \"BH_Label_d2n010.npy\", \"BH_Label_d2n015.npy\", \"BH_Label_d2n020.npy\"]\n",
    "\n",
    "template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "AE_train_path_gt_detect = 'BH_training_dataset_gt_detect.pt' \n",
    "AE_test_path_gt_detect = 'BH_test_dataset_gt_detect.pt'\n",
    "\n",
    "AE_train_path_real_detect = 'BH_training_dataset_real_detect.pt'\n",
    "AE_test_path_real_detect = 'BH_test_dataset_real_detect.pt'\n",
    "\n",
    "\n",
    "\n",
    "thr_tot = np.array([0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7])\n",
    "cos_thr = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.85, 0.95, 0.9, 0.8, 0.95, 0.95, 0.95, 0.95, 0.8])\n",
    "\n",
    "# hyperparameter\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '1'\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "dataset_num = 16\n",
    "spike_length = 50\n",
    "num_cluster = 4  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "training_cycle = 2400 # 그 초기 몇개까지만 cluster update할지\n",
    "max_epoch = 7000\n",
    "AE_train_data = AE_train_path_gt_detect #AE_train_path_gt_detect #AE_train_path_real_detect\n",
    "AE_test_data = AE_test_path_gt_detect #AE_test_path_gt_detect  #AE_test_path_real_detect\n",
    "\n",
    "class spikedataset(Dataset):\n",
    "    def __init__(self, path, transform = None):    \n",
    "        self.transform = transform\n",
    "        self.spike = torch.load(path)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        spike = self.spike[index]            \n",
    "        if self.transform is not None:\n",
    "            spike = self.transform(spike)\n",
    "        return spike\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.spike)\n",
    "\n",
    "train_dataset = spikedataset(my_path_ground_BH + AE_train_data)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_dataset = spikedataset(my_path_ground_BH + AE_test_data)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.conv1 = nn.Conv1d(1, 32, 3, stride = 2, bias = False) # 24\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, stride = 2, bias = False) # 11\n",
    "        self.conv3 = nn.Conv1d(64, 96, 3, stride = 2, bias = False) # 4 # 병현: 여기 5인데?\n",
    "        self.fc1 = nn.Linear(96 * 5, 4, bias = False)\n",
    "        \n",
    "        # decoder\n",
    "        self.fc4 = nn.Linear(4, 5 * 96, bias = False)\n",
    "        self.deconv3 = nn.ConvTranspose1d(96, 64, 3, stride = 2, bias = False) #6 + 2 + 1= 9\n",
    "        self.deconv1 = nn.ConvTranspose1d(64, 32, 3, stride = 2, output_padding=1, bias = False) #16(9-1)*stride + 4(kernel-1) + 1 = 21\n",
    "        self.deconv2 = nn.ConvTranspose1d(32, 1, 3, stride = 2, output_padding=1, bias = False) #40 + 4 + 1 = 45\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 96 * 5)\n",
    "        mid = self.fc1(x)\n",
    "        norm = torch.sqrt(torch.sum(torch.pow(mid, 2), dim = 1))\n",
    "        h = (mid.t()/(norm + 1e-12)).t()\n",
    "\n",
    "        # decoder\n",
    "        z = F.relu(self.fc4(h))\n",
    "        z = z.view(-1, 96, 5)\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = self.deconv2(z)\n",
    "\n",
    "        return h, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): AE(\n",
      "    (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "    (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "    (conv3): Conv1d(64, 96, kernel_size=(3,), stride=(2,), bias=False)\n",
      "    (fc1): Linear(in_features=480, out_features=4, bias=False)\n",
      "    (fc4): Linear(in_features=4, out_features=480, bias=False)\n",
      "    (deconv3): ConvTranspose1d(96, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "    (deconv1): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "    (deconv2): ConvTranspose1d(32, 1, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "  )\n",
      ")\n",
      "Device: cuda\n",
      "\n",
      "Start Training, current_time = 20241231_175512\n",
      "\n",
      "epoch-0 accuracy check\n",
      "post_traincycle_acc : 75.66%\n",
      "\n",
      "epoch-0 loss : 0.14534\n",
      "\n",
      "epoch-1 accuracy check\n",
      "post_traincycle_acc : 88.77%\n",
      "\n",
      "epoch-1 loss : 0.04887\n",
      "\n",
      "epoch-2 loss : 0.04143\n",
      "\n",
      "epoch-3 loss : 0.03905\n",
      "\n",
      "epoch-4 loss : 0.03775\n",
      "\n",
      "epoch-5 loss : 0.03685\n",
      "\n",
      "epoch-6 loss : 0.03616\n",
      "\n",
      "epoch-7 loss : 0.03561\n",
      "\n",
      "epoch-8 loss : 0.03517\n",
      "\n",
      "epoch-9 loss : 0.03480\n",
      "\n",
      "epoch-10 loss : 0.03443\n",
      "\n",
      "epoch-11 loss : 0.03417\n",
      "\n",
      "epoch-12 loss : 0.03389\n",
      "\n",
      "epoch-13 loss : 0.03371\n",
      "\n",
      "epoch-14 loss : 0.03350\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# net 선택\n",
    "net = AE()\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "print(net)\n",
    "print('Device:',device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "tau = np.zeros(num_cluster)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"\\nStart Training, current_time = {current_time}\")\n",
    "best_mean_cluster_accuracy_post_training_cycle_all_dataset = 0\n",
    "for epoch in range(max_epoch):\n",
    "    cluster_accuracy_during_training_cycle_all_dataset = np.zeros(dataset_num)\n",
    "    cluster_accuracy_post_training_cycle_all_dataset = np.zeros(dataset_num)\n",
    "    cluster_accuracy_total_all_dataset = np.zeros(dataset_num)    \n",
    "    \n",
    "    if(epoch % 50 == 0 or epoch == 1): \n",
    "        print(f'\\nepoch-{epoch} accuracy check')\n",
    "        for ds in range(dataset_num):\n",
    "            # print(spike_tot[ds])\n",
    "\n",
    "            thr = thr_tot[ds]\n",
    "            spike_template = np.load(my_path_ground_BH + template[ds])\n",
    "            spike = np.load(my_path_ground_BH + spike_tot[ds])\n",
    "            label = np.load(my_path_ground_BH + label_tot[ds])\n",
    "            \n",
    "            Cluster = np.zeros((num_cluster, 4))\n",
    "            assert Cluster.shape[1] == 4, '이거 hidden dim 4 아니게 할 거면 잘 바꿔라'\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i in range(num_cluster):\n",
    "                    spike_torch = torch.from_numpy(spike_template[i, :])\n",
    "                    spike_torch = spike_torch.view(1, 1, spike_length)\n",
    "                    spike_torch = spike_torch.float().to(device)\n",
    "                    inner_inf, spike_class = net(spike_torch)\n",
    "                    Cluster[i, :] = inner_inf.cpu().detach().numpy()\n",
    "            spike_hidden = np.zeros((len(spike), 4))\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(spike)):\n",
    "                    spike_torch = torch.from_numpy(spike[i, :])\n",
    "                    spike_torch = spike_torch.view(1, 1, spike_length)\n",
    "                    spike_torch = spike_torch.float().to(device)\n",
    "                    inner_inf, spike_class = net(spike_torch)\n",
    "                    spike_hidden[i, :] = inner_inf.cpu().detach().numpy()\n",
    "                \n",
    "            spike_id = np.zeros(len(spike))\n",
    "\n",
    "\n",
    "            distance_sm = np.zeros(num_cluster)\n",
    "            tau = np.zeros(num_cluster)\n",
    "            \n",
    "            for spike_index in range(len(spike)): \n",
    "                for q in range(num_cluster):\n",
    "                    tau[q] = np.dot(spike_hidden[spike_index, :], Cluster[q, :]) # 이거 l2norm 거쳐서 나온 거니까 분모 1임.\n",
    "                # tau = np.dot(Cluster, spike_hidden[spike_index, :]) # 이거 l2norm 거쳐서 나온 거니까 분모 1임.\n",
    "\n",
    "                for i in range(num_cluster): # l2 distance\n",
    "                    distance_sm[i] = np.sum(np.power(np.abs(Cluster[i] - spike_hidden[spike_index, :]), 2))\n",
    "\n",
    "                m = np.argmin(distance_sm)\n",
    "                spike_id[spike_index] = m + 1\n",
    "                if(np.max(tau) >= cos_thr[ds] and spike_index < training_cycle): # 원래 1400 아니냐?\n",
    "                    Cluster[m] = (Cluster[m] * 15 + spike_hidden[spike_index, :])/16\n",
    "                            \n",
    "            # spike id 분포 확인하기\n",
    "            # unique_elements, counts = np.unique(spike_id, return_counts=True)\n",
    "            # print(\"Unique elements:\", unique_elements)\n",
    "            # print(\"Counts:\", counts)\n",
    "\n",
    "            cluster_accuracy_during_training_cycle = np.zeros(math.factorial(num_cluster))\n",
    "            cluster_accuracy_post_training_cycle = np.zeros(math.factorial(num_cluster))\n",
    "            cluster_accuracy_total = np.zeros(math.factorial(num_cluster))\n",
    "            \n",
    "            label_converter_ground = list(range(1, num_cluster + 1)) # [1, 2, 3, 4] 생성\n",
    "            label_converter_permutations = list(itertools.permutations(label_converter_ground)) # 모든 순열 구하기\n",
    "            perm_i = 0\n",
    "            for perm in label_converter_permutations:\n",
    "                label_converter = list(perm)\n",
    "                # print(label_converter)\n",
    "                correct_during_training_cycle = 0\n",
    "                correct_post_training_cycle = 0\n",
    "\n",
    "                assert len(spike_id) == len(label), 'spike_id랑 label 길이 같아야 됨.'\n",
    "                for i in range(len(spike_id)):\n",
    "                    if(label_converter[int(spike_id[i]-1)] == label[i]):\n",
    "                        if i < training_cycle:\n",
    "                            correct_during_training_cycle += 1\n",
    "                        else:\n",
    "                            correct_post_training_cycle += 1\n",
    "\n",
    "                cluster_accuracy_during_training_cycle[perm_i] = correct_during_training_cycle/training_cycle\n",
    "                cluster_accuracy_post_training_cycle[perm_i] = correct_post_training_cycle/(len(spike_id)-training_cycle)\n",
    "                cluster_accuracy_total[perm_i] = (correct_during_training_cycle+correct_post_training_cycle)/(len(spike_id))\n",
    "                perm_i += 1\n",
    "\n",
    "            cluster_accuracy_during_training_cycle_all_dataset[ds] = np.max(cluster_accuracy_during_training_cycle)\n",
    "            cluster_accuracy_post_training_cycle_all_dataset[ds] = cluster_accuracy_post_training_cycle[np.argmax(cluster_accuracy_during_training_cycle)]\n",
    "            cluster_accuracy_total_all_dataset[ds] = cluster_accuracy_total[np.argmax(cluster_accuracy_during_training_cycle)]\n",
    "\n",
    "        mean_cluster_accuracy_during_training_cycle_all_dataset = np.mean(cluster_accuracy_during_training_cycle_all_dataset)\n",
    "        mean_cluster_accuracy_post_training_cycle_all_dataset = np.mean(cluster_accuracy_post_training_cycle_all_dataset)\n",
    "        mean_cluster_accuracy_total_all_dataset = np.mean(cluster_accuracy_total_all_dataset)\n",
    "\n",
    "        print(f\"post_traincycle_acc : {mean_cluster_accuracy_post_training_cycle_all_dataset*100:.2f}%\")\n",
    "        \n",
    "        if mean_cluster_accuracy_post_training_cycle_all_dataset > best_mean_cluster_accuracy_post_training_cycle_all_dataset:\n",
    "            torch.save(net, f\"net_save/save_now_net_{current_time}.pth\")\n",
    "            best_mean_cluster_accuracy_post_training_cycle_all_dataset = mean_cluster_accuracy_post_training_cycle_all_dataset\n",
    "\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        spike = data\n",
    "        spike = spike.to(device)\n",
    "        spike = spike.unsqueeze(dim=1)\n",
    "        inner_inf, spike_class = net(spike)\n",
    "        loss1 = criterion(spike_class[:, 0, 5:25], spike[:, 0, 5:25])\n",
    "        loss2 = criterion(spike_class[:, 0, 0:5], spike[:, 0, 0:5])\n",
    "        loss3 = criterion(spike_class[:, 0, 25:spike_length], spike[:, 0, 25:spike_length])\n",
    "        loss = loss1 * 2.125 + (loss2 + loss3)/4\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'\\nepoch-{epoch} loss : {avg_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acc_metric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def acc_det(spike_index, spike_times, ans_times):\n",
    "        k = 0\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        spike_true_index = np.zeros(10000)\n",
    "        spike_false_index = np.zeros(10000)\n",
    "        ans_index = np.zeros(10000)\n",
    "        spike_times[spike_times == 0] = 1500000\n",
    "        det_win = 20\n",
    "\n",
    "        '''\n",
    "        f = open('./../result/TP_index.txt', 'w')\n",
    "        g = open('./../result/FP_index.txt', 'w')\n",
    "        h = open('./../result/FN_index.txt', 'w')\n",
    "        '''\n",
    "        for j in range(len(ans_times)):\n",
    "                if(ans_times[j] + det_win >= spike_times[k] and spike_times[k] >= ans_times[j] - det_win):\n",
    "                        spike_true_index[TP] = k\n",
    "                        ans_index[TP] = j\n",
    "                        #f.write('%7d %7d'%(ans_times[j], spike_times[k]) +\"\\n\")\n",
    "                        TP = TP + 1\n",
    "                        k = k + 1\n",
    "                elif(ans_times[j] + det_win < spike_times[k]):\n",
    "                        FN = FN + 1\n",
    "                        #h.write('%7d'%(ans_times[j]) +\"\\n\")\n",
    "                else:\n",
    "                        while(1):\n",
    "                                spike_false_index[FP] = k\n",
    "                                FP = FP + 1\n",
    "                                #g.write('%7d'%(spike_times[k]) + \"\\n\")\n",
    "                                k = k + 1\n",
    "                                if(ans_times[j] - det_win <= spike_times[k]):\n",
    "                                        break\n",
    "                        if(ans_times[j] + det_win >= spike_times[k]):\n",
    "                                spike_true_index[TP] = k\n",
    "                                ans_index[TP] = j\n",
    "                                #f.write('%7d %7d'%(ans_times[j], spike_times[k]) +\"\\n\")\n",
    "                                TP = TP + 1\n",
    "                                k = k + 1\n",
    "                        else:\n",
    "                                FN = FN + 1\n",
    "\t\t\t\t#h.write('%7d'%(ans_times[j]) +\"\\n\")\n",
    "        print(\"# of ans : \", len(ans_times))\n",
    "        print(\"# of TP ; \", TP)\n",
    "        print(\"# of FP ; \", FP)\n",
    "        print(\"# of FN : \", FN)\n",
    "        print(\"Det acc : \", TP/len(ans_times))\n",
    "        return spike_true_index, spike_false_index, ans_index, TP, TP/len(ans_times)\n",
    "''''\n",
    "def acc_clu(numspike, spike_id, TP, spike_true_index, ans_index, ans_cluster):\n",
    "\tcluster_accuracy = np.zeros(6)\n",
    "\tfor ep in range(6):\n",
    "\t\tif(ep == 1 or ep == 4):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 3):\n",
    "\t\t\t\t\tspike_id[i] = 2\n",
    "\t\t\t\telif(spike_id[i] == 2):\n",
    "\t\t\t\t\tspike_id[i] = 3\n",
    "\t\telif(ep == 2 or ep == 5):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 1):\n",
    "\t\t\t\t\tspike_id[i] = 2\n",
    "\t\t\t\telif(spike_id[i] == 2):\n",
    "\t\t\t\t\tspike_id[i] = 1\n",
    "\t\telif(ep == 3):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 1):\n",
    "\t\t\t\t\tspike_id[i] = 3\n",
    "\t\t\t\telif(spike_id[i] == 3):\n",
    "\t\t\t\t\tspike_id[i] = 1\n",
    "\t\ttrue_cluster = 0\n",
    "\t\tfor i in range(TP):\n",
    "\t\t\tif(spike_true_index[i] == 0) and (spike_true_index[i+1] == 0):\n",
    "\t\t\t\tprint(\"break\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tif(spike_id[int(spike_true_index[i])] == ans_cluster[int(ans_index[i])]):\n",
    "\t\t\t\t\ttrue_cluster += 1\n",
    "\t\tcluster_accuracy[ep] = true_cluster*100/TP\n",
    "\tprint('Clu acc : ', max(cluster_accuracy))\n",
    "\treturn max(cluster_accuracy)\n",
    "'''\n",
    "\n",
    "def acc(spike_index, spike_times, ans_times, spike_id, ans_cluster, training = 0):\n",
    "        k = 0\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        spike_times[spike_times == 0] = 1500000\n",
    "        det_win = 20\n",
    "        id_ssp = np.zeros(10000)\n",
    "        id_ans = np.zeros(10000)\n",
    "        id_false = np.zeros(10000)\n",
    "        training_TP = 0\n",
    "        training_ans = 0\n",
    "        training_cycle = 100\n",
    "        for j in range(len(ans_times)):\n",
    "                if(ans_times[j] + det_win >= spike_times[k] and spike_times[k] >= ans_times[j] - det_win):\n",
    "                        id_ssp[TP] = spike_id[k]\n",
    "                        id_ans[TP] = ans_cluster[j]\t\t\n",
    "                        TP = TP + 1\n",
    "                        k = k + 1\n",
    "                        if(k == training_cycle and training == 1):\n",
    "                                training_TP = TP\n",
    "                                training_ans = j\n",
    "                elif(ans_times[j] + det_win < spike_times[k]):\n",
    "                        FN = FN + 1\n",
    "                else:\n",
    "                        while(1):\n",
    "                                id_false[FP] = spike_id[k]\n",
    "                                FP = FP + 1\n",
    "\n",
    "                                k = k + 1\n",
    "                                if(k == training_cycle and training == 1):\n",
    "                                        training_TP = TP\n",
    "                                        training_ans = j\n",
    "\n",
    "                                if(ans_times[j] - det_win <= spike_times[k]):\n",
    "                                        break\n",
    "                        if(ans_times[j] + det_win >= spike_times[k]):\n",
    "                                id_ssp[TP] = spike_id[k]\n",
    "                                id_ans[TP] = ans_cluster[j]\n",
    "                                TP = TP + 1\n",
    "                                k = k + 1\n",
    "                                if(k == training_cycle and training == 1):\n",
    "                                        training_TP = TP\n",
    "                                        training_ans = j\n",
    "                        else:\n",
    "                                FN = FN + 1\n",
    "        #print(training_TP, training_ans)\n",
    "        print(\"# of ans : \", len(ans_times))\n",
    "        print(\"# of TP ; \", TP)\n",
    "        print('training miss : ', training_TP)\n",
    "        print('# of Error : ', len(ans_times)-(TP-training_TP))\n",
    "        \n",
    "        #print(\"# of FP ; \", FP)\n",
    "        print(\"# of FN : \", FN)\n",
    "        print(\"Det acc : \", (TP-training_TP)/(len(ans_times)-training_ans))\n",
    "\n",
    "        filtered_spike = 0\n",
    "        filtered_noise = 0\n",
    "        cluster_accuracy = np.zeros(6)\n",
    "        true_clusters = np.zeros(6)\n",
    "        noise = 0\n",
    "        for i in range(TP):\n",
    "                if(id_ssp[i] == 4):\n",
    "                        filtered_spike += 1\n",
    "        for i in range(FP):\n",
    "                if(id_false[i] == 4):\n",
    "                        filtered_noise += 1\n",
    "        for ep in range(6):\n",
    "                if(ep == 1 or ep == 4):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 3):\n",
    "                                        id_ssp[i] = 2\n",
    "                                elif(id_ssp[i] == 2):\n",
    "                                        id_ssp[i] = 3\n",
    "                elif(ep == 2 or ep == 5):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 1):\n",
    "                                        id_ssp[i] = 2\n",
    "                                elif(id_ssp[i] == 2):\n",
    "                                        id_ssp[i] = 1\n",
    "                elif(ep == 3):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 1):\n",
    "                                        id_ssp[i] = 3\n",
    "                                elif(id_ssp[i] == 3):\n",
    "                                        id_ssp[i] = 1\n",
    "                true_cluster = 0\n",
    "                for i in range(training_TP, TP):\n",
    "                        if(id_ssp[i] == id_ans[i]):\n",
    "                                true_cluster += 1\n",
    "                \n",
    "                cluster_accuracy[ep] = true_cluster*100/(TP-filtered_spike-training_TP)\n",
    "                true_clusters[ep] = true_cluster\n",
    "        #print('filtered noise : ', filtered_noise)\n",
    "        print('filtered spike : ', filtered_spike)\n",
    "        print(\"true cluster : \", max(true_clusters))\n",
    "        print('filtered FP : ', FP-filtered_noise)\n",
    "        print('Final det acc : ', (TP-filtered_spike-training_TP)/(len(ans_times)-training_ans))\n",
    "\n",
    "        print('Clu acc : ', max(cluster_accuracy))\n",
    "\t\n",
    "        return (TP-training_TP-filtered_spike)/(len(ans_times)-training_ans), max(cluster_accuracy), max(true_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (DEPRECATED) naive 템플릿 만들기. 이것도 라벨 필요 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import math\n",
    "# import os\n",
    "# from scipy import io\n",
    "\n",
    "# my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "# # 데이터 파일 목록과 템플릿 파일 목록 설정\n",
    "# filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "#             \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "#             \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "#             \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "# template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "# thr_tem = np.array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9])\n",
    "\n",
    "# dataset_num = 16\n",
    "# spike_needs = 100 # 템플릿을 만들기 위해 필요한 스파이크 수\n",
    "# spike_length = 50\n",
    "# wait_term = 20\n",
    "# num_cluster = 3  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "\n",
    "# for ds in range(dataset_num):\n",
    "#     print(\"\\ndata:\", filename[ds])\n",
    "#     spike_count = 0\n",
    "\n",
    "#     # 데이터 파일 불러오기\n",
    "#     mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "#     raw = mat1['data'][0]\n",
    "#     thr = thr_tem[ds]  # 스파이크 탐지 임계값 설정 \n",
    "#     spike = []  # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "#     # raw 데이터의 기울기 계산\n",
    "#     slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "#     # 스파이크 탐지\n",
    "#     wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "#     for i in range(len(raw)-2):\n",
    "#         wait += 1\n",
    "#         if(wait_term < wait):\n",
    "#             if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "#                 spike_count += 1\n",
    "#                 max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "#                 spike.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "#                 wait = 0  # 대기 시간 초기화 # 다시 wait_term만큼 기다려라\n",
    "#                 if spike_count == spike_needs:\n",
    "#                     break\n",
    "#     spike = np.array(spike)\n",
    "\n",
    "#     Cluster = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "#     distance_size = 0  # 거리 계산을 위한 배열 크기\n",
    "#     cluster_num = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    \n",
    "#     for i in range(num_cluster):\n",
    "#         distance_size += i + 1  # 거리 계산 배열 크기 계산\n",
    "#     distance = np.zeros(distance_size)  # 거리 배열 초기화 # num_cluster=3이면 1+2+3=6개의 거리를 계산해야 함.\n",
    "    \n",
    "#     # 훈련 사이클 시작\n",
    "#     for spike_index in range(spike_needs):\n",
    "#         spike_n = spike[spike_index, :]  # 현재 스파이크\n",
    "        \n",
    "#         if(spike_index == 0):\n",
    "#             Cluster[0, :] = spike_n  # 첫 번째 스파이크는 첫 번째 클러스터에 배정\n",
    "#             cluster_num[0] += 1  # 클러스터 데이터 수 증가\n",
    "#         else:\n",
    "#             # 각 클러스터와의 거리 계산\n",
    "#             for i in range(num_cluster): # 0, 1, 2 까지는 기존 클러스터와 지금 스파이크와의 거리\n",
    "#                 distance[i] = np.sum(abs(Cluster[i, 5:25] - spike_n[5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - spike_n[0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - spike_n[25:50])) * 2\n",
    "            \n",
    "#             k = 0\n",
    "#             for j in range(1, num_cluster):\n",
    "#                 k = k + j\n",
    "#                 for i in range(j, num_cluster):\n",
    "#                     # 훈련 초기 단계에서는 임계값을 1.5로 설정\n",
    "#                     if(spike_index < 30):\n",
    "#                         mer_thr = 1.5\n",
    "#                     else:\n",
    "#                         mer_thr = 2.5\n",
    "                    \n",
    "#                     # 클러스터의 데이터 수가 10 이상인 경우 거리를 크게 설정\n",
    "#                     if(cluster_num[j-1] > 10) or (cluster_num[i] > 10):\n",
    "#                         distance[i + j * num_cluster - k] = 1500000000000\n",
    "#                     else:\n",
    "#                         # 두 클러스터 간의 거리 계산\n",
    "#                         distance[i + j * num_cluster - k] = np.sum(abs(Cluster[j - 1, 5:25] - Cluster[i, 5:25])) * 17 + np.sum(abs(Cluster[j - 1, 0:5] - Cluster[i, 0:5])) * 2 + np.sum(abs(Cluster[j - 1, 25:50] - Cluster[i, 25:50])) * 2\n",
    "#                         distance[i + j * num_cluster - k] = distance[i + j * num_cluster - k] * mer_thr\n",
    "            \n",
    "#             # 가장 작은 거리를 가진 클러스터를 찾고 업데이트\n",
    "#             m = np.argmin(distance)\n",
    "#             if(m < num_cluster): #클러스터와 현재 스파이크간 거리에서 젤 작은 게 있을 때\n",
    "#                 Cluster[m, :] = (Cluster[m, :] * 15 + spike_n) / 16  # 클러스터 업데이트\n",
    "#                 cluster_num[m] += 1  # 클러스터 데이터 수 증가\n",
    "#             else:  #클러스터와 클러스터 간 거리에서 젤 작은 게 있을 때\n",
    "#                 x = num_cluster\n",
    "#                 for i in range(1, num_cluster):\n",
    "#                     y = x + num_cluster - i\n",
    "#                     if(x <= m and m < y):\n",
    "#                         # 새로운 클러스터와 기존 클러스터를 결합\n",
    "#                         Cluster[i - 1, :] = (Cluster[i - 1, :] + Cluster[m - x + i, :]) / 2\n",
    "#                         cluster_num[i - 1] = cluster_num[i - 1] + cluster_num[m - x + i]\n",
    "#                         Cluster[m - x + i, :] = spike_n  # 새로운 스파이크 할당\n",
    "#                         cluster_num[m - x + i] = 1  # 새로운 클러스터 데이터 수 1로 초기화\n",
    "#                     x = y\n",
    "#     # # Cluster plot\n",
    "#     # plt.figure(figsize=(12, 6))\n",
    "#     # colors = ['b', 'g', 'r']  # 클러스터별 색상 지정\n",
    "#     # x_axis = np.arange(spike_length)  # x축 값 (스파이크 길이)\n",
    "#     # print(cluster_num)\n",
    "#     # for i in range(num_cluster):\n",
    "#     #     plt.plot(x_axis, Cluster[i, :], label=f'Cluster {i+1}', color=colors[i % len(colors)])\n",
    "\n",
    "#     # plt.title(f'Cluster Templates for {filename[ds]}')\n",
    "#     # plt.xlabel('Sample Index')\n",
    "#     # plt.ylabel('Amplitude')\n",
    "#     # plt.legend()\n",
    "#     # plt.grid(True)\n",
    "#     # plt.show()\n",
    "    \n",
    "#     # 클러스터 템플릿을 파일로 저장\n",
    "#     np.save(my_path_ground_BH + template[ds], Cluster)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
