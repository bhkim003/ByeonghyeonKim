{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssp.train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch   \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from scipy import io\n",
    "import itertools\n",
    "import math\n",
    "import datetime\n",
    "import wandb\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from snntorch import spikegen\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_train_system( \n",
    "    gpu = '4',\n",
    "    Conv_net = True,\n",
    "    SAE_net = True,\n",
    "\n",
    "    # hyperparameter\n",
    "    dataset_num = 16,\n",
    "    spike_length = 50,\n",
    "    num_cluster = 4,  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "    training_cycle = 2400, # 그 초기 몇개까지만 cluster update할지\n",
    "\n",
    "\n",
    "    batch_size = 32,\n",
    "    max_epoch = 7000,\n",
    "    learning_rate = 0.001,\n",
    "    normalize_on = False, # True or False #이거 안 씀 # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "    need_bias = False,\n",
    "    # first_layer_no_train = False\n",
    "    lif_add_at_first = False,\n",
    "    my_seed = 42,\n",
    "\n",
    "    TIME = 10, # SAE일 때만 유효\n",
    "    v_decay = 0.5,\n",
    "    v_threshold = 0.5,\n",
    "    v_reset = 10000.0, # 10000이상 일 시 hard reset\n",
    "    BPTT_on = True,\n",
    "\n",
    "    SAE_hidden_nomean = True,\n",
    "    current_time = '20250101_210938_786',\n",
    "\n",
    "    optimizer = 'Adam',\n",
    "    coarse_com_mode = True,\n",
    "    coarse_com_config = (2.0, -2.0), # (max, min)\n",
    "\n",
    "    sae_l2_norm_bridge = True,\n",
    "    sae_lif_bridge = False,\n",
    "\n",
    "    accuracy_check_epoch_term = 5,\n",
    "    \n",
    "    lif_add_at_last = False,\n",
    "\n",
    "    two_channel_input = False,\n",
    "\n",
    "    lateral_feature_num = 4,\n",
    "\n",
    "    lc_adc_on = False, \n",
    "\n",
    "    converted_net_forward = False,\n",
    "\n",
    "    pretrained_net = None, \n",
    "\n",
    "    vth_mul_on = False,\n",
    "    batch_norm_on = False,\n",
    "\n",
    "    l2_norm_loss_weight = 0.0,\n",
    "\n",
    "    QCFS_neuron_on = False,\n",
    "\n",
    "    quantize_level_num = 0,\n",
    "\n",
    "    fusion_net = False, # True False\n",
    "    repeat_coding = False,\n",
    "    \n",
    "    sae_relu_on = False,\n",
    "\n",
    "    conv1d_scaling = False,\n",
    "\n",
    "    norm01 = True,\n",
    "\n",
    "    chan_loss_factor = 1.0,\n",
    "    ):\n",
    "    if coarse_com_mode == True:\n",
    "        assert coarse_com_config[0] > coarse_com_config[1], 'coarse_com_config[0] > coarse_com_config[1]이어야 함'\n",
    "        assert converted_net_forward == False\n",
    "        # assert SAE_net == True, 'coarse_com_mode는 SAE_net이 True일 때만 가능'\n",
    "    if two_channel_input == True:\n",
    "        assert Conv_net and coarse_com_mode, 'two_channel_input는 Conv_net이 True일 때만 가능'\n",
    "    if lc_adc_on == True:\n",
    "        assert coarse_com_mode and SAE_net, 'lc_adc_on은 coarse_com_mode와 SAE_net이 True일 때만 가능'\n",
    "    if converted_net_forward == True:\n",
    "        assert SAE_net == False, 'converted_net_forward는 SAE_net이 False일 때만 가능'\n",
    "    if conv1d_scaling:\n",
    "        assert Conv_net and coarse_com_mode and normalize_on\n",
    "    seed_assign(my_seed)\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print(hyperparameters)\n",
    "    # JSON으로 저장\n",
    "    with open(f\"result_save/cluster_accuracy_history_{current_time}.json\", 'w') as f:\n",
    "        json.dump(hyperparameters, f, indent=4)\n",
    "    ######################################################################################\n",
    "\n",
    "    \n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{current_time}_SAE_net_{SAE_net}_v_threshold_{v_threshold}'\n",
    "    ch_position = {\"geometry\": {\"7\": [20, 0], \"39\": [0, 20], \"8\": [20, 20], \"41\": [0, 40], \"6\": [20, 40], \"38\": [0, 60], \"9\": [20, 60], \"42\": [0, 80], \"5\": [20, 80], \"37\": [0, 100], \"10\": [20, 100], \"43\": [0, 120], \"4\": [20, 120], \"36\": [0, 140], \"11\": [20, 140], \"44\": [0, 160], \"35\": [0, 180], \"12\": [20, 180], \"45\": [0, 200], \"2\": [20, 200], \"34\": [0, 220], \"13\": [20, 220], \"46\": [0, 240], \"1\": [20, 240], \"33\": [0, 260], \"14\": [20, 260], \"47\": [0, 280], \"0\": [20, 280], \"32\": [0, 300], \"15\": [20, 300], \"31\": [20, 320], \"63\": [0, 340], \"16\": [20, 340], \"49\": [0, 360], \"30\": [20, 360], \"62\": [0, 380], \"17\": [20, 380], \"50\": [0, 400], \"29\": [20, 400], \"61\": [0, 420], \"18\": [20, 420], \"51\": [0, 440], \"28\": [20, 440], \"60\": [0, 460], \"19\": [20, 460], \"52\": [0, 480], \"59\": [0, 500], \"27\": [20, 500], \"53\": [0, 520], \"21\": [20, 520], \"58\": [0, 540], \"26\": [20, 540], \"54\": [0, 560], \"22\": [20, 560], \"57\": [0, 580], \"25\": [20, 580], \"55\": [0, 600], \"23\": [20, 600], \"56\": [0, 620], \"24\": [20, 620], \"71\": [20, 640], \"103\": [0, 660], \"72\": [20, 660], \"105\": [0, 680], \"70\": [20, 680], \"102\": [0, 700], \"73\": [20, 700], \"106\": [0, 720], \"74\": [20, 720], \"101\": [0, 740], \"69\": [20, 740], \"107\": [0, 760], \"75\": [20, 760], \"100\": [0, 780], \"68\": [20, 780], \"108\": [0, 800], \"99\": [0, 820], \"67\": [20, 820], \"109\": [0, 840], \"77\": [20, 840], \"98\": [0, 860], \"66\": [20, 860], \"110\": [0, 880], \"78\": [20, 880], \"97\": [0, 900], \"65\": [20, 900], \"111\": [0, 920], \"79\": [20, 920], \"96\": [0, 940], \"64\": [20, 940], \"80\": [20, 960], \"127\": [0, 980], \"95\": [20, 980], \"113\": [0, 1000], \"81\": [20, 1000], \"126\": [0, 1020], \"94\": [20, 1020], \"114\": [0, 1040], \"82\": [20, 1040], \"125\": [0, 1060], \"93\": [20, 1060], \"115\": [0, 1080], \"83\": [20, 1080], \"124\": [0, 1100], \"92\": [20, 1100], \"116\": [0, 1120], \"123\": [0, 1140], \"91\": [20, 1140], \"117\": [0, 1160], \"85\": [20, 1160], \"122\": [0, 1180], \"90\": [20, 1180], \"118\": [0, 1200], \"86\": [20, 1200], \"121\": [0, 1220], \"89\": [20, 1220], \"119\": [0, 1240], \"87\": [20, 1240], \"120\": [0, 1260], \"88\": [20, 1260]}}\n",
    "\n",
    "    data_path = [\"/data2/spike_sorting/neuropixels_choi/set1/winner_data_all_45000001_60000000.mat\",\n",
    "                 \"/data2/spike_sorting/neuropixels_choi/set2/winner_data_all_45000001_60000000.mat\",\n",
    "                 \"/data2/spike_sorting/neuropixels_choi/set3/winner_data_all_45000001_60000000.mat\",]\n",
    "    label_path = [\"/data2/spike_sorting/neuropixels_choi/set1/label_all_45000001_60000000.mat\",\n",
    "                 \"/data2/spike_sorting/neuropixels_choi/set2/label_all_45000001_60000000.mat\",\n",
    "                 \"/data2/spike_sorting/neuropixels_choi/set3/label_all_45000001_60000000.mat\",]\n",
    "\n",
    "    assert dataset_num == len(data_path) and dataset_num == len(label_path)\n",
    "    AE_train_data_path = \"/data2/spike_sorting/neuropixels_choi/winner_data_all_1_45000000.mat\" # gt\n",
    "    AE_train_label_path = \"/data2/spike_sorting/neuropixels_choi/label_all_1_45000000.mat\" # gt\n",
    "\n",
    "\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    if coarse_com_mode == True:\n",
    "        level_num = TIME\n",
    "        TIME = spike_length\n",
    "        spike_length = level_num\n",
    "        level_interval = (coarse_com_config[0] - coarse_com_config[1]) / (level_num-1)  # max - min\n",
    "        levels = [coarse_com_config[1] + level_interval * i for i in range(level_num)]\n",
    "        levels = torch.tensor(levels).to(torch.float32).to(device)\n",
    "        levels = levels.repeat(TIME,1) \n",
    "        # print('levels', levels, levels.shape) # TIME, level_num\n",
    "\n",
    "    n_sample = spike_length\n",
    "\n",
    "    class spikedataset(Dataset):\n",
    "        def __init__(self, path, transform = None):    \n",
    "            self.transform = transform\n",
    "            AE_train_data = io.loadmat(path)\n",
    "            winner_data_all = AE_train_data['winner_data_all']\n",
    "            self.spike = torch.tensor(winner_data_all).float()\n",
    "            \n",
    "        def __getitem__(self, index):\n",
    "            spike = self.spike[index]            \n",
    "            if self.transform is not None:\n",
    "                spike = self.transform(spike)\n",
    "            return spike\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.spike)\n",
    "\n",
    "    train_dataset = spikedataset(AE_train_data_path)\n",
    "    train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    # 모델 초기화\n",
    "    if SAE_net == False: # 여기서는 l2norm, lif bridge 둘 다 true면 l2norm먼저\n",
    "        assert two_channel_input == False\n",
    "\n",
    "        if Conv_net == True:\n",
    "            # input_channels = 2 if two_channel_input else 1\n",
    "            input_channels = TIME if coarse_com_mode else 1\n",
    "            if fusion_net == True:\n",
    "                assert False, '이거 맞음? 다시 확인'\n",
    "                net = FUSION_net_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                                    synapse_fc_trace_const1=1, \n",
    "                                    synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                                    TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                                    sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                                    sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, repeat_coding=repeat_coding).to(device)\n",
    "            else: \n",
    "                net = Autoencoder_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, need_bias=need_bias, l2norm_bridge=sae_l2_norm_bridge, relu_bridge=sae_lif_bridge, activation_collector_on=False,\n",
    "                                        batch_norm_on=batch_norm_on, QCFS_neuron_on=QCFS_neuron_on).to(device)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "            if converted_net_forward:\n",
    "                converted_net = SAE_converted_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                                    synapse_fc_trace_const1=1, \n",
    "                                    synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                                    TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                                    sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                                    sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last,\n",
    "                                    vth_mul_on=vth_mul_on, batch_norm_on=batch_norm_on).to(device) # lif bridge는 무조건 들어가게 해놨음.\n",
    "                converted_net = torch.nn.DataParallel(converted_net)\n",
    "                print('converted_net', converted_net)\n",
    "        else:\n",
    "            n_sample = n_sample * TIME if coarse_com_mode else n_sample\n",
    "            net = Autoencoder_only_FC(encoder_ch=[400, lateral_feature_num], decoder_ch=[400,n_sample], n_sample=n_sample, need_bias=need_bias, l2norm_bridge=sae_l2_norm_bridge, relu_bridge=sae_lif_bridge, activation_collector_on=False,\n",
    "                                    batch_norm_on=batch_norm_on, QCFS_neuron_on=QCFS_neuron_on).to(device)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "            if converted_net_forward:\n",
    "                converted_net = SAE_converted_fc(encoder_ch=[400, lateral_feature_num], \n",
    "                                    decoder_ch=[400, n_sample], \n",
    "                                    in_channels=n_sample, # in_channel 이 여기선 걍 lenght.\n",
    "                                    synapse_fc_trace_const1=1,\n",
    "                                    synapse_fc_trace_const2=v_decay,  #안씀 \n",
    "                                    TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                                    sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                                    sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last,\n",
    "                                    vth_mul_on=vth_mul_on, batch_norm_on=batch_norm_on).to(device) # lif bridge는 무조건 들어가게 해놨음.\n",
    "                converted_net = torch.nn.DataParallel(converted_net)\n",
    "                # print('converted_net', converted_net)\n",
    "    else:\n",
    "        if Conv_net == True: \n",
    "            input_channels = 1\n",
    "            input_channels = 2 if two_channel_input else 1\n",
    "            if fusion_net == True:  \n",
    "                assert coarse_com_mode == True\n",
    "                # net = SAE_FUSION2_net_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                #                     synapse_fc_trace_const1=1, \n",
    "                #                     synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                #                     TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                #                     sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                #                     sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on, sae_relu_on=sae_relu_on).to(device)\n",
    "                # net = SAE_FUSION3_net_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                #                     synapse_fc_trace_const1=1, \n",
    "                #                     synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                #                     TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                #                     sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                #                     sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on, sae_relu_on=sae_relu_on).to(device)\n",
    "                # net = SAE_FUSION4_net_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                #                     synapse_fc_trace_const1=1, \n",
    "                #                     synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                #                     TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                #                     sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                #                     sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on, sae_relu_on=sae_relu_on).to(device)\n",
    "                net = SAE_FUSION5_net_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                                    synapse_fc_trace_const1=1, \n",
    "                                    synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                                    TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                                    sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                                    sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on, sae_relu_on=sae_relu_on).to(device)\n",
    "                # net = SAE_FUSION6_net_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                #                     synapse_fc_trace_const1=1, \n",
    "                #                     synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                #                     TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                #                     sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                #                     sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on, sae_relu_on=sae_relu_on).to(device)\n",
    "                # net = SAE_FUSION7_net_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                #                     synapse_fc_trace_const1=1, \n",
    "                #                     synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                #                     TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                #                     sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                #                     sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on, sae_relu_on=sae_relu_on).to(device)\n",
    "            else:\n",
    "                net = SAE_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "                                    synapse_fc_trace_const1=1, \n",
    "                                    synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                                    TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                                    sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                                    sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on, sae_relu_on=sae_relu_on).to(device)\n",
    "            # net = SAE_conv1_DR(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, \n",
    "            #                     synapse_fc_trace_const1=1, \n",
    "            #                     synapse_fc_trace_const2=v_decay, #안씀 \n",
    "            #                     TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "            #                     sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "            #                     sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on).to(device)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "        else:\n",
    "            net = SAE_fc_only(encoder_ch=[400, lateral_feature_num], \n",
    "                                decoder_ch=[400, n_sample], \n",
    "                                in_channels=n_sample, # in_channel 이 여기선 걍 lenght.\n",
    "                                synapse_fc_trace_const1=1,\n",
    "                                synapse_fc_trace_const2=v_decay,  #안씀 \n",
    "                                TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                                sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                                sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge, lif_add_at_last=lif_add_at_last, batch_norm_on=batch_norm_on, sae_relu_on=sae_relu_on).to(device)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "\n",
    "    # net = torch.load('/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_AE_re_e7000.pth')\n",
    "    # net = torch.load('/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_20250101_210938_786.pth')\n",
    "    # load했으면 torch.nn.DataParallel 하지마\n",
    "    # net.module.load_state_dict(torch.load('/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_annbase_20250108_210641_941.pth'))\n",
    "    if pretrained_net != None:\n",
    "        ######################## 모델이 달라서 dict로 weight만 넣고싶을 때\n",
    "        # # 저장된 가중치 로드\n",
    "        saved_state_dict = torch.load(pretrained_net)\n",
    "        current_state_dict = net.module.state_dict()\n",
    "\n",
    "        # 함수 호출로 가중치 매핑\n",
    "        updated_state_dict = map_and_load_weights(saved_state_dict, current_state_dict)\n",
    "\n",
    "        # 업데이트된 state_dict를 네트워크에 로드\n",
    "        net.module.load_state_dict(updated_state_dict)\n",
    "        ######################## 모델이 달라서 dict로 weight만 넣고싶을 때\n",
    "\n",
    "        ############## 일반적일 때\n",
    "        # net.module.load_state_dict(torch.load(pretrained_net))\n",
    "        ############## 일반적일 때\n",
    "    \n",
    "        # pre_net = Autoencoder_conv1(input_channels=input_channels, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = lateral_feature_num, padding = 0, stride = 2, kernel_size = 3, need_bias=need_bias, l2norm_bridge=sae_l2_norm_bridge, relu_bridge=sae_lif_bridge, activation_collector_on=False,\n",
    "        #                         batch_norm_on=batch_norm_on, QCFS_neuron_on=False).to(device)\n",
    "        # pre_net = torch.nn.DataParallel(net)\n",
    "        # pre_net.module.load_state_dict(torch.load(pretrained_net))\n",
    "        # copy_weights(pre_net.module.encoder , net.module.encoder )\n",
    "        # copy_weights(pre_net.module.decoder , net.module.decoder  )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    wandb.watch(net, log=\"all\", log_freq = 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if SAE_net == True:\n",
    "        assert 'SAE' in net.module.__class__.__name__\n",
    "\n",
    "\n",
    "\n",
    "    net = net.to(device)\n",
    "    print(f\"Total number of encoder parameters: {sum(p.numel() for p in net.module.encoder.parameters())}\")\n",
    "    print(net)\n",
    "    print('Device:',device)\n",
    "\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        assert False, 'optimizer를 잘못 입력했습니다.'\n",
    "        \n",
    "    loss_history = []\n",
    "\n",
    "\n",
    "    print(f\"\\nStart Training, current_time = {current_time}\")\n",
    "\n",
    "    if SAE_net == True:\n",
    "        assert 'SAE' in net.module.__class__.__name__\n",
    "        \n",
    "    k_means_acc_best = 0\n",
    "    min_loss = 9999999\n",
    "    min_loss_normal = 9999999\n",
    "    min_loss_coarse = 9999999\n",
    "    for epoch in range(max_epoch):\n",
    "        print()\n",
    "        l2_loss_bin= 0\n",
    "        ae_train_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_loss_normal = 0.0\n",
    "        running_loss_coarse = 0.0\n",
    "        iter = 0\n",
    "        net.train()\n",
    "        # if True or max_epoch != 1:\n",
    "        wrong_element_sum = 0\n",
    "        same_data_num = 0\n",
    "        total_data_num = 0\n",
    "        if max_epoch != 1:\n",
    "            for data in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                total_data_num += len(data)\n",
    "                data = data.to(device)\n",
    "                scaling = (level_num-3)/level_num if conv1d_scaling else 1.0\n",
    "                data = zero_to_one_normalize_features(data, level_num=quantize_level_num, coarse_com_config=coarse_com_config, scaling=scaling, norm01=norm01) if normalize_on else data\n",
    "                spike_backup = data\n",
    "                spike = data\n",
    "                spike = spike.to(device) # batch, feature\n",
    "                spike_for_fusion2_net = spike\n",
    "                if coarse_com_mode == True and 'SAE' in net.module.__class__.__name__:\n",
    "                    spike = spike.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                    spike = (spike > levels).to(torch.float) \n",
    "\n",
    "                    spike = (spike == 0).cumsum(dim=-1).eq(1).to(torch.float) if lc_adc_on == True else spike\n",
    "\n",
    "                    # spike: batch, time, level_num\n",
    "                    # levels: time, level_num\n",
    "                    if Conv_net == True:\n",
    "                        spike = spike.unsqueeze(-2) # batch, time, in_channel, feature or batch in_channel,feature\n",
    "                        if two_channel_input == True:\n",
    "                            spike_backup = spike_backup.to(device)\n",
    "                            spike_backup = spike_backup.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                            spike_backup = (spike_backup <= levels).to(torch.float) \n",
    "                            spike_backup = (spike_backup == 1).cumsum(dim=-1).eq(1).to(torch.float) if lc_adc_on == True else spike_backup\n",
    "                            spike_backup = spike_backup.unsqueeze(-2)\n",
    "                            spike = torch.cat((spike, spike_backup), dim=-2)\n",
    "                    assert spike.shape[0] == batch_size and spike.shape[1] == TIME\n",
    "                elif 'SAE' in net.module.__class__.__name__:\n",
    "                    spike = spike.unsqueeze(-1).repeat(1, 1, TIME).permute(0,2,1) # (batch, time, feature)로 변환\n",
    "                    if Conv_net == True:\n",
    "                        spike = spike.unsqueeze(-2) # batch, time, in_channel, feature or batch in_channel,feature\n",
    "                else:\n",
    "                    if Conv_net == True:\n",
    "                        if coarse_com_mode == False:\n",
    "                            spike = spike.unsqueeze(-2) #batch in_channel,feature\n",
    "                        else:\n",
    "                            spike = spike.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                            spike = (spike > levels).to(torch.float) \n",
    "\n",
    "                            spike = (spike == 0).cumsum(dim=-1).eq(1).to(torch.float) if lc_adc_on == True else spike\n",
    "\n",
    "                    else:\n",
    "                        if coarse_com_mode == False:\n",
    "                            pass\n",
    "                        else:\n",
    "                            spike = spike.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                            spike = (spike > levels).to(torch.float) \n",
    "\n",
    "                            spike = (spike == 0).cumsum(dim=-1).eq(1).to(torch.float) if lc_adc_on == True else spike\n",
    "\n",
    "                            # spike: batch, time, feature\n",
    "                            spike = spike.reshape(spike.shape[0], -1)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "                    # if fusion_net == True:\n",
    "                    #     spike = spikegen.rate(spike, num_steps=TIME).transpose(0, 1)\n",
    "\n",
    "                # spike_class = net(spike) # batch, time, feature\n",
    "                encoded_spike = net.module.encoder(spike)\n",
    "                spike_class = net.module.decoder(encoded_spike)\n",
    "                \n",
    "                # # 각 레이어의 weight와 bias의 비트 수 출력\n",
    "                # print(\"\\n=== Encoder & Decoder Weight/Bias Bit Size ===\")\n",
    "                # for name, param in net.module.named_parameters():\n",
    "                #     bit_size = param.element_size() * 8  # 바이트 크기에 8을 곱하여 비트 수 계산\n",
    "                #     print(f\"{name}: {bit_size} bits ({param.dtype})\")\n",
    "\n",
    "\n",
    "                # print('encoded_spike', spike_class)\n",
    "\n",
    "                # for i in range (2):\n",
    "                #     plot_spike(spike[i,:,0,:].cpu().numpy())\n",
    "                #     # plot_spike(spike[i,:,1,:].cpu().numpy())\n",
    "                #     plot_origin_spike(spike_class.squeeze()[i].cpu().detach().numpy(), min_max_y_on = True)\n",
    "                # assert False\n",
    "                        \n",
    "\n",
    "                loss = 0\n",
    "                loss_normal = torch.tensor(0.0)\n",
    "                loss_coarse = torch.tensor(0.0)\n",
    "                if coarse_com_mode == True and 'SAE' in net.module.__class__.__name__:\n",
    "                    criterion = nn.MSELoss().to(device)\n",
    "                    # loss1 = nn.MSELoss()(spike_class[..., 5:25], spike[..., 5:25])\n",
    "                    # loss2 = nn.MSELoss()(spike_class[..., 0:5], spike[..., 0:5])\n",
    "                    # loss3 = nn.MSELoss()(spike_class[..., 25:spike_length], spike[..., 25:spike_length])\n",
    "                    # loss = loss1 * chan_loss_factor*2.125 + (loss2 + loss3) *(1/chan_loss_factor)/ 4 \n",
    "\n",
    "                    # loss1 = nn.MSELoss()(spike_class[..., 5:25, :], spike[..., 5:25, :])\n",
    "                    # loss2 = nn.MSELoss()(spike_class[..., 0:5, :], spike[..., 0:5, :])\n",
    "                    # loss3 = nn.MSELoss()(spike_class[..., 25:spike_length, :], spike[..., 25:spike_length, :])\n",
    "                    # loss = loss1 * chan_loss_factor*2.125 + (loss2 + loss3) *(1/chan_loss_factor)/ 4 \n",
    "                    if fusion_net:\n",
    "                        # print('1', spike.shape) # batch, time, in_channel, feature [32, 50, 1, 50]\n",
    "                        \n",
    "                        # ### coarse에서 ann loss 만들기 ######\n",
    "                        # spike = spike.squeeze()\n",
    "                        # assert two_channel_input == False\n",
    "                        # zero_mask = (spike == 0)  # 0이 있는 위치\n",
    "                        # first_zero_idx = torch.where(zero_mask, torch.arange(spike.shape[-1]).to(device), spike.shape[-1]-1).min(dim=-1).values\n",
    "                        # spike = levels[0][first_zero_idx]\n",
    "                        # # plot_origin_spike(spike[0].cpu().detach().numpy())\n",
    "                        # ### coarse에서 ann loss 만들기 ######\n",
    "\n",
    "                        ### 그냥 원래 스파이크로 ann loss 만들기 ######\n",
    "                        spike = spike_for_fusion2_net\n",
    "                        ### 그냥 원래 스파이크로 ann loss 만들기 ######\n",
    "\n",
    "                        spike = spike.squeeze()\n",
    "                        spike_class = spike_class.squeeze()\n",
    "                        \n",
    "                        ### normal loss################\n",
    "                        # loss = criterion(spike_class, spike)\n",
    "                        ### normal loss################\n",
    "                        \n",
    "                        ### chan loss################\n",
    "                        loss1 = criterion(spike_class[..., 5:25], spike[..., 5:25])\n",
    "                        loss2 = criterion(spike_class[..., 0:5], spike[..., 0:5])\n",
    "                        loss3 = criterion(spike_class[..., 25:], spike[..., 25:])\n",
    "                        loss = loss1 * chan_loss_factor*2.125 + (loss2 + loss3) *(1/chan_loss_factor)/ 4 \n",
    "                        ### chan loss################\n",
    "\n",
    "\n",
    "                        # #########################################\n",
    "                        # # 손실 함수 정의 (예: MSELoss 사용)\n",
    "                        # criterion_joke = torch.nn.MSELoss(reduction='none')  # 개별 요소별 손실을 유지\n",
    "\n",
    "                        # # 손실 계산\n",
    "                        # loss1_joke = criterion_joke(spike_class[..., 5:25], spike[..., 5:25]).mean(dim=-1)  # (batch,)\n",
    "                        # loss2_joke = criterion_joke(spike_class[..., 0:5], spike[..., 0:5]).mean(dim=-1)    # (batch,)\n",
    "                        # loss3_joke = criterion_joke(spike_class[..., 25:], spike[..., 25:]).mean(dim=-1)    # (batch,)\n",
    "\n",
    "                        # # 주어진 가중치를 적용한 최종 손실\n",
    "                        # loss_joke = loss1_joke * chan_loss_factor*2.125 + (loss2_joke + loss3_joke) *(1/chan_loss_factor)/ 4 \n",
    "\n",
    "                        # # 가장 큰 손실을 갖는 샘플의 인덱스 찾기\n",
    "                        # max_loss_idx_joke = torch.argmax(loss_joke)\n",
    "\n",
    "                        # # 해당 샘플 선택\n",
    "                        # selected_sample_class = spike_class[max_loss_idx_joke]\n",
    "                        # selected_sample_spike = spike[max_loss_idx_joke]\n",
    "\n",
    "                        # # 선택한 샘플의 손실 값 출력\n",
    "                        # print(\"Index of max loss sample:\", max_loss_idx_joke.item())\n",
    "                        # print(\"Max loss value:\", loss_joke[max_loss_idx_joke].item())\n",
    "                        # mean_loss_joke = loss_joke.mean().item()\n",
    "                        # print(\"Mean loss across the batch:\", mean_loss_joke)\n",
    "\n",
    "                        # # 선택한 샘플을 시각화\n",
    "                        # plot_origin_spike(selected_sample_class.cpu().detach().numpy())\n",
    "                        # plot_origin_spike(selected_sample_spike.cpu().detach().numpy())\n",
    "                        # #########################################\n",
    "\n",
    "                        # coarse loss ######################################################\n",
    "                        loss_normal = criterion(spike_class, spike)\n",
    "                        level_num_in_loss = spike_length\n",
    "                        level_interval = (coarse_com_config[0] - coarse_com_config[1]) / (level_num_in_loss-1)  # max - min\n",
    "                        levels = [coarse_com_config[1] + level_interval * i for i in range(level_num_in_loss)]\n",
    "                        levels = torch.tensor(levels).to(torch.float).to(device)\n",
    "                        # print('coarse leves', levels)\n",
    "                        levels = levels.repeat(spike_length,1) \n",
    "\n",
    "                        spike = spike.squeeze()\n",
    "                        spike_class = spike_class.squeeze()\n",
    "                        # plot_origin_spike(spike_class[0].cpu().detach().numpy())\n",
    "                        spike = spike.unsqueeze(2).repeat(1, 1, level_num_in_loss) \n",
    "                        spike = (spike > levels).to(torch.float) \n",
    "                        spike_class = spike_class.unsqueeze(2).repeat(1, 1, level_num_in_loss) \n",
    "                        spike_class = (spike_class > levels).to(torch.float) \n",
    "                        # spike = spike[..., 0:-3, :]\n",
    "                        # spike_class = spike_class[..., 0:-3, :]\n",
    "                        loss_coarse = criterion(spike_class, spike)\n",
    "                        wrong_element_sum += torch.sum(torch.abs(spike - spike_class)).item() \n",
    "\n",
    "                        # plot_spike(spike_class[0].cpu().detach().numpy())\n",
    "                        # assert False\n",
    "                        # coarse loss ######################################################\n",
    "                    else:\n",
    "                        spike = spike.squeeze()\n",
    "                        spike_class = spike_class.squeeze()\n",
    "                        loss = criterion(spike_class, spike)\n",
    "\n",
    "                    for iii in range(spike.shape[0]):\n",
    "                        same_data_num = same_data_num + 1 if torch.eq(spike[iii], spike_class[iii]).all() else same_data_num\n",
    "                    wrong_element_sum += torch.sum(torch.abs(spike - spike_class)).item() \n",
    "\n",
    "                    # spike = spike.squeeze()\n",
    "                    # spike_class = spike_class.squeeze()\n",
    "                    # plot_spike(spike[0].cpu().detach().numpy())\n",
    "                    # plot_spike(spike_class[0].cpu().detach().numpy())\n",
    "                    # print('손실 절대값 합',np.sum(np.abs(spike[0].cpu().detach().numpy() - spike_class[0].cpu().detach().numpy())))\n",
    "                    # # assert False\n",
    "                elif 'SAE' in net.module.__class__.__name__:\n",
    "                    criterion = nn.MSELoss().to(device)\n",
    "                    loss1 = criterion(spike_class[..., 5:25], spike[..., 5:25])\n",
    "                    loss2 = criterion(spike_class[..., 0:5], spike[..., 0:5])\n",
    "                    loss3 = criterion(spike_class[..., 25:spike_length], spike[..., 25:spike_length])\n",
    "                    # loss = loss1 * 2.125 + (loss2 + loss3)/4 # chan_loss_factor = 1\n",
    "                    loss = loss1 * chan_loss_factor*2.125 + (loss2 + loss3) *(1/chan_loss_factor)/ 4 \n",
    "                    assert spike_length > 25, 'spike_length가 25보다 작음'\n",
    "                    # wrong_element_sum += torch.sum(torch.abs(spike - spike_class)).item() \n",
    "                else:\n",
    "                    criterion = nn.MSELoss().to(device)\n",
    "                    loss1 = criterion(spike_class[..., 5:25], spike[..., 5:25])\n",
    "                    loss2 = criterion(spike_class[..., 0:5], spike[..., 0:5])\n",
    "                    loss3 = criterion(spike_class[..., 25:spike_length], spike[..., 25:spike_length])\n",
    "                    loss = loss1 * chan_loss_factor*2.125 + (loss2 + loss3) *(1/chan_loss_factor)/ 4 \n",
    "                    assert spike_length > 25, 'spike_length가 25보다 작음'\n",
    "                    # wrong_element_sum += torch.sum(torch.abs(spike - spike_class)).item() \n",
    "\n",
    "\n",
    "                    if l2_norm_loss_weight > 0:\n",
    "                        assert len(encoded_spike.shape) == 2, 'time 성분 없는 걸로'\n",
    "                        l2_loss = l2_norm_loss(encoded_spike, target_norm=1.0)  # L2Norm Loss 계산, l2 1.0되게.\n",
    "                        loss = loss + l2_loss*l2_norm_loss_weight\n",
    "                        l2_loss_bin += l2_loss.item()\n",
    "\n",
    "                    # coarse loss ######################################################\n",
    "                    loss_normal = criterion(spike_class, spike)\n",
    "                    level_num_in_loss = quantize_level_num\n",
    "                    level_interval = (coarse_com_config[0] - coarse_com_config[1]) / (level_num_in_loss-1)  # max - min\n",
    "                    \n",
    "                    levels = [coarse_com_config[1] + level_interval * i for i in range(level_num_in_loss)]\n",
    "                    levels = torch.tensor(levels).to(torch.float).to(device)\n",
    "                    levels = levels.repeat(spike_length,1) \n",
    "\n",
    "                    spike = spike.squeeze()\n",
    "                    spike_class = spike_class.squeeze()\n",
    "                    spike = spike.squeeze()\n",
    "                    spike_class = spike_class.squeeze()\n",
    "\n",
    "                    # 차원이 1이면 0차원에 크기 1인 차원 추가\n",
    "                    if spike.ndimension() == 1:\n",
    "                        spike = spike.unsqueeze(0)\n",
    "\n",
    "                    if spike_class.ndimension() == 1:\n",
    "                        spike_class = spike_class.unsqueeze(0)\n",
    "                    spike_class = spike_class.squeeze()\n",
    "                    # plot_origin_spike(spike_class[0].cpu().detach().numpy())\n",
    "                    spike = spike.unsqueeze(2).repeat(1, 1, level_num_in_loss) \n",
    "                    spike = (spike > levels).to(torch.float) \n",
    "                    spike_class = spike_class.unsqueeze(2).repeat(1, 1, level_num_in_loss) \n",
    "                    spike_class = (spike_class > levels).to(torch.float) \n",
    "                    # spike = spike[..., 0:-3, :]\n",
    "                    # spike_class = spike_class[..., 0:-3, :]\n",
    "                    loss_coarse = criterion(spike_class, spike)\n",
    "                    wrong_element_sum += torch.sum(torch.abs(spike - spike_class)).item() \n",
    "\n",
    "                    # plot_spike(spike_class[0].cpu().detach().numpy())\n",
    "                    # assert False\n",
    "                    # coarse loss ######################################################\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                running_loss_normal += loss_normal.item()\n",
    "                running_loss_coarse += loss_coarse.item()\n",
    "                # print(f'\\nepoch-{epoch}, running_loss : {running_loss:.5f}, iter percent {iter/len(train_loader)*100:.2f}%')\n",
    "                iter += 1\n",
    "        else:\n",
    "            print('\\n\\n\\n max_epoch 1이면 Train 안함!!!!!!!!!!!!!!!!!!!!!')\n",
    "        if l2_norm_loss_weight > 0:\n",
    "            print('l2_loss_bin', l2_loss_bin/len(train_loader))\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        assert not np.isnan(avg_loss), f\"Error: avg_loss is NaN! Running loss: {running_loss}, Length of train_loader: {len(train_loader)}\"\n",
    "        loss_history.append((epoch, avg_loss))\n",
    "        min_loss = min(min_loss, avg_loss)\n",
    "        min_loss_normal = min(min_loss_normal, running_loss_normal/len(train_loader))\n",
    "        min_loss_coarse = min(min_loss_coarse, running_loss_coarse/len(train_loader))\n",
    "        print(f'\\nepoch-{epoch} loss : {avg_loss:.8f}, loss_normal : {running_loss_normal/len(train_loader):.8f}, loss_coarse : {running_loss_coarse/len(train_loader):.8f}, min_loss : {min_loss:.8f}, min_loss_normal : {min_loss_normal:.8f}, min_loss_coarse : {min_loss_coarse:.8f}, wrong_element_sum : {wrong_element_sum:.8f}, same_data : {100*same_data_num/(total_data_num+1e-12):.2f}%')\n",
    "        print(f\"ae train 실행 시간: {time.time()-ae_train_start_time:.3f}초, 전체 시작 시간 {current_time}\")\n",
    "\n",
    "        # plot_activation_distribution(net)\n",
    "\n",
    "        if SAE_net == False and converted_net_forward == True:\n",
    "            source_encoder = net.module.encoder \n",
    "            target_encoder = converted_net.module.encoder  \n",
    "            copy_weights(source_encoder, target_encoder)\n",
    "\n",
    "        k_means_acc = 0\n",
    "        converted_k_means_acc = 0\n",
    "        if(epoch % accuracy_check_epoch_term == 0 or epoch == 1 or epoch == max_epoch-1): \n",
    "            accuracy_check_start_time = time.time()\n",
    "            print(f'\\nepoch-{epoch} accuracy check')\n",
    "            k_means_bin_origin_feature = []\n",
    "            k_means_bin = []\n",
    "            converted_k_means_bin = []\n",
    "            for ds in range(dataset_num):\n",
    "                num_cluster = 8 if ds == 2 else 7\n",
    "                # print('\\n', data_path[ds])\n",
    "\n",
    "                spike = torch.tensor(io.loadmat(data_path[ds])['winner_data_all']).float()\n",
    "                label = torch.tensor(io.loadmat(label_path[ds])['label_all']).float()\n",
    "                # unit, ch, start, end, gt, max_slope_index, ch_winner_determinant, set_idx\n",
    "\n",
    "                unit = label[:, 0].to(torch.int)\n",
    "                ch = label[:, 1].to(torch.int)\n",
    "                start = label[:, 2].to(torch.int)\n",
    "                end = label[:, 3].to(torch.int)\n",
    "                gt = label[:, 4].to(torch.int)\n",
    "                max_slope_index = label[:, 5].to(torch.int)\n",
    "                ch_winner_determinant = label[:, 6]\n",
    "                set_idx = label[:, 7].to(torch.int)\n",
    "\n",
    "                label = unit\n",
    "\n",
    "                scaling = (level_num-3)/level_num if conv1d_scaling else 1.0\n",
    "                spike = zero_to_one_normalize_features(spike, level_num=quantize_level_num, coarse_com_config=coarse_com_config, scaling=scaling, norm01=norm01) if normalize_on else spike\n",
    "                \n",
    "                hidden_size = lateral_feature_num*TIME if 'SAE' in net.module.__class__.__name__ and SAE_hidden_nomean == True and fusion_net == False or 'SAE_FUSION5' in net.module.__class__.__name__ else lateral_feature_num\n",
    "                hidden_size = lateral_feature_num if '_DR' in net.module.__class__.__name__  else hidden_size\n",
    "\n",
    "                encoder_batch = 128\n",
    "                spike_hidden = np.zeros((len(spike), hidden_size))\n",
    "                converted_spike_hidden = np.zeros((len(spike), hidden_size))\n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "                    now_index = 0\n",
    "                    while (1):\n",
    "                        now_end_index = now_index+encoder_batch if now_index+encoder_batch < len(spike) else len(spike)\n",
    "                        spike_batch = spike[now_index:now_end_index] \n",
    "                        spike_torch = spike_batch\n",
    "                        spike_torch = spike_torch.float()\n",
    "                        spike_backup = spike_torch\n",
    "                        spike_torch = spike_torch.to(device)\n",
    "                        if coarse_com_mode == True and 'SAE' in net.module.__class__.__name__:\n",
    "                            spike_torch = spike_torch.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                            spike_torch = (spike_torch > levels).to(torch.float) \n",
    "                            spike_torch = (spike_torch == 0).cumsum(dim=-1).eq(1).to(torch.float) if lc_adc_on == True else spike_torch\n",
    "                            if Conv_net == True:\n",
    "                                spike_torch = spike_torch.unsqueeze(-2) # batch, time, in_channel, feature or batch in_channel,feature\n",
    "                                if two_channel_input == True:\n",
    "                                    spike_backup = spike_backup.to(device)\n",
    "                                    spike_backup = spike_backup.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                                    spike_backup = (spike_backup <= levels).to(torch.float) \n",
    "                                    spike_backup = (spike_backup == 1).cumsum(dim=-1).eq(1).to(torch.float) if lc_adc_on == True else spike_backup\n",
    "                                    spike_backup = spike_backup.unsqueeze(-2)\n",
    "                                    spike_torch = torch.cat((spike_torch, spike_backup), dim=-2)\n",
    "                        elif 'SAE' in net.module.__class__.__name__:\n",
    "                            spike_torch = spike_torch.unsqueeze(1).repeat(1, TIME, 1) # (batch, time, feature)로 변환\n",
    "                            if Conv_net == True:\n",
    "                                spike_torch = spike_torch.unsqueeze(-2) # batch, time, in_channel, feature or batch in_channel,feature\n",
    "                        else:\n",
    "                            # if Conv_net == True:\n",
    "                            #     spike_torch = spike_torch.unsqueeze(-2) #batch in_channel,feature\n",
    "                            if Conv_net == True:\n",
    "                                if coarse_com_mode == False:\n",
    "                                    spike_torch = spike_torch.unsqueeze(-2) #batch in_channel,feature\n",
    "                                else:\n",
    "                                    spike_torch = spike_torch.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                                    spike_torch = (spike_torch > levels).to(torch.float) \n",
    "\n",
    "                                    spike_torch = (spike_torch == 0).cumsum(dim=-1).eq(1).to(torch.float) if lc_adc_on == True else spike_torch\n",
    "\n",
    "                            else:\n",
    "                                if coarse_com_mode == False:\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    spike_torch = spike_torch.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                                    spike_torch = (spike_torch > levels).to(torch.float) \n",
    "\n",
    "                                    spike_torch = (spike_torch == 0).cumsum(dim=-1).eq(1).to(torch.float) if lc_adc_on == True else spike_torch\n",
    "\n",
    "                                    # spike: batch, time, feature\n",
    "                                    spike_torch = spike_torch.reshape(spike_torch.shape[0], -1)\n",
    "                            if converted_net_forward == True:\n",
    "                                spike_torch_spikegen = spikegen.rate(spike_torch, num_steps=TIME).transpose(0, 1)\n",
    "                            # if fusion_net == True:\n",
    "                            #     spike_torch = spikegen.rate(spike_torch, num_steps=TIME).transpose(0, 1)\n",
    "                        ### forward #######################################################\n",
    "                        inner_inf = net.module.encoder(spike_torch)\n",
    "                        if SAE_net == False and converted_net_forward == True:\n",
    "                            converted_inner_inf = converted_net.module.encoder(spike_torch_spikegen)\n",
    "                        ### forward #######################################################\n",
    "                            \n",
    "                        if 'SAE' in net.module.__class__.__name__:\n",
    "                            if SAE_hidden_nomean == True:\n",
    "                                inner_inf = inner_inf.reshape(spike_batch.shape[0],-1)# 펼치기\n",
    "                            else:\n",
    "                                inner_inf = inner_inf.mean(dim=1)# Time 방향으로 평균\n",
    "                            # inner_inf = F.normalize(inner_inf, p=2, dim=1)\n",
    "                        spike_hidden[now_index:now_end_index] = inner_inf.cpu().detach().numpy()\n",
    "                        if SAE_net == False and converted_net_forward == True:\n",
    "                            converted_spike_hidden[now_index:now_end_index] = converted_inner_inf.cpu().detach().numpy()\n",
    "                        now_index += encoder_batch\n",
    "                        if (now_index >= len(spike)):\n",
    "                            break\n",
    "\n",
    "                origin_kmeans_accuracy = cluster_spikes_with_accuracy_torch(features= spike, true_labels=label, n_clusters=num_cluster, init_point=None)\n",
    "                kmeans_accuracy = cluster_spikes_with_accuracy_torch(features= torch.tensor(spike_hidden).to(device), true_labels=label, n_clusters=num_cluster, init_point=None)\n",
    "                \n",
    "                \n",
    "                # plot_tsne(spike_tot[ds], kmeans_accuracy, spike_hidden, label-1, n_components=2, perplexity=30, random_state=42)\n",
    "                \n",
    "                k_means_bin_origin_feature.append(origin_kmeans_accuracy)\n",
    "                k_means_bin.append(kmeans_accuracy)\n",
    "                if SAE_net == False and converted_net_forward == True:\n",
    "                    converted_kmeans_accuracy = cluster_spikes_with_accuracy_torch(features= torch.tensor(converted_spike_hidden).to(device), true_labels=label-1, n_clusters=num_cluster, init_point=None)\n",
    "                    converted_k_means_bin.append(converted_kmeans_accuracy)\n",
    "\n",
    "            print(f'k_means origin feature average accuracy : {100*sum(k_means_bin_origin_feature)/(len(k_means_bin_origin_feature)+1e-12):.8f}%, total {k_means_bin_origin_feature}')\n",
    "\n",
    "            if SAE_net == False and converted_net_forward == True:\n",
    "                converted_k_means_acc = 100*sum(converted_k_means_bin)/len(converted_k_means_bin)\n",
    "                print(f'converted_kmeans average accuracy : {converted_k_means_acc:.8f}%, total {converted_k_means_bin}')\n",
    "            k_means_acc = 100*sum(k_means_bin)/len(k_means_bin)\n",
    "            if k_means_acc > k_means_acc_best:\n",
    "                # torch.save(net, f\"net_save/save_now_net_{current_time}.pth\")\n",
    "                torch.save(net.module.state_dict(), f\"net_save/save_now_net_{current_time}.pth\")\n",
    "                print('save model')\n",
    "            \n",
    "            k_means_acc_best = max(k_means_acc_best, k_means_acc)\n",
    "            print(f'kmeans average accuracy best : {k_means_acc_best:.2f}%, kmeans average accuracy : {k_means_acc:.8f}%, total {k_means_bin}')\n",
    "            print(f\"accuracy_check 실행 시간: {time.time()-accuracy_check_start_time:.3f}초\")\n",
    "\n",
    "        wandb.log({\"avg_loss\": avg_loss})\n",
    "        wandb.log({\"k_means_acc\": k_means_acc})\n",
    "        wandb.log({\"k_means_acc_best\": k_means_acc_best})\n",
    "        wandb.log({\"converted_k_means_acc\": converted_k_means_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20250321_054928-dh5v3w1i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run%20np/runs/dh5v3w1i' target=\"_blank\">genial-galaxy-43</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run%20np' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run%20np' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run%20np</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run%20np/runs/dh5v3w1i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run%20np/runs/dh5v3w1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu': '4', 'Conv_net': True, 'SAE_net': False, 'dataset_num': 3, 'spike_length': 50, 'num_cluster': 7, 'training_cycle': 1400, 'batch_size': 32, 'max_epoch': 10000, 'learning_rate': 0.001, 'normalize_on': False, 'need_bias': False, 'lif_add_at_first': False, 'my_seed': 42, 'TIME': 50, 'v_decay': 0.5, 'v_threshold': 0.25, 'v_reset': 0.0, 'BPTT_on': True, 'SAE_hidden_nomean': True, 'current_time': '20250321_054926_895', 'optimizer': 'Adam', 'coarse_com_mode': False, 'sae_l2_norm_bridge': True, 'sae_lif_bridge': False, 'accuracy_check_epoch_term': 1, 'lif_add_at_last': False, 'two_channel_input': False, 'lateral_feature_num': 4, 'lc_adc_on': False, 'converted_net_forward': False, 'pretrained_net': None, 'vth_mul_on': False, 'batch_norm_on': False, 'l2_norm_loss_weight': 0, 'QCFS_neuron_on': False, 'quantize_level_num': 50, 'fusion_net': False, 'repeat_coding': False, 'sae_relu_on': False, 'conv1d_scaling': False, 'norm01': True, 'chan_loss_factor': 1, 'coarse_com_config': (0.999, -0.0)}\n",
      "ae conv lenght [50, 24, 11, 5]\n",
      "Total number of encoder parameters: 26592\n",
      "DataParallel(\n",
      "  (module): Autoencoder_conv1(\n",
      "    (activation_function): ReLU()\n",
      "    (encoder): Sequential(\n",
      "      (0): Conv1d(1, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): Conv1d(32, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (3): ReLU()\n",
      "      (4): Conv1d(64, 96, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (5): ReLU()\n",
      "      (6): SSBH_DimChanger_for_fc()\n",
      "      (7): Linear(in_features=480, out_features=4, bias=False)\n",
      "      (8): SSBH_L2NormLayer()\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=480, bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): SSBH_DimChanger_for_conv1()\n",
      "      (3): ConvTranspose1d(96, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (4): ReLU()\n",
      "      (5): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "      (6): ReLU()\n",
      "      (7): ConvTranspose1d(32, 1, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Device: cuda\n",
      "\n",
      "Start Training, current_time = 20250321_054926_895\n",
      "\n",
      "\n",
      "epoch-0 loss : 16726.65692750, loss_normal : 4210.29523422, loss_coarse : 0.11050843, min_loss : 16726.65692750, min_loss_normal : 4210.29523422, min_loss_coarse : 0.11050843, wrong_element_sum : 24441090.00000000, same_data : 0.00%\n",
      "ae train 실행 시간: 19.070초, 전체 시작 시간 20250321_054926_895\n",
      "\n",
      "epoch-0 accuracy check\n",
      "k_means origin feature average accuracy : 23.13300254%, total [0.207864599168947, 0.1972923527259422, 0.2888331242158093]\n",
      "save model\n",
      "kmeans average accuracy best : 23.59%, kmeans average accuracy : 23.58796686%, total [0.20806729502381677, 0.1974387120380534, 0.30213299874529487]\n",
      "accuracy_check 실행 시간: 16.174초\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpu = '4'\n",
    "Conv_net = True # True False\n",
    "SAE_net = False # True False\n",
    "\n",
    "# hyperparameter\n",
    "dataset_num = 3\n",
    "spike_length = 50 # coarse_com_mode일 때는 time step이 됨.\n",
    "num_cluster = 7 #무의미 #데이터셋마다 다름!! # 클러스터 수 설정 \n",
    "training_cycle = 1400 #1400 2400 # 그 초기 몇개까지만 cluster update할지\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "max_epoch = 10000 # 10000 # 1\n",
    "learning_rate = 0.001\n",
    "normalize_on = False # True or False # 0부터1까지 normalize\n",
    "need_bias = False\n",
    "# first_layer_no_train = False\n",
    "lif_add_at_first = False\n",
    "my_seed = 42\n",
    "\n",
    "TIME = 50 # SAE일 때만 유효. coarse_com_mode일 때는 level_num이 됨. 즉 feature 개수.\n",
    "v_decay = 0.5 # -cor\n",
    "v_threshold = 0.25 # -cor\n",
    "v_reset = 0.0 # -cor # 10000이상 일 시 hard reset\n",
    "BPTT_on = True # +cor # True False\n",
    "\n",
    "SAE_hidden_nomean = True # True False\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"_{str(int(datetime.datetime.now().microsecond / 1000)).zfill(3)}\"\n",
    "\n",
    "optimizer = 'Adam' #'Adam', 'SGD' # 둘다 준수함. loss 줄이는 거는 adam이 좋긴한데, cluster accuracy는 비슷함.\n",
    "\n",
    "coarse_com_mode = False # True False\n",
    "coarse_com_config = (0.999, -0.0) # (max, min) (0.999, -0.0) (1.0, -0.0) (2.0, -2.0) (3.0, -3.0)\n",
    "\n",
    "sae_l2_norm_bridge = True # True False\n",
    "sae_lif_bridge = False # True False\n",
    "\n",
    "accuracy_check_epoch_term = 1\n",
    "\n",
    "lif_add_at_last = False # True False\n",
    "\n",
    "two_channel_input = False # True False\n",
    "\n",
    "lateral_feature_num = 4\n",
    "\n",
    "lc_adc_on = False # True False\n",
    "\n",
    "converted_net_forward = False # True False\n",
    "\n",
    "pretrained_net = None\n",
    "# pretrained_net = '/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_중요_20250110_203117_390.pth'\n",
    "# pretrained_net = '/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_중요_20250113_134126_881_이거_94나오는거.pth'\n",
    "# pretrained_net = '/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_20250205_184901_132.pth'\n",
    "# pretrained_net = '/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_20250306_134219_133.pth'\n",
    "# pretrained_net = '/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_20250304_130322_661.pth'\n",
    "\n",
    "vth_mul_on = False # True False\n",
    "batch_norm_on = False # True False\n",
    "\n",
    "l2_norm_loss_weight = 0 #0.0001 #0.1 #  0 # 0초과면 작동\n",
    "\n",
    "QCFS_neuron_on = False # True False\n",
    "\n",
    "quantize_level_num = 50 # 0이면 quantize 안함. 1이상이면 그 수만큼 quantize함. # normalize_on 켜져야됨. 음수면 0~1norm안하고 quant함\n",
    "\n",
    "fusion_net = False # True False # SAE_net False, Conv_net True로 해라. TIME 적절하게 설정해주고.\n",
    "repeat_coding = False # True False #fusion_net에서 쓰이는 거임 # True면 repeat, False면 rate coding.\n",
    "\n",
    "sae_relu_on = False # True False\n",
    "\n",
    "conv1d_scaling = False # True False # conv1d때매 norm하고 (level_num-3)/level_num 곱해줌 # Conv_net and coarse_com_mode and normalize_on\n",
    "\n",
    "norm01 = True # True False # normalize_on = True일 때 01norm하는지 아님 걍 quant만 하는지.\n",
    "\n",
    "chan_loss_factor = 1\n",
    "\n",
    "# multi_timestep_insert = (20,10) # (20,10) # None # (한번에 넣을 timestep,stride)\n",
    "\n",
    "wandb.init(project= f'spike_sorting just run np',save_code=False)\n",
    "\n",
    "\n",
    "cluster_train_system( \n",
    "    gpu = gpu,\n",
    "    Conv_net = Conv_net,\n",
    "    SAE_net = SAE_net,\n",
    "\n",
    "    # hyperparameter\n",
    "    dataset_num = dataset_num,\n",
    "    spike_length = spike_length,\n",
    "    num_cluster = num_cluster,  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "    training_cycle = training_cycle, # 그 초기 몇개까지만 cluster update할지\n",
    "\n",
    "\n",
    "    batch_size = batch_size,\n",
    "    max_epoch = max_epoch,\n",
    "    learning_rate = learning_rate,\n",
    "    normalize_on = normalize_on, # True or False #이거 안 씀 # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "    need_bias = need_bias,\n",
    "    # first_layer_no_train = False\n",
    "    lif_add_at_first = lif_add_at_first,\n",
    "    my_seed = my_seed,\n",
    "\n",
    "    TIME = TIME, # SAE일 때만 유효\n",
    "    v_decay = v_decay,\n",
    "    v_threshold = v_threshold,\n",
    "    v_reset = v_reset, # 10000이상 일 시 hard reset\n",
    "    BPTT_on = BPTT_on,\n",
    "\n",
    "    SAE_hidden_nomean = SAE_hidden_nomean,\n",
    "    \n",
    "    current_time = current_time,\n",
    "\n",
    "    optimizer = optimizer, #'Adam', 'SGD'\n",
    "\n",
    "    coarse_com_mode = coarse_com_mode,\n",
    "    coarse_com_config = coarse_com_config, # (max, min)\n",
    "\n",
    "    \n",
    "    sae_l2_norm_bridge = sae_l2_norm_bridge,\n",
    "    sae_lif_bridge = sae_lif_bridge,\n",
    "\n",
    "    accuracy_check_epoch_term = accuracy_check_epoch_term,\n",
    "    \n",
    "    lif_add_at_last = lif_add_at_last,\n",
    "\n",
    "    two_channel_input = two_channel_input,\n",
    "\n",
    "    lateral_feature_num = lateral_feature_num,\n",
    "\n",
    "    lc_adc_on = lc_adc_on, \n",
    "\n",
    "    converted_net_forward = converted_net_forward,\n",
    "\n",
    "    pretrained_net = pretrained_net,\n",
    "\n",
    "    vth_mul_on = vth_mul_on,\n",
    "    batch_norm_on = batch_norm_on,\n",
    "\n",
    "    l2_norm_loss_weight = l2_norm_loss_weight,\n",
    "    \n",
    "    QCFS_neuron_on = QCFS_neuron_on, # True False\n",
    "\n",
    "    quantize_level_num = quantize_level_num,\n",
    "\n",
    "    fusion_net = fusion_net, # True False\n",
    "    repeat_coding = repeat_coding,\n",
    "\n",
    "    sae_relu_on = sae_relu_on,\n",
    "\n",
    "    conv1d_scaling = conv1d_scaling,\n",
    "\n",
    "    norm01 = norm01,\n",
    "\n",
    "    chan_loss_factor = chan_loss_factor,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sweep code\n",
    "\n",
    "\n",
    "# from unittest import TextTestRunner\n",
    "\n",
    "\n",
    "# unique_name_hyper = 'cluster_train_system'\n",
    "# # run_name = 'spike_sorting'\n",
    "# sweep_start_time =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"_{str(int(datetime.datetime.now().microsecond / 1000)).zfill(3)}\"\n",
    "# sweep_configuration = {\n",
    "#     'method': 'grid', # 'random', 'bayes', 'grid'\n",
    "#     'name': f'spike_sorting_{sweep_start_time}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'k_means_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"gpu\": {\"values\": ['1']},  # 이건 sweep parameter아님. hyper_iter에서 직접 설정\n",
    "#         \"Conv_net\": {\"values\": [True]}, \n",
    "#         \"SAE_net\": {\"values\": [True]}, \n",
    "\n",
    "#         \"dataset_num\": {\"values\": [16]}, \n",
    "#         \"spike_length\": {\"values\": [50]},  \n",
    "#         \"num_cluster\": {\"values\": [4]}, \n",
    "#         \"training_cycle\": {\"values\": [1400]}, # [1400, 2400]\n",
    "\n",
    "#         \"batch_size\": {\"values\": [32]}, \n",
    "#         \"max_epoch\": {\"values\": [20]}, \n",
    "#         \"learning_rate\": {\"values\": [0.001, 0.0001]},\n",
    "#         \"normalize_on\": {\"values\": [True]},\n",
    "#         \"need_bias\": {\"values\": [False]}, # [True, False]\n",
    "\n",
    "#         \"lif_add_at_first\": {\"values\": [False]}, # [True, False]\n",
    "#         \"my_seed\": {\"values\": [42]}, \n",
    "\n",
    "#         \"TIME\": {\"values\": [50]}, #  [4,6,8,10]\n",
    "#         \"v_decay\": {\"values\": [0.5]}, # [0.25,0.50,0.75]\n",
    "#         \"v_threshold\": {\"values\": [0.25]}, # [0.25,0.50,0.75]\n",
    "#         \"v_reset\": {\"values\": [0.0]},  # [0.0, 10000.0]\n",
    "#         \"BPTT_on\": {\"values\": [True]},  # [True, False]\n",
    "\n",
    "#         \"SAE_hidden_nomean\": {\"values\": [True]}, # [True, False]\n",
    "\n",
    "#         # \"current_time\": {\"values\": [current_time]} #밑에서 직접설정됨.\n",
    "\n",
    "#         \"optimizer\": {\"values\": ['Adam']}, # ['Adam', 'SGD']\n",
    "\n",
    "#         \"coarse_com_mode\": {\"values\": [True]}, # [True, False]\n",
    "#         \"coarse_com_config\": {\"values\": [(0.999, -0.0)]}, # ['Adam', 'SGD']\n",
    "\n",
    "#         \"sae_l2_norm_bridge\": {\"values\": [True]}, # [True, False]\n",
    "#         \"sae_lif_bridge\": {\"values\": [False]}, # [False, True]\n",
    "        \n",
    "#         \"accuracy_check_epoch_term\": {\"values\": [1]}, \n",
    "\n",
    "#         \"lif_add_at_last\": {\"values\": [False]},# [True, False]\n",
    "\n",
    "#         \"two_channel_input\": {\"values\": [False]},# [True, False]\n",
    "\n",
    "#         \"lateral_feature_num\": {\"values\": [2, 4]},# [True, False]\n",
    "\n",
    "#         \"lc_adc_on\": {\"values\": [False]},# [True, False]\n",
    "        \n",
    "#         \"converted_net_forward\": {\"values\": [False]},# [True, False]\n",
    "\n",
    "#         \"pretrained_net\": {\"values\": [None]},# [None]\n",
    "\n",
    "#         \"vth_mul_on\": {\"values\": [False]},# [True, False]\n",
    "#         \"batch_norm_on\": {\"values\": [False]},# [True, False]\n",
    "\n",
    "#         \"l2_norm_loss_weight\": {\"values\": [0]},\n",
    "\n",
    "#         \"QCFS_neuron_on\": {\"values\": [False]},   # [True, False]\n",
    "\n",
    "#         \"quantize_level_num\": {\"values\": [0]}, \n",
    "\n",
    "#         \"fusion_net\": {\"values\": [True]}, \n",
    "#         \"repeat_coding\": {\"values\": [False]}, \n",
    "\n",
    "#         \"sae_relu_on\": {\"values\": [False]}, \n",
    "\n",
    "#         \"conv1d_scaling\": {\"values\": [False]}, \n",
    "\n",
    "#         \"norm01\": {\"values\": [True]}, \n",
    "\n",
    "#         \"chan_loss_factor\": {\"values\": [0.25,0.5,0.75,1.0,1.25,1.5,1.75,2.0,2.5,3.0,3.5]}, \n",
    "\n",
    "        \n",
    "#      }\n",
    "# }\n",
    "\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code = False)\n",
    "#     gpu  =  '2'\n",
    "#     Conv_net  =  wandb.config.Conv_net\n",
    "#     SAE_net  =  wandb.config.SAE_net\n",
    "\n",
    "#     dataset_num  =  wandb.config.dataset_num\n",
    "#     spike_length  =  wandb.config.spike_length\n",
    "#     num_cluster  =  wandb.config.num_cluster\n",
    "#     training_cycle  =  wandb.config.training_cycle\n",
    "\n",
    "#     batch_size  =  wandb.config.batch_size\n",
    "#     max_epoch  =  wandb.config.max_epoch\n",
    "#     learning_rate  =  wandb.config.learning_rate\n",
    "#     normalize_on  =  wandb.config.normalize_on\n",
    "#     need_bias  =  wandb.config.need_bias\n",
    "\n",
    "#     lif_add_at_first  =  wandb.config.lif_add_at_first\n",
    "#     my_seed  =  wandb.config.my_seed\n",
    "\n",
    "\n",
    "#     TIME  =  wandb.config.TIME\n",
    "#     v_decay  =  wandb.config.v_decay\n",
    "#     v_threshold  =  wandb.config.v_threshold\n",
    "#     v_reset  =  wandb.config.v_reset\n",
    "#     BPTT_on  =  wandb.config.BPTT_on\n",
    "\n",
    "#     SAE_hidden_nomean  =  wandb.config.SAE_hidden_nomean\n",
    "    \n",
    "#     current_time =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"_{str(int(datetime.datetime.now().microsecond / 1000)).zfill(3)}\"\n",
    "\n",
    "#     optimizer  =  wandb.config.optimizer\n",
    "\n",
    "#     coarse_com_mode = wandb.config.coarse_com_mode\n",
    "#     coarse_com_config = wandb.config.coarse_com_config # (max, min)\n",
    "\n",
    "#     sae_l2_norm_bridge = wandb.config.sae_l2_norm_bridge\n",
    "#     sae_lif_bridge = wandb.config.sae_lif_bridge\n",
    "\n",
    "#     accuracy_check_epoch_term = wandb.config.accuracy_check_epoch_term\n",
    "\n",
    "#     lif_add_at_last = wandb.config.lif_add_at_last\n",
    "\n",
    "#     two_channel_input = wandb.config.two_channel_input\n",
    "\n",
    "#     lateral_feature_num = wandb.config.lateral_feature_num\n",
    "\n",
    "#     lc_adc_on = wandb.config.lc_adc_on\n",
    "\n",
    "#     converted_net_forward = wandb.config.converted_net_forward\n",
    "\n",
    "#     pretrained_net = wandb.config.pretrained_net\n",
    "\n",
    "#     vth_mul_on = wandb.config.vth_mul_on\n",
    "#     batch_norm_on = wandb.config.batch_norm_on\n",
    "\n",
    "#     l2_norm_loss_weight = wandb.config.l2_norm_loss_weight\n",
    "\n",
    "#     QCFS_neuron_on = wandb.config.QCFS_neuron_on\n",
    "\n",
    "#     quantize_level_num = wandb.config.quantize_level_num\n",
    "\n",
    "#     fusion_net = wandb.config.fusion_net\n",
    "#     repeat_coding = wandb.config.repeat_coding\n",
    "\n",
    "#     sae_relu_on = wandb.config.sae_relu_on\n",
    "\n",
    "#     conv1d_scaling = wandb.config.conv1d_scaling\n",
    "\n",
    "#     norm01 = wandb.config.norm01\n",
    "\n",
    "#     chan_loss_factor = wandb.config.chan_loss_factor\n",
    "\n",
    "#     cluster_train_system( \n",
    "#         gpu = gpu,\n",
    "#         Conv_net = Conv_net,\n",
    "#         SAE_net = SAE_net,\n",
    "\n",
    "#         # hyperparameter\n",
    "#         dataset_num = dataset_num,\n",
    "#         spike_length = spike_length,\n",
    "#         num_cluster = num_cluster,  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "#         training_cycle = training_cycle, # 그 초기 몇개까지만 cluster update할지\n",
    "\n",
    "\n",
    "#         batch_size = batch_size,\n",
    "#         max_epoch = max_epoch,\n",
    "#         learning_rate = learning_rate,\n",
    "#         normalize_on = normalize_on, # True or False #이거 안 씀 # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "#         need_bias = need_bias,\n",
    "#         # first_layer_no_train = False\n",
    "#         lif_add_at_first = lif_add_at_first,\n",
    "#         my_seed = my_seed,\n",
    "\n",
    "#         TIME = TIME, # SAE일 때만 유효\n",
    "#         v_decay = v_decay,\n",
    "#         v_threshold = v_threshold,\n",
    "#         v_reset = v_reset, # 10000이상 일 시 hard reset\n",
    "#         BPTT_on = BPTT_on,\n",
    "\n",
    "#         SAE_hidden_nomean = SAE_hidden_nomean,\n",
    "\n",
    "#         current_time = current_time,\n",
    "\n",
    "#         optimizer = optimizer, #'Adam', 'SGD'\n",
    "\n",
    "#         coarse_com_mode = coarse_com_mode,\n",
    "#         coarse_com_config = coarse_com_config, # (max, min)\n",
    "        \n",
    "#         sae_l2_norm_bridge = sae_l2_norm_bridge,\n",
    "#         sae_lif_bridge = sae_lif_bridge,\n",
    "\n",
    "#         accuracy_check_epoch_term = accuracy_check_epoch_term,\n",
    "\n",
    "#         lif_add_at_last = lif_add_at_last,\n",
    "        \n",
    "#         two_channel_input = two_channel_input,\n",
    "        \n",
    "#         lateral_feature_num = lateral_feature_num,\n",
    "\n",
    "#         lc_adc_on = lc_adc_on,\n",
    "\n",
    "#         converted_net_forward = converted_net_forward,\n",
    "\n",
    "#         pretrained_net = pretrained_net,\n",
    "\n",
    "#         vth_mul_on = vth_mul_on,\n",
    "#         batch_norm_on = batch_norm_on,\n",
    "\n",
    "#         l2_norm_loss_weight = l2_norm_loss_weight,\n",
    "\n",
    "#         QCFS_neuron_on = QCFS_neuron_on,\n",
    "\n",
    "#         quantize_level_num = quantize_level_num,\n",
    "\n",
    "#         fusion_net = fusion_net, \n",
    "#         repeat_coding = repeat_coding, \n",
    "\n",
    "#         sae_relu_on = sae_relu_on,\n",
    "\n",
    "#         conv1d_scaling = conv1d_scaling,\n",
    "\n",
    "#         norm01 = norm01,\n",
    "\n",
    "#         chan_loss_factor = chan_loss_factor,\n",
    "#         )\n",
    "    \n",
    "# # sweep_id = 'ygoj9jt4'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'spike_sorting {unique_name_hyper} np')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=100000, project=f'spike_sorting {unique_name_hyper}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "# import pickle\n",
    "# import json\n",
    "\n",
    "# # current_time = '20250102_225243_972'\n",
    "\n",
    "# with open(f\"result_save/cluster_accuracy_history_{current_time}.pkl\", \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "\n",
    "# # JSON으로 저장\n",
    "# with open(f\"result_save/cluster_accuracy_history_{current_time}.json\", 'r') as f:\n",
    "#     loaded_hyperparameters = json.load(f)\n",
    "\n",
    "# loss_history = data['loss_history']\n",
    "# mean_cluster_accuracy_during_training_cycle_all_dataset_history = data['mean_cluster_accuracy_during_training_cycle_all_dataset_history']\n",
    "# mean_cluster_accuracy_post_training_cycle_all_dataset_history = data['mean_cluster_accuracy_post_training_cycle_all_dataset_history']\n",
    "# mean_cluster_accuracy_total_all_dataset_history = data['mean_cluster_accuracy_total_all_dataset_history']\n",
    "# print(data)\n",
    "# max_acc = 0\n",
    "# for i in mean_cluster_accuracy_post_training_cycle_all_dataset_history:\n",
    "#     if i[1] > max_acc:\n",
    "#         max_acc = i[1]\n",
    "\n",
    "# # 설정 정보 제목 작성\n",
    "# title = (\n",
    "#     f\"Dataset Num: {loaded_hyperparameters['dataset_num']}, Conv {loaded_hyperparameters['Conv_net']}, SAE {loaded_hyperparameters['SAE_net']}, Current time {loaded_hyperparameters['current_time']}, Spike Length: {loaded_hyperparameters['spike_length']}, Num Cluster: {loaded_hyperparameters['num_cluster']}, \"\n",
    "#     f\"Training Cycle: {loaded_hyperparameters['training_cycle']}, Batch Size: {loaded_hyperparameters['batch_size']}, Max Epoch: {loaded_hyperparameters['max_epoch']}, \\n\"\n",
    "#     f\"Learning Rate: {loaded_hyperparameters['learning_rate']}, Input Normalize: {loaded_hyperparameters['normalize_on']}, Need Bias: {loaded_hyperparameters['need_bias']}, \"\n",
    "#     f\"LIF Add at First: {loaded_hyperparameters['lif_add_at_first']}, TIME: {loaded_hyperparameters['TIME']}, Seed: {loaded_hyperparameters['my_seed']}, Best ACC: {max_acc:.2f}%\"\n",
    "# )\n",
    "\n",
    "# # 데이터 리스트와 라벨 설정 (Loss 제외)\n",
    "# data_list = [\n",
    "#     (\"Mean Cluster Accuracy (During Training Cycle)\", mean_cluster_accuracy_during_training_cycle_all_dataset_history),\n",
    "#     (\"Mean Cluster Accuracy (Post Training Cycle)\", mean_cluster_accuracy_post_training_cycle_all_dataset_history),\n",
    "#     (\"Mean Cluster Accuracy (Total)\", mean_cluster_accuracy_total_all_dataset_history),\n",
    "# ]\n",
    "\n",
    "# # 플롯 생성\n",
    "# fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# # 첫 번째 y축: Accuracy 관련 데이터\n",
    "# for label, data in data_list:\n",
    "#     epochs, values = zip(*data)  # epoch, value 분리\n",
    "#     ax1.plot(epochs, values, label=label)\n",
    "\n",
    "# ax1.set_xlabel(\"Epoch\")\n",
    "# ax1.set_ylabel(\"Clurstering Accuracy [%]\", color=\"blue\")\n",
    "# ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "# ax1.legend(loc=\"center right\")\n",
    "# ax1.grid(True)\n",
    "\n",
    "# # x축을 정수만 표시하도록 설정\n",
    "# ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# # 두 번째 y축: Loss History\n",
    "# ax2 = ax1.twinx()\n",
    "# epochs, values = zip(*loss_history)\n",
    "# ax2.plot(epochs, values, label=\"AE Loss History\", color=\"red\", linestyle=\"--\")\n",
    "# ax2.set_ylabel(\"Loss\", color=\"red\")\n",
    "# ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "# ax2.legend(loc=\"center left\")\n",
    "\n",
    "# # 제목 추가\n",
    "# plt.title(title, fontsize=10)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'net_save/{current_time}', dpi=300, bbox_inches=\"tight\")  # dpi=300은 고해상도로 저장, bbox_inches=\"tight\"는 여백 최소화\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def create_image_grid(image_paths, grid_size=(4, 4)):\n",
    "    \"\"\"\n",
    "    여러 개의 이미지를 4x4 그리드로 병합하여 저장하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "        image_paths (list of str): 불러올 이미지 경로 리스트 (16개 필요).\n",
    "        grid_size (tuple): (rows, cols) 형태의 그리드 크기 (기본값: (4, 4)).\n",
    "        save_path (str): 저장할 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    rows, cols = grid_size\n",
    "    assert len(image_paths) == rows * cols, f\"Need {rows * cols} images, but got {len(image_paths)}\"\n",
    "\n",
    "    # 이미지 불러오기\n",
    "    images = [Image.open(img_path) for img_path in image_paths]\n",
    "\n",
    "    # 모든 이미지 크기 맞추기 (첫 번째 이미지 기준)\n",
    "    img_width, img_height = images[0].size\n",
    "    new_image = Image.new(\"RGB\", (cols * img_width, rows * img_height))\n",
    "\n",
    "    # 이미지 붙이기\n",
    "    for i, img in enumerate(images):\n",
    "        x_offset = (i % cols) * img_width\n",
    "        y_offset = (i // cols) * img_height\n",
    "        new_image.paste(img, (x_offset, y_offset))\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    plt.imshow(new_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# 사용 예시\n",
    "    \n",
    "\n",
    "spike_tot = [\"BH_Spike_e1n005.npy\", \"BH_Spike_e1n010.npy\", \"BH_Spike_e1n015.npy\", \"BH_Spike_e1n020.npy\",\n",
    "            \"BH_Spike_e2n005.npy\", \"BH_Spike_e2n010.npy\", \"BH_Spike_e2n015.npy\", \"BH_Spike_e2n020.npy\",\n",
    "            \"BH_Spike_d1n005.npy\", \"BH_Spike_d1n010.npy\", \"BH_Spike_d1n015.npy\", \"BH_Spike_d1n020.npy\",\n",
    "            \"BH_Spike_d2n005.npy\", \"BH_Spike_d2n010.npy\", \"BH_Spike_d2n015.npy\", \"BH_Spike_d2n020.npy\"]\n",
    "image_paths = [f\"/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/picture/{spike_tot[i]}.png\" for i in range(len(spike_tot))]  # 저장된 16개의 이미지\n",
    "create_image_grid(image_paths, grid_size=(4, 4))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
