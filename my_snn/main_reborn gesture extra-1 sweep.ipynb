{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14871/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 20664,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kodzvtqc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251028_193000-kodzvtqc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kodzvtqc' target=\"_blank\">decent-sweep-228</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kodzvtqc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kodzvtqc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251028_193007_965', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 378.0\n",
      "lif layer 1 self.abs_max_v: 378.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 408.0\n",
      "lif layer 1 self.abs_max_v: 528.0\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "lif layer 1 self.abs_max_v: 555.5\n",
      "lif layer 2 self.abs_max_v: 101.5\n",
      "fc layer 1 self.abs_max_out: 473.0\n",
      "lif layer 1 self.abs_max_v: 630.0\n",
      "fc layer 2 self.abs_max_out: 214.0\n",
      "lif layer 2 self.abs_max_v: 248.5\n",
      "lif layer 1 self.abs_max_v: 669.5\n",
      "lif layer 2 self.abs_max_v: 254.5\n",
      "lif layer 2 self.abs_max_v: 285.5\n",
      "fc layer 1 self.abs_max_out: 565.0\n",
      "lif layer 1 self.abs_max_v: 854.0\n",
      "fc layer 2 self.abs_max_out: 334.0\n",
      "lif layer 2 self.abs_max_v: 452.0\n",
      "fc layer 1 self.abs_max_out: 623.0\n",
      "fc layer 2 self.abs_max_out: 358.0\n",
      "lif layer 2 self.abs_max_v: 464.0\n",
      "fc layer 1 self.abs_max_out: 728.0\n",
      "lif layer 1 self.abs_max_v: 865.0\n",
      "fc layer 2 self.abs_max_out: 413.0\n",
      "fc layer 1 self.abs_max_out: 967.0\n",
      "lif layer 1 self.abs_max_v: 1064.5\n",
      "lif layer 2 self.abs_max_v: 470.5\n",
      "fc layer 1 self.abs_max_out: 1125.0\n",
      "lif layer 1 self.abs_max_v: 1125.0\n",
      "fc layer 2 self.abs_max_out: 526.0\n",
      "lif layer 2 self.abs_max_v: 709.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 1 self.abs_max_out: 1148.0\n",
      "lif layer 1 self.abs_max_v: 1148.0\n",
      "lif layer 2 self.abs_max_v: 772.0\n",
      "fc layer 1 self.abs_max_out: 1182.0\n",
      "lif layer 1 self.abs_max_v: 1182.0\n",
      "lif layer 1 self.abs_max_v: 1230.5\n",
      "fc layer 2 self.abs_max_out: 665.0\n",
      "lif layer 2 self.abs_max_v: 839.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 1 self.abs_max_out: 1266.0\n",
      "lif layer 1 self.abs_max_v: 1266.0\n",
      "fc layer 1 self.abs_max_out: 1416.0\n",
      "lif layer 1 self.abs_max_v: 1416.0\n",
      "lif layer 2 self.abs_max_v: 880.5\n",
      "lif layer 2 self.abs_max_v: 891.5\n",
      "fc layer 1 self.abs_max_out: 1496.0\n",
      "lif layer 1 self.abs_max_v: 1496.0\n",
      "fc layer 2 self.abs_max_out: 704.0\n",
      "lif layer 2 self.abs_max_v: 947.5\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "lif layer 2 self.abs_max_v: 991.0\n",
      "lif layer 2 self.abs_max_v: 1036.5\n",
      "fc layer 2 self.abs_max_out: 756.0\n",
      "fc layer 2 self.abs_max_out: 867.0\n",
      "lif layer 2 self.abs_max_v: 1207.5\n",
      "fc layer 2 self.abs_max_out: 940.0\n",
      "lif layer 2 self.abs_max_v: 1302.0\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "fc layer 1 self.abs_max_out: 1520.0\n",
      "lif layer 1 self.abs_max_v: 1520.0\n",
      "fc layer 1 self.abs_max_out: 1596.0\n",
      "lif layer 1 self.abs_max_v: 1596.0\n",
      "fc layer 2 self.abs_max_out: 965.0\n",
      "fc layer 1 self.abs_max_out: 1620.0\n",
      "lif layer 1 self.abs_max_v: 1620.0\n",
      "fc layer 1 self.abs_max_out: 1793.0\n",
      "lif layer 1 self.abs_max_v: 1793.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "fc layer 1 self.abs_max_out: 1899.0\n",
      "lif layer 1 self.abs_max_v: 1899.0\n",
      "fc layer 2 self.abs_max_out: 1098.0\n",
      "fc layer 1 self.abs_max_out: 1984.0\n",
      "lif layer 1 self.abs_max_v: 1984.0\n",
      "fc layer 1 self.abs_max_out: 2139.0\n",
      "lif layer 1 self.abs_max_v: 2139.0\n",
      "lif layer 2 self.abs_max_v: 1507.5\n",
      "fc layer 1 self.abs_max_out: 2243.0\n",
      "lif layer 1 self.abs_max_v: 2243.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 1 self.abs_max_out: 2366.0\n",
      "lif layer 1 self.abs_max_v: 2366.0\n",
      "lif layer 2 self.abs_max_v: 1679.5\n",
      "lif layer 2 self.abs_max_v: 1926.0\n",
      "lif layer 2 self.abs_max_v: 2027.0\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "fc layer 2 self.abs_max_out: 1160.0\n",
      "fc layer 2 self.abs_max_out: 1186.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "fc layer 2 self.abs_max_out: 1284.0\n",
      "fc layer 2 self.abs_max_out: 1373.0\n",
      "fc layer 2 self.abs_max_out: 1455.0\n",
      "fc layer 1 self.abs_max_out: 2560.0\n",
      "lif layer 1 self.abs_max_v: 2560.0\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "fc layer 2 self.abs_max_out: 1484.0\n",
      "fc layer 2 self.abs_max_out: 1508.0\n",
      "fc layer 2 self.abs_max_out: 1538.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 1 self.abs_max_out: 2812.0\n",
      "lif layer 1 self.abs_max_v: 2812.0\n",
      "fc layer 2 self.abs_max_out: 1620.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "fc layer 1 self.abs_max_out: 2889.0\n",
      "lif layer 1 self.abs_max_v: 2889.0\n",
      "lif layer 2 self.abs_max_v: 2122.5\n",
      "lif layer 2 self.abs_max_v: 2200.5\n",
      "fc layer 2 self.abs_max_out: 1624.0\n",
      "fc layer 2 self.abs_max_out: 1628.0\n",
      "fc layer 1 self.abs_max_out: 3027.0\n",
      "lif layer 1 self.abs_max_v: 3027.0\n",
      "fc layer 2 self.abs_max_out: 1714.0\n",
      "fc layer 1 self.abs_max_out: 3176.0\n",
      "lif layer 1 self.abs_max_v: 3176.0\n",
      "fc layer 2 self.abs_max_out: 1801.0\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 1 self.abs_max_out: 3198.0\n",
      "lif layer 1 self.abs_max_v: 3198.0\n",
      "fc layer 1 self.abs_max_out: 3233.0\n",
      "lif layer 1 self.abs_max_v: 3233.0\n",
      "fc layer 1 self.abs_max_out: 3380.0\n",
      "lif layer 1 self.abs_max_v: 3380.0\n",
      "fc layer 1 self.abs_max_out: 3438.0\n",
      "lif layer 1 self.abs_max_v: 3438.0\n",
      "fc layer 1 self.abs_max_out: 3607.0\n",
      "lif layer 1 self.abs_max_v: 3607.0\n",
      "fc layer 2 self.abs_max_out: 1806.0\n",
      "fc layer 1 self.abs_max_out: 3640.0\n",
      "lif layer 1 self.abs_max_v: 3640.0\n",
      "fc layer 1 self.abs_max_out: 3660.0\n",
      "lif layer 1 self.abs_max_v: 3660.0\n",
      "fc layer 1 self.abs_max_out: 3721.0\n",
      "lif layer 1 self.abs_max_v: 3721.0\n",
      "fc layer 2 self.abs_max_out: 1880.0\n",
      "fc layer 1 self.abs_max_out: 3749.0\n",
      "lif layer 1 self.abs_max_v: 3749.0\n",
      "fc layer 2 self.abs_max_out: 1883.0\n",
      "fc layer 2 self.abs_max_out: 1924.0\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "fc layer 1 self.abs_max_out: 3860.0\n",
      "lif layer 1 self.abs_max_v: 3860.0\n",
      "fc layer 2 self.abs_max_out: 1945.0\n",
      "fc layer 1 self.abs_max_out: 4101.0\n",
      "lif layer 1 self.abs_max_v: 4101.0\n",
      "lif layer 2 self.abs_max_v: 2284.5\n",
      "fc layer 2 self.abs_max_out: 2033.0\n",
      "fc layer 2 self.abs_max_out: 2070.0\n",
      "lif layer 2 self.abs_max_v: 2349.0\n",
      "lif layer 2 self.abs_max_v: 2458.5\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "fc layer 1 self.abs_max_out: 4352.0\n",
      "lif layer 1 self.abs_max_v: 4352.0\n",
      "lif layer 2 self.abs_max_v: 2486.0\n",
      "lif layer 2 self.abs_max_v: 2509.0\n",
      "lif layer 2 self.abs_max_v: 2650.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.108559/  2.123199, val:  35.42%, val_best:  35.42%, tr:  78.24%, tr_best:  78.24%, epoch time: 59.63 seconds, 0.99 minutes\n",
      "total_backward_count 9790 real_backward_count 4291  43.830%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 2 self.abs_max_out: 2079.0\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "fc layer 1 self.abs_max_out: 4608.0\n",
      "lif layer 1 self.abs_max_v: 4608.0\n",
      "fc layer 1 self.abs_max_out: 4683.0\n",
      "lif layer 1 self.abs_max_v: 4683.0\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 2 self.abs_max_out: 2188.0\n",
      "fc layer 1 self.abs_max_out: 4747.0\n",
      "lif layer 1 self.abs_max_v: 4747.0\n",
      "fc layer 2 self.abs_max_out: 2251.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "fc layer 1 self.abs_max_out: 4761.0\n",
      "lif layer 1 self.abs_max_v: 4761.0\n",
      "lif layer 1 self.abs_max_v: 4958.5\n",
      "lif layer 1 self.abs_max_v: 5399.5\n",
      "lif layer 2 self.abs_max_v: 2791.0\n",
      "fc layer 1 self.abs_max_out: 4917.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.050009/  2.136398, val:  44.17%, val_best:  44.17%, tr:  96.63%, tr_best:  96.63%, epoch time: 58.38 seconds, 0.97 minutes\n",
      "total_backward_count 19580 real_backward_count 6762  34.535%\n",
      "fc layer 2 self.abs_max_out: 2340.0\n",
      "fc layer 2 self.abs_max_out: 2413.0\n",
      "fc layer 1 self.abs_max_out: 5022.0\n",
      "fc layer 1 self.abs_max_out: 5110.0\n",
      "fc layer 1 self.abs_max_out: 5157.0\n",
      "fc layer 2 self.abs_max_out: 2424.0\n",
      "fc layer 1 self.abs_max_out: 5382.0\n",
      "lif layer 1 self.abs_max_v: 5685.0\n",
      "lif layer 2 self.abs_max_v: 2909.5\n",
      "lif layer 2 self.abs_max_v: 3132.0\n",
      "fc layer 1 self.abs_max_out: 5481.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.058161/  2.137817, val:  46.25%, val_best:  46.25%, tr:  98.26%, tr_best:  98.26%, epoch time: 59.21 seconds, 0.99 minutes\n",
      "total_backward_count 29370 real_backward_count 8890  30.269%\n",
      "fc layer 1 self.abs_max_out: 5567.0\n",
      "fc layer 1 self.abs_max_out: 5747.0\n",
      "lif layer 1 self.abs_max_v: 5747.0\n",
      "fc layer 2 self.abs_max_out: 2589.0\n",
      "fc layer 2 self.abs_max_out: 2669.0\n",
      "lif layer 1 self.abs_max_v: 6132.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.068822/  2.146090, val:  40.83%, val_best:  46.25%, tr:  98.67%, tr_best:  98.67%, epoch time: 58.89 seconds, 0.98 minutes\n",
      "total_backward_count 39160 real_backward_count 10805  27.592%\n",
      "lif layer 1 self.abs_max_v: 6402.0\n",
      "lif layer 1 self.abs_max_v: 7094.0\n",
      "lif layer 2 self.abs_max_v: 3233.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.075545/  2.159961, val:  38.33%, val_best:  46.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 48950 real_backward_count 12644  25.830%\n",
      "fc layer 1 self.abs_max_out: 5901.0\n",
      "lif layer 1 self.abs_max_v: 7461.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.065536/  2.147517, val:  50.00%, val_best:  50.00%, tr:  98.77%, tr_best:  99.49%, epoch time: 58.80 seconds, 0.98 minutes\n",
      "total_backward_count 58740 real_backward_count 14429  24.564%\n",
      "fc layer 2 self.abs_max_out: 2678.0\n",
      "fc layer 2 self.abs_max_out: 2784.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.076669/  2.150235, val:  44.17%, val_best:  50.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 58.26 seconds, 0.97 minutes\n",
      "total_backward_count 68530 real_backward_count 16171  23.597%\n",
      "fc layer 1 self.abs_max_out: 6134.0\n",
      "lif layer 1 self.abs_max_v: 7698.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.071681/  2.138699, val:  54.58%, val_best:  54.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 57.91 seconds, 0.97 minutes\n",
      "total_backward_count 78320 real_backward_count 17799  22.726%\n",
      "fc layer 1 self.abs_max_out: 6229.0\n",
      "lif layer 1 self.abs_max_v: 8129.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.080394/  2.127344, val:  62.50%, val_best:  62.50%, tr:  98.88%, tr_best:  99.49%, epoch time: 59.47 seconds, 0.99 minutes\n",
      "total_backward_count 88110 real_backward_count 19522  22.156%\n",
      "fc layer 1 self.abs_max_out: 6232.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.077148/  2.150138, val:  52.92%, val_best:  62.50%, tr:  99.18%, tr_best:  99.49%, epoch time: 58.04 seconds, 0.97 minutes\n",
      "total_backward_count 97900 real_backward_count 21184  21.638%\n",
      "fc layer 1 self.abs_max_out: 6372.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.074777/  2.147768, val:  50.00%, val_best:  62.50%, tr:  98.98%, tr_best:  99.49%, epoch time: 57.82 seconds, 0.96 minutes\n",
      "total_backward_count 107690 real_backward_count 22836  21.205%\n",
      "fc layer 1 self.abs_max_out: 6658.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.070469/  2.127114, val:  60.00%, val_best:  62.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 58.73 seconds, 0.98 minutes\n",
      "total_backward_count 117480 real_backward_count 24442  20.805%\n",
      "fc layer 2 self.abs_max_out: 2794.0\n",
      "fc layer 2 self.abs_max_out: 3094.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.067548/  2.145314, val:  55.00%, val_best:  62.50%, tr:  99.59%, tr_best:  99.69%, epoch time: 59.33 seconds, 0.99 minutes\n",
      "total_backward_count 127270 real_backward_count 25969  20.405%\n",
      "fc layer 1 self.abs_max_out: 6795.0\n",
      "lif layer 2 self.abs_max_v: 3247.5\n",
      "fc layer 1 self.abs_max_out: 6835.0\n",
      "lif layer 1 self.abs_max_v: 8641.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.074422/  2.147930, val:  49.58%, val_best:  62.50%, tr:  99.18%, tr_best:  99.69%, epoch time: 57.04 seconds, 0.95 minutes\n",
      "total_backward_count 137060 real_backward_count 27534  20.089%\n",
      "fc layer 1 self.abs_max_out: 6968.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.077356/  2.135033, val:  62.08%, val_best:  62.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 55.76 seconds, 0.93 minutes\n",
      "total_backward_count 146850 real_backward_count 28968  19.726%\n",
      "lif layer 2 self.abs_max_v: 3275.5\n",
      "lif layer 2 self.abs_max_v: 3322.5\n",
      "fc layer 2 self.abs_max_out: 3123.0\n",
      "lif layer 2 self.abs_max_v: 3367.5\n",
      "lif layer 1 self.abs_max_v: 8925.5\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.078113/  2.139361, val:  55.42%, val_best:  62.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 55.62 seconds, 0.93 minutes\n",
      "total_backward_count 156640 real_backward_count 30485  19.462%\n",
      "fc layer 1 self.abs_max_out: 7068.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.071181/  2.129601, val:  58.75%, val_best:  62.50%, tr:  99.39%, tr_best:  99.69%, epoch time: 56.10 seconds, 0.94 minutes\n",
      "total_backward_count 166430 real_backward_count 31987  19.219%\n",
      "lif layer 1 self.abs_max_v: 9124.5\n",
      "fc layer 1 self.abs_max_out: 7128.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.068293/  2.126557, val:  68.33%, val_best:  68.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 56.14 seconds, 0.94 minutes\n",
      "total_backward_count 176220 real_backward_count 33544  19.035%\n",
      "fc layer 1 self.abs_max_out: 7426.0\n",
      "lif layer 1 self.abs_max_v: 9409.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.072977/  2.129737, val:  51.25%, val_best:  68.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 55.97 seconds, 0.93 minutes\n",
      "total_backward_count 186010 real_backward_count 35108  18.874%\n",
      "lif layer 2 self.abs_max_v: 3452.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.059284/  2.133618, val:  46.67%, val_best:  68.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 56.27 seconds, 0.94 minutes\n",
      "total_backward_count 195800 real_backward_count 36505  18.644%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.060528/  2.135899, val:  55.83%, val_best:  68.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 57.80 seconds, 0.96 minutes\n",
      "total_backward_count 205590 real_backward_count 37920  18.444%\n",
      "fc layer 1 self.abs_max_out: 7581.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.069271/  2.140122, val:  57.50%, val_best:  68.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 57.92 seconds, 0.97 minutes\n",
      "total_backward_count 215380 real_backward_count 39415  18.300%\n",
      "lif layer 2 self.abs_max_v: 3483.0\n",
      "lif layer 2 self.abs_max_v: 3593.5\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.061863/  2.115463, val:  62.08%, val_best:  68.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 57.92 seconds, 0.97 minutes\n",
      "total_backward_count 225170 real_backward_count 40856  18.145%\n",
      "fc layer 1 self.abs_max_out: 7754.0\n",
      "lif layer 2 self.abs_max_v: 3692.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.057483/  2.126547, val:  60.83%, val_best:  68.33%, tr:  99.69%, tr_best:  99.80%, epoch time: 57.23 seconds, 0.95 minutes\n",
      "total_backward_count 234960 real_backward_count 42325  18.014%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.064507/  2.123296, val:  68.75%, val_best:  68.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 56.97 seconds, 0.95 minutes\n",
      "total_backward_count 244750 real_backward_count 43727  17.866%\n",
      "lif layer 2 self.abs_max_v: 3738.5\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.069736/  2.124479, val:  77.08%, val_best:  77.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 57.41 seconds, 0.96 minutes\n",
      "total_backward_count 254540 real_backward_count 45163  17.743%\n",
      "lif layer 2 self.abs_max_v: 3790.5\n",
      "fc layer 1 self.abs_max_out: 7995.0\n",
      "lif layer 1 self.abs_max_v: 9440.5\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.057230/  2.105456, val:  67.08%, val_best:  77.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 56.63 seconds, 0.94 minutes\n",
      "total_backward_count 264330 real_backward_count 46515  17.597%\n",
      "lif layer 1 self.abs_max_v: 9471.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.046983/  2.117501, val:  75.00%, val_best:  77.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 56.96 seconds, 0.95 minutes\n",
      "total_backward_count 274120 real_backward_count 47929  17.485%\n",
      "lif layer 2 self.abs_max_v: 3967.5\n",
      "lif layer 1 self.abs_max_v: 9937.5\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.046399/  2.109456, val:  55.83%, val_best:  77.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 57.76 seconds, 0.96 minutes\n",
      "total_backward_count 283910 real_backward_count 49272  17.355%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.049471/  2.103735, val:  55.00%, val_best:  77.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 58.39 seconds, 0.97 minutes\n",
      "total_backward_count 293700 real_backward_count 50616  17.234%\n",
      "lif layer 2 self.abs_max_v: 4006.0\n",
      "lif layer 2 self.abs_max_v: 4120.5\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.049055/  2.110528, val:  72.92%, val_best:  77.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 58.39 seconds, 0.97 minutes\n",
      "total_backward_count 303490 real_backward_count 51979  17.127%\n",
      "lif layer 2 self.abs_max_v: 4123.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.055739/  2.134297, val:  58.75%, val_best:  77.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 58.78 seconds, 0.98 minutes\n",
      "total_backward_count 313280 real_backward_count 53327  17.022%\n",
      "lif layer 2 self.abs_max_v: 4147.5\n",
      "lif layer 1 self.abs_max_v: 10003.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.062059/  2.123918, val:  62.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.87 seconds, 0.96 minutes\n",
      "total_backward_count 323070 real_backward_count 54582  16.895%\n",
      "fc layer 1 self.abs_max_out: 8022.0\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.051909/  2.099012, val:  77.50%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 57.83 seconds, 0.96 minutes\n",
      "total_backward_count 332860 real_backward_count 55911  16.797%\n",
      "lif layer 2 self.abs_max_v: 4211.0\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.043959/  2.094951, val:  62.50%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.18 seconds, 0.97 minutes\n",
      "total_backward_count 342650 real_backward_count 57208  16.696%\n",
      "fc layer 1 self.abs_max_out: 8168.0\n",
      "fc layer 1 self.abs_max_out: 8248.0\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.035864/  2.101155, val:  74.17%, val_best:  77.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 56.58 seconds, 0.94 minutes\n",
      "total_backward_count 352440 real_backward_count 58469  16.590%\n",
      "lif layer 2 self.abs_max_v: 4231.0\n",
      "lif layer 2 self.abs_max_v: 4284.5\n",
      "fc layer 2 self.abs_max_out: 3183.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.038266/  2.089599, val:  73.75%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.72 seconds, 0.95 minutes\n",
      "total_backward_count 362230 real_backward_count 59708  16.483%\n",
      "lif layer 1 self.abs_max_v: 10127.5\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.030912/  2.098361, val:  62.08%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.14 seconds, 0.94 minutes\n",
      "total_backward_count 372020 real_backward_count 60879  16.364%\n",
      "fc layer 1 self.abs_max_out: 8456.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.039222/  2.097055, val:  63.33%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 56.54 seconds, 0.94 minutes\n",
      "total_backward_count 381810 real_backward_count 62123  16.271%\n",
      "lif layer 2 self.abs_max_v: 4306.0\n",
      "lif layer 2 self.abs_max_v: 4310.5\n",
      "lif layer 2 self.abs_max_v: 4325.5\n",
      "lif layer 2 self.abs_max_v: 4437.0\n",
      "lif layer 1 self.abs_max_v: 10539.5\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.041765/  2.103223, val:  58.75%, val_best:  77.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 55.68 seconds, 0.93 minutes\n",
      "total_backward_count 391600 real_backward_count 63399  16.190%\n",
      "fc layer 2 self.abs_max_out: 3234.0\n",
      "lif layer 1 self.abs_max_v: 10626.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.033333/  2.098807, val:  62.08%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.26 seconds, 0.94 minutes\n",
      "total_backward_count 401390 real_backward_count 64666  16.111%\n",
      "fc layer 1 self.abs_max_out: 8530.0\n",
      "fc layer 2 self.abs_max_out: 3312.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.037562/  2.095945, val:  63.33%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.72 seconds, 0.96 minutes\n",
      "total_backward_count 411180 real_backward_count 65858  16.017%\n",
      "fc layer 1 self.abs_max_out: 8672.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.031904/  2.097287, val:  66.67%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.13 seconds, 0.97 minutes\n",
      "total_backward_count 420970 real_backward_count 67011  15.918%\n",
      "lif layer 2 self.abs_max_v: 4470.5\n",
      "lif layer 2 self.abs_max_v: 4570.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.038625/  2.092946, val:  75.83%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 430760 real_backward_count 68267  15.848%\n",
      "fc layer 1 self.abs_max_out: 8715.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.042848/  2.088149, val:  69.17%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.71 seconds, 0.96 minutes\n",
      "total_backward_count 440550 real_backward_count 69476  15.770%\n",
      "fc layer 1 self.abs_max_out: 8740.0\n",
      "lif layer 2 self.abs_max_v: 4646.5\n",
      "lif layer 2 self.abs_max_v: 4682.5\n",
      "lif layer 1 self.abs_max_v: 10703.5\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.029168/  2.075307, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 450340 real_backward_count 70656  15.689%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.025546/  2.084158, val:  69.58%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.61 seconds, 0.98 minutes\n",
      "total_backward_count 460130 real_backward_count 71845  15.614%\n",
      "lif layer 2 self.abs_max_v: 4705.5\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.029693/  2.101057, val:  57.08%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.93 seconds, 0.97 minutes\n",
      "total_backward_count 469920 real_backward_count 72991  15.533%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.031099/  2.089421, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 479710 real_backward_count 74183  15.464%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.019773/  2.081884, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.67 seconds, 0.96 minutes\n",
      "total_backward_count 489500 real_backward_count 75297  15.382%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.023803/  2.082467, val:  72.08%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 58.45 seconds, 0.97 minutes\n",
      "total_backward_count 499290 real_backward_count 76437  15.309%\n",
      "lif layer 1 self.abs_max_v: 10713.0\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.028283/  2.095803, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.61 seconds, 0.96 minutes\n",
      "total_backward_count 509080 real_backward_count 77557  15.235%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.038655/  2.094332, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.23 seconds, 0.95 minutes\n",
      "total_backward_count 518870 real_backward_count 78701  15.168%\n",
      "fc layer 1 self.abs_max_out: 8810.0\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.028144/  2.069095, val:  75.83%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.97 seconds, 0.97 minutes\n",
      "total_backward_count 528660 real_backward_count 79854  15.105%\n",
      "lif layer 2 self.abs_max_v: 4733.0\n",
      "fc layer 1 self.abs_max_out: 8844.0\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.019620/  2.085532, val:  79.58%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.70 seconds, 0.94 minutes\n",
      "total_backward_count 538450 real_backward_count 80962  15.036%\n",
      "lif layer 1 self.abs_max_v: 11122.0\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.029799/  2.079860, val:  73.33%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.31 seconds, 0.94 minutes\n",
      "total_backward_count 548240 real_backward_count 82074  14.970%\n",
      "fc layer 1 self.abs_max_out: 9025.0\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.012592/  2.062327, val:  73.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.89 seconds, 0.93 minutes\n",
      "total_backward_count 558030 real_backward_count 83184  14.907%\n",
      "lif layer 2 self.abs_max_v: 4741.5\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.013182/  2.073833, val:  69.17%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.26 seconds, 0.92 minutes\n",
      "total_backward_count 567820 real_backward_count 84237  14.835%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.017545/  2.080277, val:  80.83%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.06 seconds, 0.93 minutes\n",
      "total_backward_count 577610 real_backward_count 85332  14.773%\n",
      "fc layer 1 self.abs_max_out: 9056.0\n",
      "lif layer 2 self.abs_max_v: 4971.0\n",
      "lif layer 2 self.abs_max_v: 5160.5\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.022481/  2.077907, val:  58.75%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 55.93 seconds, 0.93 minutes\n",
      "total_backward_count 587400 real_backward_count 86379  14.705%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.019178/  2.068557, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.03 seconds, 0.93 minutes\n",
      "total_backward_count 597190 real_backward_count 87458  14.645%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.021796/  2.063942, val:  74.17%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 56.12 seconds, 0.94 minutes\n",
      "total_backward_count 606980 real_backward_count 88502  14.581%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.013439/  2.086842, val:  73.33%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 55.91 seconds, 0.93 minutes\n",
      "total_backward_count 616770 real_backward_count 89566  14.522%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.016253/  2.069115, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.37 seconds, 0.97 minutes\n",
      "total_backward_count 626560 real_backward_count 90576  14.456%\n",
      "fc layer 2 self.abs_max_out: 3433.0\n",
      "fc layer 1 self.abs_max_out: 9152.0\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.013211/  2.067772, val:  74.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.93 seconds, 0.97 minutes\n",
      "total_backward_count 636350 real_backward_count 91635  14.400%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.007335/  2.073572, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.25 seconds, 0.95 minutes\n",
      "total_backward_count 646140 real_backward_count 92655  14.340%\n",
      "fc layer 1 self.abs_max_out: 9183.0\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.011174/  2.067390, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.17 seconds, 0.97 minutes\n",
      "total_backward_count 655930 real_backward_count 93635  14.275%\n",
      "fc layer 1 self.abs_max_out: 9229.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.002372/  2.057434, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.91 seconds, 0.97 minutes\n",
      "total_backward_count 665720 real_backward_count 94645  14.217%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.005545/  2.073937, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.80 seconds, 0.96 minutes\n",
      "total_backward_count 675510 real_backward_count 95655  14.160%\n",
      "lif layer 2 self.abs_max_v: 5165.5\n",
      "lif layer 1 self.abs_max_v: 11318.5\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.003347/  2.070558, val:  64.17%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.73 seconds, 0.96 minutes\n",
      "total_backward_count 685300 real_backward_count 96650  14.103%\n",
      "lif layer 1 self.abs_max_v: 11522.0\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.001955/  2.071958, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 695090 real_backward_count 97644  14.048%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.007161/  2.078504, val:  62.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.59 seconds, 0.96 minutes\n",
      "total_backward_count 704880 real_backward_count 98628  13.992%\n",
      "lif layer 2 self.abs_max_v: 5275.0\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.006792/  2.065239, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.61 seconds, 0.98 minutes\n",
      "total_backward_count 714670 real_backward_count 99617  13.939%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.003209/  2.052828, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.89 seconds, 0.98 minutes\n",
      "total_backward_count 724460 real_backward_count 100538  13.878%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.998730/  2.064347, val:  75.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.62 seconds, 0.96 minutes\n",
      "total_backward_count 734250 real_backward_count 101513  13.825%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.001566/  2.062040, val:  79.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.97 seconds, 0.97 minutes\n",
      "total_backward_count 744040 real_backward_count 102424  13.766%\n",
      "fc layer 1 self.abs_max_out: 9474.0\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.008327/  2.064678, val:  79.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.95 seconds, 0.97 minutes\n",
      "total_backward_count 753830 real_backward_count 103408  13.718%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.996784/  2.063823, val:  62.92%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.28 seconds, 0.97 minutes\n",
      "total_backward_count 763620 real_backward_count 104383  13.669%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.993976/  2.055837, val:  73.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.62 seconds, 0.94 minutes\n",
      "total_backward_count 773410 real_backward_count 105283  13.613%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.996529/  2.054035, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.17 seconds, 0.94 minutes\n",
      "total_backward_count 783200 real_backward_count 106135  13.551%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.999372/  2.056727, val:  79.17%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 55.87 seconds, 0.93 minutes\n",
      "total_backward_count 792990 real_backward_count 107027  13.497%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.003136/  2.063342, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.01 seconds, 0.93 minutes\n",
      "total_backward_count 802780 real_backward_count 107935  13.445%\n",
      "fc layer 1 self.abs_max_out: 9485.0\n",
      "lif layer 1 self.abs_max_v: 11539.0\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.006531/  2.051452, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.76 seconds, 0.95 minutes\n",
      "total_backward_count 812570 real_backward_count 108869  13.398%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.005646/  2.061816, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.40 seconds, 0.94 minutes\n",
      "total_backward_count 822360 real_backward_count 109779  13.349%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.004568/  2.058760, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.98 seconds, 0.93 minutes\n",
      "total_backward_count 832150 real_backward_count 110735  13.307%\n",
      "lif layer 1 self.abs_max_v: 11570.0\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.997299/  2.058578, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.74 seconds, 0.98 minutes\n",
      "total_backward_count 841940 real_backward_count 111692  13.266%\n",
      "lif layer 1 self.abs_max_v: 11631.5\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.004490/  2.067407, val:  77.08%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.23 seconds, 0.95 minutes\n",
      "total_backward_count 851730 real_backward_count 112612  13.222%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.007729/  2.067342, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.90 seconds, 0.96 minutes\n",
      "total_backward_count 861520 real_backward_count 113513  13.176%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.006154/  2.051888, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.68 seconds, 0.96 minutes\n",
      "total_backward_count 871310 real_backward_count 114384  13.128%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.995465/  2.052576, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.08 seconds, 0.97 minutes\n",
      "total_backward_count 881100 real_backward_count 115334  13.090%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.994046/  2.057669, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.84 seconds, 0.96 minutes\n",
      "total_backward_count 890890 real_backward_count 116228  13.046%\n",
      "fc layer 1 self.abs_max_out: 9624.0\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.996805/  2.047770, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.90 seconds, 0.98 minutes\n",
      "total_backward_count 900680 real_backward_count 117077  12.999%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.003478/  2.058392, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.33 seconds, 0.97 minutes\n",
      "total_backward_count 910470 real_backward_count 117921  12.952%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.003619/  2.065875, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.65 seconds, 0.96 minutes\n",
      "total_backward_count 920260 real_backward_count 118772  12.906%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.997460/  2.057562, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 930050 real_backward_count 119630  12.863%\n",
      "lif layer 1 self.abs_max_v: 11717.5\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.994972/  2.051929, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.60 seconds, 0.98 minutes\n",
      "total_backward_count 939840 real_backward_count 120514  12.823%\n",
      "fc layer 1 self.abs_max_out: 9687.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.996316/  2.058323, val:  79.58%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.37 seconds, 0.97 minutes\n",
      "total_backward_count 949630 real_backward_count 121374  12.781%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.990702/  2.046266, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.53 seconds, 0.96 minutes\n",
      "total_backward_count 959420 real_backward_count 122173  12.734%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.988651/  2.066151, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.25 seconds, 0.97 minutes\n",
      "total_backward_count 969210 real_backward_count 122988  12.690%\n",
      "lif layer 1 self.abs_max_v: 11819.0\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.980901/  2.056823, val:  72.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.75 seconds, 0.96 minutes\n",
      "total_backward_count 979000 real_backward_count 123778  12.643%\n",
      "fc layer 1 self.abs_max_out: 9745.0\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.981809/  2.040246, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.93 seconds, 0.93 minutes\n",
      "total_backward_count 988790 real_backward_count 124622  12.603%\n",
      "fc layer 1 self.abs_max_out: 9755.0\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.987214/  2.045731, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.54 seconds, 0.94 minutes\n",
      "total_backward_count 998580 real_backward_count 125496  12.567%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.986432/  2.046844, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.34 seconds, 0.94 minutes\n",
      "total_backward_count 1008370 real_backward_count 126278  12.523%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.989073/  2.054403, val:  79.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.42 seconds, 0.94 minutes\n",
      "total_backward_count 1018160 real_backward_count 127097  12.483%\n",
      "lif layer 1 self.abs_max_v: 11920.5\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.982499/  2.039268, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.04 seconds, 0.93 minutes\n",
      "total_backward_count 1027950 real_backward_count 127882  12.440%\n",
      "lif layer 1 self.abs_max_v: 12065.0\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.980963/  2.039634, val:  73.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.02 seconds, 0.95 minutes\n",
      "total_backward_count 1037740 real_backward_count 128653  12.397%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.972051/  2.032811, val:  79.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.26 seconds, 0.94 minutes\n",
      "total_backward_count 1047530 real_backward_count 129454  12.358%\n",
      "fc layer 1 self.abs_max_out: 9877.0\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.963555/  2.017191, val:  78.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 1057320 real_backward_count 130281  12.322%\n",
      "fc layer 1 self.abs_max_out: 9892.0\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.964432/  2.027966, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.42 seconds, 0.96 minutes\n",
      "total_backward_count 1067110 real_backward_count 131086  12.284%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.968610/  2.027992, val:  82.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.38 seconds, 0.96 minutes\n",
      "total_backward_count 1076900 real_backward_count 131889  12.247%\n",
      "lif layer 1 self.abs_max_v: 12070.5\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.962902/  2.027005, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.58 seconds, 0.96 minutes\n",
      "total_backward_count 1086690 real_backward_count 132646  12.206%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.972719/  2.032460, val:  77.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 1096480 real_backward_count 133425  12.168%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.965181/  2.029613, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.56 seconds, 0.96 minutes\n",
      "total_backward_count 1106270 real_backward_count 134215  12.132%\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.964278/  2.035545, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.70 seconds, 0.96 minutes\n",
      "total_backward_count 1116060 real_backward_count 134956  12.092%\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.973031/  2.043687, val:  74.17%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.40 seconds, 0.96 minutes\n",
      "total_backward_count 1125850 real_backward_count 135736  12.056%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.974923/  2.044422, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.76 seconds, 0.96 minutes\n",
      "total_backward_count 1135640 real_backward_count 136537  12.023%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.981233/  2.041452, val:  76.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.16 seconds, 0.97 minutes\n",
      "total_backward_count 1145430 real_backward_count 137298  11.987%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.974493/  2.034583, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.21 seconds, 0.97 minutes\n",
      "total_backward_count 1155220 real_backward_count 138059  11.951%\n",
      "lif layer 2 self.abs_max_v: 5326.5\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.975013/  2.036461, val:  78.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.62 seconds, 0.96 minutes\n",
      "total_backward_count 1165010 real_backward_count 138843  11.918%\n",
      "fc layer 1 self.abs_max_out: 9904.0\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.977188/  2.038753, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.60 seconds, 0.94 minutes\n",
      "total_backward_count 1174800 real_backward_count 139569  11.880%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.971707/  2.043335, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.84 seconds, 0.95 minutes\n",
      "total_backward_count 1184590 real_backward_count 140302  11.844%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.975378/  2.037067, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.26 seconds, 0.94 minutes\n",
      "total_backward_count 1194380 real_backward_count 141051  11.810%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.968497/  2.027742, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.93 seconds, 0.93 minutes\n",
      "total_backward_count 1204170 real_backward_count 141806  11.776%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.967079/  2.030913, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.79 seconds, 0.93 minutes\n",
      "total_backward_count 1213960 real_backward_count 142514  11.740%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.973299/  2.032355, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.85 seconds, 0.93 minutes\n",
      "total_backward_count 1223750 real_backward_count 143265  11.707%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.973494/  2.035838, val:  78.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.12 seconds, 0.95 minutes\n",
      "total_backward_count 1233540 real_backward_count 143999  11.674%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.977725/  2.043811, val:  78.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.35 seconds, 0.94 minutes\n",
      "total_backward_count 1243330 real_backward_count 144716  11.639%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.972257/  2.038184, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.88 seconds, 0.93 minutes\n",
      "total_backward_count 1253120 real_backward_count 145438  11.606%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.963597/  2.024274, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.00 seconds, 0.95 minutes\n",
      "total_backward_count 1262910 real_backward_count 146132  11.571%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.957733/  2.017091, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.33 seconds, 0.97 minutes\n",
      "total_backward_count 1272700 real_backward_count 146835  11.537%\n",
      "fc layer 1 self.abs_max_out: 10014.0\n",
      "lif layer 2 self.abs_max_v: 5477.5\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.958409/  2.016707, val:  77.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.22 seconds, 0.97 minutes\n",
      "total_backward_count 1282490 real_backward_count 147522  11.503%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.957793/  2.021553, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.97 seconds, 0.97 minutes\n",
      "total_backward_count 1292280 real_backward_count 148273  11.474%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.956856/  2.035863, val:  71.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.77 seconds, 0.96 minutes\n",
      "total_backward_count 1302070 real_backward_count 148955  11.440%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.955355/  2.005809, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 1311860 real_backward_count 149646  11.407%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.950865/  2.026021, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.45 seconds, 0.96 minutes\n",
      "total_backward_count 1321650 real_backward_count 150332  11.375%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.957503/  2.014202, val:  84.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 1331440 real_backward_count 151048  11.345%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.955993/  2.017355, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.50 seconds, 0.96 minutes\n",
      "total_backward_count 1341230 real_backward_count 151737  11.313%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.958721/  2.018878, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.24 seconds, 0.95 minutes\n",
      "total_backward_count 1351020 real_backward_count 152439  11.283%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.954738/  2.021302, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.40 seconds, 0.96 minutes\n",
      "total_backward_count 1360810 real_backward_count 153138  11.253%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.961033/  2.028777, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.30 seconds, 0.96 minutes\n",
      "total_backward_count 1370600 real_backward_count 153855  11.225%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.958863/  2.019667, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.39 seconds, 0.96 minutes\n",
      "total_backward_count 1380390 real_backward_count 154551  11.196%\n",
      "fc layer 1 self.abs_max_out: 10096.0\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.958306/  2.033294, val:  81.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.03 seconds, 0.97 minutes\n",
      "total_backward_count 1390180 real_backward_count 155257  11.168%\n",
      "fc layer 2 self.abs_max_out: 3443.0\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.963131/  2.020714, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.94 seconds, 0.97 minutes\n",
      "total_backward_count 1399970 real_backward_count 155962  11.140%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.959698/  2.020681, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.16 seconds, 0.95 minutes\n",
      "total_backward_count 1409760 real_backward_count 156623  11.110%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.963856/  2.024347, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.23 seconds, 0.94 minutes\n",
      "total_backward_count 1419550 real_backward_count 157289  11.080%\n",
      "fc layer 1 self.abs_max_out: 10124.0\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.964441/  2.028177, val:  78.33%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.55 seconds, 0.93 minutes\n",
      "total_backward_count 1429340 real_backward_count 157941  11.050%\n",
      "fc layer 1 self.abs_max_out: 10131.0\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.960722/  2.020678, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.44 seconds, 0.94 minutes\n",
      "total_backward_count 1439130 real_backward_count 158601  11.021%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.953978/  2.013634, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.83 seconds, 0.93 minutes\n",
      "total_backward_count 1448920 real_backward_count 159246  10.991%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.954077/  2.023406, val:  75.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.93 seconds, 0.93 minutes\n",
      "total_backward_count 1458710 real_backward_count 159913  10.963%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.950260/  2.011672, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.64 seconds, 0.94 minutes\n",
      "total_backward_count 1468500 real_backward_count 160595  10.936%\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.950545/  2.028054, val:  79.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.94 seconds, 0.95 minutes\n",
      "total_backward_count 1478290 real_backward_count 161270  10.909%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.968033/  2.034951, val:  77.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.60 seconds, 0.94 minutes\n",
      "total_backward_count 1488080 real_backward_count 161971  10.885%\n",
      "fc layer 1 self.abs_max_out: 10237.0\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.962864/  2.023094, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.07 seconds, 0.95 minutes\n",
      "total_backward_count 1497870 real_backward_count 162653  10.859%\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.955588/  2.019298, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 1507660 real_backward_count 163359  10.835%\n",
      "fc layer 1 self.abs_max_out: 10264.0\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.955338/  2.026104, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.28 seconds, 0.95 minutes\n",
      "total_backward_count 1517450 real_backward_count 163980  10.806%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.950970/  2.015377, val:  70.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.82 seconds, 0.96 minutes\n",
      "total_backward_count 1527240 real_backward_count 164651  10.781%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.948251/  2.013698, val:  83.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.49 seconds, 0.96 minutes\n",
      "total_backward_count 1537030 real_backward_count 165286  10.754%\n",
      "fc layer 2 self.abs_max_out: 3504.0\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.946984/  2.015126, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 1546820 real_backward_count 165919  10.726%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.949089/  2.012891, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.05 seconds, 0.97 minutes\n",
      "total_backward_count 1556610 real_backward_count 166523  10.698%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.947302/  2.015825, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.54 seconds, 0.98 minutes\n",
      "total_backward_count 1566400 real_backward_count 167165  10.672%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.954880/  2.011002, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.98 seconds, 0.97 minutes\n",
      "total_backward_count 1576190 real_backward_count 167758  10.643%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.957105/  2.015084, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 1585980 real_backward_count 168408  10.619%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.951044/  2.013989, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.77 seconds, 0.96 minutes\n",
      "total_backward_count 1595770 real_backward_count 169032  10.593%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.949443/  2.022336, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 1605560 real_backward_count 169663  10.567%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.955493/  2.014277, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.88 seconds, 0.96 minutes\n",
      "total_backward_count 1615350 real_backward_count 170300  10.543%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.953457/  2.012960, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.56 seconds, 0.94 minutes\n",
      "total_backward_count 1625140 real_backward_count 170904  10.516%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.949788/  2.024912, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.09 seconds, 0.93 minutes\n",
      "total_backward_count 1634930 real_backward_count 171510  10.490%\n",
      "fc layer 1 self.abs_max_out: 10400.0\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.961208/  2.029749, val:  80.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.80 seconds, 0.93 minutes\n",
      "total_backward_count 1644720 real_backward_count 172152  10.467%\n",
      "fc layer 1 self.abs_max_out: 10411.0\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.958966/  2.023063, val:  77.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.52 seconds, 0.94 minutes\n",
      "total_backward_count 1654510 real_backward_count 172738  10.440%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.958293/  2.010608, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.14 seconds, 0.94 minutes\n",
      "total_backward_count 1664300 real_backward_count 173330  10.415%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.957678/  2.014812, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.90 seconds, 0.93 minutes\n",
      "total_backward_count 1674090 real_backward_count 173910  10.388%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.952386/  2.013443, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.58 seconds, 0.94 minutes\n",
      "total_backward_count 1683880 real_backward_count 174448  10.360%\n",
      "fc layer 1 self.abs_max_out: 10435.0\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.950673/  2.014596, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.08 seconds, 0.97 minutes\n",
      "total_backward_count 1693670 real_backward_count 175046  10.335%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.948574/  2.010170, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.57 seconds, 0.96 minutes\n",
      "total_backward_count 1703460 real_backward_count 175622  10.310%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.940717/  2.001810, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.34 seconds, 0.96 minutes\n",
      "total_backward_count 1713250 real_backward_count 176210  10.285%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.937024/  2.000844, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.71 seconds, 0.96 minutes\n",
      "total_backward_count 1723040 real_backward_count 176858  10.264%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.940303/  2.010424, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.37 seconds, 0.97 minutes\n",
      "total_backward_count 1732830 real_backward_count 177454  10.241%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.943633/  2.011708, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.94 seconds, 0.97 minutes\n",
      "total_backward_count 1742620 real_backward_count 178038  10.217%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.947387/  2.008227, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.22 seconds, 0.97 minutes\n",
      "total_backward_count 1752410 real_backward_count 178601  10.192%\n",
      "lif layer 1 self.abs_max_v: 12076.5\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.947008/  2.016523, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.92 seconds, 0.97 minutes\n",
      "total_backward_count 1762200 real_backward_count 179223  10.170%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.947206/  2.004785, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 59.06 seconds, 0.98 minutes\n",
      "total_backward_count 1771990 real_backward_count 179847  10.149%\n",
      "fc layer 1 self.abs_max_out: 10488.0\n",
      "lif layer 1 self.abs_max_v: 12125.0\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.946606/  2.014520, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.56 seconds, 0.96 minutes\n",
      "total_backward_count 1781780 real_backward_count 180418  10.126%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.946437/  2.018983, val:  78.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 59.25 seconds, 0.99 minutes\n",
      "total_backward_count 1791570 real_backward_count 181007  10.103%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.950048/  2.016143, val:  79.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.79 seconds, 0.95 minutes\n",
      "total_backward_count 1801360 real_backward_count 181596  10.081%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.947787/  2.017904, val:  80.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 1811150 real_backward_count 182178  10.059%\n",
      "fc layer 1 self.abs_max_out: 10522.0\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.947949/  2.014833, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.96 seconds, 0.95 minutes\n",
      "total_backward_count 1820940 real_backward_count 182753  10.036%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.951435/  2.018358, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.48 seconds, 0.96 minutes\n",
      "total_backward_count 1830730 real_backward_count 183329  10.014%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.943388/  2.014939, val:  79.58%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.33 seconds, 0.94 minutes\n",
      "total_backward_count 1840520 real_backward_count 183935   9.994%\n",
      "fc layer 1 self.abs_max_out: 10540.0\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.939785/  2.005690, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.81 seconds, 0.93 minutes\n",
      "total_backward_count 1850310 real_backward_count 184460   9.969%\n",
      "fc layer 1 self.abs_max_out: 10573.0\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.946047/  2.001875, val:  80.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.91 seconds, 0.95 minutes\n",
      "total_backward_count 1860100 real_backward_count 185019   9.947%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.939194/  2.010684, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.88 seconds, 0.93 minutes\n",
      "total_backward_count 1869890 real_backward_count 185547   9.923%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.944108/  2.005055, val:  80.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.97 seconds, 0.93 minutes\n",
      "total_backward_count 1879680 real_backward_count 186088   9.900%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.933448/  1.999969, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.98 seconds, 0.93 minutes\n",
      "total_backward_count 1889470 real_backward_count 186655   9.879%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.940624/  2.008492, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.85 seconds, 0.95 minutes\n",
      "total_backward_count 1899260 real_backward_count 187197   9.856%\n",
      "fc layer 2 self.abs_max_out: 3570.0\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.940130/  2.006960, val:  82.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.64 seconds, 0.96 minutes\n",
      "total_backward_count 1909050 real_backward_count 187733   9.834%\n",
      "lif layer 1 self.abs_max_v: 12127.5\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.933737/  2.005612, val:  87.08%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.17 seconds, 0.97 minutes\n",
      "total_backward_count 1918840 real_backward_count 188244   9.810%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.930717/  1.997285, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.51 seconds, 0.96 minutes\n",
      "total_backward_count 1928630 real_backward_count 188839   9.791%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.929575/  2.001356, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.81 seconds, 0.96 minutes\n",
      "total_backward_count 1938420 real_backward_count 189390   9.770%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.927508/  2.005605, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.57 seconds, 0.96 minutes\n",
      "total_backward_count 1948210 real_backward_count 189909   9.748%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.934427/  2.005831, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.88 seconds, 0.96 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8630d5ec6268441097b4d18b2d932959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.93443</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.00583</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sweep-228</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kodzvtqc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kodzvtqc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251028_193000-kodzvtqc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ierkzf1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee832626fe39434c8eef5c2bda754b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113265487882827, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251028_224200-5ierkzf1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ierkzf1' target=\"_blank\">olive-sweep-228</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ierkzf1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ierkzf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251028_224211_822', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 378.0\n",
      "lif layer 1 self.abs_max_v: 378.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 408.0\n",
      "lif layer 1 self.abs_max_v: 528.0\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "lif layer 1 self.abs_max_v: 555.5\n",
      "lif layer 2 self.abs_max_v: 101.5\n",
      "fc layer 1 self.abs_max_out: 473.0\n",
      "lif layer 1 self.abs_max_v: 630.0\n",
      "fc layer 2 self.abs_max_out: 214.0\n",
      "lif layer 2 self.abs_max_v: 248.5\n",
      "fc layer 1 self.abs_max_out: 637.0\n",
      "lif layer 1 self.abs_max_v: 828.0\n",
      "fc layer 2 self.abs_max_out: 307.0\n",
      "lif layer 2 self.abs_max_v: 354.5\n",
      "fc layer 1 self.abs_max_out: 1013.0\n",
      "lif layer 1 self.abs_max_v: 1108.5\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 458.0\n",
      "fc layer 1 self.abs_max_out: 1364.0\n",
      "lif layer 1 self.abs_max_v: 1364.0\n",
      "fc layer 2 self.abs_max_out: 477.0\n",
      "lif layer 2 self.abs_max_v: 675.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "fc layer 1 self.abs_max_out: 1421.0\n",
      "lif layer 1 self.abs_max_v: 1421.0\n",
      "lif layer 2 self.abs_max_v: 740.5\n",
      "fc layer 1 self.abs_max_out: 1747.0\n",
      "lif layer 1 self.abs_max_v: 1747.0\n",
      "fc layer 2 self.abs_max_out: 478.0\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 2 self.abs_max_out: 514.0\n",
      "lif layer 2 self.abs_max_v: 871.5\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 2 self.abs_max_out: 602.0\n",
      "fc layer 2 self.abs_max_out: 711.0\n",
      "fc layer 2 self.abs_max_out: 884.0\n",
      "lif layer 2 self.abs_max_v: 993.5\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "lif layer 2 self.abs_max_v: 1087.0\n",
      "fc layer 3 self.abs_max_out: 201.0\n",
      "fc layer 1 self.abs_max_out: 1924.0\n",
      "lif layer 1 self.abs_max_v: 1924.0\n",
      "fc layer 1 self.abs_max_out: 2365.0\n",
      "lif layer 1 self.abs_max_v: 2365.0\n",
      "fc layer 2 self.abs_max_out: 961.0\n",
      "fc layer 2 self.abs_max_out: 1061.0\n",
      "lif layer 2 self.abs_max_v: 1137.0\n",
      "lif layer 2 self.abs_max_v: 1138.0\n",
      "lif layer 2 self.abs_max_v: 1186.0\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1261.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "lif layer 2 self.abs_max_v: 1333.0\n",
      "lif layer 2 self.abs_max_v: 1388.5\n",
      "fc layer 2 self.abs_max_out: 1211.0\n",
      "fc layer 2 self.abs_max_out: 1228.0\n",
      "lif layer 2 self.abs_max_v: 1476.0\n",
      "fc layer 1 self.abs_max_out: 2478.0\n",
      "lif layer 1 self.abs_max_v: 2478.0\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "fc layer 2 self.abs_max_out: 1304.0\n",
      "fc layer 2 self.abs_max_out: 1381.0\n",
      "lif layer 2 self.abs_max_v: 1701.5\n",
      "fc layer 2 self.abs_max_out: 1386.0\n",
      "fc layer 2 self.abs_max_out: 1495.0\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 2 self.abs_max_out: 1539.0\n",
      "lif layer 2 self.abs_max_v: 1830.0\n",
      "lif layer 2 self.abs_max_v: 2017.0\n",
      "fc layer 2 self.abs_max_out: 1603.0\n",
      "fc layer 2 self.abs_max_out: 1610.0\n",
      "fc layer 2 self.abs_max_out: 1692.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 1 self.abs_max_out: 2710.0\n",
      "lif layer 1 self.abs_max_v: 2710.0\n",
      "fc layer 1 self.abs_max_out: 3005.0\n",
      "lif layer 1 self.abs_max_v: 3005.0\n",
      "fc layer 1 self.abs_max_out: 3214.0\n",
      "lif layer 1 self.abs_max_v: 3214.0\n",
      "fc layer 2 self.abs_max_out: 1896.0\n",
      "fc layer 1 self.abs_max_out: 3320.0\n",
      "lif layer 1 self.abs_max_v: 3320.0\n",
      "fc layer 1 self.abs_max_out: 3528.0\n",
      "lif layer 1 self.abs_max_v: 3528.0\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "lif layer 2 self.abs_max_v: 2041.0\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "lif layer 1 self.abs_max_v: 3652.0\n",
      "lif layer 1 self.abs_max_v: 3908.0\n",
      "lif layer 2 self.abs_max_v: 2242.0\n",
      "fc layer 2 self.abs_max_out: 1925.0\n",
      "lif layer 2 self.abs_max_v: 2338.0\n",
      "fc layer 2 self.abs_max_out: 1947.0\n",
      "fc layer 2 self.abs_max_out: 2016.0\n",
      "lif layer 2 self.abs_max_v: 2355.5\n",
      "fc layer 2 self.abs_max_out: 2115.0\n",
      "lif layer 2 self.abs_max_v: 2399.5\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 1 self.abs_max_out: 3785.0\n",
      "lif layer 2 self.abs_max_v: 2446.5\n",
      "fc layer 2 self.abs_max_out: 2123.0\n",
      "fc layer 2 self.abs_max_out: 2150.0\n",
      "fc layer 2 self.abs_max_out: 2351.0\n",
      "lif layer 2 self.abs_max_v: 2500.0\n",
      "lif layer 1 self.abs_max_v: 4287.0\n",
      "lif layer 2 self.abs_max_v: 2635.0\n",
      "lif layer 1 self.abs_max_v: 4398.5\n",
      "lif layer 2 self.abs_max_v: 2725.0\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "lif layer 2 self.abs_max_v: 2795.5\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "lif layer 2 self.abs_max_v: 2891.5\n",
      "lif layer 2 self.abs_max_v: 3003.0\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "fc layer 2 self.abs_max_out: 2363.0\n",
      "lif layer 2 self.abs_max_v: 3065.5\n",
      "lif layer 2 self.abs_max_v: 3086.5\n",
      "lif layer 2 self.abs_max_v: 3098.5\n",
      "fc layer 2 self.abs_max_out: 2453.0\n",
      "fc layer 2 self.abs_max_out: 2526.0\n",
      "fc layer 1 self.abs_max_out: 3829.0\n",
      "fc layer 1 self.abs_max_out: 3861.0\n",
      "fc layer 1 self.abs_max_out: 4539.0\n",
      "lif layer 1 self.abs_max_v: 4539.0\n",
      "fc layer 2 self.abs_max_out: 2577.0\n",
      "fc layer 2 self.abs_max_out: 2611.0\n",
      "fc layer 2 self.abs_max_out: 2643.0\n",
      "fc layer 2 self.abs_max_out: 2652.0\n",
      "fc layer 2 self.abs_max_out: 2743.0\n",
      "lif layer 2 self.abs_max_v: 3114.0\n",
      "lif layer 2 self.abs_max_v: 3178.0\n",
      "lif layer 2 self.abs_max_v: 3209.0\n",
      "lif layer 2 self.abs_max_v: 3346.5\n",
      "fc layer 2 self.abs_max_out: 2767.0\n",
      "fc layer 1 self.abs_max_out: 4624.0\n",
      "lif layer 1 self.abs_max_v: 4624.0\n",
      "fc layer 2 self.abs_max_out: 2856.0\n",
      "fc layer 3 self.abs_max_out: 714.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "fc layer 3 self.abs_max_out: 816.0\n",
      "fc layer 3 self.abs_max_out: 878.0\n",
      "fc layer 2 self.abs_max_out: 2880.0\n",
      "fc layer 2 self.abs_max_out: 2952.0\n",
      "lif layer 2 self.abs_max_v: 3411.5\n",
      "lif layer 2 self.abs_max_v: 3477.0\n",
      "lif layer 2 self.abs_max_v: 3590.5\n",
      "lif layer 2 self.abs_max_v: 3647.0\n",
      "lif layer 2 self.abs_max_v: 3769.5\n",
      "lif layer 2 self.abs_max_v: 4085.0\n",
      "lif layer 1 self.abs_max_v: 4636.0\n",
      "lif layer 1 self.abs_max_v: 4834.0\n",
      "lif layer 1 self.abs_max_v: 5131.0\n",
      "fc layer 1 self.abs_max_out: 4650.0\n",
      "lif layer 1 self.abs_max_v: 5773.5\n",
      "fc layer 2 self.abs_max_out: 2976.0\n",
      "fc layer 2 self.abs_max_out: 2979.0\n",
      "fc layer 2 self.abs_max_out: 3087.0\n",
      "fc layer 2 self.abs_max_out: 3158.0\n",
      "fc layer 2 self.abs_max_out: 3238.0\n",
      "fc layer 2 self.abs_max_out: 3592.0\n",
      "fc layer 2 self.abs_max_out: 3681.0\n",
      "fc layer 2 self.abs_max_out: 3737.0\n",
      "fc layer 2 self.abs_max_out: 3804.0\n",
      "fc layer 2 self.abs_max_out: 3844.0\n",
      "fc layer 2 self.abs_max_out: 3860.0\n",
      "lif layer 2 self.abs_max_v: 4111.0\n",
      "lif layer 2 self.abs_max_v: 4150.0\n",
      "lif layer 2 self.abs_max_v: 4286.5\n",
      "lif layer 2 self.abs_max_v: 4306.5\n",
      "lif layer 2 self.abs_max_v: 4395.0\n",
      "lif layer 2 self.abs_max_v: 4627.0\n",
      "lif layer 1 self.abs_max_v: 5868.0\n",
      "lif layer 1 self.abs_max_v: 5869.0\n",
      "fc layer 3 self.abs_max_out: 894.0\n",
      "lif layer 2 self.abs_max_v: 4666.5\n",
      "lif layer 1 self.abs_max_v: 5896.5\n",
      "lif layer 1 self.abs_max_v: 6502.5\n",
      "fc layer 1 self.abs_max_out: 4722.0\n",
      "fc layer 1 self.abs_max_out: 5198.0\n",
      "fc layer 1 self.abs_max_out: 5326.0\n",
      "fc layer 2 self.abs_max_out: 3911.0\n",
      "lif layer 1 self.abs_max_v: 6946.0\n",
      "lif layer 1 self.abs_max_v: 6966.0\n",
      "lif layer 1 self.abs_max_v: 7108.0\n",
      "fc layer 2 self.abs_max_out: 3930.0\n",
      "fc layer 2 self.abs_max_out: 3978.0\n",
      "fc layer 2 self.abs_max_out: 4092.0\n",
      "fc layer 1 self.abs_max_out: 5768.0\n",
      "fc layer 1 self.abs_max_out: 5783.0\n",
      "lif layer 1 self.abs_max_v: 7354.5\n",
      "lif layer 1 self.abs_max_v: 7445.0\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "fc layer 2 self.abs_max_out: 4257.0\n",
      "lif layer 1 self.abs_max_v: 7976.5\n",
      "lif layer 1 self.abs_max_v: 8374.5\n",
      "lif layer 1 self.abs_max_v: 8440.5\n",
      "fc layer 3 self.abs_max_out: 933.0\n",
      "lif layer 1 self.abs_max_v: 8702.0\n",
      "lif layer 1 self.abs_max_v: 9212.0\n",
      "lif layer 1 self.abs_max_v: 9708.0\n",
      "lif layer 1 self.abs_max_v: 10036.0\n",
      "fc layer 3 self.abs_max_out: 959.0\n",
      "fc layer 1 self.abs_max_out: 5833.0\n",
      "fc layer 1 self.abs_max_out: 6004.0\n",
      "lif layer 2 self.abs_max_v: 4678.0\n",
      "fc layer 3 self.abs_max_out: 1035.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.670431/  1.897289, val:  33.75%, val_best:  33.75%, tr:  98.47%, tr_best:  98.47%, epoch time: 58.99 seconds, 0.98 minutes\n",
      "total_backward_count 9790 real_backward_count 1881  19.213%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 4490.0\n",
      "lif layer 2 self.abs_max_v: 4702.0\n",
      "lif layer 2 self.abs_max_v: 5052.0\n",
      "fc layer 3 self.abs_max_out: 1040.0\n",
      "lif layer 2 self.abs_max_v: 5101.0\n",
      "lif layer 2 self.abs_max_v: 5241.0\n",
      "lif layer 2 self.abs_max_v: 5313.5\n",
      "lif layer 2 self.abs_max_v: 5746.0\n",
      "lif layer 2 self.abs_max_v: 6081.0\n",
      "fc layer 1 self.abs_max_out: 6100.0\n",
      "fc layer 1 self.abs_max_out: 6473.0\n",
      "lif layer 1 self.abs_max_v: 10325.5\n",
      "lif layer 2 self.abs_max_v: 6288.5\n",
      "fc layer 1 self.abs_max_out: 6604.0\n",
      "fc layer 1 self.abs_max_out: 6687.0\n",
      "lif layer 1 self.abs_max_v: 10913.0\n",
      "fc layer 1 self.abs_max_out: 6862.0\n",
      "lif layer 1 self.abs_max_v: 11030.0\n",
      "lif layer 1 self.abs_max_v: 11106.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.564369/  1.857548, val:  42.08%, val_best:  42.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 58.80 seconds, 0.98 minutes\n",
      "total_backward_count 19580 real_backward_count 3386  17.293%\n",
      "lif layer 2 self.abs_max_v: 6341.5\n",
      "fc layer 3 self.abs_max_out: 1089.0\n",
      "fc layer 1 self.abs_max_out: 7232.0\n",
      "fc layer 1 self.abs_max_out: 7278.0\n",
      "fc layer 1 self.abs_max_out: 7508.0\n",
      "fc layer 1 self.abs_max_out: 7572.0\n",
      "lif layer 1 self.abs_max_v: 11549.5\n",
      "lif layer 1 self.abs_max_v: 12472.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.503041/  1.811162, val:  52.08%, val_best:  52.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 59.07 seconds, 0.98 minutes\n",
      "total_backward_count 29370 real_backward_count 4887  16.639%\n",
      "fc layer 3 self.abs_max_out: 1183.0\n",
      "fc layer 1 self.abs_max_out: 7587.0\n",
      "fc layer 1 self.abs_max_out: 7612.0\n",
      "fc layer 1 self.abs_max_out: 7710.0\n",
      "fc layer 1 self.abs_max_out: 8045.0\n",
      "lif layer 2 self.abs_max_v: 6479.0\n",
      "lif layer 2 self.abs_max_v: 6637.0\n",
      "fc layer 3 self.abs_max_out: 1202.0\n",
      "fc layer 3 self.abs_max_out: 1204.0\n",
      "lif layer 1 self.abs_max_v: 13046.0\n",
      "lif layer 1 self.abs_max_v: 13878.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.490953/  1.817588, val:  38.33%, val_best:  52.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 58.21 seconds, 0.97 minutes\n",
      "total_backward_count 39160 real_backward_count 6270  16.011%\n",
      "lif layer 2 self.abs_max_v: 6775.5\n",
      "lif layer 2 self.abs_max_v: 7102.0\n",
      "fc layer 1 self.abs_max_out: 8686.0\n",
      "lif layer 2 self.abs_max_v: 7369.0\n",
      "fc layer 3 self.abs_max_out: 1221.0\n",
      "lif layer 2 self.abs_max_v: 7390.0\n",
      "lif layer 2 self.abs_max_v: 7680.0\n",
      "lif layer 2 self.abs_max_v: 7722.0\n",
      "lif layer 2 self.abs_max_v: 8153.5\n",
      "lif layer 1 self.abs_max_v: 14644.0\n",
      "fc layer 2 self.abs_max_out: 4491.0\n",
      "lif layer 2 self.abs_max_v: 8168.5\n",
      "fc layer 2 self.abs_max_out: 4579.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.452908/  1.814329, val:  47.08%, val_best:  52.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 58.95 seconds, 0.98 minutes\n",
      "total_backward_count 48950 real_backward_count 7639  15.606%\n",
      "fc layer 2 self.abs_max_out: 4714.0\n",
      "lif layer 2 self.abs_max_v: 8512.0\n",
      "lif layer 2 self.abs_max_v: 8704.0\n",
      "lif layer 1 self.abs_max_v: 15479.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.427033/  1.746846, val:  50.83%, val_best:  52.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 58.87 seconds, 0.98 minutes\n",
      "total_backward_count 58740 real_backward_count 9008  15.335%\n",
      "fc layer 1 self.abs_max_out: 8820.0\n",
      "fc layer 1 self.abs_max_out: 9058.0\n",
      "fc layer 1 self.abs_max_out: 9210.0\n",
      "lif layer 1 self.abs_max_v: 16180.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.397803/  1.789959, val:  50.83%, val_best:  52.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 58.23 seconds, 0.97 minutes\n",
      "total_backward_count 68530 real_backward_count 10390  15.161%\n",
      "fc layer 3 self.abs_max_out: 1259.0\n",
      "fc layer 3 self.abs_max_out: 1310.0\n",
      "fc layer 3 self.abs_max_out: 1393.0\n",
      "fc layer 1 self.abs_max_out: 9284.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.413489/  1.662405, val:  57.92%, val_best:  57.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 58.67 seconds, 0.98 minutes\n",
      "total_backward_count 78320 real_backward_count 11730  14.977%\n",
      "fc layer 2 self.abs_max_out: 4731.0\n",
      "fc layer 2 self.abs_max_out: 4748.0\n",
      "lif layer 2 self.abs_max_v: 8829.5\n",
      "fc layer 2 self.abs_max_out: 4847.0\n",
      "fc layer 2 self.abs_max_out: 5043.0\n",
      "lif layer 2 self.abs_max_v: 9103.5\n",
      "fc layer 1 self.abs_max_out: 9367.0\n",
      "fc layer 1 self.abs_max_out: 9571.0\n",
      "fc layer 1 self.abs_max_out: 9676.0\n",
      "lif layer 1 self.abs_max_v: 17170.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.362002/  1.713245, val:  49.17%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 57.13 seconds, 0.95 minutes\n",
      "total_backward_count 88110 real_backward_count 13118  14.888%\n",
      "fc layer 1 self.abs_max_out: 10116.0\n",
      "lif layer 1 self.abs_max_v: 17734.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.317764/  1.729147, val:  47.92%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.88 seconds, 0.95 minutes\n",
      "total_backward_count 97900 real_backward_count 14478  14.789%\n",
      "fc layer 1 self.abs_max_out: 10184.0\n",
      "lif layer 1 self.abs_max_v: 17894.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.348642/  1.773664, val:  41.67%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.14 seconds, 0.94 minutes\n",
      "total_backward_count 107690 real_backward_count 15775  14.649%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.361506/  1.674395, val:  57.50%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 56.63 seconds, 0.94 minutes\n",
      "total_backward_count 117480 real_backward_count 17121  14.574%\n",
      "fc layer 1 self.abs_max_out: 10517.0\n",
      "lif layer 1 self.abs_max_v: 18354.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.349732/  1.690908, val:  47.08%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.21 seconds, 0.94 minutes\n",
      "total_backward_count 127270 real_backward_count 18379  14.441%\n",
      "fc layer 2 self.abs_max_out: 5323.0\n",
      "fc layer 3 self.abs_max_out: 1401.0\n",
      "fc layer 1 self.abs_max_out: 11079.0\n",
      "lif layer 1 self.abs_max_v: 19021.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.343997/  1.786044, val:  34.58%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 56.18 seconds, 0.94 minutes\n",
      "total_backward_count 137060 real_backward_count 19616  14.312%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.330550/  1.705654, val:  47.08%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 56.44 seconds, 0.94 minutes\n",
      "total_backward_count 146850 real_backward_count 20859  14.204%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.310699/  1.723034, val:  50.42%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.78 seconds, 0.96 minutes\n",
      "total_backward_count 156640 real_backward_count 22143  14.136%\n",
      "fc layer 3 self.abs_max_out: 1454.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.274041/  1.658453, val:  57.92%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 56.84 seconds, 0.95 minutes\n",
      "total_backward_count 166430 real_backward_count 23397  14.058%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.327133/  1.654875, val:  54.17%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 56.89 seconds, 0.95 minutes\n",
      "total_backward_count 176220 real_backward_count 24633  13.979%\n",
      "fc layer 3 self.abs_max_out: 1485.0\n",
      "fc layer 1 self.abs_max_out: 11135.0\n",
      "fc layer 1 self.abs_max_out: 11609.0\n",
      "lif layer 1 self.abs_max_v: 20773.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.304670/  1.616756, val:  52.08%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.32 seconds, 0.96 minutes\n",
      "total_backward_count 186010 real_backward_count 25923  13.936%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.271230/  1.682174, val:  42.08%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 57.83 seconds, 0.96 minutes\n",
      "total_backward_count 195800 real_backward_count 27099  13.840%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.267454/  1.684407, val:  44.58%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.26 seconds, 0.95 minutes\n",
      "total_backward_count 205590 real_backward_count 28321  13.775%\n",
      "fc layer 3 self.abs_max_out: 1528.0\n",
      "fc layer 3 self.abs_max_out: 1551.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.271642/  1.659863, val:  56.25%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 58.31 seconds, 0.97 minutes\n",
      "total_backward_count 215380 real_backward_count 29606  13.746%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.269136/  1.591063, val:  56.25%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.89 seconds, 0.96 minutes\n",
      "total_backward_count 225170 real_backward_count 30847  13.699%\n",
      "fc layer 3 self.abs_max_out: 1587.0\n",
      "fc layer 3 self.abs_max_out: 1604.0\n",
      "fc layer 3 self.abs_max_out: 1662.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.248076/  1.599936, val:  65.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 234960 real_backward_count 32054  13.642%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.216547/  1.590172, val:  61.67%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 57.92 seconds, 0.97 minutes\n",
      "total_backward_count 244750 real_backward_count 33266  13.592%\n",
      "fc layer 3 self.abs_max_out: 1670.0\n",
      "fc layer 3 self.abs_max_out: 1730.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.241003/  1.590591, val:  58.75%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.43 seconds, 0.97 minutes\n",
      "total_backward_count 254540 real_backward_count 34540  13.570%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.272751/  1.564927, val:  56.67%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.94 seconds, 0.98 minutes\n",
      "total_backward_count 264330 real_backward_count 35752  13.526%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.242660/  1.587190, val:  62.08%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.52 seconds, 0.98 minutes\n",
      "total_backward_count 274120 real_backward_count 37046  13.515%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.247401/  1.606837, val:  55.00%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.49 seconds, 0.97 minutes\n",
      "total_backward_count 283910 real_backward_count 38228  13.465%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.276481/  1.620246, val:  51.25%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.69 seconds, 0.96 minutes\n",
      "total_backward_count 293700 real_backward_count 39445  13.430%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.227802/  1.587446, val:  57.50%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 55.88 seconds, 0.93 minutes\n",
      "total_backward_count 303490 real_backward_count 40620  13.384%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.251931/  1.692037, val:  39.17%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 56.20 seconds, 0.94 minutes\n",
      "total_backward_count 313280 real_backward_count 41831  13.353%\n",
      "fc layer 1 self.abs_max_out: 11773.0\n",
      "fc layer 1 self.abs_max_out: 12423.0\n",
      "lif layer 1 self.abs_max_v: 21924.5\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.207472/  1.592719, val:  57.08%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.19 seconds, 0.95 minutes\n",
      "total_backward_count 323070 real_backward_count 43010  13.313%\n",
      "lif layer 2 self.abs_max_v: 9145.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.213924/  1.566528, val:  56.25%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.69 seconds, 0.94 minutes\n",
      "total_backward_count 332860 real_backward_count 44196  13.278%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.223401/  1.599008, val:  57.92%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.00 seconds, 0.95 minutes\n",
      "total_backward_count 342650 real_backward_count 45382  13.244%\n",
      "fc layer 3 self.abs_max_out: 1766.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.213536/  1.614167, val:  54.17%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.48 seconds, 0.94 minutes\n",
      "total_backward_count 352440 real_backward_count 46544  13.206%\n",
      "fc layer 3 self.abs_max_out: 1780.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.191296/  1.546793, val:  64.58%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.35 seconds, 0.96 minutes\n",
      "total_backward_count 362230 real_backward_count 47705  13.170%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.192793/  1.537355, val:  60.83%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.29 seconds, 0.97 minutes\n",
      "total_backward_count 372020 real_backward_count 48774  13.111%\n",
      "fc layer 3 self.abs_max_out: 1795.0\n",
      "lif layer 2 self.abs_max_v: 9467.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.210827/  1.619453, val:  57.92%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.71 seconds, 0.96 minutes\n",
      "total_backward_count 381810 real_backward_count 49937  13.079%\n",
      "lif layer 2 self.abs_max_v: 9487.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.249186/  1.659693, val:  43.33%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 58.42 seconds, 0.97 minutes\n",
      "total_backward_count 391600 real_backward_count 51110  13.052%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.258245/  1.558070, val:  62.92%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.56 seconds, 0.98 minutes\n",
      "total_backward_count 401390 real_backward_count 52281  13.025%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.265891/  1.583369, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.92 seconds, 0.98 minutes\n",
      "total_backward_count 411180 real_backward_count 53413  12.990%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.270377/  1.607293, val:  57.50%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.88 seconds, 0.98 minutes\n",
      "total_backward_count 420970 real_backward_count 54510  12.949%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.249292/  1.539055, val:  65.00%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.02 seconds, 0.97 minutes\n",
      "total_backward_count 430760 real_backward_count 55615  12.911%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.260110/  1.563348, val:  56.25%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.66 seconds, 0.96 minutes\n",
      "total_backward_count 440550 real_backward_count 56793  12.891%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.223977/  1.545866, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.87 seconds, 0.96 minutes\n",
      "total_backward_count 450340 real_backward_count 57894  12.856%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.255871/  1.596587, val:  67.92%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 57.09 seconds, 0.95 minutes\n",
      "total_backward_count 460130 real_backward_count 59066  12.837%\n",
      "fc layer 1 self.abs_max_out: 12600.0\n",
      "lif layer 1 self.abs_max_v: 22724.5\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.265298/  1.672689, val:  45.42%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 56.91 seconds, 0.95 minutes\n",
      "total_backward_count 469920 real_backward_count 60206  12.812%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.287274/  1.622659, val:  65.42%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.13 seconds, 0.95 minutes\n",
      "total_backward_count 479710 real_backward_count 61341  12.787%\n",
      "lif layer 2 self.abs_max_v: 9501.5\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.286569/  1.595597, val:  64.58%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.18 seconds, 0.97 minutes\n",
      "total_backward_count 489500 real_backward_count 62493  12.767%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.258912/  1.545150, val:  55.00%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.60 seconds, 0.96 minutes\n",
      "total_backward_count 499290 real_backward_count 63583  12.735%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.243678/  1.580944, val:  73.33%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.61 seconds, 0.94 minutes\n",
      "total_backward_count 509080 real_backward_count 64720  12.713%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.271885/  1.565000, val:  64.58%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.39 seconds, 0.94 minutes\n",
      "total_backward_count 518870 real_backward_count 65872  12.695%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.217209/  1.505199, val:  62.92%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.30 seconds, 0.94 minutes\n",
      "total_backward_count 528660 real_backward_count 67018  12.677%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.224742/  1.566482, val:  61.25%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.31 seconds, 0.94 minutes\n",
      "total_backward_count 538450 real_backward_count 68118  12.651%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.215595/  1.539648, val:  70.42%, val_best:  73.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 56.73 seconds, 0.95 minutes\n",
      "total_backward_count 548240 real_backward_count 69266  12.634%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.213997/  1.642301, val:  46.67%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.16 seconds, 0.92 minutes\n",
      "total_backward_count 558030 real_backward_count 70446  12.624%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.233341/  1.577608, val:  58.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.44 seconds, 0.94 minutes\n",
      "total_backward_count 567820 real_backward_count 71538  12.599%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.252512/  1.571164, val:  59.58%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.63 seconds, 0.98 minutes\n",
      "total_backward_count 577610 real_backward_count 72666  12.580%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.242993/  1.619331, val:  55.42%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.94 seconds, 0.97 minutes\n",
      "total_backward_count 587400 real_backward_count 73743  12.554%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.256192/  1.618410, val:  50.83%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.07 seconds, 0.97 minutes\n",
      "total_backward_count 597190 real_backward_count 74852  12.534%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.262553/  1.571826, val:  65.42%, val_best:  73.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 58.35 seconds, 0.97 minutes\n",
      "total_backward_count 606980 real_backward_count 76050  12.529%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.294535/  1.588073, val:  65.42%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.65 seconds, 0.98 minutes\n",
      "total_backward_count 616770 real_backward_count 77183  12.514%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.281750/  1.603532, val:  57.50%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.32 seconds, 0.97 minutes\n",
      "total_backward_count 626560 real_backward_count 78299  12.497%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.263223/  1.566484, val:  54.58%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.42 seconds, 0.97 minutes\n",
      "total_backward_count 636350 real_backward_count 79420  12.481%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.220544/  1.600968, val:  70.42%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 646140 real_backward_count 80501  12.459%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.264178/  1.582456, val:  60.42%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.91 seconds, 0.97 minutes\n",
      "total_backward_count 655930 real_backward_count 81604  12.441%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.269374/  1.536987, val:  61.67%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.98 seconds, 0.97 minutes\n",
      "total_backward_count 665720 real_backward_count 82699  12.422%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.301104/  1.537660, val:  65.42%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.04 seconds, 0.97 minutes\n",
      "total_backward_count 675510 real_backward_count 83836  12.411%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.254931/  1.553264, val:  55.42%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.36 seconds, 0.97 minutes\n",
      "total_backward_count 685300 real_backward_count 84859  12.383%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.248857/  1.552355, val:  74.58%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.84 seconds, 0.96 minutes\n",
      "total_backward_count 695090 real_backward_count 85990  12.371%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.259264/  1.611061, val:  60.42%, val_best:  74.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 57.63 seconds, 0.96 minutes\n",
      "total_backward_count 704880 real_backward_count 87079  12.354%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.249623/  1.585437, val:  72.08%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.44 seconds, 0.96 minutes\n",
      "total_backward_count 714670 real_backward_count 88133  12.332%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.247409/  1.626746, val:  51.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.42 seconds, 0.94 minutes\n",
      "total_backward_count 724460 real_backward_count 89194  12.312%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.278068/  1.590568, val:  62.92%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.15 seconds, 0.94 minutes\n",
      "total_backward_count 734250 real_backward_count 90276  12.295%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.256232/  1.543556, val:  65.83%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.83 seconds, 0.93 minutes\n",
      "total_backward_count 744040 real_backward_count 91357  12.279%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.286167/  1.562950, val:  68.75%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 55.57 seconds, 0.93 minutes\n",
      "total_backward_count 753830 real_backward_count 92509  12.272%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.250662/  1.659880, val:  44.58%, val_best:  74.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 56.35 seconds, 0.94 minutes\n",
      "total_backward_count 763620 real_backward_count 93561  12.252%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.253114/  1.577529, val:  62.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.97 seconds, 0.93 minutes\n",
      "total_backward_count 773410 real_backward_count 94633  12.236%\n",
      "fc layer 1 self.abs_max_out: 12837.0\n",
      "lif layer 1 self.abs_max_v: 23234.5\n",
      "fc layer 1 self.abs_max_out: 13334.0\n",
      "lif layer 1 self.abs_max_v: 24163.5\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.292721/  1.571826, val:  64.17%, val_best:  74.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 56.71 seconds, 0.95 minutes\n",
      "total_backward_count 783200 real_backward_count 95688  12.218%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.307793/  1.578283, val:  55.42%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.37 seconds, 0.96 minutes\n",
      "total_backward_count 792990 real_backward_count 96728  12.198%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.274621/  1.614389, val:  49.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.45 seconds, 0.96 minutes\n",
      "total_backward_count 802780 real_backward_count 97778  12.180%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.288680/  1.583075, val:  73.33%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.96 seconds, 0.95 minutes\n",
      "total_backward_count 812570 real_backward_count 98911  12.173%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.263191/  1.575942, val:  67.08%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.43 seconds, 0.97 minutes\n",
      "total_backward_count 822360 real_backward_count 100052  12.166%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.286855/  1.534649, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.46 seconds, 0.96 minutes\n",
      "total_backward_count 832150 real_backward_count 101169  12.158%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.272638/  1.610733, val:  77.08%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.42 seconds, 0.96 minutes\n",
      "total_backward_count 841940 real_backward_count 102312  12.152%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.317151/  1.620145, val:  66.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.23 seconds, 0.97 minutes\n",
      "total_backward_count 851730 real_backward_count 103405  12.141%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.308461/  1.598576, val:  70.00%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.51 seconds, 0.98 minutes\n",
      "total_backward_count 861520 real_backward_count 104478  12.127%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.309869/  1.600190, val:  66.67%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.71 seconds, 0.96 minutes\n",
      "total_backward_count 871310 real_backward_count 105489  12.107%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.294473/  1.576903, val:  74.17%, val_best:  77.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 58.31 seconds, 0.97 minutes\n",
      "total_backward_count 881100 real_backward_count 106572  12.095%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.282884/  1.619935, val:  65.42%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.87 seconds, 0.96 minutes\n",
      "total_backward_count 890890 real_backward_count 107652  12.084%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.233178/  1.486780, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.78 seconds, 0.96 minutes\n",
      "total_backward_count 900680 real_backward_count 108661  12.064%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.246121/  1.581249, val:  68.33%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.48 seconds, 0.96 minutes\n",
      "total_backward_count 910470 real_backward_count 109697  12.048%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.288187/  1.648427, val:  65.42%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.07 seconds, 0.97 minutes\n",
      "total_backward_count 920260 real_backward_count 110792  12.039%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.304649/  1.590634, val:  72.08%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.53 seconds, 0.96 minutes\n",
      "total_backward_count 930050 real_backward_count 111864  12.028%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.349074/  1.633348, val:  63.33%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.37 seconds, 0.96 minutes\n",
      "total_backward_count 939840 real_backward_count 112934  12.016%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.335880/  1.654247, val:  52.92%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 949630 real_backward_count 114009  12.006%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.329706/  1.591924, val:  65.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.32 seconds, 0.94 minutes\n",
      "total_backward_count 959420 real_backward_count 115035  11.990%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.302432/  1.695674, val:  55.42%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.13 seconds, 0.94 minutes\n",
      "total_backward_count 969210 real_backward_count 116040  11.973%\n",
      "fc layer 1 self.abs_max_out: 13414.0\n",
      "lif layer 1 self.abs_max_v: 24607.0\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.319062/  1.575953, val:  68.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.62 seconds, 0.94 minutes\n",
      "total_backward_count 979000 real_backward_count 117080  11.959%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.341293/  1.618322, val:  70.00%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.58 seconds, 0.94 minutes\n",
      "total_backward_count 988790 real_backward_count 118139  11.948%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.393902/  1.659663, val:  62.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.15 seconds, 0.97 minutes\n",
      "total_backward_count 998580 real_backward_count 119255  11.942%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.422338/  1.706748, val:  65.83%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.12 seconds, 0.97 minutes\n",
      "total_backward_count 1008370 real_backward_count 120330  11.933%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.384710/  1.614532, val:  64.58%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.68 seconds, 0.98 minutes\n",
      "total_backward_count 1018160 real_backward_count 121348  11.918%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.392975/  1.627569, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.16 seconds, 0.95 minutes\n",
      "total_backward_count 1027950 real_backward_count 122409  11.908%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.369511/  1.649133, val:  67.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.84 seconds, 0.98 minutes\n",
      "total_backward_count 1037740 real_backward_count 123483  11.899%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.393462/  1.712751, val:  55.42%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.16 seconds, 0.97 minutes\n",
      "total_backward_count 1047530 real_backward_count 124494  11.885%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.384691/  1.707721, val:  56.67%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.15 seconds, 0.97 minutes\n",
      "total_backward_count 1057320 real_backward_count 125611  11.880%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.366909/  1.621684, val:  70.42%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.41 seconds, 0.96 minutes\n",
      "total_backward_count 1067110 real_backward_count 126666  11.870%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.354679/  1.663896, val:  69.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.20 seconds, 0.97 minutes\n",
      "total_backward_count 1076900 real_backward_count 127699  11.858%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.398022/  1.646516, val:  69.17%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.70 seconds, 0.96 minutes\n",
      "total_backward_count 1086690 real_backward_count 128762  11.849%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.367930/  1.654587, val:  60.00%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.08 seconds, 0.95 minutes\n",
      "total_backward_count 1096480 real_backward_count 129789  11.837%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.378572/  1.610482, val:  73.75%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 57.25 seconds, 0.95 minutes\n",
      "total_backward_count 1106270 real_backward_count 130884  11.831%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.330168/  1.646445, val:  64.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.20 seconds, 0.95 minutes\n",
      "total_backward_count 1116060 real_backward_count 131936  11.822%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.293066/  1.607864, val:  64.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.64 seconds, 0.96 minutes\n",
      "total_backward_count 1125850 real_backward_count 132977  11.811%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.305340/  1.612633, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.90 seconds, 0.96 minutes\n",
      "total_backward_count 1135640 real_backward_count 134049  11.804%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.310895/  1.576498, val:  70.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.05 seconds, 0.95 minutes\n",
      "total_backward_count 1145430 real_backward_count 135103  11.795%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.287082/  1.560824, val:  70.42%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.78 seconds, 0.95 minutes\n",
      "total_backward_count 1155220 real_backward_count 136127  11.784%\n",
      "fc layer 1 self.abs_max_out: 13470.0\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.298906/  1.606938, val:  63.75%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 56.87 seconds, 0.95 minutes\n",
      "total_backward_count 1165010 real_backward_count 137183  11.775%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.369712/  1.652932, val:  62.92%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.06 seconds, 0.93 minutes\n",
      "total_backward_count 1174800 real_backward_count 138214  11.765%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.372681/  1.711646, val:  60.42%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.84 seconds, 0.95 minutes\n",
      "total_backward_count 1184590 real_backward_count 139245  11.755%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.354407/  1.642313, val:  70.42%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.64 seconds, 0.94 minutes\n",
      "total_backward_count 1194380 real_backward_count 140292  11.746%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.369919/  1.637815, val:  61.25%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 1204170 real_backward_count 141330  11.737%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.343474/  1.592890, val:  70.42%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.56 seconds, 0.96 minutes\n",
      "total_backward_count 1213960 real_backward_count 142365  11.727%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.345307/  1.593944, val:  73.75%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.01 seconds, 0.97 minutes\n",
      "total_backward_count 1223750 real_backward_count 143412  11.719%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.360115/  1.686905, val:  59.17%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.84 seconds, 0.96 minutes\n",
      "total_backward_count 1233540 real_backward_count 144494  11.714%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.341870/  1.670842, val:  60.83%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.08 seconds, 0.95 minutes\n",
      "total_backward_count 1243330 real_backward_count 145514  11.704%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.340345/  1.594627, val:  66.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.52 seconds, 0.96 minutes\n",
      "total_backward_count 1253120 real_backward_count 146500  11.691%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.371972/  1.687522, val:  66.25%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 57.99 seconds, 0.97 minutes\n",
      "total_backward_count 1262910 real_backward_count 147508  11.680%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.385260/  1.689813, val:  59.17%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 57.83 seconds, 0.96 minutes\n",
      "total_backward_count 1272700 real_backward_count 148493  11.668%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.400844/  1.636109, val:  58.75%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.15 seconds, 0.97 minutes\n",
      "total_backward_count 1282490 real_backward_count 149567  11.662%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.355579/  1.695688, val:  64.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.62 seconds, 0.98 minutes\n",
      "total_backward_count 1292280 real_backward_count 150626  11.656%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.375761/  1.644463, val:  65.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.73 seconds, 0.96 minutes\n",
      "total_backward_count 1302070 real_backward_count 151629  11.645%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.367631/  1.694098, val:  68.75%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.27 seconds, 0.95 minutes\n",
      "total_backward_count 1311860 real_backward_count 152650  11.636%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.362954/  1.695145, val:  50.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.25 seconds, 0.97 minutes\n",
      "total_backward_count 1321650 real_backward_count 153677  11.628%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.369487/  1.652179, val:  60.42%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.63 seconds, 0.98 minutes\n",
      "total_backward_count 1331440 real_backward_count 154804  11.627%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.317806/  1.578236, val:  76.67%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.43 seconds, 0.96 minutes\n",
      "total_backward_count 1341230 real_backward_count 155823  11.618%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.364527/  1.669196, val:  65.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.02 seconds, 0.95 minutes\n",
      "total_backward_count 1351020 real_backward_count 156844  11.609%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.366070/  1.590393, val:  71.67%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.56 seconds, 0.94 minutes\n",
      "total_backward_count 1360810 real_backward_count 157907  11.604%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.330932/  1.625795, val:  61.25%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.02 seconds, 0.95 minutes\n",
      "total_backward_count 1370600 real_backward_count 158956  11.598%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.341204/  1.625394, val:  74.58%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.98 seconds, 0.95 minutes\n",
      "total_backward_count 1380390 real_backward_count 160005  11.591%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.326780/  1.643253, val:  72.50%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.34 seconds, 0.94 minutes\n",
      "total_backward_count 1390180 real_backward_count 161027  11.583%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.332213/  1.626139, val:  63.75%, val_best:  79.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 56.11 seconds, 0.94 minutes\n",
      "total_backward_count 1399970 real_backward_count 162066  11.576%\n",
      "fc layer 1 self.abs_max_out: 13493.0\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.324161/  1.632446, val:  57.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.13 seconds, 0.94 minutes\n",
      "total_backward_count 1409760 real_backward_count 163098  11.569%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.325066/  1.624862, val:  65.42%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.48 seconds, 0.94 minutes\n",
      "total_backward_count 1419550 real_backward_count 164128  11.562%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.369028/  1.638963, val:  67.08%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.92 seconds, 0.95 minutes\n",
      "total_backward_count 1429340 real_backward_count 165185  11.557%\n",
      "fc layer 1 self.abs_max_out: 13560.0\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.367465/  1.599914, val:  62.50%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.40 seconds, 0.96 minutes\n",
      "total_backward_count 1439130 real_backward_count 166247  11.552%\n",
      "fc layer 1 self.abs_max_out: 13562.0\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.285048/  1.637914, val:  62.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.94 seconds, 0.97 minutes\n",
      "total_backward_count 1448920 real_backward_count 167240  11.542%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.342490/  1.610930, val:  64.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.87 seconds, 0.96 minutes\n",
      "total_backward_count 1458710 real_backward_count 168259  11.535%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.336180/  1.607208, val:  70.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.21 seconds, 0.95 minutes\n",
      "total_backward_count 1468500 real_backward_count 169283  11.528%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.346578/  1.665631, val:  68.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.05 seconds, 0.97 minutes\n",
      "total_backward_count 1478290 real_backward_count 170352  11.524%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.419373/  1.672334, val:  71.25%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.40 seconds, 0.96 minutes\n",
      "total_backward_count 1488080 real_backward_count 171398  11.518%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.385803/  1.639031, val:  67.50%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.14 seconds, 0.95 minutes\n",
      "total_backward_count 1497870 real_backward_count 172425  11.511%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.392389/  1.584772, val:  72.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.51 seconds, 0.96 minutes\n",
      "total_backward_count 1507660 real_backward_count 173507  11.508%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.396755/  1.697680, val:  71.25%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 58.35 seconds, 0.97 minutes\n",
      "total_backward_count 1517450 real_backward_count 174503  11.500%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.414617/  1.684466, val:  65.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 1527240 real_backward_count 175448  11.488%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.432379/  1.724180, val:  43.33%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.58 seconds, 0.96 minutes\n",
      "total_backward_count 1537030 real_backward_count 176530  11.485%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.449500/  1.698556, val:  65.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.11 seconds, 0.97 minutes\n",
      "total_backward_count 1546820 real_backward_count 177558  11.479%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.427438/  1.667010, val:  69.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.49 seconds, 0.96 minutes\n",
      "total_backward_count 1556610 real_backward_count 178590  11.473%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.402491/  1.678543, val:  67.08%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.35 seconds, 0.94 minutes\n",
      "total_backward_count 1566400 real_backward_count 179625  11.467%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.407310/  1.648426, val:  60.42%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.22 seconds, 0.95 minutes\n",
      "total_backward_count 1576190 real_backward_count 180611  11.459%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.430234/  1.669243, val:  79.58%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 56.68 seconds, 0.94 minutes\n",
      "total_backward_count 1585980 real_backward_count 181622  11.452%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.426320/  1.741993, val:  47.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.51 seconds, 0.94 minutes\n",
      "total_backward_count 1595770 real_backward_count 182614  11.444%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.384426/  1.666012, val:  68.33%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.23 seconds, 0.94 minutes\n",
      "total_backward_count 1605560 real_backward_count 183637  11.438%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.372981/  1.642612, val:  67.08%, val_best:  79.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 56.35 seconds, 0.94 minutes\n",
      "total_backward_count 1615350 real_backward_count 184637  11.430%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.363421/  1.638628, val:  71.67%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.99 seconds, 0.95 minutes\n",
      "total_backward_count 1625140 real_backward_count 185703  11.427%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.361256/  1.612978, val:  62.92%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.38 seconds, 0.96 minutes\n",
      "total_backward_count 1634930 real_backward_count 186661  11.417%\n",
      "fc layer 2 self.abs_max_out: 5518.0\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.347401/  1.693223, val:  51.25%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.63 seconds, 0.96 minutes\n",
      "total_backward_count 1644720 real_backward_count 187705  11.413%\n",
      "fc layer 1 self.abs_max_out: 13658.0\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.367525/  1.688296, val:  58.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.66 seconds, 0.98 minutes\n",
      "total_backward_count 1654510 real_backward_count 188656  11.403%\n",
      "lif layer 2 self.abs_max_v: 9533.5\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.355667/  1.639971, val:  69.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.29 seconds, 0.97 minutes\n",
      "total_backward_count 1664300 real_backward_count 189632  11.394%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.353727/  1.655105, val:  54.17%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.98 seconds, 0.97 minutes\n",
      "total_backward_count 1674090 real_backward_count 190619  11.386%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.360310/  1.642974, val:  61.25%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.26 seconds, 0.97 minutes\n",
      "total_backward_count 1683880 real_backward_count 191575  11.377%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.371015/  1.588191, val:  70.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 59.04 seconds, 0.98 minutes\n",
      "total_backward_count 1693670 real_backward_count 192618  11.373%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.335467/  1.624160, val:  69.17%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.45 seconds, 0.96 minutes\n",
      "total_backward_count 1703460 real_backward_count 193571  11.363%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.345955/  1.589167, val:  71.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.55 seconds, 0.96 minutes\n",
      "total_backward_count 1713250 real_backward_count 194565  11.356%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.331786/  1.584927, val:  72.50%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.29 seconds, 0.95 minutes\n",
      "total_backward_count 1723040 real_backward_count 195532  11.348%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.352007/  1.618066, val:  65.00%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.39 seconds, 0.96 minutes\n",
      "total_backward_count 1732830 real_backward_count 196570  11.344%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.346858/  1.555333, val:  72.92%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.94 seconds, 0.95 minutes\n",
      "total_backward_count 1742620 real_backward_count 197579  11.338%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.345289/  1.686921, val:  52.92%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.91 seconds, 0.95 minutes\n",
      "total_backward_count 1752410 real_backward_count 198545  11.330%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.342928/  1.593677, val:  78.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.58 seconds, 0.96 minutes\n",
      "total_backward_count 1762200 real_backward_count 199564  11.325%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.317116/  1.582229, val:  64.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 1771990 real_backward_count 200581  11.320%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.275352/  1.574238, val:  67.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.38 seconds, 0.94 minutes\n",
      "total_backward_count 1781780 real_backward_count 201557  11.312%\n",
      "fc layer 1 self.abs_max_out: 13700.0\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.308064/  1.644839, val:  70.83%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.70 seconds, 0.95 minutes\n",
      "total_backward_count 1791570 real_backward_count 202554  11.306%\n",
      "lif layer 1 self.abs_max_v: 24862.0\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.323496/  1.681414, val:  50.83%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.32 seconds, 0.94 minutes\n",
      "total_backward_count 1801360 real_backward_count 203553  11.300%\n",
      "fc layer 1 self.abs_max_out: 14210.0\n",
      "lif layer 1 self.abs_max_v: 25174.5\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.321541/  1.612288, val:  61.67%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.60 seconds, 0.94 minutes\n",
      "total_backward_count 1811150 real_backward_count 204478  11.290%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.325145/  1.647294, val:  57.50%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.45 seconds, 0.94 minutes\n",
      "total_backward_count 1820940 real_backward_count 205525  11.287%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.340037/  1.599475, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.22 seconds, 0.95 minutes\n",
      "total_backward_count 1830730 real_backward_count 206551  11.282%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.327334/  1.667287, val:  68.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.48 seconds, 0.97 minutes\n",
      "total_backward_count 1840520 real_backward_count 207585  11.279%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.337259/  1.622699, val:  69.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.94 seconds, 0.97 minutes\n",
      "total_backward_count 1850310 real_backward_count 208519  11.269%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.334683/  1.585215, val:  69.17%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.85 seconds, 0.98 minutes\n",
      "total_backward_count 1860100 real_backward_count 209493  11.262%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.302524/  1.525202, val:  77.50%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 1869890 real_backward_count 210450  11.255%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.350832/  1.672128, val:  58.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.78 seconds, 0.96 minutes\n",
      "total_backward_count 1879680 real_backward_count 211471  11.250%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.341911/  1.614395, val:  70.00%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.38 seconds, 0.97 minutes\n",
      "total_backward_count 1889470 real_backward_count 212438  11.243%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.320719/  1.645831, val:  62.92%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.77 seconds, 0.98 minutes\n",
      "total_backward_count 1899260 real_backward_count 213374  11.235%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.296897/  1.567350, val:  60.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.33 seconds, 0.97 minutes\n",
      "total_backward_count 1909050 real_backward_count 214294  11.225%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.317407/  1.564288, val:  72.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.52 seconds, 0.96 minutes\n",
      "total_backward_count 1918840 real_backward_count 215238  11.217%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.301607/  1.576331, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.98 seconds, 0.97 minutes\n",
      "total_backward_count 1928630 real_backward_count 216206  11.210%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.294913/  1.606262, val:  57.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.36 seconds, 0.96 minutes\n",
      "total_backward_count 1938420 real_backward_count 217238  11.207%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.272201/  1.588344, val:  60.00%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.49 seconds, 0.97 minutes\n",
      "total_backward_count 1948210 real_backward_count 218112  11.196%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.272215/  1.604920, val:  71.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.52 seconds, 0.96 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b316e15a87e5405aafd8b14a9a661960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÇ‚ñÇ‚ñá‚ñÑ‚ñÇ‚ñà‚ñÅ‚ñà‚ñÖ‚ñá‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñÖ‚ñá‚ñÖ‚ñá‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñà‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.27221</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.71667</td></tr><tr><td>val_loss</td><td>1.60492</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-sweep-228</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ierkzf1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ierkzf1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251028_224200-5ierkzf1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1jhrrq0x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251029_015435-1jhrrq0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jhrrq0x' target=\"_blank\">splendid-sweep-234</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jhrrq0x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jhrrq0x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251029_015444_487', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 188.0\n",
      "lif layer 1 self.abs_max_v: 188.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 279.0\n",
      "lif layer 2 self.abs_max_v: 279.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 1 self.abs_max_out: 204.0\n",
      "lif layer 1 self.abs_max_v: 235.0\n",
      "fc layer 2 self.abs_max_out: 518.0\n",
      "lif layer 2 self.abs_max_v: 626.5\n",
      "fc layer 1 self.abs_max_out: 209.0\n",
      "lif layer 1 self.abs_max_v: 279.0\n",
      "lif layer 2 self.abs_max_v: 757.5\n",
      "fc layer 1 self.abs_max_out: 232.0\n",
      "lif layer 1 self.abs_max_v: 290.5\n",
      "lif layer 1 self.abs_max_v: 342.5\n",
      "fc layer 1 self.abs_max_out: 270.0\n",
      "lif layer 1 self.abs_max_v: 432.0\n",
      "fc layer 1 self.abs_max_out: 360.0\n",
      "fc layer 3 self.abs_max_out: 201.0\n",
      "lif layer 1 self.abs_max_v: 438.0\n",
      "fc layer 1 self.abs_max_out: 376.0\n",
      "lif layer 1 self.abs_max_v: 510.5\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 2 self.abs_max_out: 523.0\n",
      "lif layer 2 self.abs_max_v: 792.0\n",
      "fc layer 1 self.abs_max_out: 388.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "fc layer 2 self.abs_max_out: 528.0\n",
      "fc layer 1 self.abs_max_out: 394.0\n",
      "lif layer 1 self.abs_max_v: 562.5\n",
      "fc layer 1 self.abs_max_out: 485.0\n",
      "lif layer 1 self.abs_max_v: 724.5\n",
      "fc layer 1 self.abs_max_out: 500.0\n",
      "fc layer 2 self.abs_max_out: 555.0\n",
      "lif layer 1 self.abs_max_v: 737.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "lif layer 2 self.abs_max_v: 904.5\n",
      "fc layer 2 self.abs_max_out: 618.0\n",
      "fc layer 1 self.abs_max_out: 541.0\n",
      "lif layer 1 self.abs_max_v: 780.0\n",
      "fc layer 2 self.abs_max_out: 658.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "lif layer 2 self.abs_max_v: 974.0\n",
      "lif layer 2 self.abs_max_v: 994.0\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "lif layer 1 self.abs_max_v: 808.0\n",
      "lif layer 1 self.abs_max_v: 814.0\n",
      "lif layer 2 self.abs_max_v: 1001.5\n",
      "lif layer 2 self.abs_max_v: 1032.5\n",
      "lif layer 2 self.abs_max_v: 1072.0\n",
      "fc layer 1 self.abs_max_out: 561.0\n",
      "fc layer 2 self.abs_max_out: 705.0\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "lif layer 1 self.abs_max_v: 866.0\n",
      "lif layer 2 self.abs_max_v: 1174.5\n",
      "fc layer 1 self.abs_max_out: 580.0\n",
      "lif layer 1 self.abs_max_v: 942.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.325739/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.24%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 9790 real_backward_count 8473  86.547%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "fc layer 2 self.abs_max_out: 723.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 2 self.abs_max_out: 762.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.319657/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  16.45%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 19580 real_backward_count 16935  86.491%\n",
      "lif layer 1 self.abs_max_v: 992.5\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.322608/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  16.45%, epoch time: 56.49 seconds, 0.94 minutes\n",
      "total_backward_count 29370 real_backward_count 25367  86.370%\n",
      "fc layer 1 self.abs_max_out: 581.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.325054/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.45%, epoch time: 57.35 seconds, 0.96 minutes\n",
      "total_backward_count 39160 real_backward_count 33860  86.466%\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.325322/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  16.45%, epoch time: 56.59 seconds, 0.94 minutes\n",
      "total_backward_count 48950 real_backward_count 42336  86.488%\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.322008/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.45%, epoch time: 56.59 seconds, 0.94 minutes\n",
      "total_backward_count 58740 real_backward_count 50799  86.481%\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.324630/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.45%, epoch time: 55.66 seconds, 0.93 minutes\n",
      "total_backward_count 68530 real_backward_count 59266  86.482%\n",
      "fc layer 1 self.abs_max_out: 589.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.323931/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  16.45%, epoch time: 55.99 seconds, 0.93 minutes\n",
      "total_backward_count 78320 real_backward_count 67710  86.453%\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.321740/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.45%, epoch time: 55.86 seconds, 0.93 minutes\n",
      "total_backward_count 88110 real_backward_count 76139  86.414%\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.322703/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  16.45%, epoch time: 57.12 seconds, 0.95 minutes\n",
      "total_backward_count 97900 real_backward_count 84614  86.429%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.325741/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.45%, epoch time: 57.27 seconds, 0.95 minutes\n",
      "total_backward_count 107690 real_backward_count 93069  86.423%\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.324161/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  16.45%, epoch time: 58.12 seconds, 0.97 minutes\n",
      "total_backward_count 117480 real_backward_count 101518  86.413%\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.321888/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.85%, tr_best:  16.85%, epoch time: 58.57 seconds, 0.98 minutes\n",
      "total_backward_count 127270 real_backward_count 109971  86.408%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.323989/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 57.53 seconds, 0.96 minutes\n",
      "total_backward_count 137060 real_backward_count 118448  86.421%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.325376/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 59.21 seconds, 0.99 minutes\n",
      "total_backward_count 146850 real_backward_count 126894  86.411%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.323114/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 58.05 seconds, 0.97 minutes\n",
      "total_backward_count 156640 real_backward_count 135379  86.427%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.325227/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  16.85%, epoch time: 57.26 seconds, 0.95 minutes\n",
      "total_backward_count 166430 real_backward_count 143832  86.422%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.324114/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 176220 real_backward_count 152305  86.429%\n",
      "lif layer 1 self.abs_max_v: 1030.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.323063/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.85%, epoch time: 58.17 seconds, 0.97 minutes\n",
      "total_backward_count 186010 real_backward_count 160769  86.430%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.322181/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 57.83 seconds, 0.96 minutes\n",
      "total_backward_count 195800 real_backward_count 169207  86.418%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.321162/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  16.85%, epoch time: 57.53 seconds, 0.96 minutes\n",
      "total_backward_count 205590 real_backward_count 177679  86.424%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.327621/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.02%, tr_best:  16.85%, epoch time: 58.52 seconds, 0.98 minutes\n",
      "total_backward_count 215380 real_backward_count 186150  86.429%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.323303/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 58.23 seconds, 0.97 minutes\n",
      "total_backward_count 225170 real_backward_count 194589  86.419%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.324633/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 57.08 seconds, 0.95 minutes\n",
      "total_backward_count 234960 real_backward_count 203028  86.410%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.322214/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  16.85%, epoch time: 56.16 seconds, 0.94 minutes\n",
      "total_backward_count 244750 real_backward_count 211494  86.412%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.321650/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  16.85%, epoch time: 56.18 seconds, 0.94 minutes\n",
      "total_backward_count 254540 real_backward_count 219934  86.404%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.325432/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.65%, tr_best:  16.85%, epoch time: 56.45 seconds, 0.94 minutes\n",
      "total_backward_count 264330 real_backward_count 228370  86.396%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.325854/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.42%, tr_best:  16.85%, epoch time: 55.92 seconds, 0.93 minutes\n",
      "total_backward_count 274120 real_backward_count 236850  86.404%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.323102/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  16.85%, epoch time: 56.26 seconds, 0.94 minutes\n",
      "total_backward_count 283910 real_backward_count 245315  86.406%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.324426/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  16.85%, epoch time: 56.75 seconds, 0.95 minutes\n",
      "total_backward_count 293700 real_backward_count 253775  86.406%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.322130/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.53%, tr_best:  16.85%, epoch time: 58.61 seconds, 0.98 minutes\n",
      "total_backward_count 303490 real_backward_count 262206  86.397%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.324562/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  16.85%, epoch time: 57.48 seconds, 0.96 minutes\n",
      "total_backward_count 313280 real_backward_count 270649  86.392%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.323519/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 56.97 seconds, 0.95 minutes\n",
      "total_backward_count 323070 real_backward_count 279075  86.382%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.323928/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.32%, tr_best:  16.85%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 332860 real_backward_count 287586  86.398%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.324293/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  16.85%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 342650 real_backward_count 296040  86.397%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.322043/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 57.48 seconds, 0.96 minutes\n",
      "total_backward_count 352440 real_backward_count 304478  86.391%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.323213/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  16.85%, epoch time: 57.70 seconds, 0.96 minutes\n",
      "total_backward_count 362230 real_backward_count 312924  86.388%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.323028/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 58.41 seconds, 0.97 minutes\n",
      "total_backward_count 372020 real_backward_count 321365  86.384%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.324174/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.02%, tr_best:  16.85%, epoch time: 56.86 seconds, 0.95 minutes\n",
      "total_backward_count 381810 real_backward_count 329818  86.383%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.321960/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 57.56 seconds, 0.96 minutes\n",
      "total_backward_count 391600 real_backward_count 338247  86.376%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.322287/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 57.73 seconds, 0.96 minutes\n",
      "total_backward_count 401390 real_backward_count 346717  86.379%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.320219/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 58.28 seconds, 0.97 minutes\n",
      "total_backward_count 411180 real_backward_count 355144  86.372%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.322637/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  16.85%, epoch time: 57.12 seconds, 0.95 minutes\n",
      "total_backward_count 420970 real_backward_count 363572  86.365%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.324344/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 58.30 seconds, 0.97 minutes\n",
      "total_backward_count 430760 real_backward_count 372019  86.363%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.325536/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.32%, tr_best:  16.85%, epoch time: 59.30 seconds, 0.99 minutes\n",
      "total_backward_count 440550 real_backward_count 380491  86.367%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.321388/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.85%, epoch time: 57.30 seconds, 0.95 minutes\n",
      "total_backward_count 450340 real_backward_count 388963  86.371%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.325407/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.85%, epoch time: 56.97 seconds, 0.95 minutes\n",
      "total_backward_count 460130 real_backward_count 397459  86.380%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.326031/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.22%, tr_best:  16.85%, epoch time: 56.16 seconds, 0.94 minutes\n",
      "total_backward_count 469920 real_backward_count 405957  86.389%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.323782/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 56.87 seconds, 0.95 minutes\n",
      "total_backward_count 479710 real_backward_count 414444  86.395%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.322958/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 56.11 seconds, 0.94 minutes\n",
      "total_backward_count 489500 real_backward_count 422896  86.393%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.323869/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  16.85%, epoch time: 56.11 seconds, 0.94 minutes\n",
      "total_backward_count 499290 real_backward_count 431361  86.395%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.321823/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  16.85%, epoch time: 57.02 seconds, 0.95 minutes\n",
      "total_backward_count 509080 real_backward_count 439786  86.388%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.322900/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.75%, tr_best:  16.85%, epoch time: 57.45 seconds, 0.96 minutes\n",
      "total_backward_count 518870 real_backward_count 448233  86.386%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.325852/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.42%, tr_best:  16.85%, epoch time: 57.68 seconds, 0.96 minutes\n",
      "total_backward_count 528660 real_backward_count 456717  86.391%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.325526/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 58.30 seconds, 0.97 minutes\n",
      "total_backward_count 538450 real_backward_count 465151  86.387%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.325742/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.85%, epoch time: 58.17 seconds, 0.97 minutes\n",
      "total_backward_count 548240 real_backward_count 473621  86.389%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.323983/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  16.85%, epoch time: 58.06 seconds, 0.97 minutes\n",
      "total_backward_count 558030 real_backward_count 482059  86.386%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.324157/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  16.85%, epoch time: 58.11 seconds, 0.97 minutes\n",
      "total_backward_count 567820 real_backward_count 490542  86.390%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.325111/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.85%, epoch time: 57.66 seconds, 0.96 minutes\n",
      "total_backward_count 577610 real_backward_count 499014  86.393%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.324652/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  16.85%, epoch time: 57.58 seconds, 0.96 minutes\n",
      "total_backward_count 587400 real_backward_count 507508  86.399%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.320703/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.85%, tr_best:  16.85%, epoch time: 57.34 seconds, 0.96 minutes\n",
      "total_backward_count 597190 real_backward_count 515956  86.397%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.323084/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 606980 real_backward_count 524423  86.399%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.326963/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 57.72 seconds, 0.96 minutes\n",
      "total_backward_count 616770 real_backward_count 532919  86.405%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.324723/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.75%, tr_best:  16.85%, epoch time: 57.70 seconds, 0.96 minutes\n",
      "total_backward_count 626560 real_backward_count 541350  86.400%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.322884/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.65%, tr_best:  16.85%, epoch time: 58.16 seconds, 0.97 minutes\n",
      "total_backward_count 636350 real_backward_count 549795  86.398%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.321916/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  16.85%, epoch time: 58.21 seconds, 0.97 minutes\n",
      "total_backward_count 646140 real_backward_count 558286  86.403%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.327581/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  16.85%, epoch time: 58.69 seconds, 0.98 minutes\n",
      "total_backward_count 655930 real_backward_count 566765  86.406%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.326080/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  16.85%, epoch time: 55.72 seconds, 0.93 minutes\n",
      "total_backward_count 665720 real_backward_count 575241  86.409%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.326352/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.85%, epoch time: 56.31 seconds, 0.94 minutes\n",
      "total_backward_count 675510 real_backward_count 583759  86.418%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.323567/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 56.62 seconds, 0.94 minutes\n",
      "total_backward_count 685300 real_backward_count 592205  86.415%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.324813/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  16.85%, epoch time: 56.08 seconds, 0.93 minutes\n",
      "total_backward_count 695090 real_backward_count 600681  86.418%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.324013/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  16.85%, epoch time: 55.30 seconds, 0.92 minutes\n",
      "total_backward_count 704880 real_backward_count 609139  86.417%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.322472/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.55%, tr_best:  16.85%, epoch time: 55.24 seconds, 0.92 minutes\n",
      "total_backward_count 714670 real_backward_count 617600  86.418%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.323944/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.75%, tr_best:  16.85%, epoch time: 57.04 seconds, 0.95 minutes\n",
      "total_backward_count 724460 real_backward_count 626070  86.419%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.322181/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 57.19 seconds, 0.95 minutes\n",
      "total_backward_count 734250 real_backward_count 634494  86.414%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.324180/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  16.85%, epoch time: 57.34 seconds, 0.96 minutes\n",
      "total_backward_count 744040 real_backward_count 642980  86.417%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.324205/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  16.85%, epoch time: 57.70 seconds, 0.96 minutes\n",
      "total_backward_count 753830 real_backward_count 651473  86.422%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.323714/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.02%, tr_best:  16.85%, epoch time: 57.62 seconds, 0.96 minutes\n",
      "total_backward_count 763620 real_backward_count 659924  86.420%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.323168/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  16.85%, epoch time: 58.15 seconds, 0.97 minutes\n",
      "total_backward_count 773410 real_backward_count 668391  86.421%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.325239/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.32%, tr_best:  16.85%, epoch time: 56.81 seconds, 0.95 minutes\n",
      "total_backward_count 783200 real_backward_count 676917  86.430%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.321201/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 58.56 seconds, 0.98 minutes\n",
      "total_backward_count 792990 real_backward_count 685363  86.428%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.323689/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 59.16 seconds, 0.99 minutes\n",
      "total_backward_count 802780 real_backward_count 693835  86.429%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.323243/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.85%, tr_best:  16.85%, epoch time: 58.26 seconds, 0.97 minutes\n",
      "total_backward_count 812570 real_backward_count 702306  86.430%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.323055/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  16.85%, epoch time: 58.99 seconds, 0.98 minutes\n",
      "total_backward_count 822360 real_backward_count 710748  86.428%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.325619/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.42%, tr_best:  16.85%, epoch time: 58.04 seconds, 0.97 minutes\n",
      "total_backward_count 832150 real_backward_count 719234  86.431%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.324079/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 58.80 seconds, 0.98 minutes\n",
      "total_backward_count 841940 real_backward_count 727687  86.430%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.323095/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 57.03 seconds, 0.95 minutes\n",
      "total_backward_count 851730 real_backward_count 736120  86.426%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.326030/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  16.85%, epoch time: 57.28 seconds, 0.95 minutes\n",
      "total_backward_count 861520 real_backward_count 744550  86.423%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.324392/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  16.85%, epoch time: 57.03 seconds, 0.95 minutes\n",
      "total_backward_count 871310 real_backward_count 753013  86.423%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.322048/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.85%, tr_best:  16.85%, epoch time: 56.55 seconds, 0.94 minutes\n",
      "total_backward_count 881100 real_backward_count 761426  86.418%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.323906/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  16.85%, epoch time: 56.12 seconds, 0.94 minutes\n",
      "total_backward_count 890890 real_backward_count 769886  86.418%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.323493/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  16.85%, epoch time: 56.74 seconds, 0.95 minutes\n",
      "total_backward_count 900680 real_backward_count 778322  86.415%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.326706/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  16.85%, epoch time: 56.07 seconds, 0.93 minutes\n",
      "total_backward_count 910470 real_backward_count 786815  86.419%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.323520/  2.325624, val:  18.75%, val_best:  18.75%, tr:  17.26%, tr_best:  17.26%, epoch time: 56.18 seconds, 0.94 minutes\n",
      "total_backward_count 920260 real_backward_count 795269  86.418%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.325479/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.53%, tr_best:  17.26%, epoch time: 56.83 seconds, 0.95 minutes\n",
      "total_backward_count 930050 real_backward_count 803761  86.421%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.319580/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  17.26%, epoch time: 57.75 seconds, 0.96 minutes\n",
      "total_backward_count 939840 real_backward_count 812205  86.419%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.322881/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 949630 real_backward_count 820664  86.419%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.325047/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 57.75 seconds, 0.96 minutes\n",
      "total_backward_count 959420 real_backward_count 829133  86.420%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.324712/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.32%, tr_best:  17.26%, epoch time: 58.31 seconds, 0.97 minutes\n",
      "total_backward_count 969210 real_backward_count 837600  86.421%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.324380/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.32%, tr_best:  17.26%, epoch time: 57.81 seconds, 0.96 minutes\n",
      "total_backward_count 979000 real_backward_count 846068  86.422%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.319132/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 58.08 seconds, 0.97 minutes\n",
      "total_backward_count 988790 real_backward_count 854506  86.419%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.321418/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.65%, tr_best:  17.26%, epoch time: 57.51 seconds, 0.96 minutes\n",
      "total_backward_count 998580 real_backward_count 862938  86.417%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.323128/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  17.26%, epoch time: 57.39 seconds, 0.96 minutes\n",
      "total_backward_count 1008370 real_backward_count 871378  86.415%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.323427/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.55%, tr_best:  17.26%, epoch time: 57.18 seconds, 0.95 minutes\n",
      "total_backward_count 1018160 real_backward_count 879813  86.412%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.323816/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 57.23 seconds, 0.95 minutes\n",
      "total_backward_count 1027950 real_backward_count 888297  86.414%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.323002/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 57.48 seconds, 0.96 minutes\n",
      "total_backward_count 1037740 real_backward_count 896780  86.417%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.325829/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  17.26%, epoch time: 56.94 seconds, 0.95 minutes\n",
      "total_backward_count 1047530 real_backward_count 905242  86.417%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.322777/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 58.12 seconds, 0.97 minutes\n",
      "total_backward_count 1057320 real_backward_count 913726  86.419%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.325984/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 58.31 seconds, 0.97 minutes\n",
      "total_backward_count 1067110 real_backward_count 922200  86.420%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.325089/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 58.46 seconds, 0.97 minutes\n",
      "total_backward_count 1076900 real_backward_count 930695  86.424%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.324023/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 56.32 seconds, 0.94 minutes\n",
      "total_backward_count 1086690 real_backward_count 939138  86.422%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.323969/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 56.46 seconds, 0.94 minutes\n",
      "total_backward_count 1096480 real_backward_count 947613  86.423%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.324186/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 56.53 seconds, 0.94 minutes\n",
      "total_backward_count 1106270 real_backward_count 956074  86.423%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.325192/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 56.09 seconds, 0.93 minutes\n",
      "total_backward_count 1116060 real_backward_count 964532  86.423%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.326312/  2.325624, val:  18.75%, val_best:  18.75%, tr:  14.91%, tr_best:  17.26%, epoch time: 56.37 seconds, 0.94 minutes\n",
      "total_backward_count 1125850 real_backward_count 973005  86.424%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.325689/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 56.57 seconds, 0.94 minutes\n",
      "total_backward_count 1135640 real_backward_count 981475  86.425%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.324305/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 56.67 seconds, 0.94 minutes\n",
      "total_backward_count 1145430 real_backward_count 989923  86.424%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.323438/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 57.53 seconds, 0.96 minutes\n",
      "total_backward_count 1155220 real_backward_count 998353  86.421%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.324696/  2.325624, val:  18.75%, val_best:  18.75%, tr:  17.06%, tr_best:  17.26%, epoch time: 57.93 seconds, 0.97 minutes\n",
      "total_backward_count 1165010 real_backward_count 1006856  86.425%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.323947/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.53%, tr_best:  17.26%, epoch time: 59.03 seconds, 0.98 minutes\n",
      "total_backward_count 1174800 real_backward_count 1015331  86.426%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.322727/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.12%, tr_best:  17.26%, epoch time: 58.65 seconds, 0.98 minutes\n",
      "total_backward_count 1184590 real_backward_count 1023829  86.429%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.324377/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.55%, tr_best:  17.26%, epoch time: 57.50 seconds, 0.96 minutes\n",
      "total_backward_count 1194380 real_backward_count 1032284  86.428%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.320821/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  17.26%, epoch time: 58.46 seconds, 0.97 minutes\n",
      "total_backward_count 1204170 real_backward_count 1040710  86.426%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.327846/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 57.53 seconds, 0.96 minutes\n",
      "total_backward_count 1213960 real_backward_count 1049213  86.429%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.323024/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 57.37 seconds, 0.96 minutes\n",
      "total_backward_count 1223750 real_backward_count 1057700  86.431%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.325702/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 57.39 seconds, 0.96 minutes\n",
      "total_backward_count 1233540 real_backward_count 1066174  86.432%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.326778/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 58.12 seconds, 0.97 minutes\n",
      "total_backward_count 1243330 real_backward_count 1074676  86.435%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.323793/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 57.78 seconds, 0.96 minutes\n",
      "total_backward_count 1253120 real_backward_count 1083130  86.435%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.327699/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.53%, tr_best:  17.26%, epoch time: 57.71 seconds, 0.96 minutes\n",
      "total_backward_count 1262910 real_backward_count 1091628  86.438%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.325089/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 1272700 real_backward_count 1100081  86.437%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.321870/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 58.94 seconds, 0.98 minutes\n",
      "total_backward_count 1282490 real_backward_count 1108513  86.434%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.322629/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 56.88 seconds, 0.95 minutes\n",
      "total_backward_count 1292280 real_backward_count 1116997  86.436%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.320791/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 55.62 seconds, 0.93 minutes\n",
      "total_backward_count 1302070 real_backward_count 1125473  86.437%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.321916/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 56.64 seconds, 0.94 minutes\n",
      "total_backward_count 1311860 real_backward_count 1133954  86.439%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.328279/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 56.66 seconds, 0.94 minutes\n",
      "total_backward_count 1321650 real_backward_count 1142440  86.440%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.324555/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 55.66 seconds, 0.93 minutes\n",
      "total_backward_count 1331440 real_backward_count 1150899  86.440%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.322896/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.65%, tr_best:  17.26%, epoch time: 55.23 seconds, 0.92 minutes\n",
      "total_backward_count 1341230 real_backward_count 1159331  86.438%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.324440/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 56.60 seconds, 0.94 minutes\n",
      "total_backward_count 1351020 real_backward_count 1167779  86.437%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.327246/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 57.27 seconds, 0.95 minutes\n",
      "total_backward_count 1360810 real_backward_count 1176240  86.437%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.323778/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 57.22 seconds, 0.95 minutes\n",
      "total_backward_count 1370600 real_backward_count 1184668  86.434%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.321792/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 57.66 seconds, 0.96 minutes\n",
      "total_backward_count 1380390 real_backward_count 1193101  86.432%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.323312/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 58.18 seconds, 0.97 minutes\n",
      "total_backward_count 1390180 real_backward_count 1201569  86.433%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.325192/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 58.00 seconds, 0.97 minutes\n",
      "total_backward_count 1399970 real_backward_count 1210054  86.434%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.323803/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 57.82 seconds, 0.96 minutes\n",
      "total_backward_count 1409760 real_backward_count 1218507  86.434%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.324558/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  17.26%, epoch time: 58.38 seconds, 0.97 minutes\n",
      "total_backward_count 1419550 real_backward_count 1226957  86.433%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.321548/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 58.72 seconds, 0.98 minutes\n",
      "total_backward_count 1429340 real_backward_count 1235395  86.431%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.321873/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 58.02 seconds, 0.97 minutes\n",
      "total_backward_count 1439130 real_backward_count 1243880  86.433%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.323730/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 58.50 seconds, 0.97 minutes\n",
      "total_backward_count 1448920 real_backward_count 1252342  86.433%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.325762/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 58.68 seconds, 0.98 minutes\n",
      "total_backward_count 1458710 real_backward_count 1260807  86.433%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.320830/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 57.54 seconds, 0.96 minutes\n",
      "total_backward_count 1468500 real_backward_count 1269275  86.433%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.323803/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 57.69 seconds, 0.96 minutes\n",
      "total_backward_count 1478290 real_backward_count 1277762  86.435%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.325158/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.34%, tr_best:  17.26%, epoch time: 58.57 seconds, 0.98 minutes\n",
      "total_backward_count 1488080 real_backward_count 1286193  86.433%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.324450/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 57.56 seconds, 0.96 minutes\n",
      "total_backward_count 1497870 real_backward_count 1294651  86.433%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.324360/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 57.00 seconds, 0.95 minutes\n",
      "total_backward_count 1507660 real_backward_count 1303121  86.433%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.328347/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 56.42 seconds, 0.94 minutes\n",
      "total_backward_count 1517450 real_backward_count 1311595  86.434%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.326539/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.42%, tr_best:  17.26%, epoch time: 56.39 seconds, 0.94 minutes\n",
      "total_backward_count 1527240 real_backward_count 1320108  86.437%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.327524/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 55.65 seconds, 0.93 minutes\n",
      "total_backward_count 1537030 real_backward_count 1328611  86.440%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.325547/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 56.07 seconds, 0.93 minutes\n",
      "total_backward_count 1546820 real_backward_count 1337092  86.441%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.322264/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 57.02 seconds, 0.95 minutes\n",
      "total_backward_count 1556610 real_backward_count 1345519  86.439%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.326462/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 56.90 seconds, 0.95 minutes\n",
      "total_backward_count 1566400 real_backward_count 1353986  86.439%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.325346/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 57.88 seconds, 0.96 minutes\n",
      "total_backward_count 1576190 real_backward_count 1362434  86.438%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.324581/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 57.96 seconds, 0.97 minutes\n",
      "total_backward_count 1585980 real_backward_count 1370888  86.438%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.326557/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.85%, tr_best:  17.26%, epoch time: 58.23 seconds, 0.97 minutes\n",
      "total_backward_count 1595770 real_backward_count 1379291  86.434%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.320527/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 57.14 seconds, 0.95 minutes\n",
      "total_backward_count 1605560 real_backward_count 1387770  86.435%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.323258/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 58.80 seconds, 0.98 minutes\n",
      "total_backward_count 1615350 real_backward_count 1396249  86.436%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.324293/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.22%, tr_best:  17.26%, epoch time: 58.74 seconds, 0.98 minutes\n",
      "total_backward_count 1625140 real_backward_count 1404706  86.436%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.326862/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.55%, tr_best:  17.26%, epoch time: 57.54 seconds, 0.96 minutes\n",
      "total_backward_count 1634930 real_backward_count 1413181  86.437%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.325046/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 57.16 seconds, 0.95 minutes\n",
      "total_backward_count 1644720 real_backward_count 1421634  86.436%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.322606/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 56.57 seconds, 0.94 minutes\n",
      "total_backward_count 1654510 real_backward_count 1430085  86.436%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.325268/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 56.95 seconds, 0.95 minutes\n",
      "total_backward_count 1664300 real_backward_count 1438569  86.437%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.325312/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 56.61 seconds, 0.94 minutes\n",
      "total_backward_count 1674090 real_backward_count 1447039  86.437%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.325839/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.63%, tr_best:  17.26%, epoch time: 57.78 seconds, 0.96 minutes\n",
      "total_backward_count 1683880 real_backward_count 1455528  86.439%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.323336/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 57.77 seconds, 0.96 minutes\n",
      "total_backward_count 1693670 real_backward_count 1463964  86.437%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.326060/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 57.99 seconds, 0.97 minutes\n",
      "total_backward_count 1703460 real_backward_count 1472400  86.436%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.327021/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  17.26%, epoch time: 58.45 seconds, 0.97 minutes\n",
      "total_backward_count 1713250 real_backward_count 1480892  86.438%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.324990/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.42%, tr_best:  17.26%, epoch time: 56.35 seconds, 0.94 minutes\n",
      "total_backward_count 1723040 real_backward_count 1489369  86.438%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.325341/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.53%, tr_best:  17.26%, epoch time: 56.37 seconds, 0.94 minutes\n",
      "total_backward_count 1732830 real_backward_count 1497849  86.439%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.327298/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.32%, tr_best:  17.26%, epoch time: 56.59 seconds, 0.94 minutes\n",
      "total_backward_count 1742620 real_backward_count 1506345  86.441%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.322459/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.85%, tr_best:  17.26%, epoch time: 56.68 seconds, 0.94 minutes\n",
      "total_backward_count 1752410 real_backward_count 1514796  86.441%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.319747/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 56.43 seconds, 0.94 minutes\n",
      "total_backward_count 1762200 real_backward_count 1523266  86.441%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.324829/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  17.26%, epoch time: 56.54 seconds, 0.94 minutes\n",
      "total_backward_count 1771990 real_backward_count 1531739  86.442%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.325249/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.42%, tr_best:  17.26%, epoch time: 57.01 seconds, 0.95 minutes\n",
      "total_backward_count 1781780 real_backward_count 1540187  86.441%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.326006/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 58.16 seconds, 0.97 minutes\n",
      "total_backward_count 1791570 real_backward_count 1548656  86.441%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.325977/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.04%, tr_best:  17.26%, epoch time: 57.72 seconds, 0.96 minutes\n",
      "total_backward_count 1801360 real_backward_count 1557128  86.442%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.329172/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.42%, tr_best:  17.26%, epoch time: 58.32 seconds, 0.97 minutes\n",
      "total_backward_count 1811150 real_backward_count 1565615  86.443%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.321515/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 57.48 seconds, 0.96 minutes\n",
      "total_backward_count 1820940 real_backward_count 1574046  86.441%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.324544/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  17.26%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 1830730 real_backward_count 1582496  86.441%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.322292/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.24%, tr_best:  17.26%, epoch time: 58.53 seconds, 0.98 minutes\n",
      "total_backward_count 1840520 real_backward_count 1590930  86.439%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.320994/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.73%, tr_best:  17.26%, epoch time: 57.53 seconds, 0.96 minutes\n",
      "total_backward_count 1850310 real_backward_count 1599392  86.439%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.321337/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.93%, tr_best:  17.26%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 1860100 real_backward_count 1607856  86.439%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.323400/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 57.67 seconds, 0.96 minutes\n",
      "total_backward_count 1869890 real_backward_count 1616319  86.439%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.325859/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.53%, tr_best:  17.26%, epoch time: 58.03 seconds, 0.97 minutes\n",
      "total_backward_count 1879680 real_backward_count 1624820  86.441%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.325101/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 57.69 seconds, 0.96 minutes\n",
      "total_backward_count 1889470 real_backward_count 1633270  86.441%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.324141/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  17.26%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 1899260 real_backward_count 1641707  86.439%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.323805/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.14%, tr_best:  17.26%, epoch time: 57.02 seconds, 0.95 minutes\n",
      "total_backward_count 1909050 real_backward_count 1650149  86.438%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.323327/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 58.02 seconds, 0.97 minutes\n",
      "total_backward_count 1918840 real_backward_count 1658617  86.439%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.328250/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 57.19 seconds, 0.95 minutes\n",
      "total_backward_count 1928630 real_backward_count 1667066  86.438%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.325035/  2.325624, val:  18.75%, val_best:  18.75%, tr:  15.83%, tr_best:  17.26%, epoch time: 56.37 seconds, 0.94 minutes\n",
      "total_backward_count 1938420 real_backward_count 1675553  86.439%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.321279/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.45%, tr_best:  17.26%, epoch time: 55.89 seconds, 0.93 minutes\n",
      "total_backward_count 1948210 real_backward_count 1683988  86.438%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.323805/  2.325624, val:  18.75%, val_best:  18.75%, tr:  16.85%, tr_best:  17.26%, epoch time: 56.14 seconds, 0.94 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922fa19459484bffae7f4589a466a90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÇ‚ñÜ‚ñá‚ñà‚ñá‚ñÉ‚ñÖ‚ñÖ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.16854</td></tr><tr><td>tr_epoch_loss</td><td>2.32381</td></tr><tr><td>val_acc_best</td><td>0.1875</td></tr><tr><td>val_acc_now</td><td>0.1875</td></tr><tr><td>val_loss</td><td>2.32562</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">splendid-sweep-234</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jhrrq0x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jhrrq0x</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251029_015435-1jhrrq0x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z0b0o2e5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251029_050640-z0b0o2e5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z0b0o2e5' target=\"_blank\">sunny-sweep-240</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z0b0o2e5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z0b0o2e5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251029_050649_065', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 756.0\n",
      "lif layer 1 self.abs_max_v: 756.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 814.0\n",
      "lif layer 1 self.abs_max_v: 1052.5\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "fc layer 1 self.abs_max_out: 833.0\n",
      "lif layer 1 self.abs_max_v: 1101.0\n",
      "lif layer 2 self.abs_max_v: 192.0\n",
      "fc layer 1 self.abs_max_out: 936.0\n",
      "lif layer 1 self.abs_max_v: 1267.0\n",
      "fc layer 2 self.abs_max_out: 373.0\n",
      "lif layer 2 self.abs_max_v: 438.5\n",
      "lif layer 1 self.abs_max_v: 1338.5\n",
      "fc layer 2 self.abs_max_out: 480.0\n",
      "lif layer 2 self.abs_max_v: 532.0\n",
      "fc layer 1 self.abs_max_out: 1253.0\n",
      "lif layer 1 self.abs_max_v: 1621.5\n",
      "lif layer 2 self.abs_max_v: 565.5\n",
      "fc layer 1 self.abs_max_out: 1641.0\n",
      "lif layer 1 self.abs_max_v: 1704.0\n",
      "fc layer 2 self.abs_max_out: 594.0\n",
      "lif layer 2 self.abs_max_v: 863.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "lif layer 2 self.abs_max_v: 969.5\n",
      "fc layer 1 self.abs_max_out: 2058.0\n",
      "lif layer 1 self.abs_max_v: 2058.0\n",
      "fc layer 2 self.abs_max_out: 826.0\n",
      "lif layer 2 self.abs_max_v: 1237.5\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "fc layer 1 self.abs_max_out: 2084.0\n",
      "lif layer 1 self.abs_max_v: 2084.0\n",
      "fc layer 2 self.abs_max_out: 1032.0\n",
      "lif layer 2 self.abs_max_v: 1312.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "lif layer 2 self.abs_max_v: 1373.5\n",
      "lif layer 1 self.abs_max_v: 2274.5\n",
      "fc layer 1 self.abs_max_out: 3569.0\n",
      "lif layer 1 self.abs_max_v: 3569.0\n",
      "fc layer 2 self.abs_max_out: 1195.0\n",
      "lif layer 2 self.abs_max_v: 1777.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 1 self.abs_max_out: 4185.0\n",
      "lif layer 1 self.abs_max_v: 4185.0\n",
      "fc layer 2 self.abs_max_out: 1317.0\n",
      "lif layer 2 self.abs_max_v: 1828.5\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 2 self.abs_max_out: 1577.0\n",
      "lif layer 2 self.abs_max_v: 1996.0\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "lif layer 2 self.abs_max_v: 2005.0\n",
      "lif layer 2 self.abs_max_v: 2058.5\n",
      "lif layer 2 self.abs_max_v: 2337.5\n",
      "fc layer 2 self.abs_max_out: 1585.0\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 2 self.abs_max_out: 1821.0\n",
      "lif layer 2 self.abs_max_v: 2411.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "fc layer 2 self.abs_max_out: 2300.0\n",
      "lif layer 2 self.abs_max_v: 2465.0\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 2487.5\n",
      "lif layer 2 self.abs_max_v: 2897.0\n",
      "lif layer 2 self.abs_max_v: 2950.5\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 2 self.abs_max_out: 2617.0\n",
      "lif layer 2 self.abs_max_v: 3036.5\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 1 self.abs_max_out: 4784.0\n",
      "lif layer 1 self.abs_max_v: 4784.0\n",
      "lif layer 2 self.abs_max_v: 3810.0\n",
      "fc layer 1 self.abs_max_out: 5388.0\n",
      "lif layer 1 self.abs_max_v: 5388.0\n",
      "fc layer 1 self.abs_max_out: 5658.0\n",
      "lif layer 1 self.abs_max_v: 5658.0\n",
      "lif layer 2 self.abs_max_v: 3842.0\n",
      "lif layer 2 self.abs_max_v: 4188.0\n",
      "lif layer 2 self.abs_max_v: 4207.0\n",
      "fc layer 2 self.abs_max_out: 2620.0\n",
      "fc layer 2 self.abs_max_out: 2661.0\n",
      "fc layer 2 self.abs_max_out: 2725.0\n",
      "fc layer 2 self.abs_max_out: 2793.0\n",
      "fc layer 3 self.abs_max_out: 558.0\n",
      "fc layer 2 self.abs_max_out: 2816.0\n",
      "fc layer 2 self.abs_max_out: 3119.0\n",
      "fc layer 2 self.abs_max_out: 3206.0\n",
      "fc layer 3 self.abs_max_out: 572.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 604.0\n",
      "lif layer 2 self.abs_max_v: 4396.5\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "fc layer 3 self.abs_max_out: 673.0\n",
      "fc layer 1 self.abs_max_out: 5736.0\n",
      "lif layer 1 self.abs_max_v: 5736.0\n",
      "fc layer 1 self.abs_max_out: 6171.0\n",
      "lif layer 1 self.abs_max_v: 6171.0\n",
      "fc layer 1 self.abs_max_out: 6256.0\n",
      "lif layer 1 self.abs_max_v: 6256.0\n",
      "fc layer 1 self.abs_max_out: 6480.0\n",
      "lif layer 1 self.abs_max_v: 6480.0\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "fc layer 2 self.abs_max_out: 3268.0\n",
      "fc layer 2 self.abs_max_out: 3349.0\n",
      "fc layer 2 self.abs_max_out: 3360.0\n",
      "lif layer 2 self.abs_max_v: 4434.5\n",
      "fc layer 2 self.abs_max_out: 3532.0\n",
      "lif layer 1 self.abs_max_v: 6919.0\n",
      "fc layer 1 self.abs_max_out: 6727.0\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "fc layer 3 self.abs_max_out: 731.0\n",
      "fc layer 1 self.abs_max_out: 7061.0\n",
      "lif layer 1 self.abs_max_v: 7061.0\n",
      "fc layer 1 self.abs_max_out: 7154.0\n",
      "lif layer 1 self.abs_max_v: 7154.0\n",
      "lif layer 2 self.abs_max_v: 4439.0\n",
      "lif layer 2 self.abs_max_v: 4471.5\n",
      "fc layer 2 self.abs_max_out: 3785.0\n",
      "fc layer 1 self.abs_max_out: 7201.0\n",
      "lif layer 1 self.abs_max_v: 7201.0\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "fc layer 1 self.abs_max_out: 7271.0\n",
      "lif layer 1 self.abs_max_v: 7271.0\n",
      "fc layer 1 self.abs_max_out: 7824.0\n",
      "lif layer 1 self.abs_max_v: 7824.0\n",
      "fc layer 2 self.abs_max_out: 3825.0\n",
      "fc layer 2 self.abs_max_out: 3938.0\n",
      "lif layer 2 self.abs_max_v: 4511.0\n",
      "lif layer 2 self.abs_max_v: 4679.5\n",
      "lif layer 2 self.abs_max_v: 4790.0\n",
      "fc layer 1 self.abs_max_out: 7900.0\n",
      "lif layer 1 self.abs_max_v: 7900.0\n",
      "fc layer 2 self.abs_max_out: 4430.0\n",
      "fc layer 1 self.abs_max_out: 8511.0\n",
      "lif layer 1 self.abs_max_v: 8511.0\n",
      "fc layer 1 self.abs_max_out: 8708.0\n",
      "lif layer 1 self.abs_max_v: 8708.0\n",
      "fc layer 1 self.abs_max_out: 9255.0\n",
      "lif layer 1 self.abs_max_v: 9255.0\n",
      "fc layer 1 self.abs_max_out: 9318.0\n",
      "lif layer 1 self.abs_max_v: 9318.0\n",
      "fc layer 3 self.abs_max_out: 838.0\n",
      "lif layer 1 self.abs_max_v: 9991.0\n",
      "fc layer 1 self.abs_max_out: 9373.0\n",
      "fc layer 3 self.abs_max_out: 866.0\n",
      "fc layer 1 self.abs_max_out: 9655.0\n",
      "fc layer 1 self.abs_max_out: 10231.0\n",
      "lif layer 1 self.abs_max_v: 10231.0\n",
      "lif layer 1 self.abs_max_v: 10505.0\n",
      "lif layer 1 self.abs_max_v: 11264.5\n",
      "lif layer 1 self.abs_max_v: 11330.0\n",
      "lif layer 1 self.abs_max_v: 11708.5\n",
      "lif layer 1 self.abs_max_v: 12188.5\n",
      "lif layer 2 self.abs_max_v: 4909.5\n",
      "lif layer 1 self.abs_max_v: 12372.0\n",
      "lif layer 1 self.abs_max_v: 12724.0\n",
      "lif layer 2 self.abs_max_v: 4944.0\n",
      "fc layer 1 self.abs_max_out: 10580.0\n",
      "lif layer 2 self.abs_max_v: 5068.5\n",
      "lif layer 2 self.abs_max_v: 5190.5\n",
      "fc layer 2 self.abs_max_out: 4693.0\n",
      "lif layer 1 self.abs_max_v: 13086.5\n",
      "lif layer 1 self.abs_max_v: 13449.0\n",
      "lif layer 1 self.abs_max_v: 13715.5\n",
      "lif layer 2 self.abs_max_v: 5341.0\n",
      "fc layer 2 self.abs_max_out: 4956.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  2.065428/  2.131881, val:  29.17%, val_best:  29.17%, tr:  90.81%, tr_best:  90.81%, epoch time: 55.48 seconds, 0.92 minutes\n",
      "total_backward_count 9790 real_backward_count 3059  31.246%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 5446.0\n",
      "lif layer 2 self.abs_max_v: 5518.0\n",
      "fc layer 1 self.abs_max_out: 11440.0\n",
      "lif layer 2 self.abs_max_v: 5572.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  2.011539/  2.089283, val:  44.58%, val_best:  44.58%, tr:  98.26%, tr_best:  98.26%, epoch time: 56.21 seconds, 0.94 minutes\n",
      "total_backward_count 19580 real_backward_count 5108  26.088%\n",
      "lif layer 2 self.abs_max_v: 5624.0\n",
      "fc layer 1 self.abs_max_out: 12008.0\n",
      "fc layer 3 self.abs_max_out: 872.0\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "fc layer 3 self.abs_max_out: 931.0\n",
      "lif layer 2 self.abs_max_v: 5808.5\n",
      "lif layer 1 self.abs_max_v: 13853.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.999360/  2.120428, val:  43.33%, val_best:  44.58%, tr:  98.47%, tr_best:  98.47%, epoch time: 55.42 seconds, 0.92 minutes\n",
      "total_backward_count 29370 real_backward_count 6998  23.827%\n",
      "lif layer 2 self.abs_max_v: 5871.5\n",
      "fc layer 1 self.abs_max_out: 12142.0\n",
      "fc layer 1 self.abs_max_out: 13429.0\n",
      "lif layer 2 self.abs_max_v: 5960.0\n",
      "lif layer 2 self.abs_max_v: 5980.5\n",
      "lif layer 2 self.abs_max_v: 6675.5\n",
      "lif layer 2 self.abs_max_v: 6676.0\n",
      "lif layer 2 self.abs_max_v: 6787.0\n",
      "lif layer 1 self.abs_max_v: 15039.5\n",
      "lif layer 1 self.abs_max_v: 16016.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.999353/  2.086255, val:  39.17%, val_best:  44.58%, tr:  98.98%, tr_best:  98.98%, epoch time: 57.80 seconds, 0.96 minutes\n",
      "total_backward_count 39160 real_backward_count 8824  22.533%\n",
      "fc layer 1 self.abs_max_out: 13971.0\n",
      "lif layer 1 self.abs_max_v: 16218.5\n",
      "lif layer 1 self.abs_max_v: 17450.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.980375/  2.107941, val:  35.83%, val_best:  44.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 48950 real_backward_count 10621  21.698%\n",
      "lif layer 2 self.abs_max_v: 7033.5\n",
      "fc layer 1 self.abs_max_out: 14438.0\n",
      "lif layer 1 self.abs_max_v: 19134.0\n",
      "lif layer 1 self.abs_max_v: 20793.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.978803/  2.120542, val:  46.25%, val_best:  46.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 57.44 seconds, 0.96 minutes\n",
      "total_backward_count 58740 real_backward_count 12412  21.130%\n",
      "fc layer 3 self.abs_max_out: 953.0\n",
      "fc layer 2 self.abs_max_out: 4972.0\n",
      "fc layer 2 self.abs_max_out: 4998.0\n",
      "fc layer 2 self.abs_max_out: 4999.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.980433/  2.101923, val:  46.25%, val_best:  46.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 58.07 seconds, 0.97 minutes\n",
      "total_backward_count 68530 real_backward_count 14253  20.798%\n",
      "fc layer 2 self.abs_max_out: 5132.0\n",
      "fc layer 2 self.abs_max_out: 5174.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.983098/  2.070493, val:  48.75%, val_best:  48.75%, tr:  99.08%, tr_best:  99.49%, epoch time: 58.97 seconds, 0.98 minutes\n",
      "total_backward_count 78320 real_backward_count 15949  20.364%\n",
      "fc layer 1 self.abs_max_out: 14780.0\n",
      "fc layer 3 self.abs_max_out: 983.0\n",
      "fc layer 3 self.abs_max_out: 1042.0\n",
      "lif layer 1 self.abs_max_v: 22374.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.960403/  2.069847, val:  48.75%, val_best:  48.75%, tr:  99.18%, tr_best:  99.49%, epoch time: 58.06 seconds, 0.97 minutes\n",
      "total_backward_count 88110 real_backward_count 17715  20.106%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.958362/  2.077761, val:  49.58%, val_best:  49.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 57.72 seconds, 0.96 minutes\n",
      "total_backward_count 97900 real_backward_count 19397  19.813%\n",
      "lif layer 2 self.abs_max_v: 7169.0\n",
      "fc layer 2 self.abs_max_out: 5217.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.963915/  2.095754, val:  41.67%, val_best:  49.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 57.67 seconds, 0.96 minutes\n",
      "total_backward_count 107690 real_backward_count 21054  19.551%\n",
      "fc layer 3 self.abs_max_out: 1055.0\n",
      "fc layer 3 self.abs_max_out: 1089.0\n",
      "fc layer 3 self.abs_max_out: 1094.0\n",
      "lif layer 2 self.abs_max_v: 7341.0\n",
      "lif layer 2 self.abs_max_v: 7372.0\n",
      "fc layer 1 self.abs_max_out: 14796.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.964020/  2.058364, val:  46.67%, val_best:  49.58%, tr:  99.49%, tr_best:  99.59%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 117480 real_backward_count 22725  19.344%\n",
      "fc layer 3 self.abs_max_out: 1108.0\n",
      "fc layer 3 self.abs_max_out: 1141.0\n",
      "fc layer 1 self.abs_max_out: 15876.0\n",
      "lif layer 2 self.abs_max_v: 7517.0\n",
      "lif layer 2 self.abs_max_v: 7716.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.946455/  2.075129, val:  45.83%, val_best:  49.58%, tr:  99.18%, tr_best:  99.59%, epoch time: 57.84 seconds, 0.96 minutes\n",
      "total_backward_count 127270 real_backward_count 24380  19.156%\n",
      "fc layer 3 self.abs_max_out: 1155.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.960611/  2.083656, val:  40.83%, val_best:  49.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 57.49 seconds, 0.96 minutes\n",
      "total_backward_count 137060 real_backward_count 25994  18.965%\n",
      "fc layer 3 self.abs_max_out: 1168.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.945141/  2.058783, val:  42.92%, val_best:  49.58%, tr:  98.88%, tr_best:  99.59%, epoch time: 58.74 seconds, 0.98 minutes\n",
      "total_backward_count 146850 real_backward_count 27611  18.802%\n",
      "fc layer 3 self.abs_max_out: 1171.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.944422/  2.064458, val:  52.50%, val_best:  52.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 58.11 seconds, 0.97 minutes\n",
      "total_backward_count 156640 real_backward_count 29260  18.680%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.930399/  2.043339, val:  55.00%, val_best:  55.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 57.51 seconds, 0.96 minutes\n",
      "total_backward_count 166430 real_backward_count 30821  18.519%\n",
      "fc layer 3 self.abs_max_out: 1184.0\n",
      "fc layer 2 self.abs_max_out: 5236.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.943353/  2.043699, val:  62.08%, val_best:  62.08%, tr:  98.98%, tr_best:  99.69%, epoch time: 57.89 seconds, 0.96 minutes\n",
      "total_backward_count 176220 real_backward_count 32447  18.413%\n",
      "fc layer 3 self.abs_max_out: 1197.0\n",
      "lif layer 1 self.abs_max_v: 23205.0\n",
      "fc layer 2 self.abs_max_out: 5401.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.946461/  2.066006, val:  48.75%, val_best:  62.08%, tr:  99.28%, tr_best:  99.69%, epoch time: 56.50 seconds, 0.94 minutes\n",
      "total_backward_count 186010 real_backward_count 34063  18.312%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.936863/  2.053398, val:  41.25%, val_best:  62.08%, tr:  99.39%, tr_best:  99.69%, epoch time: 56.39 seconds, 0.94 minutes\n",
      "total_backward_count 195800 real_backward_count 35625  18.195%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.942154/  2.066306, val:  55.00%, val_best:  62.08%, tr:  99.28%, tr_best:  99.69%, epoch time: 56.23 seconds, 0.94 minutes\n",
      "total_backward_count 205590 real_backward_count 37254  18.121%\n",
      "fc layer 3 self.abs_max_out: 1217.0\n",
      "fc layer 3 self.abs_max_out: 1285.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.944961/  2.063585, val:  46.25%, val_best:  62.08%, tr:  99.59%, tr_best:  99.69%, epoch time: 56.41 seconds, 0.94 minutes\n",
      "total_backward_count 215380 real_backward_count 38908  18.065%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.934356/  2.046644, val:  55.00%, val_best:  62.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 56.20 seconds, 0.94 minutes\n",
      "total_backward_count 225170 real_backward_count 40488  17.981%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.939947/  2.056930, val:  50.42%, val_best:  62.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 56.84 seconds, 0.95 minutes\n",
      "total_backward_count 234960 real_backward_count 42086  17.912%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.954035/  2.047195, val:  49.58%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 56.58 seconds, 0.94 minutes\n",
      "total_backward_count 244750 real_backward_count 43649  17.834%\n",
      "fc layer 2 self.abs_max_out: 5476.0\n",
      "fc layer 2 self.abs_max_out: 5567.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.952259/  2.044482, val:  56.67%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 57.63 seconds, 0.96 minutes\n",
      "total_backward_count 254540 real_backward_count 45362  17.821%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.933414/  2.036964, val:  58.75%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 264330 real_backward_count 46937  17.757%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.919594/  2.053541, val:  63.75%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 57.70 seconds, 0.96 minutes\n",
      "total_backward_count 274120 real_backward_count 48557  17.714%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.918808/  2.038099, val:  55.83%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 57.37 seconds, 0.96 minutes\n",
      "total_backward_count 283910 real_backward_count 50126  17.656%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.927157/  2.050809, val:  46.67%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 57.38 seconds, 0.96 minutes\n",
      "total_backward_count 293700 real_backward_count 51613  17.573%\n",
      "lif layer 1 self.abs_max_v: 23873.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.940506/  2.050280, val:  48.33%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 303490 real_backward_count 53179  17.522%\n",
      "lif layer 2 self.abs_max_v: 7954.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.931293/  2.057478, val:  46.67%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 56.96 seconds, 0.95 minutes\n",
      "total_backward_count 313280 real_backward_count 54769  17.482%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.928807/  2.065084, val:  50.42%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 57.34 seconds, 0.96 minutes\n",
      "total_backward_count 323070 real_backward_count 56263  17.415%\n",
      "fc layer 1 self.abs_max_out: 16038.0\n",
      "fc layer 2 self.abs_max_out: 5625.0\n",
      "fc layer 2 self.abs_max_out: 5877.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.940113/  2.044151, val:  46.67%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 57.64 seconds, 0.96 minutes\n",
      "total_backward_count 332860 real_backward_count 57877  17.388%\n",
      "fc layer 2 self.abs_max_out: 5968.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.934299/  2.051164, val:  46.25%, val_best:  63.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 56.83 seconds, 0.95 minutes\n",
      "total_backward_count 342650 real_backward_count 59462  17.354%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.929903/  2.055025, val:  51.25%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 57.59 seconds, 0.96 minutes\n",
      "total_backward_count 352440 real_backward_count 61073  17.329%\n",
      "fc layer 2 self.abs_max_out: 6072.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.920029/  2.051323, val:  57.08%, val_best:  63.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 57.69 seconds, 0.96 minutes\n",
      "total_backward_count 362230 real_backward_count 62620  17.287%\n",
      "fc layer 2 self.abs_max_out: 6245.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.921244/  2.070666, val:  42.08%, val_best:  63.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 58.18 seconds, 0.97 minutes\n",
      "total_backward_count 372020 real_backward_count 64189  17.254%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.939520/  2.061532, val:  60.83%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 57.44 seconds, 0.96 minutes\n",
      "total_backward_count 381810 real_backward_count 65852  17.247%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.949187/  2.068369, val:  39.17%, val_best:  63.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 56.64 seconds, 0.94 minutes\n",
      "total_backward_count 391600 real_backward_count 67432  17.220%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.946971/  2.047401, val:  62.08%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 56.40 seconds, 0.94 minutes\n",
      "total_backward_count 401390 real_backward_count 69005  17.192%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.950835/  2.070694, val:  49.58%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 56.36 seconds, 0.94 minutes\n",
      "total_backward_count 411180 real_backward_count 70598  17.170%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.950890/  2.054359, val:  51.67%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 56.30 seconds, 0.94 minutes\n",
      "total_backward_count 420970 real_backward_count 72131  17.134%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.960550/  2.044536, val:  47.92%, val_best:  63.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 56.36 seconds, 0.94 minutes\n",
      "total_backward_count 430760 real_backward_count 73697  17.109%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.939098/  2.071784, val:  52.08%, val_best:  63.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 56.45 seconds, 0.94 minutes\n",
      "total_backward_count 440550 real_backward_count 75275  17.087%\n",
      "lif layer 2 self.abs_max_v: 8063.0\n",
      "lif layer 2 self.abs_max_v: 8163.0\n",
      "lif layer 2 self.abs_max_v: 8642.5\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.953156/  2.066081, val:  57.92%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 56.43 seconds, 0.94 minutes\n",
      "total_backward_count 450340 real_backward_count 76850  17.065%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.974828/  2.082858, val:  47.92%, val_best:  63.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 57.05 seconds, 0.95 minutes\n",
      "total_backward_count 460130 real_backward_count 78424  17.044%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.963457/  2.079199, val:  46.25%, val_best:  63.75%, tr:  98.98%, tr_best:  99.80%, epoch time: 57.44 seconds, 0.96 minutes\n",
      "total_backward_count 469920 real_backward_count 79946  17.013%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.962582/  2.048416, val:  53.33%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 57.49 seconds, 0.96 minutes\n",
      "total_backward_count 479710 real_backward_count 81461  16.981%\n",
      "fc layer 1 self.abs_max_out: 16196.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.963923/  2.074500, val:  49.58%, val_best:  63.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 57.50 seconds, 0.96 minutes\n",
      "total_backward_count 489500 real_backward_count 83050  16.966%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.964118/  2.085642, val:  52.50%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 57.77 seconds, 0.96 minutes\n",
      "total_backward_count 499290 real_backward_count 84583  16.941%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.959160/  2.061395, val:  52.92%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 57.12 seconds, 0.95 minutes\n",
      "total_backward_count 509080 real_backward_count 86112  16.915%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.955451/  2.071198, val:  55.83%, val_best:  63.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 57.84 seconds, 0.96 minutes\n",
      "total_backward_count 518870 real_backward_count 87660  16.894%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.965854/  2.052633, val:  47.08%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 57.83 seconds, 0.96 minutes\n",
      "total_backward_count 528660 real_backward_count 89207  16.874%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.968659/  2.090603, val:  61.25%, val_best:  63.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 57.11 seconds, 0.95 minutes\n",
      "total_backward_count 538450 real_backward_count 90736  16.851%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.985411/  2.112461, val:  53.33%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 57.38 seconds, 0.96 minutes\n",
      "total_backward_count 548240 real_backward_count 92340  16.843%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.987230/  2.084936, val:  53.75%, val_best:  63.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 58.08 seconds, 0.97 minutes\n",
      "total_backward_count 558030 real_backward_count 93955  16.837%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.981794/  2.090735, val:  53.33%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 57.44 seconds, 0.96 minutes\n",
      "total_backward_count 567820 real_backward_count 95432  16.807%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.974154/  2.063938, val:  54.17%, val_best:  63.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 57.42 seconds, 0.96 minutes\n",
      "total_backward_count 577610 real_backward_count 96935  16.782%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.977540/  2.074540, val:  56.25%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 58.40 seconds, 0.97 minutes\n",
      "total_backward_count 587400 real_backward_count 98467  16.763%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.985071/  2.088562, val:  50.00%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 57.86 seconds, 0.96 minutes\n",
      "total_backward_count 597190 real_backward_count 100051  16.754%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.985656/  2.074947, val:  55.42%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 56.59 seconds, 0.94 minutes\n",
      "total_backward_count 606980 real_backward_count 101624  16.743%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.995620/  2.093273, val:  54.58%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 56.68 seconds, 0.94 minutes\n",
      "total_backward_count 616770 real_backward_count 103194  16.731%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.996480/  2.095955, val:  52.92%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 55.77 seconds, 0.93 minutes\n",
      "total_backward_count 626560 real_backward_count 104724  16.714%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.976818/  2.076702, val:  49.17%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 55.84 seconds, 0.93 minutes\n",
      "total_backward_count 636350 real_backward_count 106255  16.698%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.981516/  2.107328, val:  51.67%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 55.65 seconds, 0.93 minutes\n",
      "total_backward_count 646140 real_backward_count 107754  16.677%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.999611/  2.090044, val:  57.08%, val_best:  63.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 55.35 seconds, 0.92 minutes\n",
      "total_backward_count 655930 real_backward_count 109289  16.662%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.996246/  2.097595, val:  56.67%, val_best:  63.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 56.35 seconds, 0.94 minutes\n",
      "total_backward_count 665720 real_backward_count 110823  16.647%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.999421/  2.077561, val:  68.33%, val_best:  68.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 56.67 seconds, 0.94 minutes\n",
      "total_backward_count 675510 real_backward_count 112400  16.639%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.991169/  2.108932, val:  47.08%, val_best:  68.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 58.24 seconds, 0.97 minutes\n",
      "total_backward_count 685300 real_backward_count 113875  16.617%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.989915/  2.092170, val:  57.50%, val_best:  68.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 58.33 seconds, 0.97 minutes\n",
      "total_backward_count 695090 real_backward_count 115422  16.605%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.981863/  2.095886, val:  52.08%, val_best:  68.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 57.38 seconds, 0.96 minutes\n",
      "total_backward_count 704880 real_backward_count 116943  16.590%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.982616/  2.110967, val:  51.25%, val_best:  68.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 57.48 seconds, 0.96 minutes\n",
      "total_backward_count 714670 real_backward_count 118508  16.582%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.973585/  2.093796, val:  54.17%, val_best:  68.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 57.57 seconds, 0.96 minutes\n",
      "total_backward_count 724460 real_backward_count 120007  16.565%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.984973/  2.109808, val:  51.25%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.28 seconds, 0.95 minutes\n",
      "total_backward_count 734250 real_backward_count 121525  16.551%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.983854/  2.062339, val:  55.83%, val_best:  68.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 58.06 seconds, 0.97 minutes\n",
      "total_backward_count 744040 real_backward_count 123054  16.539%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.987262/  2.067863, val:  57.50%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.20 seconds, 0.95 minutes\n",
      "total_backward_count 753830 real_backward_count 124606  16.530%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.972502/  2.101612, val:  32.08%, val_best:  68.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.03 seconds, 0.95 minutes\n",
      "total_backward_count 763620 real_backward_count 126105  16.514%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.974404/  2.080624, val:  47.50%, val_best:  68.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 58.03 seconds, 0.97 minutes\n",
      "total_backward_count 773410 real_backward_count 127633  16.503%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.983019/  2.079223, val:  58.33%, val_best:  68.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.23 seconds, 0.95 minutes\n",
      "total_backward_count 783200 real_backward_count 129123  16.487%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.987964/  2.080597, val:  57.08%, val_best:  68.33%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.16 seconds, 0.95 minutes\n",
      "total_backward_count 792990 real_backward_count 130630  16.473%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.973184/  2.101603, val:  35.00%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.95 seconds, 0.97 minutes\n",
      "total_backward_count 802780 real_backward_count 132145  16.461%\n",
      "fc layer 1 self.abs_max_out: 16357.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.980807/  2.078408, val:  50.00%, val_best:  68.33%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.81 seconds, 0.96 minutes\n",
      "total_backward_count 812570 real_backward_count 133820  16.469%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.962417/  2.115863, val:  41.67%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 56.11 seconds, 0.94 minutes\n",
      "total_backward_count 822360 real_backward_count 135375  16.462%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.985538/  2.069725, val:  65.42%, val_best:  68.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.42 seconds, 0.94 minutes\n",
      "total_backward_count 832150 real_backward_count 136925  16.454%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.991125/  2.111414, val:  55.00%, val_best:  68.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 56.86 seconds, 0.95 minutes\n",
      "total_backward_count 841940 real_backward_count 138484  16.448%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.975654/  2.065692, val:  51.25%, val_best:  68.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.19 seconds, 0.94 minutes\n",
      "total_backward_count 851730 real_backward_count 139960  16.432%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.962402/  2.070206, val:  49.58%, val_best:  68.33%, tr:  99.18%, tr_best:  99.90%, epoch time: 55.52 seconds, 0.93 minutes\n",
      "total_backward_count 861520 real_backward_count 141480  16.422%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.964666/  2.048328, val:  58.33%, val_best:  68.33%, tr:  99.08%, tr_best:  99.90%, epoch time: 56.34 seconds, 0.94 minutes\n",
      "total_backward_count 871310 real_backward_count 143012  16.413%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.975811/  2.079697, val:  69.17%, val_best:  69.17%, tr:  98.88%, tr_best:  99.90%, epoch time: 56.85 seconds, 0.95 minutes\n",
      "total_backward_count 881100 real_backward_count 144596  16.411%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.972153/  2.100954, val:  53.33%, val_best:  69.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.96 seconds, 0.97 minutes\n",
      "total_backward_count 890890 real_backward_count 146149  16.405%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.971306/  2.080317, val:  69.58%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 58.50 seconds, 0.98 minutes\n",
      "total_backward_count 900680 real_backward_count 147688  16.397%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.984305/  2.106750, val:  54.58%, val_best:  69.58%, tr:  99.08%, tr_best:  99.90%, epoch time: 57.60 seconds, 0.96 minutes\n",
      "total_backward_count 910470 real_backward_count 149239  16.391%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.984354/  2.090146, val:  50.00%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 58.03 seconds, 0.97 minutes\n",
      "total_backward_count 920260 real_backward_count 150734  16.380%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.993828/  2.088059, val:  58.75%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.87 seconds, 0.96 minutes\n",
      "total_backward_count 930050 real_backward_count 152189  16.364%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.986276/  2.079489, val:  62.50%, val_best:  69.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 57.94 seconds, 0.97 minutes\n",
      "total_backward_count 939840 real_backward_count 153744  16.359%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.998101/  2.096582, val:  60.00%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.24 seconds, 0.95 minutes\n",
      "total_backward_count 949630 real_backward_count 155192  16.342%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.992440/  2.071014, val:  67.08%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.20 seconds, 0.95 minutes\n",
      "total_backward_count 959420 real_backward_count 156631  16.326%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.975609/  2.083481, val:  54.58%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.70 seconds, 0.96 minutes\n",
      "total_backward_count 969210 real_backward_count 158072  16.309%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.963161/  2.048170, val:  58.33%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.51 seconds, 0.94 minutes\n",
      "total_backward_count 979000 real_backward_count 159527  16.295%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.972368/  2.110566, val:  54.58%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.80 seconds, 0.96 minutes\n",
      "total_backward_count 988790 real_backward_count 161050  16.288%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.989742/  2.088516, val:  56.25%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.21 seconds, 0.95 minutes\n",
      "total_backward_count 998580 real_backward_count 162573  16.280%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.986460/  2.104663, val:  42.92%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.14 seconds, 0.95 minutes\n",
      "total_backward_count 1008370 real_backward_count 164062  16.270%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.994414/  2.099664, val:  60.00%, val_best:  69.58%, tr:  99.08%, tr_best:  99.90%, epoch time: 57.19 seconds, 0.95 minutes\n",
      "total_backward_count 1018160 real_backward_count 165586  16.263%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.988667/  2.065843, val:  60.00%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.45 seconds, 0.96 minutes\n",
      "total_backward_count 1027950 real_backward_count 167092  16.255%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.989496/  2.095703, val:  64.58%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.08 seconds, 0.93 minutes\n",
      "total_backward_count 1037740 real_backward_count 168650  16.252%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.995115/  2.134709, val:  36.25%, val_best:  69.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 55.92 seconds, 0.93 minutes\n",
      "total_backward_count 1047530 real_backward_count 170110  16.239%\n",
      "fc layer 1 self.abs_max_out: 16522.0\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.977771/  2.081959, val:  47.08%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.17 seconds, 0.94 minutes\n",
      "total_backward_count 1057320 real_backward_count 171645  16.234%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.977970/  2.096092, val:  49.17%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.91 seconds, 0.95 minutes\n",
      "total_backward_count 1067110 real_backward_count 173201  16.231%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  2.003542/  2.080690, val:  58.75%, val_best:  69.58%, tr:  99.08%, tr_best:  99.90%, epoch time: 55.78 seconds, 0.93 minutes\n",
      "total_backward_count 1076900 real_backward_count 174790  16.231%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  2.000651/  2.097673, val:  55.83%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 55.03 seconds, 0.92 minutes\n",
      "total_backward_count 1086690 real_backward_count 176301  16.224%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.992064/  2.070603, val:  54.58%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.98 seconds, 0.97 minutes\n",
      "total_backward_count 1096480 real_backward_count 177859  16.221%\n",
      "fc layer 1 self.abs_max_out: 16646.0\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.994411/  2.093946, val:  62.50%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 1106270 real_backward_count 179442  16.220%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.989547/  2.105099, val:  50.83%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.58 seconds, 0.96 minutes\n",
      "total_backward_count 1116060 real_backward_count 181000  16.218%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.994677/  2.119057, val:  41.67%, val_best:  69.58%, tr:  99.08%, tr_best:  99.90%, epoch time: 58.41 seconds, 0.97 minutes\n",
      "total_backward_count 1125850 real_backward_count 182546  16.214%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.983252/  2.096161, val:  62.08%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 1135640 real_backward_count 184093  16.211%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.979149/  2.084591, val:  54.17%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.34 seconds, 0.96 minutes\n",
      "total_backward_count 1145430 real_backward_count 185571  16.201%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  2.005824/  2.097597, val:  57.92%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.46 seconds, 0.96 minutes\n",
      "total_backward_count 1155220 real_backward_count 187094  16.196%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.997362/  2.103239, val:  56.25%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.67 seconds, 0.96 minutes\n",
      "total_backward_count 1165010 real_backward_count 188644  16.192%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  2.002302/  2.106652, val:  60.42%, val_best:  69.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 58.02 seconds, 0.97 minutes\n",
      "total_backward_count 1174800 real_backward_count 190177  16.188%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.998167/  2.101298, val:  58.33%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.71 seconds, 0.96 minutes\n",
      "total_backward_count 1184590 real_backward_count 191657  16.179%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.984126/  2.121367, val:  49.17%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 58.31 seconds, 0.97 minutes\n",
      "total_backward_count 1194380 real_backward_count 193146  16.171%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  2.003289/  2.108399, val:  55.83%, val_best:  69.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 57.87 seconds, 0.96 minutes\n",
      "total_backward_count 1204170 real_backward_count 194663  16.166%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.978935/  2.092104, val:  57.08%, val_best:  69.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 58.05 seconds, 0.97 minutes\n",
      "total_backward_count 1213960 real_backward_count 196175  16.160%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.986704/  2.097826, val:  68.75%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 1223750 real_backward_count 197702  16.155%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.979918/  2.067624, val:  62.50%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 1233540 real_backward_count 199190  16.148%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.972228/  2.110711, val:  46.25%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.28 seconds, 0.95 minutes\n",
      "total_backward_count 1243330 real_backward_count 200649  16.138%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.987887/  2.092489, val:  53.33%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 55.66 seconds, 0.93 minutes\n",
      "total_backward_count 1253120 real_backward_count 202141  16.131%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.971097/  2.078776, val:  48.33%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 55.49 seconds, 0.92 minutes\n",
      "total_backward_count 1262910 real_backward_count 203620  16.123%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.969197/  2.097598, val:  57.92%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 55.36 seconds, 0.92 minutes\n",
      "total_backward_count 1272700 real_backward_count 205087  16.114%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.965784/  2.068949, val:  51.25%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 55.93 seconds, 0.93 minutes\n",
      "total_backward_count 1282490 real_backward_count 206600  16.109%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.968075/  2.063232, val:  61.25%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 56.51 seconds, 0.94 minutes\n",
      "total_backward_count 1292280 real_backward_count 208208  16.112%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.964717/  2.103858, val:  51.67%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 56.09 seconds, 0.93 minutes\n",
      "total_backward_count 1302070 real_backward_count 209655  16.102%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.986391/  2.077504, val:  58.75%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.30 seconds, 0.96 minutes\n",
      "total_backward_count 1311860 real_backward_count 211129  16.094%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.976832/  2.103305, val:  45.00%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 58.85 seconds, 0.98 minutes\n",
      "total_backward_count 1321650 real_backward_count 212640  16.089%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.967178/  2.060442, val:  57.08%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.54 seconds, 0.96 minutes\n",
      "total_backward_count 1331440 real_backward_count 214166  16.085%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.946613/  2.064177, val:  60.83%, val_best:  69.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 58.01 seconds, 0.97 minutes\n",
      "total_backward_count 1341230 real_backward_count 215651  16.079%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.947965/  2.056103, val:  60.00%, val_best:  69.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 58.13 seconds, 0.97 minutes\n",
      "total_backward_count 1351020 real_backward_count 217081  16.068%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.952741/  2.056283, val:  62.08%, val_best:  69.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.41 seconds, 0.96 minutes\n",
      "total_backward_count 1360810 real_backward_count 218599  16.064%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.945218/  2.093755, val:  58.33%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.54 seconds, 0.96 minutes\n",
      "total_backward_count 1370600 real_backward_count 220136  16.061%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.961776/  2.066596, val:  58.33%, val_best:  69.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 58.52 seconds, 0.98 minutes\n",
      "total_backward_count 1380390 real_backward_count 221643  16.057%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.960033/  2.098850, val:  50.00%, val_best:  69.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 57.91 seconds, 0.97 minutes\n",
      "total_backward_count 1390180 real_backward_count 223129  16.050%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.951694/  2.071609, val:  67.08%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.36 seconds, 0.96 minutes\n",
      "total_backward_count 1399970 real_backward_count 224609  16.044%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.947417/  2.063273, val:  55.00%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.67 seconds, 0.96 minutes\n",
      "total_backward_count 1409760 real_backward_count 226041  16.034%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.968723/  2.078445, val:  57.08%, val_best:  69.58%, tr:  98.98%, tr_best:  99.90%, epoch time: 58.62 seconds, 0.98 minutes\n",
      "total_backward_count 1419550 real_backward_count 227590  16.033%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.965236/  2.093225, val:  38.33%, val_best:  69.58%, tr:  98.98%, tr_best:  99.90%, epoch time: 57.42 seconds, 0.96 minutes\n",
      "total_backward_count 1429340 real_backward_count 229061  16.026%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.958377/  2.079796, val:  48.75%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 58.40 seconds, 0.97 minutes\n",
      "total_backward_count 1439130 real_backward_count 230579  16.022%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.966010/  2.084511, val:  41.25%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 58.37 seconds, 0.97 minutes\n",
      "total_backward_count 1448920 real_backward_count 232035  16.014%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.947599/  2.050307, val:  58.75%, val_best:  69.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 56.61 seconds, 0.94 minutes\n",
      "total_backward_count 1458710 real_backward_count 233483  16.006%\n",
      "lif layer 1 self.abs_max_v: 23925.5\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.952567/  2.055238, val:  59.58%, val_best:  69.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 56.94 seconds, 0.95 minutes\n",
      "total_backward_count 1468500 real_backward_count 234998  16.003%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.970402/  2.086049, val:  51.25%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.41 seconds, 0.94 minutes\n",
      "total_backward_count 1478290 real_backward_count 236440  15.994%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.970915/  2.063048, val:  58.33%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.09 seconds, 0.93 minutes\n",
      "total_backward_count 1488080 real_backward_count 237911  15.988%\n",
      "fc layer 1 self.abs_max_out: 16651.0\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.951256/  2.068315, val:  59.58%, val_best:  69.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 55.72 seconds, 0.93 minutes\n",
      "total_backward_count 1497870 real_backward_count 239402  15.983%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.971303/  2.066312, val:  59.58%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.51 seconds, 0.94 minutes\n",
      "total_backward_count 1507660 real_backward_count 240972  15.983%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.973171/  2.069319, val:  71.25%, val_best:  71.25%, tr:  99.18%, tr_best:  99.90%, epoch time: 56.93 seconds, 0.95 minutes\n",
      "total_backward_count 1517450 real_backward_count 242481  15.980%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.974748/  2.100792, val:  60.00%, val_best:  71.25%, tr:  98.88%, tr_best:  99.90%, epoch time: 56.98 seconds, 0.95 minutes\n",
      "total_backward_count 1527240 real_backward_count 243964  15.974%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.968763/  2.077789, val:  54.58%, val_best:  71.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.68 seconds, 0.96 minutes\n",
      "total_backward_count 1537030 real_backward_count 245432  15.968%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.966946/  2.083020, val:  65.00%, val_best:  71.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.66 seconds, 0.96 minutes\n",
      "total_backward_count 1546820 real_backward_count 246905  15.962%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.983278/  2.114915, val:  54.17%, val_best:  71.25%, tr:  98.77%, tr_best:  99.90%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 1556610 real_backward_count 248442  15.960%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.959653/  2.066333, val:  41.67%, val_best:  71.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.12 seconds, 0.95 minutes\n",
      "total_backward_count 1566400 real_backward_count 249973  15.958%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.949833/  2.062380, val:  66.67%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 1576190 real_backward_count 251397  15.950%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.955390/  2.072688, val:  57.50%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.99 seconds, 0.95 minutes\n",
      "total_backward_count 1585980 real_backward_count 252884  15.945%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.973910/  2.103303, val:  48.33%, val_best:  71.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.49 seconds, 0.96 minutes\n",
      "total_backward_count 1595770 real_backward_count 254334  15.938%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.966071/  2.099416, val:  42.50%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 58.24 seconds, 0.97 minutes\n",
      "total_backward_count 1605560 real_backward_count 255770  15.930%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.963273/  2.086714, val:  60.42%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 58.01 seconds, 0.97 minutes\n",
      "total_backward_count 1615350 real_backward_count 257267  15.926%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.957756/  2.071491, val:  60.00%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.91 seconds, 0.97 minutes\n",
      "total_backward_count 1625140 real_backward_count 258725  15.920%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.950608/  2.084859, val:  58.33%, val_best:  71.25%, tr:  98.98%, tr_best:  99.90%, epoch time: 58.61 seconds, 0.98 minutes\n",
      "total_backward_count 1634930 real_backward_count 260183  15.914%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.953704/  2.082180, val:  50.83%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 58.37 seconds, 0.97 minutes\n",
      "total_backward_count 1644720 real_backward_count 261753  15.915%\n",
      "fc layer 1 self.abs_max_out: 16729.0\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.959697/  2.069154, val:  42.50%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 58.01 seconds, 0.97 minutes\n",
      "total_backward_count 1654510 real_backward_count 263196  15.908%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.937041/  2.076142, val:  54.17%, val_best:  71.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.41 seconds, 0.96 minutes\n",
      "total_backward_count 1664300 real_backward_count 264565  15.896%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.976968/  2.087748, val:  47.08%, val_best:  71.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 56.04 seconds, 0.93 minutes\n",
      "total_backward_count 1674090 real_backward_count 266044  15.892%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.947358/  2.062083, val:  54.58%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.48 seconds, 0.94 minutes\n",
      "total_backward_count 1683880 real_backward_count 267469  15.884%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.943856/  2.059430, val:  67.92%, val_best:  71.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 55.84 seconds, 0.93 minutes\n",
      "total_backward_count 1693670 real_backward_count 268930  15.879%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.958447/  2.063626, val:  57.08%, val_best:  71.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 56.44 seconds, 0.94 minutes\n",
      "total_backward_count 1703460 real_backward_count 270344  15.870%\n",
      "lif layer 1 self.abs_max_v: 24122.5\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.951984/  2.036350, val:  63.75%, val_best:  71.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 56.69 seconds, 0.94 minutes\n",
      "total_backward_count 1713250 real_backward_count 271795  15.864%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.959196/  2.090757, val:  70.00%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.06 seconds, 0.93 minutes\n",
      "total_backward_count 1723040 real_backward_count 273249  15.859%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.953159/  2.069707, val:  48.33%, val_best:  71.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 57.70 seconds, 0.96 minutes\n",
      "total_backward_count 1732830 real_backward_count 274717  15.854%\n",
      "lif layer 1 self.abs_max_v: 24324.0\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.941003/  2.053488, val:  61.67%, val_best:  71.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 57.33 seconds, 0.96 minutes\n",
      "total_backward_count 1742620 real_backward_count 276135  15.846%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.919916/  2.073437, val:  52.50%, val_best:  71.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 57.65 seconds, 0.96 minutes\n",
      "total_backward_count 1752410 real_backward_count 277593  15.841%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.937739/  2.075168, val:  70.42%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 58.05 seconds, 0.97 minutes\n",
      "total_backward_count 1762200 real_backward_count 279054  15.836%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.953773/  2.074126, val:  65.00%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.54 seconds, 0.96 minutes\n",
      "total_backward_count 1771990 real_backward_count 280520  15.831%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.951997/  2.060951, val:  58.75%, val_best:  71.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 58.09 seconds, 0.97 minutes\n",
      "total_backward_count 1781780 real_backward_count 281945  15.824%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.965544/  2.098276, val:  62.92%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.41 seconds, 0.96 minutes\n",
      "total_backward_count 1791570 real_backward_count 283403  15.819%\n",
      "lif layer 1 self.abs_max_v: 24568.5\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.964675/  2.088939, val:  56.25%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.60 seconds, 0.96 minutes\n",
      "total_backward_count 1801360 real_backward_count 284882  15.815%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.970447/  2.066371, val:  63.75%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.79 seconds, 0.95 minutes\n",
      "total_backward_count 1811150 real_backward_count 286324  15.809%\n",
      "fc layer 1 self.abs_max_out: 16771.0\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.965065/  2.080298, val:  53.75%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 58.30 seconds, 0.97 minutes\n",
      "total_backward_count 1820940 real_backward_count 287753  15.802%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.960949/  2.045055, val:  66.67%, val_best:  71.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 1830730 real_backward_count 289229  15.799%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.948777/  2.074646, val:  50.42%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 58.77 seconds, 0.98 minutes\n",
      "total_backward_count 1840520 real_backward_count 290746  15.797%\n",
      "fc layer 1 self.abs_max_out: 16792.0\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.971399/  2.071565, val:  68.33%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 58.74 seconds, 0.98 minutes\n",
      "total_backward_count 1850310 real_backward_count 292225  15.793%\n",
      "fc layer 1 self.abs_max_out: 16987.0\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.961084/  2.067353, val:  51.25%, val_best:  71.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 58.49 seconds, 0.97 minutes\n",
      "total_backward_count 1860100 real_backward_count 293656  15.787%\n",
      "lif layer 1 self.abs_max_v: 24801.0\n",
      "lif layer 1 self.abs_max_v: 24840.5\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.956052/  2.055476, val:  60.83%, val_best:  71.25%, tr:  98.98%, tr_best:  99.90%, epoch time: 57.83 seconds, 0.96 minutes\n",
      "total_backward_count 1869890 real_backward_count 295106  15.782%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.949375/  2.062464, val:  50.83%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 55.95 seconds, 0.93 minutes\n",
      "total_backward_count 1879680 real_backward_count 296579  15.778%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.971373/  2.093755, val:  58.33%, val_best:  71.25%, tr:  98.98%, tr_best:  99.90%, epoch time: 55.91 seconds, 0.93 minutes\n",
      "total_backward_count 1889470 real_backward_count 298078  15.776%\n",
      "lif layer 1 self.abs_max_v: 25292.0\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.953714/  2.066036, val:  55.00%, val_best:  71.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 56.33 seconds, 0.94 minutes\n",
      "total_backward_count 1899260 real_backward_count 299476  15.768%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.951190/  2.066528, val:  51.25%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.81 seconds, 0.95 minutes\n",
      "total_backward_count 1909050 real_backward_count 300868  15.760%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.958307/  2.055606, val:  59.58%, val_best:  71.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 55.78 seconds, 0.93 minutes\n",
      "total_backward_count 1918840 real_backward_count 302291  15.754%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.945043/  2.069845, val:  55.00%, val_best:  71.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 56.93 seconds, 0.95 minutes\n",
      "total_backward_count 1928630 real_backward_count 303746  15.749%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.969222/  2.098699, val:  57.08%, val_best:  71.25%, tr:  99.18%, tr_best:  99.90%, epoch time: 57.54 seconds, 0.96 minutes\n",
      "total_backward_count 1938420 real_backward_count 305269  15.748%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.959805/  2.086790, val:  57.50%, val_best:  71.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.53 seconds, 0.96 minutes\n",
      "total_backward_count 1948210 real_backward_count 306639  15.740%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.975156/  2.075131, val:  68.33%, val_best:  71.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.84 seconds, 0.96 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4612d866094596b62fc3dc24bc542a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99489</td></tr><tr><td>tr_epoch_loss</td><td>1.97516</td></tr><tr><td>val_acc_best</td><td>0.7125</td></tr><tr><td>val_acc_now</td><td>0.68333</td></tr><tr><td>val_loss</td><td>2.07513</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-sweep-240</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z0b0o2e5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z0b0o2e5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251029_050640-z0b0o2e5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4h34xrqv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251029_081821-4h34xrqv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4h34xrqv' target=\"_blank\">giddy-sweep-246</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4h34xrqv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4h34xrqv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251029_081830_020', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 188.0\n",
      "lif layer 1 self.abs_max_v: 188.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 279.0\n",
      "lif layer 2 self.abs_max_v: 279.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 1 self.abs_max_out: 204.0\n",
      "lif layer 1 self.abs_max_v: 235.0\n",
      "fc layer 2 self.abs_max_out: 518.0\n",
      "lif layer 2 self.abs_max_v: 626.5\n",
      "fc layer 1 self.abs_max_out: 209.0\n",
      "lif layer 1 self.abs_max_v: 279.0\n",
      "lif layer 2 self.abs_max_v: 757.5\n",
      "fc layer 1 self.abs_max_out: 237.0\n",
      "lif layer 1 self.abs_max_v: 338.5\n",
      "fc layer 1 self.abs_max_out: 403.0\n",
      "lif layer 1 self.abs_max_v: 403.0\n",
      "lif layer 2 self.abs_max_v: 785.5\n",
      "lif layer 1 self.abs_max_v: 405.5\n",
      "fc layer 1 self.abs_max_out: 469.0\n",
      "lif layer 1 self.abs_max_v: 508.5\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "lif layer 2 self.abs_max_v: 818.0\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "fc layer 3 self.abs_max_out: 297.0\n",
      "fc layer 2 self.abs_max_out: 589.0\n",
      "lif layer 2 self.abs_max_v: 839.5\n",
      "fc layer 1 self.abs_max_out: 507.0\n",
      "lif layer 1 self.abs_max_v: 557.5\n",
      "fc layer 1 self.abs_max_out: 554.0\n",
      "lif layer 1 self.abs_max_v: 565.0\n",
      "fc layer 1 self.abs_max_out: 576.0\n",
      "lif layer 1 self.abs_max_v: 615.5\n",
      "fc layer 2 self.abs_max_out: 694.0\n",
      "fc layer 1 self.abs_max_out: 589.0\n",
      "fc layer 1 self.abs_max_out: 620.0\n",
      "lif layer 1 self.abs_max_v: 620.0\n",
      "fc layer 1 self.abs_max_out: 645.0\n",
      "lif layer 1 self.abs_max_v: 665.0\n",
      "fc layer 1 self.abs_max_out: 826.0\n",
      "lif layer 1 self.abs_max_v: 826.0\n",
      "fc layer 1 self.abs_max_out: 1105.0\n",
      "lif layer 1 self.abs_max_v: 1105.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 903.0\n",
      "fc layer 2 self.abs_max_out: 753.0\n",
      "lif layer 2 self.abs_max_v: 981.5\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "lif layer 1 self.abs_max_v: 1163.5\n",
      "lif layer 1 self.abs_max_v: 1167.0\n",
      "lif layer 2 self.abs_max_v: 1019.5\n",
      "lif layer 2 self.abs_max_v: 1199.0\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "lif layer 1 self.abs_max_v: 1275.5\n",
      "lif layer 1 self.abs_max_v: 1351.5\n",
      "lif layer 1 self.abs_max_v: 1516.0\n",
      "lif layer 1 self.abs_max_v: 1642.5\n",
      "lif layer 2 self.abs_max_v: 1230.0\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "fc layer 1 self.abs_max_out: 1148.0\n",
      "fc layer 1 self.abs_max_out: 1164.0\n",
      "fc layer 1 self.abs_max_out: 1186.0\n",
      "fc layer 1 self.abs_max_out: 1226.0\n",
      "lif layer 1 self.abs_max_v: 1655.5\n",
      "lif layer 2 self.abs_max_v: 1248.0\n",
      "fc layer 2 self.abs_max_out: 861.0\n",
      "lif layer 2 self.abs_max_v: 1261.0\n",
      "lif layer 2 self.abs_max_v: 1303.0\n",
      "fc layer 2 self.abs_max_out: 913.0\n",
      "fc layer 2 self.abs_max_out: 919.0\n",
      "fc layer 1 self.abs_max_out: 1311.0\n",
      "lif layer 1 self.abs_max_v: 2132.5\n",
      "lif layer 2 self.abs_max_v: 1329.5\n",
      "lif layer 2 self.abs_max_v: 1345.5\n",
      "fc layer 2 self.abs_max_out: 986.0\n",
      "lif layer 2 self.abs_max_v: 1397.0\n",
      "fc layer 2 self.abs_max_out: 989.0\n",
      "lif layer 2 self.abs_max_v: 1409.5\n",
      "lif layer 2 self.abs_max_v: 1451.5\n",
      "lif layer 2 self.abs_max_v: 1492.0\n",
      "lif layer 2 self.abs_max_v: 1501.0\n",
      "lif layer 2 self.abs_max_v: 1512.5\n",
      "fc layer 2 self.abs_max_out: 1068.0\n",
      "lif layer 2 self.abs_max_v: 1565.5\n",
      "lif layer 2 self.abs_max_v: 1589.0\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "fc layer 1 self.abs_max_out: 1322.0\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "lif layer 2 self.abs_max_v: 1594.0\n",
      "lif layer 2 self.abs_max_v: 1676.0\n",
      "fc layer 1 self.abs_max_out: 1674.0\n",
      "fc layer 1 self.abs_max_out: 1901.0\n",
      "lif layer 1 self.abs_max_v: 2221.0\n",
      "lif layer 1 self.abs_max_v: 2224.0\n",
      "lif layer 2 self.abs_max_v: 1686.0\n",
      "lif layer 2 self.abs_max_v: 1731.0\n",
      "lif layer 2 self.abs_max_v: 1766.5\n",
      "lif layer 2 self.abs_max_v: 1778.5\n",
      "lif layer 2 self.abs_max_v: 1867.5\n",
      "fc layer 2 self.abs_max_out: 1126.0\n",
      "lif layer 2 self.abs_max_v: 1902.0\n",
      "lif layer 2 self.abs_max_v: 2004.5\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "fc layer 2 self.abs_max_out: 1131.0\n",
      "lif layer 2 self.abs_max_v: 2049.0\n",
      "lif layer 1 self.abs_max_v: 2282.5\n",
      "lif layer 1 self.abs_max_v: 2306.0\n",
      "lif layer 1 self.abs_max_v: 2308.0\n",
      "lif layer 1 self.abs_max_v: 2422.0\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "lif layer 1 self.abs_max_v: 2503.5\n",
      "lif layer 1 self.abs_max_v: 2596.5\n",
      "lif layer 1 self.abs_max_v: 2711.0\n",
      "lif layer 1 self.abs_max_v: 2713.0\n",
      "lif layer 1 self.abs_max_v: 2738.0\n",
      "lif layer 1 self.abs_max_v: 2867.0\n",
      "lif layer 1 self.abs_max_v: 2974.0\n",
      "lif layer 1 self.abs_max_v: 3100.5\n",
      "lif layer 1 self.abs_max_v: 3181.5\n",
      "lif layer 1 self.abs_max_v: 3182.0\n",
      "lif layer 2 self.abs_max_v: 2050.0\n",
      "fc layer 3 self.abs_max_out: 545.0\n",
      "fc layer 3 self.abs_max_out: 558.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "fc layer 3 self.abs_max_out: 569.0\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "fc layer 1 self.abs_max_out: 1927.0\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "fc layer 1 self.abs_max_out: 2266.0\n",
      "lif layer 1 self.abs_max_v: 3305.5\n",
      "fc layer 1 self.abs_max_out: 2392.0\n",
      "lif layer 1 self.abs_max_v: 3339.0\n",
      "lif layer 1 self.abs_max_v: 3467.5\n",
      "fc layer 2 self.abs_max_out: 1150.0\n",
      "lif layer 2 self.abs_max_v: 2082.0\n",
      "lif layer 2 self.abs_max_v: 2098.0\n",
      "fc layer 2 self.abs_max_out: 1161.0\n",
      "lif layer 2 self.abs_max_v: 2159.5\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "fc layer 2 self.abs_max_out: 1211.0\n",
      "fc layer 2 self.abs_max_out: 1290.0\n",
      "lif layer 2 self.abs_max_v: 2205.5\n",
      "lif layer 1 self.abs_max_v: 3504.5\n",
      "lif layer 2 self.abs_max_v: 2287.5\n",
      "lif layer 2 self.abs_max_v: 2365.0\n",
      "fc layer 2 self.abs_max_out: 1337.0\n",
      "fc layer 3 self.abs_max_out: 637.0\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "fc layer 2 self.abs_max_out: 1340.0\n",
      "lif layer 2 self.abs_max_v: 2474.0\n",
      "lif layer 2 self.abs_max_v: 2576.0\n",
      "lif layer 2 self.abs_max_v: 2595.0\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "fc layer 2 self.abs_max_out: 1377.0\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "lif layer 2 self.abs_max_v: 2609.5\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "lif layer 1 self.abs_max_v: 3533.0\n",
      "lif layer 1 self.abs_max_v: 3570.5\n",
      "lif layer 1 self.abs_max_v: 3691.0\n",
      "lif layer 1 self.abs_max_v: 3834.0\n",
      "fc layer 2 self.abs_max_out: 1468.0\n",
      "lif layer 2 self.abs_max_v: 2712.5\n",
      "lif layer 2 self.abs_max_v: 2727.0\n",
      "fc layer 2 self.abs_max_out: 1530.0\n",
      "fc layer 1 self.abs_max_out: 2476.0\n",
      "lif layer 1 self.abs_max_v: 4106.0\n",
      "lif layer 1 self.abs_max_v: 4264.0\n",
      "lif layer 1 self.abs_max_v: 4337.5\n",
      "lif layer 1 self.abs_max_v: 4585.0\n",
      "lif layer 1 self.abs_max_v: 4747.5\n",
      "lif layer 1 self.abs_max_v: 4809.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.396704/  1.770218, val:  38.75%, val_best:  38.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 58.29 seconds, 0.97 minutes\n",
      "total_backward_count 9790 real_backward_count 1311  13.391%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2497.0\n",
      "fc layer 2 self.abs_max_out: 1555.0\n",
      "fc layer 3 self.abs_max_out: 735.0\n",
      "fc layer 3 self.abs_max_out: 755.0\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "lif layer 2 self.abs_max_v: 2833.5\n",
      "lif layer 2 self.abs_max_v: 2886.0\n",
      "fc layer 1 self.abs_max_out: 2599.0\n",
      "fc layer 2 self.abs_max_out: 1597.0\n",
      "lif layer 2 self.abs_max_v: 2969.0\n",
      "lif layer 2 self.abs_max_v: 2974.5\n",
      "fc layer 1 self.abs_max_out: 2675.0\n",
      "fc layer 2 self.abs_max_out: 1614.0\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "lif layer 2 self.abs_max_v: 3086.5\n",
      "lif layer 2 self.abs_max_v: 3101.5\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "fc layer 1 self.abs_max_out: 2983.0\n",
      "lif layer 1 self.abs_max_v: 5017.5\n",
      "lif layer 1 self.abs_max_v: 5305.0\n",
      "lif layer 1 self.abs_max_v: 5326.5\n",
      "lif layer 1 self.abs_max_v: 5631.5\n",
      "fc layer 1 self.abs_max_out: 3039.0\n",
      "lif layer 1 self.abs_max_v: 5756.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.245529/  1.819206, val:  36.25%, val_best:  38.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 57.97 seconds, 0.97 minutes\n",
      "total_backward_count 19580 real_backward_count 2510  12.819%\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "lif layer 2 self.abs_max_v: 3154.5\n",
      "fc layer 1 self.abs_max_out: 3392.0\n",
      "fc layer 2 self.abs_max_out: 1681.0\n",
      "lif layer 2 self.abs_max_v: 3255.0\n",
      "lif layer 2 self.abs_max_v: 3278.5\n",
      "fc layer 2 self.abs_max_out: 1718.0\n",
      "lif layer 2 self.abs_max_v: 3357.5\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "fc layer 2 self.abs_max_out: 1803.0\n",
      "fc layer 2 self.abs_max_out: 1809.0\n",
      "fc layer 2 self.abs_max_out: 1832.0\n",
      "lif layer 2 self.abs_max_v: 3371.5\n",
      "fc layer 2 self.abs_max_out: 1863.0\n",
      "lif layer 2 self.abs_max_v: 3460.5\n",
      "fc layer 3 self.abs_max_out: 867.0\n",
      "fc layer 3 self.abs_max_out: 894.0\n",
      "fc layer 2 self.abs_max_out: 1930.0\n",
      "lif layer 2 self.abs_max_v: 3491.0\n",
      "lif layer 2 self.abs_max_v: 3577.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "lif layer 1 self.abs_max_v: 5960.5\n",
      "lif layer 1 self.abs_max_v: 6040.0\n",
      "lif layer 1 self.abs_max_v: 6363.0\n",
      "lif layer 1 self.abs_max_v: 6455.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.205531/  1.699991, val:  38.75%, val_best:  38.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 29370 real_backward_count 3684  12.543%\n",
      "fc layer 2 self.abs_max_out: 1967.0\n",
      "fc layer 2 self.abs_max_out: 2216.0\n",
      "lif layer 2 self.abs_max_v: 3707.0\n",
      "lif layer 2 self.abs_max_v: 3873.5\n",
      "fc layer 1 self.abs_max_out: 3450.0\n",
      "lif layer 1 self.abs_max_v: 6509.5\n",
      "fc layer 1 self.abs_max_out: 3616.0\n",
      "lif layer 1 self.abs_max_v: 6871.0\n",
      "fc layer 1 self.abs_max_out: 3670.0\n",
      "fc layer 1 self.abs_max_out: 3708.0\n",
      "fc layer 1 self.abs_max_out: 3786.0\n",
      "lif layer 1 self.abs_max_v: 7091.5\n",
      "lif layer 1 self.abs_max_v: 7247.0\n",
      "fc layer 1 self.abs_max_out: 3885.0\n",
      "lif layer 1 self.abs_max_v: 7508.5\n",
      "lif layer 2 self.abs_max_v: 4035.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.178955/  1.699908, val:  39.58%, val_best:  39.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 57.69 seconds, 0.96 minutes\n",
      "total_backward_count 39160 real_backward_count 4877  12.454%\n",
      "fc layer 1 self.abs_max_out: 3887.0\n",
      "fc layer 1 self.abs_max_out: 4002.0\n",
      "fc layer 1 self.abs_max_out: 4071.0\n",
      "fc layer 1 self.abs_max_out: 4322.0\n",
      "lif layer 1 self.abs_max_v: 7544.5\n",
      "lif layer 1 self.abs_max_v: 7600.5\n",
      "lif layer 1 self.abs_max_v: 7968.5\n",
      "fc layer 1 self.abs_max_out: 4514.0\n",
      "lif layer 1 self.abs_max_v: 8498.5\n",
      "lif layer 1 self.abs_max_v: 8663.5\n",
      "fc layer 1 self.abs_max_out: 4604.0\n",
      "lif layer 1 self.abs_max_v: 8936.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.168030/  1.607749, val:  52.08%, val_best:  52.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 58.06 seconds, 0.97 minutes\n",
      "total_backward_count 48950 real_backward_count 6058  12.376%\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.142824/  1.558771, val:  56.67%, val_best:  56.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 58740 real_backward_count 7266  12.370%\n",
      "fc layer 2 self.abs_max_out: 2320.0\n",
      "fc layer 3 self.abs_max_out: 916.0\n",
      "fc layer 3 self.abs_max_out: 922.0\n",
      "fc layer 3 self.abs_max_out: 939.0\n",
      "fc layer 3 self.abs_max_out: 970.0\n",
      "fc layer 1 self.abs_max_out: 4625.0\n",
      "fc layer 1 self.abs_max_out: 4676.0\n",
      "lif layer 1 self.abs_max_v: 9083.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.112813/  1.564069, val:  50.00%, val_best:  56.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 58.12 seconds, 0.97 minutes\n",
      "total_backward_count 68530 real_backward_count 8434  12.307%\n",
      "fc layer 3 self.abs_max_out: 1028.0\n",
      "fc layer 2 self.abs_max_out: 2338.0\n",
      "fc layer 2 self.abs_max_out: 2418.0\n",
      "fc layer 2 self.abs_max_out: 2561.0\n",
      "lif layer 2 self.abs_max_v: 4127.0\n",
      "lif layer 2 self.abs_max_v: 4329.5\n",
      "fc layer 1 self.abs_max_out: 4686.0\n",
      "fc layer 1 self.abs_max_out: 4738.0\n",
      "lif layer 1 self.abs_max_v: 9218.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.091040/  1.556604, val:  49.17%, val_best:  56.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 58.48 seconds, 0.97 minutes\n",
      "total_backward_count 78320 real_backward_count 9561  12.208%\n",
      "fc layer 1 self.abs_max_out: 4832.0\n",
      "lif layer 1 self.abs_max_v: 9392.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.097472/  1.544483, val:  52.50%, val_best:  56.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 57.06 seconds, 0.95 minutes\n",
      "total_backward_count 88110 real_backward_count 10706  12.151%\n",
      "fc layer 1 self.abs_max_out: 4860.0\n",
      "fc layer 1 self.abs_max_out: 4863.0\n",
      "fc layer 1 self.abs_max_out: 4874.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.057603/  1.551850, val:  49.17%, val_best:  56.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 57.43 seconds, 0.96 minutes\n",
      "total_backward_count 97900 real_backward_count 11809  12.062%\n",
      "lif layer 2 self.abs_max_v: 4423.5\n",
      "lif layer 2 self.abs_max_v: 4563.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.024558/  1.544765, val:  45.83%, val_best:  56.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 57.78 seconds, 0.96 minutes\n",
      "total_backward_count 107690 real_backward_count 12890  11.970%\n",
      "lif layer 2 self.abs_max_v: 4566.5\n",
      "fc layer 1 self.abs_max_out: 4970.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.029211/  1.455982, val:  57.50%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 117480 real_backward_count 13984  11.903%\n",
      "fc layer 1 self.abs_max_out: 5039.0\n",
      "lif layer 1 self.abs_max_v: 9704.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.987548/  1.414950, val:  50.42%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 56.66 seconds, 0.94 minutes\n",
      "total_backward_count 127270 real_backward_count 14989  11.777%\n",
      "fc layer 2 self.abs_max_out: 2633.0\n",
      "fc layer 3 self.abs_max_out: 1039.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.984557/  1.511202, val:  45.42%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 57.07 seconds, 0.95 minutes\n",
      "total_backward_count 137060 real_backward_count 16013  11.683%\n",
      "fc layer 1 self.abs_max_out: 5104.0\n",
      "fc layer 1 self.abs_max_out: 5220.0\n",
      "fc layer 1 self.abs_max_out: 5389.0\n",
      "lif layer 1 self.abs_max_v: 9969.5\n",
      "fc layer 1 self.abs_max_out: 5496.0\n",
      "lif layer 1 self.abs_max_v: 10481.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.990012/  1.440857, val:  61.67%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 56.24 seconds, 0.94 minutes\n",
      "total_backward_count 146850 real_backward_count 17001  11.577%\n",
      "fc layer 1 self.abs_max_out: 5528.0\n",
      "fc layer 3 self.abs_max_out: 1064.0\n",
      "fc layer 3 self.abs_max_out: 1070.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.986641/  1.427523, val:  52.50%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 56.70 seconds, 0.94 minutes\n",
      "total_backward_count 156640 real_backward_count 18012  11.499%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.943297/  1.406037, val:  55.42%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 56.30 seconds, 0.94 minutes\n",
      "total_backward_count 166430 real_backward_count 19016  11.426%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.945532/  1.369876, val:  65.00%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.87 seconds, 0.95 minutes\n",
      "total_backward_count 176220 real_backward_count 20034  11.369%\n",
      "fc layer 1 self.abs_max_out: 5590.0\n",
      "fc layer 1 self.abs_max_out: 5845.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.947650/  1.425852, val:  42.92%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.36 seconds, 0.96 minutes\n",
      "total_backward_count 186010 real_backward_count 21063  11.324%\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "lif layer 2 self.abs_max_v: 4650.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.883112/  1.373936, val:  50.42%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 195800 real_backward_count 22041  11.257%\n",
      "fc layer 2 self.abs_max_out: 2661.0\n",
      "fc layer 2 self.abs_max_out: 2691.0\n",
      "fc layer 1 self.abs_max_out: 5870.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.905874/  1.401982, val:  55.42%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 59.21 seconds, 0.99 minutes\n",
      "total_backward_count 205590 real_backward_count 22966  11.171%\n",
      "fc layer 1 self.abs_max_out: 5990.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.909039/  1.373621, val:  52.50%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.66 seconds, 0.96 minutes\n",
      "total_backward_count 215380 real_backward_count 23989  11.138%\n",
      "lif layer 2 self.abs_max_v: 4743.0\n",
      "fc layer 1 self.abs_max_out: 6010.0\n",
      "fc layer 2 self.abs_max_out: 2717.0\n",
      "fc layer 3 self.abs_max_out: 1081.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.882032/  1.357068, val:  56.67%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.90 seconds, 0.97 minutes\n",
      "total_backward_count 225170 real_backward_count 24934  11.073%\n",
      "fc layer 3 self.abs_max_out: 1108.0\n",
      "fc layer 3 self.abs_max_out: 1126.0\n",
      "fc layer 2 self.abs_max_out: 2827.0\n",
      "lif layer 2 self.abs_max_v: 4766.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.869455/  1.320823, val:  59.58%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.25 seconds, 0.95 minutes\n",
      "total_backward_count 234960 real_backward_count 25889  11.018%\n",
      "fc layer 3 self.abs_max_out: 1180.0\n",
      "fc layer 3 self.abs_max_out: 1200.0\n",
      "fc layer 3 self.abs_max_out: 1202.0\n",
      "fc layer 3 self.abs_max_out: 1240.0\n",
      "fc layer 3 self.abs_max_out: 1250.0\n",
      "lif layer 2 self.abs_max_v: 4780.0\n",
      "lif layer 1 self.abs_max_v: 10938.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.851398/  1.283153, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.49 seconds, 0.96 minutes\n",
      "total_backward_count 244750 real_backward_count 26815  10.956%\n",
      "lif layer 2 self.abs_max_v: 4813.0\n",
      "lif layer 2 self.abs_max_v: 4911.5\n",
      "fc layer 2 self.abs_max_out: 2836.0\n",
      "fc layer 2 self.abs_max_out: 2875.0\n",
      "lif layer 2 self.abs_max_v: 4953.0\n",
      "fc layer 2 self.abs_max_out: 2912.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.852586/  1.249321, val:  67.92%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.11 seconds, 0.95 minutes\n",
      "total_backward_count 254540 real_backward_count 27766  10.908%\n",
      "fc layer 2 self.abs_max_out: 3005.0\n",
      "lif layer 2 self.abs_max_v: 5076.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.847500/  1.312478, val:  62.08%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.10 seconds, 0.95 minutes\n",
      "total_backward_count 264330 real_backward_count 28642  10.836%\n",
      "lif layer 2 self.abs_max_v: 5124.0\n",
      "fc layer 1 self.abs_max_out: 6043.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.853201/  1.263523, val:  64.58%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.82 seconds, 0.96 minutes\n",
      "total_backward_count 274120 real_backward_count 29585  10.793%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.787706/  1.292335, val:  58.33%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.11 seconds, 0.97 minutes\n",
      "total_backward_count 283910 real_backward_count 30455  10.727%\n",
      "lif layer 2 self.abs_max_v: 5127.0\n",
      "lif layer 2 self.abs_max_v: 5264.5\n",
      "fc layer 2 self.abs_max_out: 3135.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.811252/  1.419771, val:  47.92%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 293700 real_backward_count 31362  10.678%\n",
      "lif layer 2 self.abs_max_v: 5361.0\n",
      "lif layer 2 self.abs_max_v: 5447.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.794660/  1.193740, val:  68.33%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.35 seconds, 0.97 minutes\n",
      "total_backward_count 303490 real_backward_count 32242  10.624%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.794319/  1.254778, val:  64.17%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 57.94 seconds, 0.97 minutes\n",
      "total_backward_count 313280 real_backward_count 33119  10.572%\n",
      "fc layer 2 self.abs_max_out: 3160.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.772592/  1.181822, val:  68.33%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.38 seconds, 0.97 minutes\n",
      "total_backward_count 323070 real_backward_count 33972  10.515%\n",
      "fc layer 1 self.abs_max_out: 6057.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.765980/  1.169492, val:  70.00%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.99 seconds, 0.97 minutes\n",
      "total_backward_count 332860 real_backward_count 34810  10.458%\n",
      "fc layer 1 self.abs_max_out: 6316.0\n",
      "lif layer 1 self.abs_max_v: 11016.5\n",
      "lif layer 1 self.abs_max_v: 11335.0\n",
      "lif layer 1 self.abs_max_v: 11376.5\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.732544/  1.229422, val:  58.33%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.67 seconds, 0.94 minutes\n",
      "total_backward_count 342650 real_backward_count 35662  10.408%\n",
      "lif layer 2 self.abs_max_v: 5617.5\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.730044/  1.232194, val:  67.08%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.15 seconds, 0.94 minutes\n",
      "total_backward_count 352440 real_backward_count 36499  10.356%\n",
      "fc layer 1 self.abs_max_out: 6356.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.720376/  1.185239, val:  71.67%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.22 seconds, 0.94 minutes\n",
      "total_backward_count 362230 real_backward_count 37296  10.296%\n",
      "fc layer 1 self.abs_max_out: 6645.0\n",
      "lif layer 1 self.abs_max_v: 11388.5\n",
      "lif layer 1 self.abs_max_v: 11453.5\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.721596/  1.192886, val:  67.92%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.14 seconds, 0.94 minutes\n",
      "total_backward_count 372020 real_backward_count 38063  10.231%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.713034/  1.207777, val:  72.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.48 seconds, 0.92 minutes\n",
      "total_backward_count 381810 real_backward_count 38888  10.185%\n",
      "lif layer 1 self.abs_max_v: 11537.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.710480/  1.280863, val:  62.92%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.96 seconds, 0.93 minutes\n",
      "total_backward_count 391600 real_backward_count 39720  10.143%\n",
      "fc layer 2 self.abs_max_out: 3245.0\n",
      "fc layer 2 self.abs_max_out: 3271.0\n",
      "lif layer 1 self.abs_max_v: 11588.0\n",
      "lif layer 1 self.abs_max_v: 11852.0\n",
      "lif layer 1 self.abs_max_v: 11875.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.704094/  1.184458, val:  60.42%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.51 seconds, 0.96 minutes\n",
      "total_backward_count 401390 real_backward_count 40520  10.095%\n",
      "fc layer 3 self.abs_max_out: 1303.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.709064/  1.182632, val:  66.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.51 seconds, 0.96 minutes\n",
      "total_backward_count 411180 real_backward_count 41324  10.050%\n",
      "fc layer 2 self.abs_max_out: 3478.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.708487/  1.180746, val:  72.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.46 seconds, 0.96 minutes\n",
      "total_backward_count 420970 real_backward_count 42108  10.003%\n",
      "fc layer 3 self.abs_max_out: 1316.0\n",
      "fc layer 2 self.abs_max_out: 3515.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.701149/  1.104803, val:  69.17%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.23 seconds, 0.97 minutes\n",
      "total_backward_count 430760 real_backward_count 42893   9.958%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.693633/  1.155177, val:  70.83%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.87 seconds, 0.95 minutes\n",
      "total_backward_count 440550 real_backward_count 43698   9.919%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.698021/  1.186312, val:  67.08%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.22 seconds, 0.97 minutes\n",
      "total_backward_count 450340 real_backward_count 44509   9.883%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.694525/  1.129053, val:  72.08%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 59.70 seconds, 1.00 minutes\n",
      "total_backward_count 460130 real_backward_count 45317   9.849%\n",
      "fc layer 1 self.abs_max_out: 6799.0\n",
      "lif layer 1 self.abs_max_v: 12150.5\n",
      "lif layer 1 self.abs_max_v: 12336.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.661434/  1.185884, val:  64.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.72 seconds, 0.98 minutes\n",
      "total_backward_count 469920 real_backward_count 46086   9.807%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.657118/  1.059652, val:  80.00%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.95 seconds, 0.97 minutes\n",
      "total_backward_count 479710 real_backward_count 46819   9.760%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.670606/  1.088543, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 59.10 seconds, 0.99 minutes\n",
      "total_backward_count 489500 real_backward_count 47567   9.717%\n",
      "fc layer 3 self.abs_max_out: 1335.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.652726/  1.143205, val:  72.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.83 seconds, 0.96 minutes\n",
      "total_backward_count 499290 real_backward_count 48286   9.671%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.655944/  1.099104, val:  72.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.87 seconds, 0.98 minutes\n",
      "total_backward_count 509080 real_backward_count 49004   9.626%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.655863/  1.044137, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.79 seconds, 0.96 minutes\n",
      "total_backward_count 518870 real_backward_count 49789   9.596%\n",
      "lif layer 1 self.abs_max_v: 12489.5\n",
      "lif layer 1 self.abs_max_v: 12575.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.644307/  1.107447, val:  76.67%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.90 seconds, 0.96 minutes\n",
      "total_backward_count 528660 real_backward_count 50494   9.551%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.644024/  1.080995, val:  72.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.18 seconds, 0.95 minutes\n",
      "total_backward_count 538450 real_backward_count 51225   9.513%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.629798/  1.059757, val:  72.50%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.78 seconds, 0.93 minutes\n",
      "total_backward_count 548240 real_backward_count 51970   9.479%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.607305/  1.067000, val:  74.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.84 seconds, 0.93 minutes\n",
      "total_backward_count 558030 real_backward_count 52703   9.444%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.612638/  1.087666, val:  63.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.52 seconds, 0.93 minutes\n",
      "total_backward_count 567820 real_backward_count 53420   9.408%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.615356/  1.026989, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.63 seconds, 0.93 minutes\n",
      "total_backward_count 577610 real_backward_count 54083   9.363%\n",
      "fc layer 1 self.abs_max_out: 6830.0\n",
      "fc layer 1 self.abs_max_out: 6929.0\n",
      "lif layer 1 self.abs_max_v: 13122.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.618464/  1.152413, val:  58.75%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.07 seconds, 0.93 minutes\n",
      "total_backward_count 587400 real_backward_count 54778   9.326%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.601963/  1.198386, val:  59.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.34 seconds, 0.94 minutes\n",
      "total_backward_count 597190 real_backward_count 55476   9.290%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.613642/  1.023285, val:  80.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.49 seconds, 0.94 minutes\n",
      "total_backward_count 606980 real_backward_count 56209   9.260%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.594311/  1.099594, val:  67.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.29 seconds, 0.95 minutes\n",
      "total_backward_count 616770 real_backward_count 56870   9.221%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.607393/  1.108227, val:  70.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 626560 real_backward_count 57570   9.188%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.615389/  1.122864, val:  72.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.71 seconds, 0.96 minutes\n",
      "total_backward_count 636350 real_backward_count 58220   9.149%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.596456/  1.025228, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.46 seconds, 0.96 minutes\n",
      "total_backward_count 646140 real_backward_count 58858   9.109%\n",
      "fc layer 3 self.abs_max_out: 1339.0\n",
      "lif layer 1 self.abs_max_v: 13322.5\n",
      "fc layer 1 self.abs_max_out: 7396.0\n",
      "lif layer 1 self.abs_max_v: 14057.5\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.612283/  1.057875, val:  83.33%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.10 seconds, 0.97 minutes\n",
      "total_backward_count 655930 real_backward_count 59514   9.073%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.619121/  0.988395, val:  81.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.84 seconds, 0.96 minutes\n",
      "total_backward_count 665720 real_backward_count 60178   9.040%\n",
      "fc layer 3 self.abs_max_out: 1418.0\n",
      "fc layer 3 self.abs_max_out: 1430.0\n",
      "fc layer 3 self.abs_max_out: 1438.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.605580/  0.968473, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.49 seconds, 0.96 minutes\n",
      "total_backward_count 675510 real_backward_count 60865   9.010%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.568493/  1.154115, val:  56.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.03 seconds, 0.97 minutes\n",
      "total_backward_count 685300 real_backward_count 61464   8.969%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.586538/  1.036402, val:  72.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.40 seconds, 0.96 minutes\n",
      "total_backward_count 695090 real_backward_count 62098   8.934%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.578135/  1.017074, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.23 seconds, 0.97 minutes\n",
      "total_backward_count 704880 real_backward_count 62721   8.898%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.568487/  0.980667, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.20 seconds, 0.97 minutes\n",
      "total_backward_count 714670 real_backward_count 63340   8.863%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.584748/  1.127369, val:  67.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.45 seconds, 0.97 minutes\n",
      "total_backward_count 724460 real_backward_count 63953   8.828%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.572056/  1.026175, val:  72.92%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.04 seconds, 0.97 minutes\n",
      "total_backward_count 734250 real_backward_count 64580   8.795%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.567266/  0.961523, val:  77.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.24 seconds, 0.97 minutes\n",
      "total_backward_count 744040 real_backward_count 65151   8.756%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.580865/  1.000189, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.41 seconds, 0.97 minutes\n",
      "total_backward_count 753830 real_backward_count 65780   8.726%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.573067/  1.170719, val:  60.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.18 seconds, 0.94 minutes\n",
      "total_backward_count 763620 real_backward_count 66399   8.695%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.578196/  1.040116, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.62 seconds, 0.94 minutes\n",
      "total_backward_count 773410 real_backward_count 67040   8.668%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.561327/  1.005225, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.82 seconds, 0.93 minutes\n",
      "total_backward_count 783200 real_backward_count 67625   8.634%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.561252/  1.004712, val:  75.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.89 seconds, 0.95 minutes\n",
      "total_backward_count 792990 real_backward_count 68255   8.607%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.547434/  1.041280, val:  74.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.04 seconds, 0.93 minutes\n",
      "total_backward_count 802780 real_backward_count 68853   8.577%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.555680/  0.980821, val:  78.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.39 seconds, 0.94 minutes\n",
      "total_backward_count 812570 real_backward_count 69434   8.545%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.528824/  1.048747, val:  80.42%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.51 seconds, 0.96 minutes\n",
      "total_backward_count 822360 real_backward_count 70039   8.517%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.556891/  0.936248, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.42 seconds, 0.97 minutes\n",
      "total_backward_count 832150 real_backward_count 70642   8.489%\n",
      "lif layer 1 self.abs_max_v: 14174.5\n",
      "lif layer 1 self.abs_max_v: 14196.5\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.556804/  0.952610, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.76 seconds, 0.96 minutes\n",
      "total_backward_count 841940 real_backward_count 71280   8.466%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.527864/  0.993852, val:  72.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.82 seconds, 0.96 minutes\n",
      "total_backward_count 851730 real_backward_count 71832   8.434%\n",
      "fc layer 3 self.abs_max_out: 1439.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.537855/  0.974879, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.75 seconds, 0.95 minutes\n",
      "total_backward_count 861520 real_backward_count 72468   8.412%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.533390/  0.954161, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.00 seconds, 0.95 minutes\n",
      "total_backward_count 871310 real_backward_count 73073   8.387%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.527741/  0.910628, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.35 seconds, 0.96 minutes\n",
      "total_backward_count 881100 real_backward_count 73647   8.359%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.533056/  0.983685, val:  80.83%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.09 seconds, 0.95 minutes\n",
      "total_backward_count 890890 real_backward_count 74229   8.332%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.521034/  0.896988, val:  85.42%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.74 seconds, 0.95 minutes\n",
      "total_backward_count 900680 real_backward_count 74759   8.300%\n",
      "fc layer 1 self.abs_max_out: 7999.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.531924/  0.988742, val:  74.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.77 seconds, 0.96 minutes\n",
      "total_backward_count 910470 real_backward_count 75305   8.271%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.517731/  0.933733, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.69 seconds, 0.96 minutes\n",
      "total_backward_count 920260 real_backward_count 75829   8.240%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.520572/  0.938735, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.61 seconds, 0.96 minutes\n",
      "total_backward_count 930050 real_backward_count 76340   8.208%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.527727/  0.923297, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.38 seconds, 0.96 minutes\n",
      "total_backward_count 939840 real_backward_count 76895   8.182%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.525647/  0.959053, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.34 seconds, 0.97 minutes\n",
      "total_backward_count 949630 real_backward_count 77458   8.157%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.541359/  0.952860, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.47 seconds, 0.97 minutes\n",
      "total_backward_count 959420 real_backward_count 77965   8.126%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.519401/  1.104592, val:  65.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.12 seconds, 0.97 minutes\n",
      "total_backward_count 969210 real_backward_count 78480   8.097%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.505477/  0.964987, val:  75.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.27 seconds, 0.94 minutes\n",
      "total_backward_count 979000 real_backward_count 78960   8.065%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.490065/  0.866121, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.13 seconds, 0.95 minutes\n",
      "total_backward_count 988790 real_backward_count 79450   8.035%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.499175/  0.948978, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.93 seconds, 0.93 minutes\n",
      "total_backward_count 998580 real_backward_count 79961   8.007%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.488440/  0.891170, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.37 seconds, 0.94 minutes\n",
      "total_backward_count 1008370 real_backward_count 80479   7.981%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.509535/  1.003344, val:  75.83%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.47 seconds, 0.94 minutes\n",
      "total_backward_count 1018160 real_backward_count 80974   7.953%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.528956/  0.982335, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.15 seconds, 0.94 minutes\n",
      "total_backward_count 1027950 real_backward_count 81499   7.928%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.513681/  0.996906, val:  75.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.79 seconds, 0.95 minutes\n",
      "total_backward_count 1037740 real_backward_count 81999   7.902%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.489186/  0.936721, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.33 seconds, 0.96 minutes\n",
      "total_backward_count 1047530 real_backward_count 82472   7.873%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.493567/  0.951826, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 1057320 real_backward_count 82961   7.846%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.485216/  0.978293, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.48 seconds, 0.96 minutes\n",
      "total_backward_count 1067110 real_backward_count 83462   7.821%\n",
      "fc layer 3 self.abs_max_out: 1451.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.491901/  0.926882, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.66 seconds, 0.96 minutes\n",
      "total_backward_count 1076900 real_backward_count 83948   7.795%\n",
      "fc layer 3 self.abs_max_out: 1485.0\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.472609/  0.897981, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.24 seconds, 0.97 minutes\n",
      "total_backward_count 1086690 real_backward_count 84415   7.768%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.490432/  0.953515, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 1096480 real_backward_count 84899   7.743%\n",
      "lif layer 2 self.abs_max_v: 5627.0\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.471687/  0.897485, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.93 seconds, 0.98 minutes\n",
      "total_backward_count 1106270 real_backward_count 85370   7.717%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.459593/  0.906360, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.72 seconds, 0.98 minutes\n",
      "total_backward_count 1116060 real_backward_count 85872   7.694%\n",
      "fc layer 3 self.abs_max_out: 1489.0\n",
      "fc layer 3 self.abs_max_out: 1511.0\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.469776/  0.932284, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 1125850 real_backward_count 86363   7.671%\n",
      "lif layer 1 self.abs_max_v: 14282.5\n",
      "lif layer 1 self.abs_max_v: 14465.5\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.461571/  0.916525, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 59.08 seconds, 0.98 minutes\n",
      "total_backward_count 1135640 real_backward_count 86810   7.644%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.489405/  0.869466, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.61 seconds, 0.96 minutes\n",
      "total_backward_count 1145430 real_backward_count 87266   7.619%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.458250/  0.887132, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.68 seconds, 0.96 minutes\n",
      "total_backward_count 1155220 real_backward_count 87715   7.593%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.459904/  0.963870, val:  68.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.41 seconds, 0.96 minutes\n",
      "total_backward_count 1165010 real_backward_count 88184   7.569%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.450262/  0.856005, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.77 seconds, 0.95 minutes\n",
      "total_backward_count 1174800 real_backward_count 88621   7.543%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.448823/  0.898819, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.98 seconds, 0.95 minutes\n",
      "total_backward_count 1184590 real_backward_count 89070   7.519%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.453926/  0.896939, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.18 seconds, 0.92 minutes\n",
      "total_backward_count 1194380 real_backward_count 89572   7.499%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.443757/  0.900011, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.62 seconds, 0.93 minutes\n",
      "total_backward_count 1204170 real_backward_count 90031   7.477%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.443286/  0.842946, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.35 seconds, 0.94 minutes\n",
      "total_backward_count 1213960 real_backward_count 90486   7.454%\n",
      "lif layer 2 self.abs_max_v: 5708.5\n",
      "fc layer 3 self.abs_max_out: 1553.0\n",
      "fc layer 3 self.abs_max_out: 1568.0\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.455629/  0.861542, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.52 seconds, 0.94 minutes\n",
      "total_backward_count 1223750 real_backward_count 90953   7.432%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.449224/  0.862833, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.24 seconds, 0.94 minutes\n",
      "total_backward_count 1233540 real_backward_count 91372   7.407%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.440452/  0.903684, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.20 seconds, 0.94 minutes\n",
      "total_backward_count 1243330 real_backward_count 91779   7.382%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.452557/  0.909176, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.41 seconds, 0.96 minutes\n",
      "total_backward_count 1253120 real_backward_count 92208   7.358%\n",
      "lif layer 1 self.abs_max_v: 14515.0\n",
      "lif layer 1 self.abs_max_v: 14649.5\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.452519/  0.850752, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.80 seconds, 0.96 minutes\n",
      "total_backward_count 1262910 real_backward_count 92640   7.335%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.449709/  0.892651, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.16 seconds, 0.97 minutes\n",
      "total_backward_count 1272700 real_backward_count 93048   7.311%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.441684/  0.937425, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 1282490 real_backward_count 93468   7.288%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.430932/  0.840563, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.36 seconds, 0.96 minutes\n",
      "total_backward_count 1292280 real_backward_count 93920   7.268%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.417867/  0.998101, val:  73.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.54 seconds, 0.96 minutes\n",
      "total_backward_count 1302070 real_backward_count 94308   7.243%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.427127/  0.912068, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.94 seconds, 0.98 minutes\n",
      "total_backward_count 1311860 real_backward_count 94716   7.220%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.413603/  0.959880, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.79 seconds, 0.98 minutes\n",
      "total_backward_count 1321650 real_backward_count 95120   7.197%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.426142/  0.900699, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.28 seconds, 0.97 minutes\n",
      "total_backward_count 1331440 real_backward_count 95547   7.176%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.408975/  0.891056, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.56 seconds, 0.98 minutes\n",
      "total_backward_count 1341230 real_backward_count 95918   7.151%\n",
      "fc layer 3 self.abs_max_out: 1584.0\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.429377/  0.849097, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.80 seconds, 0.98 minutes\n",
      "total_backward_count 1351020 real_backward_count 96367   7.133%\n",
      "fc layer 3 self.abs_max_out: 1676.0\n",
      "lif layer 1 self.abs_max_v: 14721.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.406089/  0.844671, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.88 seconds, 0.96 minutes\n",
      "total_backward_count 1360810 real_backward_count 96794   7.113%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.407466/  0.807107, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.97 seconds, 0.97 minutes\n",
      "total_backward_count 1370600 real_backward_count 97195   7.091%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.401552/  0.865331, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.45 seconds, 0.97 minutes\n",
      "total_backward_count 1380390 real_backward_count 97632   7.073%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.414184/  0.891623, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 1390180 real_backward_count 98057   7.054%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.430717/  0.861660, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.16 seconds, 0.95 minutes\n",
      "total_backward_count 1399970 real_backward_count 98516   7.037%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.419729/  0.865063, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.16 seconds, 0.94 minutes\n",
      "total_backward_count 1409760 real_backward_count 98936   7.018%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.420736/  0.831540, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.07 seconds, 0.93 minutes\n",
      "total_backward_count 1419550 real_backward_count 99351   6.999%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.419480/  0.908230, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.01 seconds, 0.93 minutes\n",
      "total_backward_count 1429340 real_backward_count 99716   6.976%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.431357/  0.849462, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.83 seconds, 0.93 minutes\n",
      "total_backward_count 1439130 real_backward_count 100148   6.959%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.407178/  0.877924, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.77 seconds, 0.95 minutes\n",
      "total_backward_count 1448920 real_backward_count 100536   6.939%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.416707/  0.851349, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.13 seconds, 0.94 minutes\n",
      "total_backward_count 1458710 real_backward_count 100962   6.921%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.411717/  0.869685, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.04 seconds, 0.97 minutes\n",
      "total_backward_count 1468500 real_backward_count 101348   6.901%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.408860/  0.899561, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.99 seconds, 0.98 minutes\n",
      "total_backward_count 1478290 real_backward_count 101749   6.883%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.423838/  0.889031, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.16 seconds, 0.95 minutes\n",
      "total_backward_count 1488080 real_backward_count 102173   6.866%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.410821/  0.899533, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.35 seconds, 0.96 minutes\n",
      "total_backward_count 1497870 real_backward_count 102553   6.847%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.430924/  0.853316, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.24 seconds, 0.95 minutes\n",
      "total_backward_count 1507660 real_backward_count 102981   6.831%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.430738/  0.923789, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.68 seconds, 0.94 minutes\n",
      "total_backward_count 1517450 real_backward_count 103359   6.811%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.421977/  0.946290, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.46 seconds, 0.96 minutes\n",
      "total_backward_count 1527240 real_backward_count 103781   6.795%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.413783/  0.928607, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.93 seconds, 0.97 minutes\n",
      "total_backward_count 1537030 real_backward_count 104201   6.779%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.403226/  0.837001, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.29 seconds, 0.97 minutes\n",
      "total_backward_count 1546820 real_backward_count 104534   6.758%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.411059/  0.828571, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.90 seconds, 0.97 minutes\n",
      "total_backward_count 1556610 real_backward_count 104924   6.741%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.413813/  0.878742, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.50 seconds, 0.96 minutes\n",
      "total_backward_count 1566400 real_backward_count 105333   6.725%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.396511/  0.852011, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.45 seconds, 0.97 minutes\n",
      "total_backward_count 1576190 real_backward_count 105701   6.706%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.396152/  0.814844, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.68 seconds, 0.96 minutes\n",
      "total_backward_count 1585980 real_backward_count 106107   6.690%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.380042/  0.983163, val:  75.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.53 seconds, 0.98 minutes\n",
      "total_backward_count 1595770 real_backward_count 106469   6.672%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.386943/  0.883052, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.05 seconds, 0.97 minutes\n",
      "total_backward_count 1605560 real_backward_count 106837   6.654%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.384651/  0.866596, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.86 seconds, 0.93 minutes\n",
      "total_backward_count 1615350 real_backward_count 107158   6.634%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.386463/  0.883274, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.41 seconds, 0.94 minutes\n",
      "total_backward_count 1625140 real_backward_count 107493   6.614%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.380594/  0.847728, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.20 seconds, 0.94 minutes\n",
      "total_backward_count 1634930 real_backward_count 107844   6.596%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.402653/  0.907907, val:  76.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.62 seconds, 0.94 minutes\n",
      "total_backward_count 1644720 real_backward_count 108169   6.577%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.382100/  0.840752, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.14 seconds, 0.94 minutes\n",
      "total_backward_count 1654510 real_backward_count 108497   6.558%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.377784/  0.814138, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.76 seconds, 0.93 minutes\n",
      "total_backward_count 1664300 real_backward_count 108832   6.539%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.372823/  0.830900, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.88 seconds, 0.96 minutes\n",
      "total_backward_count 1674090 real_backward_count 109138   6.519%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.378972/  0.915917, val:  80.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.68 seconds, 0.96 minutes\n",
      "total_backward_count 1683880 real_backward_count 109449   6.500%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.380195/  0.854596, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.65 seconds, 0.96 minutes\n",
      "total_backward_count 1693670 real_backward_count 109809   6.483%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.365769/  0.829544, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.26 seconds, 0.97 minutes\n",
      "total_backward_count 1703460 real_backward_count 110118   6.464%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.382459/  0.874679, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 1713250 real_backward_count 110473   6.448%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.378614/  0.851019, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.23 seconds, 0.95 minutes\n",
      "total_backward_count 1723040 real_backward_count 110818   6.432%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.380597/  0.813720, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.72 seconds, 0.96 minutes\n",
      "total_backward_count 1732830 real_backward_count 111144   6.414%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.362536/  0.866284, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.29 seconds, 0.97 minutes\n",
      "total_backward_count 1742620 real_backward_count 111470   6.397%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.357403/  0.877422, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.00 seconds, 0.97 minutes\n",
      "total_backward_count 1752410 real_backward_count 111794   6.379%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.379073/  0.872312, val:  80.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.39 seconds, 0.96 minutes\n",
      "total_backward_count 1762200 real_backward_count 112167   6.365%\n",
      "fc layer 3 self.abs_max_out: 1689.0\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.373691/  0.878068, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.58 seconds, 0.98 minutes\n",
      "total_backward_count 1771990 real_backward_count 112498   6.349%\n",
      "lif layer 2 self.abs_max_v: 5874.5\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.371327/  0.806885, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.46 seconds, 0.96 minutes\n",
      "total_backward_count 1781780 real_backward_count 112819   6.332%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.360440/  0.828639, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 1791570 real_backward_count 113129   6.315%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.356654/  0.946162, val:  75.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.27 seconds, 0.97 minutes\n",
      "total_backward_count 1801360 real_backward_count 113423   6.297%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.373379/  0.958615, val:  73.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.96 seconds, 0.97 minutes\n",
      "total_backward_count 1811150 real_backward_count 113764   6.281%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.374216/  0.811123, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.48 seconds, 0.94 minutes\n",
      "total_backward_count 1820940 real_backward_count 114101   6.266%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.377665/  0.886291, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.70 seconds, 0.93 minutes\n",
      "total_backward_count 1830730 real_backward_count 114416   6.250%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.377289/  0.868515, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.52 seconds, 0.94 minutes\n",
      "total_backward_count 1840520 real_backward_count 114730   6.234%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.360998/  0.851424, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.32 seconds, 0.94 minutes\n",
      "total_backward_count 1850310 real_backward_count 115025   6.217%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.379341/  0.859256, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.30 seconds, 0.94 minutes\n",
      "total_backward_count 1860100 real_backward_count 115349   6.201%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.387704/  0.883976, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.47 seconds, 0.96 minutes\n",
      "total_backward_count 1869890 real_backward_count 115648   6.185%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.377456/  0.927073, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.37 seconds, 0.94 minutes\n",
      "total_backward_count 1879680 real_backward_count 115992   6.171%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.379995/  0.814869, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.81 seconds, 0.96 minutes\n",
      "total_backward_count 1889470 real_backward_count 116303   6.155%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.387111/  0.855699, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.48 seconds, 0.97 minutes\n",
      "total_backward_count 1899260 real_backward_count 116606   6.140%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.386963/  0.852231, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 59.67 seconds, 0.99 minutes\n",
      "total_backward_count 1909050 real_backward_count 116893   6.123%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.369814/  0.810846, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.29 seconds, 0.97 minutes\n",
      "total_backward_count 1918840 real_backward_count 117185   6.107%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.353762/  0.854411, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 1928630 real_backward_count 117466   6.091%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.367026/  0.830171, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.19 seconds, 0.97 minutes\n",
      "total_backward_count 1938420 real_backward_count 117760   6.075%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.366861/  0.820425, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.07 seconds, 0.95 minutes\n",
      "total_backward_count 1948210 real_backward_count 118031   6.058%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.350620/  0.847530, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.62 seconds, 0.96 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7589d948f63d49c989f328c8b09aafd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.35062</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.82917</td></tr><tr><td>val_loss</td><td>0.84753</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-sweep-246</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4h34xrqv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4h34xrqv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251029_081821-4h34xrqv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h3syuw32 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251029_113035-h3syuw32</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h3syuw32' target=\"_blank\">trim-sweep-252</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/cija8jrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h3syuw32' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h3syuw32</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251029_113044_338', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 188.0\n",
      "lif layer 1 self.abs_max_v: 188.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 35.0\n",
      "lif layer 2 self.abs_max_v: 35.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 204.0\n",
      "lif layer 1 self.abs_max_v: 262.5\n",
      "fc layer 2 self.abs_max_out: 253.0\n",
      "lif layer 2 self.abs_max_v: 245.0\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "fc layer 1 self.abs_max_out: 209.0\n",
      "lif layer 1 self.abs_max_v: 279.0\n",
      "lif layer 2 self.abs_max_v: 306.5\n",
      "fc layer 1 self.abs_max_out: 232.0\n",
      "lif layer 1 self.abs_max_v: 298.5\n",
      "fc layer 2 self.abs_max_out: 276.0\n",
      "lif layer 2 self.abs_max_v: 393.5\n",
      "fc layer 1 self.abs_max_out: 394.0\n",
      "lif layer 1 self.abs_max_v: 394.0\n",
      "lif layer 2 self.abs_max_v: 452.0\n",
      "fc layer 1 self.abs_max_out: 506.0\n",
      "lif layer 1 self.abs_max_v: 506.0\n",
      "fc layer 2 self.abs_max_out: 330.0\n",
      "fc layer 1 self.abs_max_out: 692.0\n",
      "lif layer 1 self.abs_max_v: 692.0\n",
      "fc layer 2 self.abs_max_out: 338.0\n",
      "lif layer 2 self.abs_max_v: 556.0\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "fc layer 1 self.abs_max_out: 694.0\n",
      "lif layer 1 self.abs_max_v: 694.0\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "fc layer 2 self.abs_max_out: 481.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 1 self.abs_max_out: 718.0\n",
      "lif layer 1 self.abs_max_v: 718.0\n",
      "fc layer 2 self.abs_max_out: 524.0\n",
      "fc layer 1 self.abs_max_out: 782.0\n",
      "lif layer 1 self.abs_max_v: 782.0\n",
      "lif layer 2 self.abs_max_v: 587.0\n",
      "fc layer 1 self.abs_max_out: 1005.0\n",
      "lif layer 1 self.abs_max_v: 1005.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 750.0\n",
      "lif layer 1 self.abs_max_v: 1029.5\n",
      "fc layer 1 self.abs_max_out: 1014.0\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "fc layer 2 self.abs_max_out: 650.0\n",
      "lif layer 1 self.abs_max_v: 1162.5\n",
      "lif layer 2 self.abs_max_v: 794.5\n",
      "fc layer 1 self.abs_max_out: 1190.0\n",
      "lif layer 1 self.abs_max_v: 1190.0\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "lif layer 2 self.abs_max_v: 808.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "lif layer 1 self.abs_max_v: 1205.5\n",
      "lif layer 2 self.abs_max_v: 845.0\n",
      "fc layer 3 self.abs_max_out: 246.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "fc layer 1 self.abs_max_out: 1221.0\n",
      "lif layer 1 self.abs_max_v: 1221.0\n",
      "lif layer 2 self.abs_max_v: 912.5\n",
      "lif layer 2 self.abs_max_v: 919.0\n",
      "lif layer 1 self.abs_max_v: 1226.0\n",
      "fc layer 1 self.abs_max_out: 1421.0\n",
      "lif layer 1 self.abs_max_v: 1795.0\n",
      "lif layer 1 self.abs_max_v: 2122.5\n",
      "lif layer 2 self.abs_max_v: 980.5\n",
      "lif layer 2 self.abs_max_v: 1013.5\n",
      "lif layer 2 self.abs_max_v: 1038.5\n",
      "lif layer 2 self.abs_max_v: 1049.0\n",
      "fc layer 2 self.abs_max_out: 665.0\n",
      "fc layer 2 self.abs_max_out: 685.0\n",
      "fc layer 2 self.abs_max_out: 700.0\n",
      "fc layer 2 self.abs_max_out: 790.0\n",
      "lif layer 2 self.abs_max_v: 1077.5\n",
      "fc layer 2 self.abs_max_out: 860.0\n",
      "fc layer 2 self.abs_max_out: 886.0\n",
      "lif layer 2 self.abs_max_v: 1109.0\n",
      "lif layer 2 self.abs_max_v: 1205.5\n",
      "lif layer 2 self.abs_max_v: 1208.0\n",
      "lif layer 2 self.abs_max_v: 1299.5\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 1 self.abs_max_out: 1520.0\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "lif layer 2 self.abs_max_v: 1403.0\n",
      "lif layer 2 self.abs_max_v: 1443.0\n",
      "lif layer 2 self.abs_max_v: 1559.5\n",
      "lif layer 2 self.abs_max_v: 1621.0\n",
      "lif layer 1 self.abs_max_v: 2148.5\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 1658.5\n",
      "lif layer 2 self.abs_max_v: 1667.0\n",
      "fc layer 1 self.abs_max_out: 1550.0\n",
      "fc layer 1 self.abs_max_out: 1835.0\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 2 self.abs_max_out: 1054.0\n",
      "lif layer 1 self.abs_max_v: 2175.0\n",
      "lif layer 1 self.abs_max_v: 2251.0\n",
      "lif layer 1 self.abs_max_v: 2279.5\n",
      "fc layer 2 self.abs_max_out: 1069.0\n",
      "lif layer 1 self.abs_max_v: 2409.0\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "lif layer 2 self.abs_max_v: 1681.0\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "lif layer 2 self.abs_max_v: 1725.5\n",
      "lif layer 1 self.abs_max_v: 2458.0\n",
      "lif layer 1 self.abs_max_v: 2543.0\n",
      "lif layer 1 self.abs_max_v: 2565.5\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "fc layer 2 self.abs_max_out: 1145.0\n",
      "fc layer 2 self.abs_max_out: 1146.0\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 3 self.abs_max_out: 508.0\n",
      "fc layer 3 self.abs_max_out: 530.0\n",
      "lif layer 1 self.abs_max_v: 2654.5\n",
      "lif layer 1 self.abs_max_v: 2811.5\n",
      "fc layer 1 self.abs_max_out: 2269.0\n",
      "lif layer 1 self.abs_max_v: 3340.0\n",
      "fc layer 1 self.abs_max_out: 2302.0\n",
      "lif layer 1 self.abs_max_v: 3619.0\n",
      "lif layer 1 self.abs_max_v: 3676.5\n",
      "lif layer 2 self.abs_max_v: 1768.0\n",
      "lif layer 2 self.abs_max_v: 1846.0\n",
      "fc layer 2 self.abs_max_out: 1189.0\n",
      "lif layer 2 self.abs_max_v: 1888.5\n",
      "lif layer 2 self.abs_max_v: 1900.0\n",
      "lif layer 2 self.abs_max_v: 1917.0\n",
      "lif layer 1 self.abs_max_v: 3834.5\n",
      "lif layer 2 self.abs_max_v: 1925.5\n",
      "lif layer 2 self.abs_max_v: 1970.5\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "lif layer 2 self.abs_max_v: 1978.5\n",
      "lif layer 2 self.abs_max_v: 2001.5\n",
      "lif layer 2 self.abs_max_v: 2062.0\n",
      "lif layer 2 self.abs_max_v: 2080.5\n",
      "lif layer 1 self.abs_max_v: 4035.5\n",
      "lif layer 1 self.abs_max_v: 4207.0\n",
      "lif layer 1 self.abs_max_v: 4212.5\n",
      "lif layer 1 self.abs_max_v: 4301.5\n",
      "lif layer 1 self.abs_max_v: 4317.0\n",
      "lif layer 2 self.abs_max_v: 2124.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.583307/  1.932267, val:  30.42%, val_best:  30.42%, tr:  99.39%, tr_best:  99.39%, epoch time: 58.42 seconds, 0.97 minutes\n",
      "total_backward_count 9790 real_backward_count 1613  16.476%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1193.0\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "fc layer 2 self.abs_max_out: 1333.0\n",
      "fc layer 3 self.abs_max_out: 606.0\n",
      "lif layer 2 self.abs_max_v: 2169.0\n",
      "fc layer 3 self.abs_max_out: 646.0\n",
      "lif layer 2 self.abs_max_v: 2186.0\n",
      "lif layer 2 self.abs_max_v: 2200.5\n",
      "fc layer 1 self.abs_max_out: 2308.0\n",
      "fc layer 1 self.abs_max_out: 2539.0\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "lif layer 2 self.abs_max_v: 2206.5\n",
      "lif layer 2 self.abs_max_v: 2222.0\n",
      "lif layer 2 self.abs_max_v: 2315.0\n",
      "lif layer 2 self.abs_max_v: 2364.0\n",
      "lif layer 2 self.abs_max_v: 2444.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "lif layer 2 self.abs_max_v: 2461.0\n",
      "fc layer 2 self.abs_max_out: 1407.0\n",
      "fc layer 2 self.abs_max_out: 1410.0\n",
      "lif layer 2 self.abs_max_v: 2534.0\n",
      "fc layer 2 self.abs_max_out: 1433.0\n",
      "lif layer 2 self.abs_max_v: 2673.5\n",
      "lif layer 1 self.abs_max_v: 4609.5\n",
      "fc layer 1 self.abs_max_out: 2624.0\n",
      "fc layer 1 self.abs_max_out: 2648.0\n",
      "lif layer 1 self.abs_max_v: 4727.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.465790/  1.814201, val:  40.83%, val_best:  40.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 58.09 seconds, 0.97 minutes\n",
      "total_backward_count 19580 real_backward_count 2990  15.271%\n",
      "fc layer 1 self.abs_max_out: 2861.0\n",
      "fc layer 3 self.abs_max_out: 672.0\n",
      "fc layer 3 self.abs_max_out: 681.0\n",
      "fc layer 2 self.abs_max_out: 1531.0\n",
      "lif layer 2 self.abs_max_v: 2727.5\n",
      "fc layer 3 self.abs_max_out: 694.0\n",
      "fc layer 3 self.abs_max_out: 741.0\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "lif layer 1 self.abs_max_v: 5038.5\n",
      "lif layer 1 self.abs_max_v: 5120.0\n",
      "lif layer 1 self.abs_max_v: 5368.0\n",
      "lif layer 1 self.abs_max_v: 5458.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.416296/  1.750618, val:  49.58%, val_best:  49.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 57.83 seconds, 0.96 minutes\n",
      "total_backward_count 29370 real_backward_count 4323  14.719%\n",
      "fc layer 1 self.abs_max_out: 2950.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.425564/  1.840422, val:  35.83%, val_best:  49.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 57.40 seconds, 0.96 minutes\n",
      "total_backward_count 39160 real_backward_count 5651  14.431%\n",
      "lif layer 2 self.abs_max_v: 2729.0\n",
      "lif layer 2 self.abs_max_v: 2784.5\n",
      "fc layer 2 self.abs_max_out: 1670.0\n",
      "lif layer 2 self.abs_max_v: 2888.0\n",
      "lif layer 2 self.abs_max_v: 2902.5\n",
      "fc layer 2 self.abs_max_out: 1692.0\n",
      "lif layer 2 self.abs_max_v: 2908.5\n",
      "lif layer 2 self.abs_max_v: 2982.5\n",
      "lif layer 2 self.abs_max_v: 2993.5\n",
      "lif layer 2 self.abs_max_v: 3031.0\n",
      "fc layer 2 self.abs_max_out: 1701.0\n",
      "lif layer 2 self.abs_max_v: 3197.5\n",
      "lif layer 2 self.abs_max_v: 3283.0\n",
      "fc layer 2 self.abs_max_out: 1746.0\n",
      "fc layer 2 self.abs_max_out: 1772.0\n",
      "fc layer 1 self.abs_max_out: 3226.0\n",
      "fc layer 1 self.abs_max_out: 3533.0\n",
      "lif layer 1 self.abs_max_v: 6234.0\n",
      "lif layer 1 self.abs_max_v: 6393.0\n",
      "lif layer 1 self.abs_max_v: 6410.5\n",
      "lif layer 2 self.abs_max_v: 3316.0\n",
      "lif layer 2 self.abs_max_v: 3345.0\n",
      "fc layer 2 self.abs_max_out: 1794.0\n",
      "lif layer 2 self.abs_max_v: 3386.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.425365/  1.788350, val:  37.08%, val_best:  49.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 48950 real_backward_count 6935  14.168%\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "fc layer 1 self.abs_max_out: 3560.0\n",
      "lif layer 1 self.abs_max_v: 6435.0\n",
      "lif layer 1 self.abs_max_v: 6472.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.400206/  1.723166, val:  49.58%, val_best:  49.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 57.88 seconds, 0.96 minutes\n",
      "total_backward_count 58740 real_backward_count 8251  14.047%\n",
      "fc layer 2 self.abs_max_out: 1810.0\n",
      "fc layer 2 self.abs_max_out: 1841.0\n",
      "fc layer 2 self.abs_max_out: 1885.0\n",
      "fc layer 2 self.abs_max_out: 1922.0\n",
      "fc layer 2 self.abs_max_out: 1983.0\n",
      "fc layer 2 self.abs_max_out: 2079.0\n",
      "fc layer 2 self.abs_max_out: 2125.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.377091/  1.732990, val:  47.08%, val_best:  49.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 56.82 seconds, 0.95 minutes\n",
      "total_backward_count 68530 real_backward_count 9574  13.971%\n",
      "lif layer 2 self.abs_max_v: 3405.5\n",
      "lif layer 2 self.abs_max_v: 3511.0\n",
      "lif layer 2 self.abs_max_v: 3602.0\n",
      "fc layer 1 self.abs_max_out: 3616.0\n",
      "lif layer 1 self.abs_max_v: 6614.0\n",
      "lif layer 1 self.abs_max_v: 6802.0\n",
      "lif layer 1 self.abs_max_v: 6870.0\n",
      "lif layer 1 self.abs_max_v: 7016.0\n",
      "fc layer 1 self.abs_max_out: 3689.0\n",
      "lif layer 1 self.abs_max_v: 7046.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.405729/  1.678567, val:  52.92%, val_best:  52.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 55.99 seconds, 0.93 minutes\n",
      "total_backward_count 78320 real_backward_count 10841  13.842%\n",
      "fc layer 2 self.abs_max_out: 2218.0\n",
      "fc layer 2 self.abs_max_out: 2263.0\n",
      "fc layer 1 self.abs_max_out: 3727.0\n",
      "fc layer 1 self.abs_max_out: 4063.0\n",
      "lif layer 1 self.abs_max_v: 7139.0\n",
      "lif layer 1 self.abs_max_v: 7494.5\n",
      "lif layer 1 self.abs_max_v: 7648.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.425605/  1.714753, val:  57.92%, val_best:  57.92%, tr:  99.18%, tr_best:  99.80%, epoch time: 56.81 seconds, 0.95 minutes\n",
      "total_backward_count 88110 real_backward_count 12156  13.796%\n",
      "fc layer 3 self.abs_max_out: 781.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.415052/  1.725762, val:  50.83%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 55.91 seconds, 0.93 minutes\n",
      "total_backward_count 97900 real_backward_count 13372  13.659%\n",
      "lif layer 2 self.abs_max_v: 3622.0\n",
      "lif layer 2 self.abs_max_v: 3632.0\n",
      "fc layer 1 self.abs_max_out: 4271.0\n",
      "lif layer 1 self.abs_max_v: 8094.5\n",
      "lif layer 1 self.abs_max_v: 8300.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.399433/  1.758921, val:  47.92%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 56.40 seconds, 0.94 minutes\n",
      "total_backward_count 107690 real_backward_count 14609  13.566%\n",
      "lif layer 2 self.abs_max_v: 3661.5\n",
      "lif layer 2 self.abs_max_v: 3825.0\n",
      "lif layer 2 self.abs_max_v: 3832.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.415137/  1.710154, val:  53.33%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 56.47 seconds, 0.94 minutes\n",
      "total_backward_count 117480 real_backward_count 15832  13.476%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.380174/  1.675128, val:  42.50%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 56.59 seconds, 0.94 minutes\n",
      "total_backward_count 127270 real_backward_count 16979  13.341%\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.356486/  1.714878, val:  43.33%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 57.37 seconds, 0.96 minutes\n",
      "total_backward_count 137060 real_backward_count 18163  13.252%\n",
      "lif layer 2 self.abs_max_v: 3835.0\n",
      "fc layer 2 self.abs_max_out: 2314.0\n",
      "lif layer 2 self.abs_max_v: 3839.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.315955/  1.620593, val:  54.17%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 57.90 seconds, 0.97 minutes\n",
      "total_backward_count 146850 real_backward_count 19307  13.147%\n",
      "lif layer 2 self.abs_max_v: 3886.0\n",
      "lif layer 2 self.abs_max_v: 4001.0\n",
      "lif layer 2 self.abs_max_v: 4146.0\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "fc layer 1 self.abs_max_out: 4768.0\n",
      "lif layer 1 self.abs_max_v: 8505.5\n",
      "lif layer 1 self.abs_max_v: 8642.0\n",
      "lif layer 1 self.abs_max_v: 9087.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.341759/  1.653533, val:  62.08%, val_best:  62.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 57.73 seconds, 0.96 minutes\n",
      "total_backward_count 156640 real_backward_count 20477  13.073%\n",
      "lif layer 2 self.abs_max_v: 4226.0\n",
      "fc layer 1 self.abs_max_out: 4821.0\n",
      "fc layer 1 self.abs_max_out: 4852.0\n",
      "lif layer 1 self.abs_max_v: 9273.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.329242/  1.654791, val:  49.17%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 57.00 seconds, 0.95 minutes\n",
      "total_backward_count 166430 real_backward_count 21603  12.980%\n",
      "fc layer 2 self.abs_max_out: 2462.0\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.305030/  1.597665, val:  58.75%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 56.90 seconds, 0.95 minutes\n",
      "total_backward_count 176220 real_backward_count 22770  12.921%\n",
      "lif layer 2 self.abs_max_v: 4279.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.281306/  1.584239, val:  56.67%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.51 seconds, 0.96 minutes\n",
      "total_backward_count 186010 real_backward_count 23943  12.872%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.256559/  1.648893, val:  50.42%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 56.97 seconds, 0.95 minutes\n",
      "total_backward_count 195800 real_backward_count 25053  12.795%\n",
      "lif layer 2 self.abs_max_v: 4291.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.256736/  1.617400, val:  53.33%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.97 seconds, 0.98 minutes\n",
      "total_backward_count 205590 real_backward_count 26163  12.726%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.281084/  1.678421, val:  46.67%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.70 seconds, 0.98 minutes\n",
      "total_backward_count 215380 real_backward_count 27320  12.685%\n",
      "fc layer 1 self.abs_max_out: 4855.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.270760/  1.559528, val:  58.33%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.28 seconds, 0.97 minutes\n",
      "total_backward_count 225170 real_backward_count 28436  12.629%\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.220387/  1.573658, val:  54.17%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.29 seconds, 0.95 minutes\n",
      "total_backward_count 234960 real_backward_count 29555  12.579%\n",
      "fc layer 3 self.abs_max_out: 866.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.258216/  1.581210, val:  59.58%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.46 seconds, 0.96 minutes\n",
      "total_backward_count 244750 real_backward_count 30625  12.513%\n",
      "lif layer 2 self.abs_max_v: 4304.5\n",
      "fc layer 2 self.abs_max_out: 2511.0\n",
      "lif layer 2 self.abs_max_v: 4663.5\n",
      "fc layer 1 self.abs_max_out: 5077.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.287987/  1.536304, val:  70.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.38 seconds, 0.96 minutes\n",
      "total_backward_count 254540 real_backward_count 31749  12.473%\n",
      "fc layer 2 self.abs_max_out: 2689.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.233823/  1.555627, val:  56.25%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.27 seconds, 0.95 minutes\n",
      "total_backward_count 264330 real_backward_count 32833  12.421%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.235119/  1.540393, val:  70.00%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.69 seconds, 0.96 minutes\n",
      "total_backward_count 274120 real_backward_count 33905  12.369%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.203154/  1.577949, val:  49.17%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.36 seconds, 0.96 minutes\n",
      "total_backward_count 283910 real_backward_count 35009  12.331%\n",
      "fc layer 1 self.abs_max_out: 5098.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.234444/  1.601932, val:  44.17%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.13 seconds, 0.94 minutes\n",
      "total_backward_count 293700 real_backward_count 36040  12.271%\n",
      "fc layer 1 self.abs_max_out: 5160.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.252567/  1.566945, val:  54.58%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.01 seconds, 0.93 minutes\n",
      "total_backward_count 303490 real_backward_count 37108  12.227%\n",
      "fc layer 1 self.abs_max_out: 5232.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.254925/  1.583478, val:  53.33%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.40 seconds, 0.92 minutes\n",
      "total_backward_count 313280 real_backward_count 38127  12.170%\n",
      "fc layer 1 self.abs_max_out: 5401.0\n",
      "fc layer 2 self.abs_max_out: 2699.0\n",
      "fc layer 1 self.abs_max_out: 5743.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.233588/  1.564975, val:  62.50%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.07 seconds, 0.93 minutes\n",
      "total_backward_count 323070 real_backward_count 39171  12.125%\n",
      "fc layer 1 self.abs_max_out: 6190.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.255553/  1.552404, val:  60.83%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.01 seconds, 0.93 minutes\n",
      "total_backward_count 332860 real_backward_count 40220  12.083%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.209600/  1.554267, val:  49.17%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.80 seconds, 0.93 minutes\n",
      "total_backward_count 342650 real_backward_count 41226  12.032%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.183835/  1.497034, val:  67.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.49 seconds, 0.97 minutes\n",
      "total_backward_count 352440 real_backward_count 42240  11.985%\n",
      "fc layer 2 self.abs_max_out: 2760.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.185853/  1.482884, val:  67.92%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.66 seconds, 0.96 minutes\n",
      "total_backward_count 362230 real_backward_count 43224  11.933%\n",
      "fc layer 1 self.abs_max_out: 6321.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.152624/  1.516406, val:  55.42%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.30 seconds, 0.95 minutes\n",
      "total_backward_count 372020 real_backward_count 44149  11.867%\n",
      "lif layer 1 self.abs_max_v: 9600.0\n",
      "lif layer 1 self.abs_max_v: 9667.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.129266/  1.449734, val:  67.92%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.27 seconds, 0.97 minutes\n",
      "total_backward_count 381810 real_backward_count 45174  11.832%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.145228/  1.549956, val:  60.42%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.64 seconds, 0.96 minutes\n",
      "total_backward_count 391600 real_backward_count 46181  11.793%\n",
      "fc layer 2 self.abs_max_out: 2775.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.112992/  1.440761, val:  62.92%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.43 seconds, 0.96 minutes\n",
      "total_backward_count 401390 real_backward_count 47187  11.756%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.100986/  1.432529, val:  70.83%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 58.52 seconds, 0.98 minutes\n",
      "total_backward_count 411180 real_backward_count 48149  11.710%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.107275/  1.441678, val:  67.50%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.09 seconds, 0.97 minutes\n",
      "total_backward_count 420970 real_backward_count 49115  11.667%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.125467/  1.416048, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.14 seconds, 0.97 minutes\n",
      "total_backward_count 430760 real_backward_count 50088  11.628%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.099101/  1.434396, val:  60.42%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.57 seconds, 0.96 minutes\n",
      "total_backward_count 440550 real_backward_count 51099  11.599%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.074646/  1.348894, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.77 seconds, 0.96 minutes\n",
      "total_backward_count 450340 real_backward_count 52087  11.566%\n",
      "fc layer 3 self.abs_max_out: 880.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.068819/  1.420607, val:  68.33%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.99 seconds, 0.97 minutes\n",
      "total_backward_count 460130 real_backward_count 53040  11.527%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.056914/  1.427996, val:  61.25%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.59 seconds, 0.94 minutes\n",
      "total_backward_count 469920 real_backward_count 53958  11.482%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.041524/  1.355774, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.45 seconds, 0.96 minutes\n",
      "total_backward_count 479710 real_backward_count 54859  11.436%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.055329/  1.361979, val:  63.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.82 seconds, 0.95 minutes\n",
      "total_backward_count 489500 real_backward_count 55798  11.399%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.051398/  1.381947, val:  62.92%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.38 seconds, 0.92 minutes\n",
      "total_backward_count 499290 real_backward_count 56714  11.359%\n",
      "fc layer 1 self.abs_max_out: 6339.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.050741/  1.374739, val:  69.58%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 56.07 seconds, 0.93 minutes\n",
      "total_backward_count 509080 real_backward_count 57635  11.321%\n",
      "fc layer 3 self.abs_max_out: 890.0\n",
      "lif layer 1 self.abs_max_v: 9924.5\n",
      "fc layer 1 self.abs_max_out: 6441.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.048947/  1.395068, val:  60.00%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 55.75 seconds, 0.93 minutes\n",
      "total_backward_count 518870 real_backward_count 58609  11.296%\n",
      "lif layer 1 self.abs_max_v: 10033.0\n",
      "lif layer 1 self.abs_max_v: 10180.5\n",
      "lif layer 1 self.abs_max_v: 10296.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.048666/  1.389323, val:  69.58%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 55.68 seconds, 0.93 minutes\n",
      "total_backward_count 528660 real_backward_count 59520  11.259%\n",
      "lif layer 1 self.abs_max_v: 10332.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.050347/  1.399801, val:  67.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.16 seconds, 0.94 minutes\n",
      "total_backward_count 538450 real_backward_count 60419  11.221%\n",
      "fc layer 2 self.abs_max_out: 2823.0\n",
      "lif layer 1 self.abs_max_v: 10332.5\n",
      "lif layer 1 self.abs_max_v: 10399.5\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.061901/  1.389283, val:  66.67%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.81 seconds, 0.93 minutes\n",
      "total_backward_count 548240 real_backward_count 61372  11.194%\n",
      "fc layer 3 self.abs_max_out: 920.0\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.042636/  1.359733, val:  71.25%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.36 seconds, 0.94 minutes\n",
      "total_backward_count 558030 real_backward_count 62280  11.161%\n",
      "lif layer 1 self.abs_max_v: 11062.0\n",
      "lif layer 1 self.abs_max_v: 11299.0\n",
      "lif layer 1 self.abs_max_v: 11534.5\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.030600/  1.371129, val:  66.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.20 seconds, 0.95 minutes\n",
      "total_backward_count 567820 real_backward_count 63164  11.124%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.036619/  1.378680, val:  67.50%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.41 seconds, 0.96 minutes\n",
      "total_backward_count 577610 real_backward_count 64075  11.093%\n",
      "fc layer 2 self.abs_max_out: 2910.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.064299/  1.439917, val:  62.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.03 seconds, 0.95 minutes\n",
      "total_backward_count 587400 real_backward_count 64972  11.061%\n",
      "fc layer 3 self.abs_max_out: 933.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.050522/  1.426087, val:  57.50%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.67 seconds, 0.96 minutes\n",
      "total_backward_count 597190 real_backward_count 65839  11.025%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.046421/  1.324395, val:  82.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 59.38 seconds, 0.99 minutes\n",
      "total_backward_count 606980 real_backward_count 66746  10.996%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.050407/  1.452480, val:  53.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.96 seconds, 0.98 minutes\n",
      "total_backward_count 616770 real_backward_count 67611  10.962%\n",
      "fc layer 2 self.abs_max_out: 2946.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.062366/  1.394164, val:  62.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.63 seconds, 0.98 minutes\n",
      "total_backward_count 626560 real_backward_count 68467  10.927%\n",
      "lif layer 1 self.abs_max_v: 11569.5\n",
      "lif layer 1 self.abs_max_v: 11607.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.057323/  1.388132, val:  70.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.13 seconds, 0.97 minutes\n",
      "total_backward_count 636350 real_backward_count 69285  10.888%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.057969/  1.408639, val:  74.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.74 seconds, 0.96 minutes\n",
      "total_backward_count 646140 real_backward_count 70164  10.859%\n",
      "fc layer 2 self.abs_max_out: 2969.0\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.076332/  1.415885, val:  75.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.50 seconds, 0.96 minutes\n",
      "total_backward_count 655930 real_backward_count 70981  10.821%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.063598/  1.381779, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.09 seconds, 0.97 minutes\n",
      "total_backward_count 665720 real_backward_count 71868  10.796%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.051705/  1.355035, val:  67.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.17 seconds, 0.97 minutes\n",
      "total_backward_count 675510 real_backward_count 72739  10.768%\n",
      "fc layer 1 self.abs_max_out: 6656.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.030105/  1.395550, val:  65.42%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 58.08 seconds, 0.97 minutes\n",
      "total_backward_count 685300 real_backward_count 73534  10.730%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.037739/  1.338288, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.65 seconds, 0.96 minutes\n",
      "total_backward_count 695090 real_backward_count 74434  10.709%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.064913/  1.423889, val:  62.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.29 seconds, 0.97 minutes\n",
      "total_backward_count 704880 real_backward_count 75245  10.675%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.042174/  1.335425, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.37 seconds, 0.92 minutes\n",
      "total_backward_count 714670 real_backward_count 76068  10.644%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.024019/  1.386607, val:  63.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.44 seconds, 0.94 minutes\n",
      "total_backward_count 724460 real_backward_count 76869  10.611%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.006784/  1.339441, val:  71.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.94 seconds, 0.93 minutes\n",
      "total_backward_count 734250 real_backward_count 77707  10.583%\n",
      "fc layer 3 self.abs_max_out: 970.0\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.971447/  1.258950, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.24 seconds, 0.94 minutes\n",
      "total_backward_count 744040 real_backward_count 78483  10.548%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.984219/  1.326984, val:  76.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.46 seconds, 0.92 minutes\n",
      "total_backward_count 753830 real_backward_count 79345  10.526%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.991847/  1.372782, val:  57.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.40 seconds, 0.92 minutes\n",
      "total_backward_count 763620 real_backward_count 80117  10.492%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.002700/  1.306473, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.90 seconds, 0.95 minutes\n",
      "total_backward_count 773410 real_backward_count 80946  10.466%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.997152/  1.290012, val:  81.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.84 seconds, 0.95 minutes\n",
      "total_backward_count 783200 real_backward_count 81748  10.438%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.998835/  1.308849, val:  75.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.16 seconds, 0.94 minutes\n",
      "total_backward_count 792990 real_backward_count 82531  10.408%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.988605/  1.372354, val:  59.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.68 seconds, 0.94 minutes\n",
      "total_backward_count 802780 real_backward_count 83308  10.377%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.996844/  1.304361, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.83 seconds, 0.95 minutes\n",
      "total_backward_count 812570 real_backward_count 84171  10.359%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.979235/  1.286305, val:  75.42%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 57.35 seconds, 0.96 minutes\n",
      "total_backward_count 822360 real_backward_count 85011  10.337%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.984271/  1.313664, val:  67.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.71 seconds, 0.96 minutes\n",
      "total_backward_count 832150 real_backward_count 85824  10.314%\n",
      "fc layer 3 self.abs_max_out: 1017.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.973618/  1.360128, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.11 seconds, 0.97 minutes\n",
      "total_backward_count 841940 real_backward_count 86655  10.292%\n",
      "fc layer 1 self.abs_max_out: 6741.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.959959/  1.331152, val:  67.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.85 seconds, 0.96 minutes\n",
      "total_backward_count 851730 real_backward_count 87444  10.267%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.970524/  1.287736, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.77 seconds, 0.96 minutes\n",
      "total_backward_count 861520 real_backward_count 88235  10.242%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.943362/  1.266459, val:  76.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 58.37 seconds, 0.97 minutes\n",
      "total_backward_count 871310 real_backward_count 89002  10.215%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.939736/  1.259648, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.59 seconds, 0.96 minutes\n",
      "total_backward_count 881100 real_backward_count 89781  10.190%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.938340/  1.280936, val:  72.08%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.82 seconds, 0.96 minutes\n",
      "total_backward_count 890890 real_backward_count 90577  10.167%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.905924/  1.214910, val:  82.92%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.54 seconds, 0.96 minutes\n",
      "total_backward_count 900680 real_backward_count 91320  10.139%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.917287/  1.242910, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.92 seconds, 0.97 minutes\n",
      "total_backward_count 910470 real_backward_count 92091  10.115%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.929237/  1.284019, val:  75.42%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.62 seconds, 0.94 minutes\n",
      "total_backward_count 920260 real_backward_count 92867  10.091%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.916168/  1.280208, val:  73.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.16 seconds, 0.94 minutes\n",
      "total_backward_count 930050 real_backward_count 93643  10.069%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.933975/  1.238245, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.55 seconds, 0.94 minutes\n",
      "total_backward_count 939840 real_backward_count 94385  10.043%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.893796/  1.240481, val:  74.58%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.20 seconds, 0.94 minutes\n",
      "total_backward_count 949630 real_backward_count 95123  10.017%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.885807/  1.210095, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.90 seconds, 0.93 minutes\n",
      "total_backward_count 959420 real_backward_count 95825   9.988%\n",
      "fc layer 2 self.abs_max_out: 3181.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.845973/  1.227429, val:  76.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.03 seconds, 0.95 minutes\n",
      "total_backward_count 969210 real_backward_count 96481   9.955%\n",
      "lif layer 1 self.abs_max_v: 11657.5\n",
      "lif layer 1 self.abs_max_v: 11846.0\n",
      "lif layer 1 self.abs_max_v: 11913.0\n",
      "lif layer 1 self.abs_max_v: 12193.5\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.885222/  1.284319, val:  73.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 55.84 seconds, 0.93 minutes\n",
      "total_backward_count 979000 real_backward_count 97176   9.926%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.869909/  1.212737, val:  73.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.35 seconds, 0.96 minutes\n",
      "total_backward_count 988790 real_backward_count 97911   9.902%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.893621/  1.193786, val:  87.50%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 57.89 seconds, 0.96 minutes\n",
      "total_backward_count 998580 real_backward_count 98580   9.872%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.888656/  1.214767, val:  77.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.88 seconds, 0.96 minutes\n",
      "total_backward_count 1008370 real_backward_count 99234   9.841%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.876145/  1.237316, val:  70.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 58.27 seconds, 0.97 minutes\n",
      "total_backward_count 1018160 real_backward_count 99931   9.815%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.871381/  1.209386, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.18 seconds, 0.95 minutes\n",
      "total_backward_count 1027950 real_backward_count 100608   9.787%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.876953/  1.248788, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.92 seconds, 0.97 minutes\n",
      "total_backward_count 1037740 real_backward_count 101299   9.762%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.863805/  1.259700, val:  72.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.28 seconds, 0.95 minutes\n",
      "total_backward_count 1047530 real_backward_count 101971   9.734%\n",
      "fc layer 1 self.abs_max_out: 7142.0\n",
      "fc layer 3 self.abs_max_out: 1023.0\n",
      "lif layer 1 self.abs_max_v: 12386.0\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.862250/  1.218109, val:  66.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.95 seconds, 0.97 minutes\n",
      "total_backward_count 1057320 real_backward_count 102681   9.711%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.859861/  1.220525, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.24 seconds, 0.95 minutes\n",
      "total_backward_count 1067110 real_backward_count 103371   9.687%\n",
      "lif layer 1 self.abs_max_v: 12431.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.886352/  1.205750, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.46 seconds, 0.96 minutes\n",
      "total_backward_count 1076900 real_backward_count 104039   9.661%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.871092/  1.188014, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.88 seconds, 0.96 minutes\n",
      "total_backward_count 1086690 real_backward_count 104690   9.634%\n",
      "lif layer 1 self.abs_max_v: 12471.5\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.861855/  1.212274, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.40 seconds, 0.94 minutes\n",
      "total_backward_count 1096480 real_backward_count 105380   9.611%\n",
      "lif layer 1 self.abs_max_v: 12508.0\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.864868/  1.186674, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 57.06 seconds, 0.95 minutes\n",
      "total_backward_count 1106270 real_backward_count 106045   9.586%\n",
      "fc layer 3 self.abs_max_out: 1065.0\n",
      "fc layer 3 self.abs_max_out: 1096.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.852101/  1.198557, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.87 seconds, 0.95 minutes\n",
      "total_backward_count 1116060 real_backward_count 106719   9.562%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.858018/  1.283593, val:  72.92%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 56.97 seconds, 0.95 minutes\n",
      "total_backward_count 1125850 real_backward_count 107421   9.541%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.855505/  1.212991, val:  82.08%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 55.85 seconds, 0.93 minutes\n",
      "total_backward_count 1135640 real_backward_count 108109   9.520%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.852118/  1.232362, val:  71.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 56.02 seconds, 0.93 minutes\n",
      "total_backward_count 1145430 real_backward_count 108755   9.495%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2, 4, 6, 8, 10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'cija8jrg'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
