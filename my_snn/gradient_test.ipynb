{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3058,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from modules.data_loader import *\n",
    "from modules.network import *\n",
    "from modules.neuron import *\n",
    "from modules.synapse import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3051,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYNAPSE_CONV_gra_test(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(SYNAPSE_CONV_gra_test, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_channels))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        return SYNAPSE_CONV_METHOD_gra_test.apply(spike, self.weight, self.bias, self.stride, self.padding)\n",
    "\n",
    "class SYNAPSE_CONV_METHOD_gra_test(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, weight, bias, stride=1, padding=1):\n",
    "        ctx.save_for_backward(spike_one_time, weight, bias, torch.tensor([stride], requires_grad=False), torch.tensor([padding], requires_grad=False))\n",
    "        return F.conv2d(spike_one_time, weight, bias=bias, stride=stride, padding=padding)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        spike_one_time, weight, bias, stride, padding = ctx.saved_tensors\n",
    "        stride=stride.item()\n",
    "        padding=padding.item()\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = F.conv_transpose2d(grad_output_current_clone, weight, stride=stride, padding=padding)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(spike_one_time, weight.shape, grad_output_current_clone,\n",
    "                                                    stride=stride, padding=padding)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output_current_clone.sum((0, -1, -2))\n",
    "\n",
    "        # print('grad_input_spike_conv', grad_input_spike)\n",
    "        # print('grad_weight_conv', grad_weight)\n",
    "        # print('grad_bias_conv', grad_bias)\n",
    "        # print('grad_input_spike_conv', ctx.needs_input_grad[0])\n",
    "        # print('grad_weight_conv', ctx.needs_input_grad[2])\n",
    "        # print('grad_bias_conv', ctx.needs_input_grad[3])\n",
    "\n",
    "        return grad_input_spike, grad_weight, grad_bias, None, None\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3052,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward outputs are equal:  True\n",
      "Input gradients are equal:  True\n",
      "Weight gradients are equal:  True\n",
      "Bias gradients are equal:  True\n"
     ]
    }
   ],
   "source": [
    "batch = 3\n",
    "in_channels = 2\n",
    "out_channels = 4\n",
    "\n",
    "image_size = 5\n",
    "\n",
    "input_tensor = torch.randn(batch, in_channels, image_size, image_size, requires_grad=True)\n",
    "\n",
    "\n",
    "# Define custom convolution layer\n",
    "custom_conv = SYNAPSE_CONV_gra_test(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "custom_conv_output = custom_conv(input_tensor)\n",
    "\n",
    "# Define standard convolution layer with the same weights and biases as custom layer\n",
    "standard_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "standard_conv.weight = nn.Parameter(custom_conv.weight.data.clone())\n",
    "standard_conv.bias = nn.Parameter(custom_conv.bias.data.clone())\n",
    "standard_conv_output = standard_conv(input_tensor)\n",
    "\n",
    "# Compare forward outputs\n",
    "print(\"Forward outputs are equal: \", torch.allclose(custom_conv_output, standard_conv_output))\n",
    "\n",
    "# Compute gradients\n",
    "grad_output = torch.randn_like(custom_conv_output)\n",
    "custom_conv_output.backward(grad_output, retain_graph=True)\n",
    "standard_conv_output.backward(grad_output)\n",
    "\n",
    "# Compare gradients w.r.t. input\n",
    "print(\"Input gradients are equal: \", torch.allclose(input_tensor.grad, input_tensor.grad))\n",
    "\n",
    "# Compare gradients w.r.t. weights\n",
    "print(\"Weight gradients are equal: \", torch.allclose(custom_conv.weight.grad, standard_conv.weight.grad))\n",
    "\n",
    "# Compare gradients w.r.t. biases\n",
    "print(\"Bias gradients are equal: \", torch.allclose(custom_conv.bias.grad, standard_conv.bias.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3053,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward outputs are equal:  True\n",
      "Input gradients are equal:  True\n",
      "First layer weight gradients are equal:  True\n",
      "First layer bias gradients are equal:  True\n",
      "Second layer weight gradients are equal:  True\n",
      "Second layer bias gradients are equal:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CustomTwoLayerConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(CustomTwoLayerConvNet, self).__init__()\n",
    "        self.layer1 = SYNAPSE_CONV_gra_test(in_channels, hidden_channels, kernel_size, stride, padding)\n",
    "        self.layer2 = SYNAPSE_CONV_gra_test(hidden_channels, out_channels, kernel_size, stride, padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)  # Adding a non-linearity for completeness\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "class StandardTwoLayerConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(StandardTwoLayerConvNet, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(in_channels, hidden_channels, kernel_size, stride, padding)\n",
    "        self.layer2 = nn.Conv2d(hidden_channels, out_channels, kernel_size, stride, padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)  # Adding a non-linearity for completeness\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "batch = 1\n",
    "in_channels = 5\n",
    "hidden_channels = 5\n",
    "out_channels = 5\n",
    "image_size = 5\n",
    "\n",
    "\n",
    "# Define input tensor\n",
    "input_tensor = torch.randn(batch, in_channels, image_size, image_size, requires_grad=True)\n",
    "\n",
    "# Define custom two-layer convolution model\n",
    "custom_model = CustomTwoLayerConvNet(in_channels, hidden_channels, out_channels)\n",
    "custom_model_output = custom_model(input_tensor)\n",
    "\n",
    "# Define standard two-layer convolution model with the same weights and biases as custom model\n",
    "standard_model = StandardTwoLayerConvNet(in_channels, hidden_channels, out_channels)\n",
    "standard_model.layer1.weight = nn.Parameter(custom_model.layer1.weight.data.clone())\n",
    "standard_model.layer1.bias = nn.Parameter(custom_model.layer1.bias.data.clone())\n",
    "standard_model.layer2.weight = nn.Parameter(custom_model.layer2.weight.data.clone())\n",
    "standard_model.layer2.bias = nn.Parameter(custom_model.layer2.bias.data.clone())\n",
    "standard_model_output = standard_model(input_tensor)\n",
    "\n",
    "# Compare forward outputs\n",
    "print(\"Forward outputs are equal: \", torch.allclose(custom_model_output, standard_model_output))\n",
    "\n",
    "# Compute gradients\n",
    "grad_output = torch.randn_like(custom_model_output)\n",
    "custom_model_output.backward(grad_output, retain_graph=True)\n",
    "standard_model_output.backward(grad_output)\n",
    "\n",
    "input_tensor.grad= torch.round(input_tensor.grad * 1e5) / 1e5\n",
    "custom_model.layer1.weight.grad= torch.round(custom_model.layer1.weight.grad * 1e5) / 1e5\n",
    "standard_model.layer1.weight.grad= torch.round(standard_model.layer1.weight.grad * 1e5) / 1e5\n",
    "custom_model.layer1.bias.grad= torch.round(custom_model.layer1.bias.grad * 1e5) / 1e5\n",
    "standard_model.layer1.bias.grad= torch.round(standard_model.layer1.bias.grad * 1e5) / 1e5\n",
    "custom_model.layer2.weight.grad= torch.round(custom_model.layer2.weight.grad * 1e5) / 1e5\n",
    "standard_model.layer2.weight.grad= torch.round(standard_model.layer2.weight.grad * 1e5) / 1e5\n",
    "custom_model.layer2.bias.grad= torch.round(custom_model.layer2.bias.grad * 1e5) / 1e5\n",
    "standard_model.layer2.bias.grad= torch.round(standard_model.layer2.bias.grad * 1e5) / 1e5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compare gradients w.r.t. input\n",
    "print(\"Input gradients are equal: \", torch.allclose(input_tensor.grad, input_tensor.grad))\n",
    "\n",
    "# Compare gradients w.r.t. first layer weights\n",
    "print(\"First layer weight gradients are equal: \", torch.allclose(custom_model.layer1.weight.grad, standard_model.layer1.weight.grad))\n",
    "\n",
    "# Compare gradients w.r.t. first layer biases\n",
    "print(\"First layer bias gradients are equal: \", torch.allclose(custom_model.layer1.bias.grad, standard_model.layer1.bias.grad))\n",
    "\n",
    "# Compare gradients w.r.t. second layer weights\n",
    "print(\"Second layer weight gradients are equal: \", torch.allclose(custom_model.layer2.weight.grad, standard_model.layer2.weight.grad))\n",
    "\n",
    "# Compare gradients w.r.t. second layer biases\n",
    "print(\"Second layer bias gradients are equal: \", torch.allclose(custom_model.layer2.bias.grad, standard_model.layer2.bias.grad))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('custom_model.layer1.weight.grad', custom_model.layer1.weight.grad.size())\n",
    "# print('standard_model.layer1.weight.grad', standard_model.layer1.weight.grad.size())\n",
    "# print('custom_model.layer1.weight.grad', custom_model.layer1.weight.grad)\n",
    "# print('standard_model.layer1.weight.grad', standard_model.layer1.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3054,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer weight gradients are equal:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"First layer weight gradients are equal: \", torch.allclose(custom_model.layer1.weight.grad, standard_model.layer1.weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -0.6744, -21.4038,   0.1721],\n",
       "          [ -5.7564,  -8.3224, -28.2213],\n",
       "          [-17.3416,  -2.1030,  -1.7222]],\n",
       "\n",
       "         [[-15.6547,  -6.5354,  -4.7486],\n",
       "          [ -0.1185,  -7.5696,  -1.3348],\n",
       "          [  7.8487,  33.8435,   4.6007]],\n",
       "\n",
       "         [[ 15.5353, -25.6578,   0.7505],\n",
       "          [ 44.0459,   9.1942,  27.1311],\n",
       "          [ -4.8918,  30.1869,  -6.4636]],\n",
       "\n",
       "         [[ -4.4907, -34.3023,  11.5691],\n",
       "          [-26.3487, -20.6629,  15.6554],\n",
       "          [-10.1458,  -4.0989,  15.2669]],\n",
       "\n",
       "         [[ 13.6383,  -7.3613,  11.5022],\n",
       "          [  9.8657,   0.8761,  17.5641],\n",
       "          [-16.2694,   8.6354,   3.4830]]],\n",
       "\n",
       "\n",
       "        [[[ 13.0467,  -8.8762, -18.4378],\n",
       "          [  5.6352,  33.0969, -13.9500],\n",
       "          [-28.1104,   6.1755,  22.3356]],\n",
       "\n",
       "         [[ -8.7892, -18.6530,  -9.2455],\n",
       "          [ 35.9025,  15.3442,   7.3019],\n",
       "          [  7.5832,  28.8663,  10.8979]],\n",
       "\n",
       "         [[  8.0235, -16.2218, -16.0273],\n",
       "          [ 39.4245,  18.9508, -31.5385],\n",
       "          [-31.0397,  54.3120,  20.4136]],\n",
       "\n",
       "         [[ -7.2389,  -2.5678,  10.8217],\n",
       "          [  5.4148, -13.3726,  -0.1557],\n",
       "          [ 22.4276,  22.2106,  24.6068]],\n",
       "\n",
       "         [[ -3.0686,   3.5221,  -8.7764],\n",
       "          [ -5.6214,   2.0203, -12.9087],\n",
       "          [  6.2924,   2.2031,  29.5844]]],\n",
       "\n",
       "\n",
       "        [[[ -6.0152,  -8.9994,   9.1030],\n",
       "          [  9.7171, -15.2237, -20.6399],\n",
       "          [  2.8646,  -8.1324, -29.0755]],\n",
       "\n",
       "         [[-34.6206, -44.1248,  -9.0569],\n",
       "          [ -3.8001,  28.2475,  14.9731],\n",
       "          [  4.9406, -25.1922,  -3.0072]],\n",
       "\n",
       "         [[ 14.4150,  22.1162,  50.2080],\n",
       "          [ -8.6576,   9.8517,  19.0411],\n",
       "          [ 30.9426,  -1.4203,  -6.0445]],\n",
       "\n",
       "         [[ -0.6027,   1.9522, -12.7123],\n",
       "          [ 17.3237,   1.4947,  -6.0024],\n",
       "          [ -8.4524, -12.5923,   5.3773]],\n",
       "\n",
       "         [[  5.3422,  23.4293,  17.2973],\n",
       "          [  2.7916,  -6.2940,   0.6187],\n",
       "          [ -8.2446,  -2.3210,  -7.0795]]],\n",
       "\n",
       "\n",
       "        [[[ 17.5780, -14.0316,  -5.2967],\n",
       "          [  4.0386,  -8.7855, -10.4085],\n",
       "          [-12.9740, -17.7072,  45.5368]],\n",
       "\n",
       "         [[ 18.9589,  40.9434,  10.8224],\n",
       "          [-14.4439, -20.2969, -34.5669],\n",
       "          [ -8.1677,  32.0701,   6.1962]],\n",
       "\n",
       "         [[-37.0735, -18.1121, -36.8626],\n",
       "          [ 28.1633,  42.5604, -10.8558],\n",
       "          [-47.5918,  12.7847,  31.6411]],\n",
       "\n",
       "         [[-17.1938,  12.2645, -18.8404],\n",
       "          [-27.8212,  13.6866, -37.9718],\n",
       "          [-29.1448,  47.7845, -49.6043]],\n",
       "\n",
       "         [[-10.0269,  -6.0637, -10.8277],\n",
       "          [  3.9965,  21.6478,  20.5336],\n",
       "          [ -9.2351,  22.5700, -21.2711]]],\n",
       "\n",
       "\n",
       "        [[[ -5.4726,   6.4014,  -1.6981],\n",
       "          [ -5.7436, -10.3349,   8.6097],\n",
       "          [ -3.1362,  24.2227,   3.9382]],\n",
       "\n",
       "         [[  0.0698, -11.5265,   5.3137],\n",
       "          [  0.8535,   7.4640,  16.7654],\n",
       "          [ 12.5671,   2.6215,  -6.2552]],\n",
       "\n",
       "         [[  5.2015,  -2.1797,  14.9558],\n",
       "          [ -4.8999,   2.0789,  -1.7470],\n",
       "          [ 16.4558,  25.0402,   5.0225]],\n",
       "\n",
       "         [[ -0.6075, -11.1420,  10.5168],\n",
       "          [-11.3811,   9.8430,  -3.2983],\n",
       "          [ 18.4229,   6.7680,   6.1895]],\n",
       "\n",
       "         [[  1.2098,  -0.1701,   0.5324],\n",
       "          [ -6.4249,   6.7437,  -8.5302],\n",
       "          [  6.0040,  -2.1343,   6.7859]]]])"
      ]
     },
     "execution_count": 3055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.layer1.weight.grad= torch.round(custom_model.layer1.weight.grad * 1e5) / 1e5\n",
    "custom_model.layer1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -0.6744, -21.4038,   0.1721],\n",
       "          [ -5.7564,  -8.3224, -28.2213],\n",
       "          [-17.3416,  -2.1030,  -1.7222]],\n",
       "\n",
       "         [[-15.6547,  -6.5354,  -4.7486],\n",
       "          [ -0.1185,  -7.5696,  -1.3348],\n",
       "          [  7.8487,  33.8435,   4.6007]],\n",
       "\n",
       "         [[ 15.5353, -25.6578,   0.7505],\n",
       "          [ 44.0459,   9.1942,  27.1311],\n",
       "          [ -4.8918,  30.1869,  -6.4636]],\n",
       "\n",
       "         [[ -4.4907, -34.3023,  11.5691],\n",
       "          [-26.3487, -20.6629,  15.6554],\n",
       "          [-10.1458,  -4.0989,  15.2669]],\n",
       "\n",
       "         [[ 13.6383,  -7.3613,  11.5022],\n",
       "          [  9.8657,   0.8761,  17.5641],\n",
       "          [-16.2694,   8.6354,   3.4830]]],\n",
       "\n",
       "\n",
       "        [[[ 13.0467,  -8.8762, -18.4378],\n",
       "          [  5.6352,  33.0969, -13.9500],\n",
       "          [-28.1104,   6.1755,  22.3356]],\n",
       "\n",
       "         [[ -8.7892, -18.6530,  -9.2455],\n",
       "          [ 35.9025,  15.3442,   7.3019],\n",
       "          [  7.5832,  28.8663,  10.8979]],\n",
       "\n",
       "         [[  8.0235, -16.2218, -16.0273],\n",
       "          [ 39.4245,  18.9508, -31.5385],\n",
       "          [-31.0397,  54.3120,  20.4136]],\n",
       "\n",
       "         [[ -7.2389,  -2.5678,  10.8217],\n",
       "          [  5.4148, -13.3726,  -0.1557],\n",
       "          [ 22.4276,  22.2106,  24.6068]],\n",
       "\n",
       "         [[ -3.0686,   3.5221,  -8.7764],\n",
       "          [ -5.6214,   2.0203, -12.9087],\n",
       "          [  6.2924,   2.2031,  29.5844]]],\n",
       "\n",
       "\n",
       "        [[[ -6.0152,  -8.9994,   9.1030],\n",
       "          [  9.7171, -15.2237, -20.6399],\n",
       "          [  2.8646,  -8.1324, -29.0755]],\n",
       "\n",
       "         [[-34.6206, -44.1248,  -9.0569],\n",
       "          [ -3.8001,  28.2475,  14.9731],\n",
       "          [  4.9406, -25.1922,  -3.0072]],\n",
       "\n",
       "         [[ 14.4150,  22.1162,  50.2080],\n",
       "          [ -8.6576,   9.8517,  19.0411],\n",
       "          [ 30.9426,  -1.4203,  -6.0445]],\n",
       "\n",
       "         [[ -0.6027,   1.9522, -12.7123],\n",
       "          [ 17.3237,   1.4947,  -6.0024],\n",
       "          [ -8.4524, -12.5923,   5.3774]],\n",
       "\n",
       "         [[  5.3422,  23.4293,  17.2973],\n",
       "          [  2.7916,  -6.2940,   0.6187],\n",
       "          [ -8.2446,  -2.3210,  -7.0795]]],\n",
       "\n",
       "\n",
       "        [[[ 17.5780, -14.0316,  -5.2967],\n",
       "          [  4.0386,  -8.7855, -10.4085],\n",
       "          [-12.9740, -17.7072,  45.5368]],\n",
       "\n",
       "         [[ 18.9589,  40.9434,  10.8224],\n",
       "          [-14.4439, -20.2969, -34.5669],\n",
       "          [ -8.1677,  32.0701,   6.1962]],\n",
       "\n",
       "         [[-37.0735, -18.1121, -36.8626],\n",
       "          [ 28.1633,  42.5604, -10.8558],\n",
       "          [-47.5918,  12.7847,  31.6411]],\n",
       "\n",
       "         [[-17.1938,  12.2645, -18.8404],\n",
       "          [-27.8212,  13.6866, -37.9718],\n",
       "          [-29.1448,  47.7845, -49.6043]],\n",
       "\n",
       "         [[-10.0269,  -6.0637, -10.8277],\n",
       "          [  3.9965,  21.6478,  20.5336],\n",
       "          [ -9.2351,  22.5700, -21.2711]]],\n",
       "\n",
       "\n",
       "        [[[ -5.4726,   6.4014,  -1.6981],\n",
       "          [ -5.7436, -10.3349,   8.6097],\n",
       "          [ -3.1362,  24.2227,   3.9382]],\n",
       "\n",
       "         [[  0.0698, -11.5265,   5.3137],\n",
       "          [  0.8535,   7.4640,  16.7654],\n",
       "          [ 12.5671,   2.6215,  -6.2552]],\n",
       "\n",
       "         [[  5.2014,  -2.1797,  14.9558],\n",
       "          [ -4.8999,   2.0789,  -1.7470],\n",
       "          [ 16.4558,  25.0402,   5.0225]],\n",
       "\n",
       "         [[ -0.6075, -11.1420,  10.5168],\n",
       "          [-11.3811,   9.8430,  -3.2983],\n",
       "          [ 18.4229,   6.7680,   6.1895]],\n",
       "\n",
       "         [[  1.2098,  -0.1701,   0.5324],\n",
       "          [ -6.4249,   6.7437,  -8.5302],\n",
       "          [  6.0040,  -2.1343,   6.7859]]]])"
      ]
     },
     "execution_count": 3056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_model.layer1.weight.grad = torch.round(standard_model.layer1.weight.grad * 1e5) / 1e5\n",
    "standard_model.layer1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3059,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6744)"
      ]
     },
     "execution_count": 3059,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.layer1.weight.grad[0][0][0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3060,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6744)"
      ]
     },
     "execution_count": 3060,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_model.layer1.weight.grad[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3057,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True, False]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True, False]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[False,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True],\n",
       "          [ True,  True,  True],\n",
       "          [ True,  True,  True]]]])"
      ]
     },
     "execution_count": 3057,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.layer1.weight.grad == standard_model.layer1.weight.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
