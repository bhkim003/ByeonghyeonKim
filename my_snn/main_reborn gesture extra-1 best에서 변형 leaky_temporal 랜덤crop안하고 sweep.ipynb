{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40448/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    \n",
    "                    random_select_ratio = 4,\n",
    "                    leaky_temporal_filter= 1.0,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "\n",
    "                now_T = inputs.shape[1]\n",
    "                window_T = temporal_filter * TIME  # Ìïú Î∞ïÏä§(time window) Í∏∏Ïù¥\n",
    "\n",
    "                # 1) Î∞ïÏä§ Í∞úÏàò Í≥ÑÏÇ∞\n",
    "                num_windows = now_T // window_T   # Îî± Îñ®Ïñ¥ÏßÄÎäî Î∞ïÏä§ Í∞úÏàò\n",
    "\n",
    "                assert num_windows >= 1\n",
    "\n",
    "                # 2) ÎÇ®Îäî ÎÇòÎ®∏ÏßÄ timestepÏùÄ ÏûòÎùºÎ≤ÑÎ¶¨Í≥†, Îî± Î∞ïÏä§ Îã®ÏúÑÍπåÏßÄÎßå ÏÇ¨Ïö©\n",
    "                valid_T = num_windows * window_T\n",
    "                inputs = inputs[:, :valid_T]   # shape: [1, valid_T, C, H, W]\n",
    "\n",
    "                # 3) (B=1, num_windows, window_T, ...) ÌòïÌÉúÎ°ú reshape\n",
    "                B, T, *rest = inputs.shape  # B=1\n",
    "                inputs_reshaped = inputs.view(B, num_windows, window_T, *rest)\n",
    "                # inputs_reshaped: [1, num_windows, window_T, C, H, W] Í∞ÄÏ†ï\n",
    "\n",
    "                # 4) Í∞Å Î∞ïÏä§Î≥Ñ Ìï© (Ïä§ÌååÏù¥ÌÅ¨ Í∞úÏàò) Í≥ÑÏÇ∞\n",
    "                #    dim=(2,3,4,5): window_T, C, H, W Ï†ÑÏ≤¥ Ìï©\n",
    "                window_sums = inputs_reshaped.sum(dim=tuple(range(2, inputs_reshaped.dim())))  # shape: [1, num_windows]\n",
    "                window_sums = window_sums[0]  # [num_windows]\n",
    "\n",
    "                # 5) ÏÉÅÏúÑ NÍ∞ú Î∞ïÏä§ ÏÑ†ÌÉù\n",
    "                #    random_select_ratio ÎπÑÏú®ÎßåÌÅº ÏÉÅÏúÑ Î∞ïÏä§Îßå ÌõÑÎ≥¥Î°ú ÏîÄ\n",
    "                # print(f'num_windows: {num_windows}, random_select_ratio: {random_select_ratio}')\n",
    "                N = min(num_windows, round(random_select_ratio))\n",
    "                # N = max(1, min(num_windows, round(num_windows * random_select_ratio)))\n",
    "                topk_vals, topk_idx = torch.topk(window_sums, k=N)  # top-N Î∞ïÏä§ Ïù∏Îç±Ïä§\n",
    "                # print(f'N: {N}, topk_vals: {topk_vals}, topk_idx: {topk_idx}')\n",
    "\n",
    "                # 6) ÏÉÅÏúÑ NÍ∞ú Î∞ïÏä§ Ï§ëÏóêÏÑú ÎûúÎç§ÌïòÍ≤å ÌïòÎÇò ÏÑ†ÌÉù\n",
    "                #    (python random ÎòêÎäî torch.randperm Îëò Îã§ Í∞ÄÎä•)\n",
    "                rand_pos = random.randint(0, N - 1)\n",
    "                chosen_win_idx = topk_idx[rand_pos].item()  # 0 ~ num_windows-1 Ï§ë ÌïòÎÇò\n",
    "                # print(f'chosen_win_idx: {chosen_win_idx}, topk_vals: {topk_vals}, topk_idx: {topk_idx}')\n",
    "\n",
    "                # 7) ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Í∑∏ Î∞ïÏä§ Íµ¨Í∞ÑÎßå ÏÇ¨Ïö©\n",
    "                start_idx = chosen_win_idx * window_T\n",
    "                end_idx = start_idx + window_T\n",
    "                inputs = inputs[:, start_idx:end_idx]\n",
    "                # shape: [1, window_T, C, H, W]\n",
    "\n",
    "                # print(f'inputs.shape after random select: {inputs.shape}')\n",
    "\n",
    "\n",
    "                if temporal_filter_accumulation == False:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # print(f'inputs.shape: {inputs.shape}')\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        for ttt in range(temporal_filter):\n",
    "                            if ttt == 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                        slice_bucket.append(slice_concat)\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                #     inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if labels.item() == 6:\n",
    "            #     # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            #     ##############################################################################################\n",
    "            #     dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            #     #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if temporal_filter_accumulation == False:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    for ttt in range(temporal_filter):\n",
    "                                        if ttt == 0:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "                            # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                            #     inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = True, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 random_select_ratio = 4,\n",
    "#                 leaky_temporal_filter= 0.0,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gm6vzct1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251126_213234-gm6vzct1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gm6vzct1' target=\"_blank\">peach-sweep-319</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gm6vzct1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gm6vzct1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251126_213241_513', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 120.0\n",
      "lif layer 1 self.abs_max_v: 120.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 190.0\n",
      "fc layer 1 self.abs_max_out: 211.0\n",
      "lif layer 1 self.abs_max_v: 306.0\n",
      "fc layer 1 self.abs_max_out: 215.0\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "fc layer 1 self.abs_max_out: 319.0\n",
      "lif layer 1 self.abs_max_v: 435.0\n",
      "fc layer 2 self.abs_max_out: 108.0\n",
      "lif layer 2 self.abs_max_v: 107.0\n",
      "fc layer 1 self.abs_max_out: 357.0\n",
      "fc layer 2 self.abs_max_out: 148.0\n",
      "lif layer 2 self.abs_max_v: 173.5\n",
      "fc layer 1 self.abs_max_out: 462.0\n",
      "lif layer 1 self.abs_max_v: 462.0\n",
      "lif layer 2 self.abs_max_v: 209.5\n",
      "fc layer 1 self.abs_max_out: 518.0\n",
      "lif layer 1 self.abs_max_v: 518.0\n",
      "fc layer 2 self.abs_max_out: 179.0\n",
      "lif layer 2 self.abs_max_v: 251.5\n",
      "fc layer 2 self.abs_max_out: 214.0\n",
      "lif layer 2 self.abs_max_v: 338.5\n",
      "lif layer 1 self.abs_max_v: 547.5\n",
      "fc layer 2 self.abs_max_out: 233.0\n",
      "fc layer 3 self.abs_max_out: 24.0\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "fc layer 1 self.abs_max_out: 709.0\n",
      "lif layer 1 self.abs_max_v: 709.0\n",
      "fc layer 2 self.abs_max_out: 262.0\n",
      "lif layer 2 self.abs_max_v: 387.0\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 1 self.abs_max_out: 1188.0\n",
      "lif layer 1 self.abs_max_v: 1188.0\n",
      "fc layer 2 self.abs_max_out: 320.0\n",
      "lif layer 2 self.abs_max_v: 475.0\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "fc layer 1 self.abs_max_out: 1206.0\n",
      "lif layer 1 self.abs_max_v: 1206.0\n",
      "fc layer 2 self.abs_max_out: 370.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 2 self.abs_max_out: 461.0\n",
      "lif layer 2 self.abs_max_v: 555.5\n",
      "fc layer 2 self.abs_max_out: 471.0\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 2 self.abs_max_out: 482.0\n",
      "fc layer 3 self.abs_max_out: 93.0\n",
      "fc layer 1 self.abs_max_out: 1209.0\n",
      "lif layer 1 self.abs_max_v: 1209.0\n",
      "fc layer 2 self.abs_max_out: 623.0\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "lif layer 2 self.abs_max_v: 646.0\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "fc layer 2 self.abs_max_out: 709.0\n",
      "lif layer 2 self.abs_max_v: 709.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 1 self.abs_max_out: 1700.0\n",
      "lif layer 1 self.abs_max_v: 1700.0\n",
      "fc layer 2 self.abs_max_out: 761.0\n",
      "lif layer 2 self.abs_max_v: 761.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "lif layer 2 self.abs_max_v: 783.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 2 self.abs_max_out: 801.0\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "fc layer 1 self.abs_max_out: 1863.0\n",
      "lif layer 1 self.abs_max_v: 1863.0\n",
      "fc layer 2 self.abs_max_out: 863.0\n",
      "lif layer 2 self.abs_max_v: 863.0\n",
      "fc layer 2 self.abs_max_out: 879.0\n",
      "lif layer 2 self.abs_max_v: 879.0\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "fc layer 3 self.abs_max_out: 180.0\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "lif layer 2 self.abs_max_v: 982.0\n",
      "lif layer 2 self.abs_max_v: 996.0\n",
      "fc layer 2 self.abs_max_out: 883.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "lif layer 2 self.abs_max_v: 1170.5\n",
      "fc layer 2 self.abs_max_out: 897.0\n",
      "fc layer 2 self.abs_max_out: 926.0\n",
      "lif layer 2 self.abs_max_v: 1176.5\n",
      "fc layer 2 self.abs_max_out: 981.0\n",
      "fc layer 3 self.abs_max_out: 226.0\n",
      "lif layer 2 self.abs_max_v: 1179.0\n",
      "fc layer 2 self.abs_max_out: 988.0\n",
      "fc layer 2 self.abs_max_out: 1023.0\n",
      "fc layer 2 self.abs_max_out: 1073.0\n",
      "lif layer 2 self.abs_max_v: 1194.0\n",
      "lif layer 2 self.abs_max_v: 1234.5\n",
      "lif layer 2 self.abs_max_v: 1288.0\n",
      "fc layer 2 self.abs_max_out: 1132.0\n",
      "fc layer 2 self.abs_max_out: 1168.0\n",
      "fc layer 2 self.abs_max_out: 1170.0\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "lif layer 2 self.abs_max_v: 1293.0\n",
      "lif layer 2 self.abs_max_v: 1315.5\n",
      "lif layer 2 self.abs_max_v: 1481.0\n",
      "lif layer 2 self.abs_max_v: 1515.0\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "fc layer 3 self.abs_max_out: 234.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "fc layer 2 self.abs_max_out: 1534.0\n",
      "lif layer 2 self.abs_max_v: 1534.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 1572.0\n",
      "lif layer 2 self.abs_max_v: 1683.0\n",
      "lif layer 2 self.abs_max_v: 1692.5\n",
      "fc layer 2 self.abs_max_out: 1572.0\n",
      "lif layer 2 self.abs_max_v: 1714.0\n",
      "fc layer 1 self.abs_max_out: 1953.0\n",
      "lif layer 1 self.abs_max_v: 1953.0\n",
      "lif layer 2 self.abs_max_v: 1714.5\n",
      "lif layer 2 self.abs_max_v: 1751.5\n",
      "fc layer 1 self.abs_max_out: 2169.0\n",
      "lif layer 1 self.abs_max_v: 2169.0\n",
      "fc layer 2 self.abs_max_out: 1598.0\n",
      "fc layer 1 self.abs_max_out: 2231.0\n",
      "lif layer 1 self.abs_max_v: 2231.0\n",
      "fc layer 2 self.abs_max_out: 1638.0\n",
      "fc layer 2 self.abs_max_out: 1656.0\n",
      "fc layer 2 self.abs_max_out: 1743.0\n",
      "fc layer 2 self.abs_max_out: 1772.0\n",
      "lif layer 2 self.abs_max_v: 1772.0\n",
      "fc layer 2 self.abs_max_out: 1802.0\n",
      "lif layer 2 self.abs_max_v: 1802.0\n",
      "fc layer 2 self.abs_max_out: 1832.0\n",
      "lif layer 2 self.abs_max_v: 1832.0\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "lif layer 2 self.abs_max_v: 1844.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "lif layer 2 self.abs_max_v: 1890.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 3 self.abs_max_out: 373.0\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 2 self.abs_max_out: 1903.0\n",
      "lif layer 2 self.abs_max_v: 1903.0\n",
      "fc layer 2 self.abs_max_out: 1962.0\n",
      "lif layer 2 self.abs_max_v: 1962.0\n",
      "fc layer 2 self.abs_max_out: 2045.0\n",
      "lif layer 2 self.abs_max_v: 2045.0\n",
      "fc layer 2 self.abs_max_out: 2092.0\n",
      "lif layer 2 self.abs_max_v: 2092.0\n",
      "fc layer 2 self.abs_max_out: 2121.0\n",
      "lif layer 2 self.abs_max_v: 2121.0\n",
      "fc layer 1 self.abs_max_out: 2244.0\n",
      "lif layer 1 self.abs_max_v: 2244.0\n",
      "fc layer 1 self.abs_max_out: 2250.0\n",
      "lif layer 1 self.abs_max_v: 2250.0\n",
      "fc layer 1 self.abs_max_out: 2466.0\n",
      "lif layer 1 self.abs_max_v: 2466.0\n",
      "fc layer 2 self.abs_max_out: 2122.0\n",
      "lif layer 2 self.abs_max_v: 2122.0\n",
      "lif layer 2 self.abs_max_v: 2137.0\n",
      "fc layer 1 self.abs_max_out: 2574.0\n",
      "lif layer 1 self.abs_max_v: 2574.0\n",
      "lif layer 2 self.abs_max_v: 2159.5\n",
      "fc layer 2 self.abs_max_out: 2301.0\n",
      "lif layer 2 self.abs_max_v: 2301.0\n",
      "fc layer 1 self.abs_max_out: 2598.0\n",
      "lif layer 1 self.abs_max_v: 2598.0\n",
      "fc layer 1 self.abs_max_out: 2911.0\n",
      "lif layer 1 self.abs_max_v: 2911.0\n",
      "fc layer 1 self.abs_max_out: 3075.0\n",
      "lif layer 1 self.abs_max_v: 3075.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "lif layer 2 self.abs_max_v: 2334.5\n",
      "lif layer 2 self.abs_max_v: 2379.5\n",
      "lif layer 1 self.abs_max_v: 3375.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.813825/  2.026991, val:  29.58%, val_best:  29.58%, tr:  96.94%, tr_best:  96.94%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 94.2308%\n",
      "layer   2  Sparsity: 78.4546%\n",
      "layer   3  Sparsity: 83.4258%\n",
      "total_backward_count 9790 real_backward_count 2220  22.676%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2434.0\n",
      "fc layer 1 self.abs_max_out: 3254.0\n",
      "fc layer 3 self.abs_max_out: 429.0\n",
      "lif layer 2 self.abs_max_v: 2471.0\n",
      "lif layer 2 self.abs_max_v: 2511.0\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "lif layer 2 self.abs_max_v: 2532.0\n",
      "lif layer 2 self.abs_max_v: 2812.5\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "fc layer 1 self.abs_max_out: 3275.0\n",
      "fc layer 1 self.abs_max_out: 3360.0\n",
      "fc layer 1 self.abs_max_out: 3369.0\n",
      "fc layer 1 self.abs_max_out: 3766.0\n",
      "lif layer 1 self.abs_max_v: 3766.0\n",
      "lif layer 2 self.abs_max_v: 2889.5\n",
      "fc layer 2 self.abs_max_out: 2323.0\n",
      "fc layer 2 self.abs_max_out: 2372.0\n",
      "fc layer 2 self.abs_max_out: 2396.0\n",
      "lif layer 2 self.abs_max_v: 2994.0\n",
      "fc layer 2 self.abs_max_out: 2477.0\n",
      "lif layer 1 self.abs_max_v: 3911.5\n",
      "fc layer 2 self.abs_max_out: 2503.0\n",
      "fc layer 2 self.abs_max_out: 2575.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.649662/  1.924001, val:  41.25%, val_best:  41.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 94.2299%\n",
      "layer   2  Sparsity: 77.1033%\n",
      "layer   3  Sparsity: 80.4226%\n",
      "total_backward_count 19580 real_backward_count 3851  19.668%\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 1 self.abs_max_out: 4088.0\n",
      "lif layer 1 self.abs_max_v: 4088.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "lif layer 2 self.abs_max_v: 2999.0\n",
      "lif layer 1 self.abs_max_v: 4175.0\n",
      "lif layer 1 self.abs_max_v: 4308.5\n",
      "lif layer 1 self.abs_max_v: 4709.5\n",
      "lif layer 1 self.abs_max_v: 5052.0\n",
      "fc layer 2 self.abs_max_out: 2603.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.578457/  1.851512, val:  39.58%, val_best:  41.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 94.2338%\n",
      "layer   2  Sparsity: 76.5240%\n",
      "layer   3  Sparsity: 79.6950%\n",
      "total_backward_count 29370 real_backward_count 5438  18.515%\n",
      "lif layer 2 self.abs_max_v: 3053.5\n",
      "fc layer 1 self.abs_max_out: 4483.0\n",
      "fc layer 3 self.abs_max_out: 520.0\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "lif layer 2 self.abs_max_v: 3234.5\n",
      "fc layer 2 self.abs_max_out: 2675.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.505211/  1.815845, val:  48.33%, val_best:  48.33%, tr:  99.59%, tr_best:  99.59%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 94.2318%\n",
      "layer   2  Sparsity: 75.9624%\n",
      "layer   3  Sparsity: 79.5740%\n",
      "total_backward_count 39160 real_backward_count 6961  17.776%\n",
      "fc layer 3 self.abs_max_out: 530.0\n",
      "fc layer 3 self.abs_max_out: 545.0\n",
      "lif layer 2 self.abs_max_v: 3362.5\n",
      "lif layer 2 self.abs_max_v: 3557.0\n",
      "fc layer 3 self.abs_max_out: 547.0\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "lif layer 2 self.abs_max_v: 3675.5\n",
      "lif layer 1 self.abs_max_v: 5157.0\n",
      "lif layer 1 self.abs_max_v: 5821.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.465308/  1.809755, val:  43.75%, val_best:  48.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2365%\n",
      "layer   2  Sparsity: 75.4691%\n",
      "layer   3  Sparsity: 78.6381%\n",
      "total_backward_count 48950 real_backward_count 8442  17.246%\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "lif layer 2 self.abs_max_v: 3760.0\n",
      "lif layer 2 self.abs_max_v: 3844.0\n",
      "fc layer 1 self.abs_max_out: 4669.0\n",
      "fc layer 1 self.abs_max_out: 4721.0\n",
      "lif layer 1 self.abs_max_v: 6172.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.419854/  1.777653, val:  52.08%, val_best:  52.08%, tr:  99.59%, tr_best:  99.69%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2262%\n",
      "layer   2  Sparsity: 75.6745%\n",
      "layer   3  Sparsity: 77.1481%\n",
      "total_backward_count 58740 real_backward_count 9897  16.849%\n",
      "fc layer 3 self.abs_max_out: 635.0\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.403448/  1.772907, val:  46.25%, val_best:  52.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2493%\n",
      "layer   2  Sparsity: 75.4881%\n",
      "layer   3  Sparsity: 77.3383%\n",
      "total_backward_count 68530 real_backward_count 11318  16.515%\n",
      "fc layer 1 self.abs_max_out: 4793.0\n",
      "lif layer 2 self.abs_max_v: 3986.0\n",
      "lif layer 2 self.abs_max_v: 4070.0\n",
      "lif layer 2 self.abs_max_v: 4192.0\n",
      "lif layer 1 self.abs_max_v: 6580.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.372814/  1.719524, val:  51.67%, val_best:  52.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2254%\n",
      "layer   2  Sparsity: 75.0646%\n",
      "layer   3  Sparsity: 78.0150%\n",
      "total_backward_count 78320 real_backward_count 12661  16.166%\n",
      "fc layer 1 self.abs_max_out: 4886.0\n",
      "lif layer 2 self.abs_max_v: 4259.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.371266/  1.712734, val:  50.00%, val_best:  52.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2383%\n",
      "layer   2  Sparsity: 74.6555%\n",
      "layer   3  Sparsity: 78.0267%\n",
      "total_backward_count 88110 real_backward_count 14103  16.006%\n",
      "fc layer 2 self.abs_max_out: 2678.0\n",
      "fc layer 3 self.abs_max_out: 671.0\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "fc layer 2 self.abs_max_out: 2685.0\n",
      "lif layer 1 self.abs_max_v: 6868.0\n",
      "lif layer 1 self.abs_max_v: 6955.0\n",
      "fc layer 2 self.abs_max_out: 2731.0\n",
      "fc layer 2 self.abs_max_out: 2757.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.352877/  1.704319, val:  45.83%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2190%\n",
      "layer   2  Sparsity: 73.9299%\n",
      "layer   3  Sparsity: 77.8427%\n",
      "total_backward_count 97900 real_backward_count 15410  15.741%\n",
      "fc layer 3 self.abs_max_out: 740.0\n",
      "fc layer 3 self.abs_max_out: 742.0\n",
      "fc layer 2 self.abs_max_out: 2785.0\n",
      "fc layer 1 self.abs_max_out: 5075.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.283880/  1.686268, val:  46.25%, val_best:  52.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2223%\n",
      "layer   2  Sparsity: 73.9144%\n",
      "layer   3  Sparsity: 77.3045%\n",
      "total_backward_count 107690 real_backward_count 16732  15.537%\n",
      "fc layer 1 self.abs_max_out: 5120.0\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "lif layer 2 self.abs_max_v: 4282.0\n",
      "lif layer 2 self.abs_max_v: 4333.0\n",
      "lif layer 1 self.abs_max_v: 6973.0\n",
      "lif layer 1 self.abs_max_v: 7044.5\n",
      "fc layer 2 self.abs_max_out: 2859.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.308764/  1.664226, val:  57.92%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.2306%\n",
      "layer   2  Sparsity: 73.5096%\n",
      "layer   3  Sparsity: 76.7432%\n",
      "total_backward_count 117480 real_backward_count 18038  15.354%\n",
      "fc layer 3 self.abs_max_out: 744.0\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "lif layer 1 self.abs_max_v: 7110.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.258831/  1.681325, val:  45.42%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2460%\n",
      "layer   2  Sparsity: 74.0417%\n",
      "layer   3  Sparsity: 76.8468%\n",
      "total_backward_count 127270 real_backward_count 19289  15.156%\n",
      "fc layer 1 self.abs_max_out: 5670.0\n",
      "fc layer 3 self.abs_max_out: 806.0\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "fc layer 2 self.abs_max_out: 2862.0\n",
      "fc layer 2 self.abs_max_out: 2878.0\n",
      "lif layer 1 self.abs_max_v: 7185.0\n",
      "lif layer 1 self.abs_max_v: 7507.0\n",
      "fc layer 2 self.abs_max_out: 2959.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.257517/  1.654168, val:  45.83%, val_best:  57.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2465%\n",
      "layer   2  Sparsity: 73.8660%\n",
      "layer   3  Sparsity: 76.5908%\n",
      "total_backward_count 137060 real_backward_count 20570  15.008%\n",
      "fc layer 1 self.abs_max_out: 5698.0\n",
      "fc layer 3 self.abs_max_out: 811.0\n",
      "lif layer 2 self.abs_max_v: 4350.5\n",
      "lif layer 1 self.abs_max_v: 7508.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.213082/  1.581722, val:  53.33%, val_best:  57.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2494%\n",
      "layer   2  Sparsity: 73.7081%\n",
      "layer   3  Sparsity: 76.4666%\n",
      "total_backward_count 146850 real_backward_count 21815  14.855%\n",
      "lif layer 2 self.abs_max_v: 4378.0\n",
      "lif layer 1 self.abs_max_v: 8257.0\n",
      "lif layer 1 self.abs_max_v: 8406.5\n",
      "lif layer 2 self.abs_max_v: 4382.0\n",
      "lif layer 2 self.abs_max_v: 4519.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.205956/  1.594330, val:  55.83%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2177%\n",
      "layer   2  Sparsity: 73.5241%\n",
      "layer   3  Sparsity: 77.3550%\n",
      "total_backward_count 156640 real_backward_count 23057  14.720%\n",
      "fc layer 2 self.abs_max_out: 3165.0\n",
      "fc layer 1 self.abs_max_out: 5849.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.203545/  1.517135, val:  52.50%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2316%\n",
      "layer   2  Sparsity: 73.6503%\n",
      "layer   3  Sparsity: 76.9602%\n",
      "total_backward_count 166430 real_backward_count 24217  14.551%\n",
      "fc layer 3 self.abs_max_out: 828.0\n",
      "lif layer 2 self.abs_max_v: 4613.5\n",
      "lif layer 2 self.abs_max_v: 4668.0\n",
      "lif layer 2 self.abs_max_v: 5346.5\n",
      "lif layer 1 self.abs_max_v: 8433.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.172518/  1.464173, val:  69.58%, val_best:  69.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2345%\n",
      "layer   2  Sparsity: 73.5419%\n",
      "layer   3  Sparsity: 76.5583%\n",
      "total_backward_count 176220 real_backward_count 25429  14.430%\n",
      "fc layer 3 self.abs_max_out: 863.0\n",
      "fc layer 3 self.abs_max_out: 872.0\n",
      "fc layer 3 self.abs_max_out: 873.0\n",
      "fc layer 3 self.abs_max_out: 881.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.159568/  1.522295, val:  57.08%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2272%\n",
      "layer   2  Sparsity: 73.5441%\n",
      "layer   3  Sparsity: 76.5639%\n",
      "total_backward_count 186010 real_backward_count 26597  14.299%\n",
      "fc layer 1 self.abs_max_out: 5993.0\n",
      "fc layer 3 self.abs_max_out: 892.0\n",
      "fc layer 3 self.abs_max_out: 945.0\n",
      "lif layer 1 self.abs_max_v: 8844.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.104386/  1.544434, val:  40.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2292%\n",
      "layer   2  Sparsity: 73.4454%\n",
      "layer   3  Sparsity: 75.7084%\n",
      "total_backward_count 195800 real_backward_count 27727  14.161%\n",
      "fc layer 2 self.abs_max_out: 3270.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.106825/  1.539559, val:  47.50%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2225%\n",
      "layer   2  Sparsity: 73.4147%\n",
      "layer   3  Sparsity: 75.6881%\n",
      "total_backward_count 205590 real_backward_count 28801  14.009%\n",
      "lif layer 1 self.abs_max_v: 8939.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.092741/  1.526603, val:  51.67%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2124%\n",
      "layer   2  Sparsity: 73.0742%\n",
      "layer   3  Sparsity: 75.8283%\n",
      "total_backward_count 215380 real_backward_count 29953  13.907%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.100556/  1.455057, val:  62.92%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2182%\n",
      "layer   2  Sparsity: 73.0966%\n",
      "layer   3  Sparsity: 75.1237%\n",
      "total_backward_count 225170 real_backward_count 31079  13.802%\n",
      "fc layer 1 self.abs_max_out: 6172.0\n",
      "fc layer 2 self.abs_max_out: 3444.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.076837/  1.486018, val:  63.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2402%\n",
      "layer   2  Sparsity: 73.3886%\n",
      "layer   3  Sparsity: 75.6110%\n",
      "total_backward_count 234960 real_backward_count 32163  13.689%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.091253/  1.422250, val:  74.58%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2143%\n",
      "layer   2  Sparsity: 73.0577%\n",
      "layer   3  Sparsity: 75.6478%\n",
      "total_backward_count 244750 real_backward_count 33208  13.568%\n",
      "fc layer 1 self.abs_max_out: 6296.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.079659/  1.397352, val:  64.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2231%\n",
      "layer   2  Sparsity: 73.2845%\n",
      "layer   3  Sparsity: 75.1053%\n",
      "total_backward_count 254540 real_backward_count 34302  13.476%\n",
      "fc layer 1 self.abs_max_out: 6418.0\n",
      "lif layer 1 self.abs_max_v: 9128.5\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.056184/  1.360897, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2207%\n",
      "layer   2  Sparsity: 73.2854%\n",
      "layer   3  Sparsity: 75.0855%\n",
      "total_backward_count 264330 real_backward_count 35338  13.369%\n",
      "lif layer 2 self.abs_max_v: 5391.0\n",
      "lif layer 2 self.abs_max_v: 5629.5\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.058739/  1.352448, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2390%\n",
      "layer   2  Sparsity: 73.2872%\n",
      "layer   3  Sparsity: 75.0574%\n",
      "total_backward_count 274120 real_backward_count 36410  13.283%\n",
      "fc layer 2 self.abs_max_out: 3490.0\n",
      "fc layer 3 self.abs_max_out: 950.0\n",
      "fc layer 3 self.abs_max_out: 955.0\n",
      "fc layer 3 self.abs_max_out: 970.0\n",
      "lif layer 1 self.abs_max_v: 9133.5\n",
      "lif layer 1 self.abs_max_v: 9945.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.010706/  1.450606, val:  59.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2442%\n",
      "layer   2  Sparsity: 73.5600%\n",
      "layer   3  Sparsity: 74.9204%\n",
      "total_backward_count 283910 real_backward_count 37375  13.164%\n",
      "fc layer 2 self.abs_max_out: 3791.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.038743/  1.441450, val:  52.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2427%\n",
      "layer   2  Sparsity: 73.3071%\n",
      "layer   3  Sparsity: 74.7696%\n",
      "total_backward_count 293700 real_backward_count 38413  13.079%\n",
      "fc layer 3 self.abs_max_out: 1004.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.031660/  1.398549, val:  57.92%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2407%\n",
      "layer   2  Sparsity: 73.1304%\n",
      "layer   3  Sparsity: 75.0169%\n",
      "total_backward_count 303490 real_backward_count 39433  12.993%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.025028/  1.377021, val:  62.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2438%\n",
      "layer   2  Sparsity: 72.9836%\n",
      "layer   3  Sparsity: 74.6661%\n",
      "total_backward_count 313280 real_backward_count 40429  12.905%\n",
      "fc layer 1 self.abs_max_out: 6576.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.002515/  1.415949, val:  61.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2200%\n",
      "layer   2  Sparsity: 73.0535%\n",
      "layer   3  Sparsity: 74.4144%\n",
      "total_backward_count 323070 real_backward_count 41397  12.814%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.998613/  1.324436, val:  64.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.2356%\n",
      "layer   2  Sparsity: 72.9416%\n",
      "layer   3  Sparsity: 74.4377%\n",
      "total_backward_count 332860 real_backward_count 42377  12.731%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.995469/  1.349599, val:  64.17%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2452%\n",
      "layer   2  Sparsity: 72.6242%\n",
      "layer   3  Sparsity: 75.1697%\n",
      "total_backward_count 342650 real_backward_count 43321  12.643%\n",
      "fc layer 3 self.abs_max_out: 1009.0\n",
      "fc layer 3 self.abs_max_out: 1080.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.984983/  1.280062, val:  79.17%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2479%\n",
      "layer   2  Sparsity: 72.3499%\n",
      "layer   3  Sparsity: 75.2825%\n",
      "total_backward_count 352440 real_backward_count 44231  12.550%\n",
      "lif layer 1 self.abs_max_v: 10144.5\n",
      "lif layer 1 self.abs_max_v: 10265.5\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.937571/  1.255137, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2313%\n",
      "layer   2  Sparsity: 72.6644%\n",
      "layer   3  Sparsity: 74.5292%\n",
      "total_backward_count 362230 real_backward_count 45130  12.459%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.919105/  1.329677, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2326%\n",
      "layer   2  Sparsity: 73.1203%\n",
      "layer   3  Sparsity: 73.8758%\n",
      "total_backward_count 372020 real_backward_count 46008  12.367%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.955942/  1.310451, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2435%\n",
      "layer   2  Sparsity: 72.7998%\n",
      "layer   3  Sparsity: 74.3208%\n",
      "total_backward_count 381810 real_backward_count 46942  12.295%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.926472/  1.291454, val:  71.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2491%\n",
      "layer   2  Sparsity: 72.9924%\n",
      "layer   3  Sparsity: 75.1353%\n",
      "total_backward_count 391600 real_backward_count 47817  12.211%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.918727/  1.222967, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2166%\n",
      "layer   2  Sparsity: 72.7708%\n",
      "layer   3  Sparsity: 74.7416%\n",
      "total_backward_count 401390 real_backward_count 48742  12.143%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.909123/  1.224081, val:  80.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2375%\n",
      "layer   2  Sparsity: 72.7699%\n",
      "layer   3  Sparsity: 74.6559%\n",
      "total_backward_count 411180 real_backward_count 49615  12.066%\n",
      "fc layer 1 self.abs_max_out: 6759.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.928576/  1.273466, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2357%\n",
      "layer   2  Sparsity: 72.4407%\n",
      "layer   3  Sparsity: 74.9376%\n",
      "total_backward_count 420970 real_backward_count 50520  12.001%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.926253/  1.222260, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2397%\n",
      "layer   2  Sparsity: 72.4743%\n",
      "layer   3  Sparsity: 75.0081%\n",
      "total_backward_count 430760 real_backward_count 51428  11.939%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.924757/  1.243198, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2491%\n",
      "layer   2  Sparsity: 72.4352%\n",
      "layer   3  Sparsity: 75.1483%\n",
      "total_backward_count 440550 real_backward_count 52297  11.871%\n",
      "fc layer 1 self.abs_max_out: 6817.0\n",
      "fc layer 3 self.abs_max_out: 1094.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.914211/  1.203217, val:  82.92%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2373%\n",
      "layer   2  Sparsity: 72.0918%\n",
      "layer   3  Sparsity: 75.4944%\n",
      "total_backward_count 450340 real_backward_count 53160  11.804%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.920604/  1.256307, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2162%\n",
      "layer   2  Sparsity: 72.2623%\n",
      "layer   3  Sparsity: 75.8086%\n",
      "total_backward_count 460130 real_backward_count 53983  11.732%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.916570/  1.304061, val:  67.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2375%\n",
      "layer   2  Sparsity: 72.3429%\n",
      "layer   3  Sparsity: 76.0568%\n",
      "total_backward_count 469920 real_backward_count 54825  11.667%\n",
      "lif layer 1 self.abs_max_v: 10657.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.924660/  1.259519, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2170%\n",
      "layer   2  Sparsity: 72.3010%\n",
      "layer   3  Sparsity: 75.9917%\n",
      "total_backward_count 479710 real_backward_count 55685  11.608%\n",
      "fc layer 1 self.abs_max_out: 6822.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.907302/  1.206570, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.61 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2270%\n",
      "layer   2  Sparsity: 71.9999%\n",
      "layer   3  Sparsity: 75.8697%\n",
      "total_backward_count 489500 real_backward_count 56529  11.548%\n",
      "lif layer 2 self.abs_max_v: 5757.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.887644/  1.237424, val:  82.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2299%\n",
      "layer   2  Sparsity: 72.0135%\n",
      "layer   3  Sparsity: 75.3692%\n",
      "total_backward_count 499290 real_backward_count 57333  11.483%\n",
      "lif layer 2 self.abs_max_v: 5860.5\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.889964/  1.257214, val:  73.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2432%\n",
      "layer   2  Sparsity: 72.1214%\n",
      "layer   3  Sparsity: 75.8208%\n",
      "total_backward_count 509080 real_backward_count 58107  11.414%\n",
      "fc layer 3 self.abs_max_out: 1097.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.920835/  1.217992, val:  85.00%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2252%\n",
      "layer   2  Sparsity: 72.1183%\n",
      "layer   3  Sparsity: 75.5255%\n",
      "total_backward_count 518870 real_backward_count 58889  11.349%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.900985/  1.241437, val:  72.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.22 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2293%\n",
      "layer   2  Sparsity: 72.1854%\n",
      "layer   3  Sparsity: 76.2404%\n",
      "total_backward_count 528660 real_backward_count 59644  11.282%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.888996/  1.159446, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2246%\n",
      "layer   2  Sparsity: 72.5079%\n",
      "layer   3  Sparsity: 76.1779%\n",
      "total_backward_count 538450 real_backward_count 60387  11.215%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.913042/  1.220360, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2251%\n",
      "layer   2  Sparsity: 72.4175%\n",
      "layer   3  Sparsity: 76.2005%\n",
      "total_backward_count 548240 real_backward_count 61200  11.163%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.875501/  1.230745, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2275%\n",
      "layer   2  Sparsity: 72.6221%\n",
      "layer   3  Sparsity: 75.6749%\n",
      "total_backward_count 558030 real_backward_count 61942  11.100%\n",
      "lif layer 2 self.abs_max_v: 6036.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.881755/  1.213666, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2452%\n",
      "layer   2  Sparsity: 72.2527%\n",
      "layer   3  Sparsity: 75.2210%\n",
      "total_backward_count 567820 real_backward_count 62703  11.043%\n",
      "fc layer 1 self.abs_max_out: 6913.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.851627/  1.156528, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2244%\n",
      "layer   2  Sparsity: 72.0447%\n",
      "layer   3  Sparsity: 75.4406%\n",
      "total_backward_count 577610 real_backward_count 63403  10.977%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.874010/  1.241319, val:  70.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2407%\n",
      "layer   2  Sparsity: 72.4112%\n",
      "layer   3  Sparsity: 76.0979%\n",
      "total_backward_count 587400 real_backward_count 64153  10.922%\n",
      "lif layer 1 self.abs_max_v: 10676.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.875200/  1.167667, val:  82.50%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.25 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2197%\n",
      "layer   2  Sparsity: 72.4356%\n",
      "layer   3  Sparsity: 75.8528%\n",
      "total_backward_count 597190 real_backward_count 64917  10.870%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.855616/  1.136133, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2273%\n",
      "layer   2  Sparsity: 72.4439%\n",
      "layer   3  Sparsity: 75.6572%\n",
      "total_backward_count 606980 real_backward_count 65682  10.821%\n",
      "lif layer 1 self.abs_max_v: 10818.5\n",
      "lif layer 1 self.abs_max_v: 10887.5\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.858645/  1.240988, val:  72.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2289%\n",
      "layer   2  Sparsity: 72.5063%\n",
      "layer   3  Sparsity: 75.8525%\n",
      "total_backward_count 616770 real_backward_count 66412  10.768%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.882203/  1.155065, val:  84.17%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2206%\n",
      "layer   2  Sparsity: 72.5283%\n",
      "layer   3  Sparsity: 75.7996%\n",
      "total_backward_count 626560 real_backward_count 67123  10.713%\n",
      "fc layer 3 self.abs_max_out: 1098.0\n",
      "fc layer 3 self.abs_max_out: 1134.0\n",
      "lif layer 1 self.abs_max_v: 10906.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.891115/  1.243947, val:  77.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.06 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.2484%\n",
      "layer   2  Sparsity: 72.5959%\n",
      "layer   3  Sparsity: 76.0739%\n",
      "total_backward_count 636350 real_backward_count 67822  10.658%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.879210/  1.232812, val:  75.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2300%\n",
      "layer   2  Sparsity: 72.4376%\n",
      "layer   3  Sparsity: 76.3394%\n",
      "total_backward_count 646140 real_backward_count 68610  10.618%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.855502/  1.251033, val:  77.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2319%\n",
      "layer   2  Sparsity: 72.6099%\n",
      "layer   3  Sparsity: 75.9883%\n",
      "total_backward_count 655930 real_backward_count 69304  10.566%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.869458/  1.164550, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2229%\n",
      "layer   2  Sparsity: 72.3594%\n",
      "layer   3  Sparsity: 76.2188%\n",
      "total_backward_count 665720 real_backward_count 70012  10.517%\n",
      "fc layer 3 self.abs_max_out: 1167.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.877385/  1.177015, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2156%\n",
      "layer   2  Sparsity: 72.3320%\n",
      "layer   3  Sparsity: 76.3676%\n",
      "total_backward_count 675510 real_backward_count 70715  10.468%\n",
      "fc layer 1 self.abs_max_out: 7025.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.845452/  1.244032, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2411%\n",
      "layer   2  Sparsity: 72.4903%\n",
      "layer   3  Sparsity: 76.2194%\n",
      "total_backward_count 685300 real_backward_count 71398  10.419%\n",
      "fc layer 3 self.abs_max_out: 1179.0\n",
      "fc layer 1 self.abs_max_out: 7043.0\n",
      "lif layer 1 self.abs_max_v: 10960.5\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.838932/  1.115450, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2523%\n",
      "layer   2  Sparsity: 72.3887%\n",
      "layer   3  Sparsity: 76.0574%\n",
      "total_backward_count 695090 real_backward_count 72099  10.373%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.829670/  1.189736, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.92 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.2287%\n",
      "layer   2  Sparsity: 72.4834%\n",
      "layer   3  Sparsity: 75.4769%\n",
      "total_backward_count 704880 real_backward_count 72793  10.327%\n",
      "lif layer 2 self.abs_max_v: 6166.5\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.830860/  1.126984, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.29 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2369%\n",
      "layer   2  Sparsity: 72.4324%\n",
      "layer   3  Sparsity: 75.3226%\n",
      "total_backward_count 714670 real_backward_count 73478  10.281%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.813367/  1.170618, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2315%\n",
      "layer   2  Sparsity: 72.5055%\n",
      "layer   3  Sparsity: 75.7536%\n",
      "total_backward_count 724460 real_backward_count 74109  10.230%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.825854/  1.150102, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2316%\n",
      "layer   2  Sparsity: 72.2796%\n",
      "layer   3  Sparsity: 75.2736%\n",
      "total_backward_count 734250 real_backward_count 74813  10.189%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.809406/  1.111205, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2131%\n",
      "layer   2  Sparsity: 72.3919%\n",
      "layer   3  Sparsity: 75.1719%\n",
      "total_backward_count 744040 real_backward_count 75506  10.148%\n",
      "fc layer 2 self.abs_max_out: 3875.0\n",
      "fc layer 2 self.abs_max_out: 3883.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.808342/  1.133844, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2438%\n",
      "layer   2  Sparsity: 72.4398%\n",
      "layer   3  Sparsity: 75.6499%\n",
      "total_backward_count 753830 real_backward_count 76109  10.096%\n",
      "fc layer 1 self.abs_max_out: 7073.0\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.794842/  1.211489, val:  74.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2160%\n",
      "layer   2  Sparsity: 72.5615%\n",
      "layer   3  Sparsity: 75.4905%\n",
      "total_backward_count 763620 real_backward_count 76750  10.051%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.810345/  1.113668, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.67 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2165%\n",
      "layer   2  Sparsity: 72.1420%\n",
      "layer   3  Sparsity: 75.1870%\n",
      "total_backward_count 773410 real_backward_count 77420  10.010%\n",
      "fc layer 2 self.abs_max_out: 3920.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.773940/  1.146222, val:  77.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2315%\n",
      "layer   2  Sparsity: 72.2160%\n",
      "layer   3  Sparsity: 74.6901%\n",
      "total_backward_count 783200 real_backward_count 78117   9.974%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.784410/  1.115989, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2357%\n",
      "layer   2  Sparsity: 72.0689%\n",
      "layer   3  Sparsity: 74.4418%\n",
      "total_backward_count 792990 real_backward_count 78798   9.937%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.780542/  1.156138, val:  77.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2204%\n",
      "layer   2  Sparsity: 72.0030%\n",
      "layer   3  Sparsity: 74.5363%\n",
      "total_backward_count 802780 real_backward_count 79450   9.897%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.786865/  1.171322, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2419%\n",
      "layer   2  Sparsity: 72.6331%\n",
      "layer   3  Sparsity: 74.6371%\n",
      "total_backward_count 812570 real_backward_count 80120   9.860%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.768425/  1.155880, val:  73.33%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2234%\n",
      "layer   2  Sparsity: 72.1701%\n",
      "layer   3  Sparsity: 74.5201%\n",
      "total_backward_count 822360 real_backward_count 80754   9.820%\n",
      "fc layer 2 self.abs_max_out: 3947.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.757957/  1.040246, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2412%\n",
      "layer   2  Sparsity: 71.9872%\n",
      "layer   3  Sparsity: 75.2037%\n",
      "total_backward_count 832150 real_backward_count 81413   9.783%\n",
      "fc layer 2 self.abs_max_out: 3969.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.747922/  1.182284, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2490%\n",
      "layer   2  Sparsity: 72.0039%\n",
      "layer   3  Sparsity: 75.0485%\n",
      "total_backward_count 841940 real_backward_count 82049   9.745%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.770812/  1.141731, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2378%\n",
      "layer   2  Sparsity: 71.9842%\n",
      "layer   3  Sparsity: 74.9160%\n",
      "total_backward_count 851730 real_backward_count 82674   9.707%\n",
      "fc layer 2 self.abs_max_out: 3995.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.757733/  1.083903, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2334%\n",
      "layer   2  Sparsity: 71.9248%\n",
      "layer   3  Sparsity: 75.3166%\n",
      "total_backward_count 861520 real_backward_count 83276   9.666%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.759679/  1.136516, val:  74.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2380%\n",
      "layer   2  Sparsity: 72.1570%\n",
      "layer   3  Sparsity: 75.1322%\n",
      "total_backward_count 871310 real_backward_count 83901   9.629%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.756066/  1.090976, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2143%\n",
      "layer   2  Sparsity: 71.9441%\n",
      "layer   3  Sparsity: 75.0333%\n",
      "total_backward_count 881100 real_backward_count 84506   9.591%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.735466/  1.109186, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2123%\n",
      "layer   2  Sparsity: 72.2260%\n",
      "layer   3  Sparsity: 75.2127%\n",
      "total_backward_count 890890 real_backward_count 85102   9.552%\n",
      "fc layer 2 self.abs_max_out: 4058.0\n",
      "fc layer 1 self.abs_max_out: 7112.0\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.754021/  1.119088, val:  78.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2335%\n",
      "layer   2  Sparsity: 71.9359%\n",
      "layer   3  Sparsity: 74.7219%\n",
      "total_backward_count 900680 real_backward_count 85765   9.522%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.750332/  1.084977, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2167%\n",
      "layer   2  Sparsity: 71.8826%\n",
      "layer   3  Sparsity: 75.1356%\n",
      "total_backward_count 910470 real_backward_count 86379   9.487%\n",
      "lif layer 1 self.abs_max_v: 10989.5\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.735730/  1.162037, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2489%\n",
      "layer   2  Sparsity: 72.1019%\n",
      "layer   3  Sparsity: 75.4264%\n",
      "total_backward_count 920260 real_backward_count 86976   9.451%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.728640/  1.077736, val:  81.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2312%\n",
      "layer   2  Sparsity: 72.2302%\n",
      "layer   3  Sparsity: 75.4503%\n",
      "total_backward_count 930050 real_backward_count 87595   9.418%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.736628/  1.074457, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2224%\n",
      "layer   2  Sparsity: 72.1202%\n",
      "layer   3  Sparsity: 75.8409%\n",
      "total_backward_count 939840 real_backward_count 88217   9.386%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.755209/  1.080691, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2362%\n",
      "layer   2  Sparsity: 72.1744%\n",
      "layer   3  Sparsity: 75.6574%\n",
      "total_backward_count 949630 real_backward_count 88802   9.351%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.748844/  1.075155, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2336%\n",
      "layer   2  Sparsity: 72.0953%\n",
      "layer   3  Sparsity: 75.2142%\n",
      "total_backward_count 959420 real_backward_count 89379   9.316%\n",
      "fc layer 2 self.abs_max_out: 4070.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.717702/  1.058318, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2253%\n",
      "layer   2  Sparsity: 72.1237%\n",
      "layer   3  Sparsity: 75.4267%\n",
      "total_backward_count 969210 real_backward_count 89950   9.281%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.726736/  1.057494, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2326%\n",
      "layer   2  Sparsity: 71.9547%\n",
      "layer   3  Sparsity: 75.8416%\n",
      "total_backward_count 979000 real_backward_count 90469   9.241%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.730002/  1.075295, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2469%\n",
      "layer   2  Sparsity: 71.9717%\n",
      "layer   3  Sparsity: 75.5747%\n",
      "total_backward_count 988790 real_backward_count 90994   9.203%\n",
      "fc layer 2 self.abs_max_out: 4266.0\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.712594/  1.071932, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2259%\n",
      "layer   2  Sparsity: 71.9531%\n",
      "layer   3  Sparsity: 76.0942%\n",
      "total_backward_count 998580 real_backward_count 91539   9.167%\n",
      "lif layer 1 self.abs_max_v: 11056.5\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.724723/  1.044265, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2341%\n",
      "layer   2  Sparsity: 72.1174%\n",
      "layer   3  Sparsity: 75.8523%\n",
      "total_backward_count 1008370 real_backward_count 92074   9.131%\n",
      "fc layer 1 self.abs_max_out: 7283.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.726171/  1.101368, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2187%\n",
      "layer   2  Sparsity: 71.9671%\n",
      "layer   3  Sparsity: 76.1469%\n",
      "total_backward_count 1018160 real_backward_count 92615   9.096%\n",
      "lif layer 1 self.abs_max_v: 11265.5\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.728308/  1.066817, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2295%\n",
      "layer   2  Sparsity: 71.9171%\n",
      "layer   3  Sparsity: 75.9025%\n",
      "total_backward_count 1027950 real_backward_count 93184   9.065%\n",
      "fc layer 1 self.abs_max_out: 7290.0\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.712599/  1.078482, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2274%\n",
      "layer   2  Sparsity: 71.8406%\n",
      "layer   3  Sparsity: 75.2568%\n",
      "total_backward_count 1037740 real_backward_count 93714   9.031%\n",
      "lif layer 1 self.abs_max_v: 11289.5\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.710942/  1.053700, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2325%\n",
      "layer   2  Sparsity: 72.0401%\n",
      "layer   3  Sparsity: 75.8247%\n",
      "total_backward_count 1047530 real_backward_count 94224   8.995%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.721947/  1.066011, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.56 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2253%\n",
      "layer   2  Sparsity: 72.1438%\n",
      "layer   3  Sparsity: 75.5533%\n",
      "total_backward_count 1057320 real_backward_count 94753   8.962%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.699528/  1.056689, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2109%\n",
      "layer   2  Sparsity: 71.9924%\n",
      "layer   3  Sparsity: 75.0888%\n",
      "total_backward_count 1067110 real_backward_count 95280   8.929%\n",
      "fc layer 3 self.abs_max_out: 1184.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.714329/  1.054913, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2424%\n",
      "layer   2  Sparsity: 72.2116%\n",
      "layer   3  Sparsity: 75.3706%\n",
      "total_backward_count 1076900 real_backward_count 95804   8.896%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.711896/  1.063071, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2308%\n",
      "layer   2  Sparsity: 72.1006%\n",
      "layer   3  Sparsity: 75.4324%\n",
      "total_backward_count 1086690 real_backward_count 96328   8.864%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.726036/  1.029177, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2208%\n",
      "layer   2  Sparsity: 72.1832%\n",
      "layer   3  Sparsity: 75.1256%\n",
      "total_backward_count 1096480 real_backward_count 96879   8.835%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.698418/  1.019506, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2221%\n",
      "layer   2  Sparsity: 71.9902%\n",
      "layer   3  Sparsity: 75.3816%\n",
      "total_backward_count 1106270 real_backward_count 97418   8.806%\n",
      "fc layer 3 self.abs_max_out: 1212.0\n",
      "fc layer 1 self.abs_max_out: 7457.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.681657/  1.034718, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2315%\n",
      "layer   2  Sparsity: 72.0932%\n",
      "layer   3  Sparsity: 75.4029%\n",
      "total_backward_count 1116060 real_backward_count 97937   8.775%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.717369/  1.194193, val:  67.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2447%\n",
      "layer   2  Sparsity: 71.9816%\n",
      "layer   3  Sparsity: 75.5333%\n",
      "total_backward_count 1125850 real_backward_count 98470   8.746%\n",
      "lif layer 2 self.abs_max_v: 6197.5\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.714441/  1.031753, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2437%\n",
      "layer   2  Sparsity: 71.7796%\n",
      "layer   3  Sparsity: 75.8482%\n",
      "total_backward_count 1135640 real_backward_count 98987   8.716%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.716113/  1.071676, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2476%\n",
      "layer   2  Sparsity: 71.9266%\n",
      "layer   3  Sparsity: 76.1638%\n",
      "total_backward_count 1145430 real_backward_count 99493   8.686%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.709664/  1.038347, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2270%\n",
      "layer   2  Sparsity: 72.0504%\n",
      "layer   3  Sparsity: 76.4595%\n",
      "total_backward_count 1155220 real_backward_count 99990   8.655%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.715984/  1.033911, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2333%\n",
      "layer   2  Sparsity: 72.2352%\n",
      "layer   3  Sparsity: 75.8489%\n",
      "total_backward_count 1165010 real_backward_count 100507   8.627%\n",
      "lif layer 2 self.abs_max_v: 6392.5\n",
      "lif layer 1 self.abs_max_v: 11419.5\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.716796/  1.058633, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2396%\n",
      "layer   2  Sparsity: 72.2146%\n",
      "layer   3  Sparsity: 75.9187%\n",
      "total_backward_count 1174800 real_backward_count 101041   8.601%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.698111/  1.061333, val:  77.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2318%\n",
      "layer   2  Sparsity: 72.2917%\n",
      "layer   3  Sparsity: 75.7636%\n",
      "total_backward_count 1184590 real_backward_count 101566   8.574%\n",
      "fc layer 3 self.abs_max_out: 1214.0\n",
      "fc layer 3 self.abs_max_out: 1233.0\n",
      "lif layer 2 self.abs_max_v: 6519.0\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.698116/  1.007915, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2263%\n",
      "layer   2  Sparsity: 72.0242%\n",
      "layer   3  Sparsity: 75.5044%\n",
      "total_backward_count 1194380 real_backward_count 102046   8.544%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.691957/  1.111434, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2400%\n",
      "layer   2  Sparsity: 72.1477%\n",
      "layer   3  Sparsity: 75.3491%\n",
      "total_backward_count 1204170 real_backward_count 102568   8.518%\n",
      "fc layer 3 self.abs_max_out: 1243.0\n",
      "fc layer 3 self.abs_max_out: 1255.0\n",
      "fc layer 3 self.abs_max_out: 1268.0\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.679560/  1.012811, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2516%\n",
      "layer   2  Sparsity: 72.1565%\n",
      "layer   3  Sparsity: 75.2393%\n",
      "total_backward_count 1213960 real_backward_count 103070   8.490%\n",
      "fc layer 3 self.abs_max_out: 1308.0\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.671161/  1.033247, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2292%\n",
      "layer   2  Sparsity: 72.2030%\n",
      "layer   3  Sparsity: 75.3362%\n",
      "total_backward_count 1223750 real_backward_count 103514   8.459%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.697433/  1.056348, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2238%\n",
      "layer   2  Sparsity: 72.2679%\n",
      "layer   3  Sparsity: 75.4790%\n",
      "total_backward_count 1233540 real_backward_count 103990   8.430%\n",
      "fc layer 1 self.abs_max_out: 7635.0\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.688040/  1.046206, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.24 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2290%\n",
      "layer   2  Sparsity: 71.9892%\n",
      "layer   3  Sparsity: 76.1549%\n",
      "total_backward_count 1243330 real_backward_count 104465   8.402%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.680744/  1.070818, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2234%\n",
      "layer   2  Sparsity: 72.0587%\n",
      "layer   3  Sparsity: 75.8829%\n",
      "total_backward_count 1253120 real_backward_count 104956   8.376%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.695545/  1.075044, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2296%\n",
      "layer   2  Sparsity: 72.1045%\n",
      "layer   3  Sparsity: 75.7388%\n",
      "total_backward_count 1262910 real_backward_count 105438   8.349%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.698739/  1.021422, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2410%\n",
      "layer   2  Sparsity: 72.1825%\n",
      "layer   3  Sparsity: 75.5752%\n",
      "total_backward_count 1272700 real_backward_count 105926   8.323%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.688200/  1.027812, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2378%\n",
      "layer   2  Sparsity: 72.0665%\n",
      "layer   3  Sparsity: 75.6194%\n",
      "total_backward_count 1282490 real_backward_count 106406   8.297%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.674045/  1.037818, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2440%\n",
      "layer   2  Sparsity: 72.1466%\n",
      "layer   3  Sparsity: 75.4676%\n",
      "total_backward_count 1292280 real_backward_count 106871   8.270%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.657830/  1.043454, val:  83.75%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2463%\n",
      "layer   2  Sparsity: 72.0717%\n",
      "layer   3  Sparsity: 75.1966%\n",
      "total_backward_count 1302070 real_backward_count 107318   8.242%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.653515/  1.025750, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2287%\n",
      "layer   2  Sparsity: 72.0938%\n",
      "layer   3  Sparsity: 74.8744%\n",
      "total_backward_count 1311860 real_backward_count 107801   8.217%\n",
      "fc layer 3 self.abs_max_out: 1325.0\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.659258/  1.034780, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2439%\n",
      "layer   2  Sparsity: 72.2315%\n",
      "layer   3  Sparsity: 74.6969%\n",
      "total_backward_count 1321650 real_backward_count 108299   8.194%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.661984/  0.998971, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2205%\n",
      "layer   2  Sparsity: 71.9748%\n",
      "layer   3  Sparsity: 74.9633%\n",
      "total_backward_count 1331440 real_backward_count 108808   8.172%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.634923/  0.953818, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2382%\n",
      "layer   2  Sparsity: 71.8993%\n",
      "layer   3  Sparsity: 75.2628%\n",
      "total_backward_count 1341230 real_backward_count 109233   8.144%\n",
      "lif layer 2 self.abs_max_v: 6538.0\n",
      "lif layer 2 self.abs_max_v: 6567.5\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.649182/  1.003349, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2127%\n",
      "layer   2  Sparsity: 71.8682%\n",
      "layer   3  Sparsity: 75.2126%\n",
      "total_backward_count 1351020 real_backward_count 109689   8.119%\n",
      "lif layer 2 self.abs_max_v: 6965.5\n",
      "fc layer 3 self.abs_max_out: 1335.0\n",
      "fc layer 3 self.abs_max_out: 1354.0\n",
      "fc layer 3 self.abs_max_out: 1371.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.661790/  1.013845, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2182%\n",
      "layer   2  Sparsity: 71.9507%\n",
      "layer   3  Sparsity: 75.5006%\n",
      "total_backward_count 1360810 real_backward_count 110162   8.095%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.661343/  1.038962, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.85 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.2426%\n",
      "layer   2  Sparsity: 72.0049%\n",
      "layer   3  Sparsity: 75.0904%\n",
      "total_backward_count 1370600 real_backward_count 110622   8.071%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.660031/  1.054494, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2462%\n",
      "layer   2  Sparsity: 71.9207%\n",
      "layer   3  Sparsity: 75.2927%\n",
      "total_backward_count 1380390 real_backward_count 111079   8.047%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.634037/  0.997656, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2207%\n",
      "layer   2  Sparsity: 72.0144%\n",
      "layer   3  Sparsity: 75.5471%\n",
      "total_backward_count 1390180 real_backward_count 111554   8.024%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.638220/  0.986978, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2415%\n",
      "layer   2  Sparsity: 72.1240%\n",
      "layer   3  Sparsity: 75.6356%\n",
      "total_backward_count 1399970 real_backward_count 112017   8.001%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.634414/  1.051077, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2603%\n",
      "layer   2  Sparsity: 72.0338%\n",
      "layer   3  Sparsity: 75.4168%\n",
      "total_backward_count 1409760 real_backward_count 112469   7.978%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.644420/  0.974242, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.40 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2483%\n",
      "layer   2  Sparsity: 71.9623%\n",
      "layer   3  Sparsity: 75.0650%\n",
      "total_backward_count 1419550 real_backward_count 112955   7.957%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.639037/  1.005198, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2226%\n",
      "layer   2  Sparsity: 72.2571%\n",
      "layer   3  Sparsity: 75.3655%\n",
      "total_backward_count 1429340 real_backward_count 113388   7.933%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.655191/  1.026402, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.93 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.2399%\n",
      "layer   2  Sparsity: 72.2369%\n",
      "layer   3  Sparsity: 75.6554%\n",
      "total_backward_count 1439130 real_backward_count 113878   7.913%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.640887/  1.017303, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2447%\n",
      "layer   2  Sparsity: 71.8845%\n",
      "layer   3  Sparsity: 76.2057%\n",
      "total_backward_count 1448920 real_backward_count 114343   7.892%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.643138/  1.001917, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2523%\n",
      "layer   2  Sparsity: 71.8370%\n",
      "layer   3  Sparsity: 75.8973%\n",
      "total_backward_count 1458710 real_backward_count 114773   7.868%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.651889/  1.024907, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2405%\n",
      "layer   2  Sparsity: 72.0181%\n",
      "layer   3  Sparsity: 76.1619%\n",
      "total_backward_count 1468500 real_backward_count 115182   7.844%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.654521/  1.017620, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.69 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2303%\n",
      "layer   2  Sparsity: 72.1719%\n",
      "layer   3  Sparsity: 76.2135%\n",
      "total_backward_count 1478290 real_backward_count 115574   7.818%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.658821/  1.000403, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2145%\n",
      "layer   2  Sparsity: 71.9441%\n",
      "layer   3  Sparsity: 75.6113%\n",
      "total_backward_count 1488080 real_backward_count 116038   7.798%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.649856/  0.990984, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2355%\n",
      "layer   2  Sparsity: 71.9207%\n",
      "layer   3  Sparsity: 75.5943%\n",
      "total_backward_count 1497870 real_backward_count 116509   7.778%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.654586/  0.967754, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.81 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.1911%\n",
      "layer   2  Sparsity: 71.9637%\n",
      "layer   3  Sparsity: 76.2009%\n",
      "total_backward_count 1507660 real_backward_count 116925   7.755%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.625251/  1.008510, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2192%\n",
      "layer   2  Sparsity: 71.8601%\n",
      "layer   3  Sparsity: 76.3923%\n",
      "total_backward_count 1517450 real_backward_count 117323   7.732%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.642223/  0.981631, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2332%\n",
      "layer   2  Sparsity: 71.9163%\n",
      "layer   3  Sparsity: 76.0932%\n",
      "total_backward_count 1527240 real_backward_count 117750   7.710%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.644495/  0.986201, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2150%\n",
      "layer   2  Sparsity: 71.8890%\n",
      "layer   3  Sparsity: 75.8857%\n",
      "total_backward_count 1537030 real_backward_count 118176   7.689%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.639688/  0.983053, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2385%\n",
      "layer   2  Sparsity: 71.8575%\n",
      "layer   3  Sparsity: 75.9658%\n",
      "total_backward_count 1546820 real_backward_count 118567   7.665%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.657206/  1.001959, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2281%\n",
      "layer   2  Sparsity: 71.8031%\n",
      "layer   3  Sparsity: 75.8987%\n",
      "total_backward_count 1556610 real_backward_count 119004   7.645%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.643111/  1.023450, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2301%\n",
      "layer   2  Sparsity: 71.7815%\n",
      "layer   3  Sparsity: 76.1325%\n",
      "total_backward_count 1566400 real_backward_count 119405   7.623%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.625802/  0.997661, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2335%\n",
      "layer   2  Sparsity: 71.9892%\n",
      "layer   3  Sparsity: 76.5411%\n",
      "total_backward_count 1576190 real_backward_count 119799   7.601%\n",
      "fc layer 3 self.abs_max_out: 1377.0\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.623695/  1.005917, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2238%\n",
      "layer   2  Sparsity: 71.9310%\n",
      "layer   3  Sparsity: 76.0066%\n",
      "total_backward_count 1585980 real_backward_count 120187   7.578%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.627038/  0.980519, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2208%\n",
      "layer   2  Sparsity: 72.0548%\n",
      "layer   3  Sparsity: 75.7904%\n",
      "total_backward_count 1595770 real_backward_count 120600   7.557%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.614143/  0.946642, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2141%\n",
      "layer   2  Sparsity: 72.0852%\n",
      "layer   3  Sparsity: 76.3297%\n",
      "total_backward_count 1605560 real_backward_count 121004   7.537%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.624112/  0.998515, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2184%\n",
      "layer   2  Sparsity: 71.9440%\n",
      "layer   3  Sparsity: 76.5494%\n",
      "total_backward_count 1615350 real_backward_count 121389   7.515%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.632005/  0.974399, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2271%\n",
      "layer   2  Sparsity: 71.7732%\n",
      "layer   3  Sparsity: 76.1954%\n",
      "total_backward_count 1625140 real_backward_count 121798   7.495%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.650197/  1.006160, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2303%\n",
      "layer   2  Sparsity: 71.7279%\n",
      "layer   3  Sparsity: 75.9622%\n",
      "total_backward_count 1634930 real_backward_count 122203   7.475%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.644648/  1.023229, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2478%\n",
      "layer   2  Sparsity: 71.6936%\n",
      "layer   3  Sparsity: 75.7556%\n",
      "total_backward_count 1644720 real_backward_count 122591   7.454%\n",
      "fc layer 1 self.abs_max_out: 7640.0\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.634565/  1.032134, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2205%\n",
      "layer   2  Sparsity: 71.8155%\n",
      "layer   3  Sparsity: 75.9288%\n",
      "total_backward_count 1654510 real_backward_count 123022   7.436%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.639595/  0.958107, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2437%\n",
      "layer   2  Sparsity: 71.8387%\n",
      "layer   3  Sparsity: 76.7531%\n",
      "total_backward_count 1664300 real_backward_count 123429   7.416%\n",
      "fc layer 1 self.abs_max_out: 7646.0\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.633329/  0.972478, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2186%\n",
      "layer   2  Sparsity: 71.8192%\n",
      "layer   3  Sparsity: 76.7184%\n",
      "total_backward_count 1674090 real_backward_count 123781   7.394%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.621597/  1.038338, val:  80.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.95 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.2178%\n",
      "layer   2  Sparsity: 71.8359%\n",
      "layer   3  Sparsity: 76.2153%\n",
      "total_backward_count 1683880 real_backward_count 124143   7.372%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.621889/  0.977130, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2432%\n",
      "layer   2  Sparsity: 72.0291%\n",
      "layer   3  Sparsity: 76.6212%\n",
      "total_backward_count 1693670 real_backward_count 124559   7.354%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.628393/  0.991149, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.47 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2368%\n",
      "layer   2  Sparsity: 71.9290%\n",
      "layer   3  Sparsity: 75.8456%\n",
      "total_backward_count 1703460 real_backward_count 124970   7.336%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.617782/  0.994016, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.60 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2424%\n",
      "layer   2  Sparsity: 71.8017%\n",
      "layer   3  Sparsity: 76.1130%\n",
      "total_backward_count 1713250 real_backward_count 125339   7.316%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.616100/  0.983620, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2027%\n",
      "layer   2  Sparsity: 71.7107%\n",
      "layer   3  Sparsity: 76.2186%\n",
      "total_backward_count 1723040 real_backward_count 125742   7.298%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.596249/  0.953519, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2395%\n",
      "layer   2  Sparsity: 71.8976%\n",
      "layer   3  Sparsity: 76.0993%\n",
      "total_backward_count 1732830 real_backward_count 126126   7.279%\n",
      "fc layer 1 self.abs_max_out: 7650.0\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.605021/  0.964259, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.38 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2490%\n",
      "layer   2  Sparsity: 71.9317%\n",
      "layer   3  Sparsity: 76.2342%\n",
      "total_backward_count 1742620 real_backward_count 126478   7.258%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.624076/  0.986623, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2315%\n",
      "layer   2  Sparsity: 71.7910%\n",
      "layer   3  Sparsity: 76.5658%\n",
      "total_backward_count 1752410 real_backward_count 126877   7.240%\n",
      "fc layer 1 self.abs_max_out: 7681.0\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.618695/  0.973626, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2272%\n",
      "layer   2  Sparsity: 71.7750%\n",
      "layer   3  Sparsity: 76.3924%\n",
      "total_backward_count 1762200 real_backward_count 127284   7.223%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.625850/  0.952382, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.39 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 94.2414%\n",
      "layer   2  Sparsity: 71.7781%\n",
      "layer   3  Sparsity: 75.9057%\n",
      "total_backward_count 1771990 real_backward_count 127678   7.205%\n",
      "fc layer 3 self.abs_max_out: 1390.0\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.595650/  0.941643, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.74 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.2198%\n",
      "layer   2  Sparsity: 71.8369%\n",
      "layer   3  Sparsity: 76.6353%\n",
      "total_backward_count 1781780 real_backward_count 128082   7.188%\n",
      "fc layer 1 self.abs_max_out: 7725.0\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.597800/  1.010484, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2313%\n",
      "layer   2  Sparsity: 72.0062%\n",
      "layer   3  Sparsity: 76.3163%\n",
      "total_backward_count 1791570 real_backward_count 128451   7.170%\n",
      "fc layer 1 self.abs_max_out: 7726.0\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.607393/  0.975746, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.49 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2179%\n",
      "layer   2  Sparsity: 71.8708%\n",
      "layer   3  Sparsity: 75.8127%\n",
      "total_backward_count 1801360 real_backward_count 128841   7.152%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.592088/  0.978640, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2371%\n",
      "layer   2  Sparsity: 71.8747%\n",
      "layer   3  Sparsity: 75.9842%\n",
      "total_backward_count 1811150 real_backward_count 129212   7.134%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.595966/  0.951111, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2237%\n",
      "layer   2  Sparsity: 71.7356%\n",
      "layer   3  Sparsity: 76.3399%\n",
      "total_backward_count 1820940 real_backward_count 129630   7.119%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.621787/  0.968281, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2310%\n",
      "layer   2  Sparsity: 71.6482%\n",
      "layer   3  Sparsity: 75.9540%\n",
      "total_backward_count 1830730 real_backward_count 130030   7.103%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.597883/  0.995289, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.2163%\n",
      "layer   2  Sparsity: 71.6735%\n",
      "layer   3  Sparsity: 75.9488%\n",
      "total_backward_count 1840520 real_backward_count 130426   7.086%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.585331/  0.944267, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2224%\n",
      "layer   2  Sparsity: 71.7431%\n",
      "layer   3  Sparsity: 75.6134%\n",
      "total_backward_count 1850310 real_backward_count 130791   7.069%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.583561/  0.901203, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2401%\n",
      "layer   2  Sparsity: 71.5590%\n",
      "layer   3  Sparsity: 76.4642%\n",
      "total_backward_count 1860100 real_backward_count 131222   7.055%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.583221/  0.946714, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.2284%\n",
      "layer   2  Sparsity: 71.5070%\n",
      "layer   3  Sparsity: 76.9891%\n",
      "total_backward_count 1869890 real_backward_count 131598   7.038%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.579984/  0.959983, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.2173%\n",
      "layer   2  Sparsity: 71.4355%\n",
      "layer   3  Sparsity: 76.5904%\n",
      "total_backward_count 1879680 real_backward_count 131956   7.020%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.576135/  0.965119, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.35 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 94.2240%\n",
      "layer   2  Sparsity: 71.7067%\n",
      "layer   3  Sparsity: 76.2388%\n",
      "total_backward_count 1889470 real_backward_count 132366   7.005%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.593603/  0.997217, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.88 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.2333%\n",
      "layer   2  Sparsity: 71.6431%\n",
      "layer   3  Sparsity: 76.3221%\n",
      "total_backward_count 1899260 real_backward_count 132756   6.990%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.593839/  0.958325, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.96 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 94.2260%\n",
      "layer   2  Sparsity: 71.6742%\n",
      "layer   3  Sparsity: 76.3942%\n",
      "total_backward_count 1909050 real_backward_count 133088   6.971%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.582585/  0.959239, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.70 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.2506%\n",
      "layer   2  Sparsity: 71.7059%\n",
      "layer   3  Sparsity: 76.1210%\n",
      "total_backward_count 1918840 real_backward_count 133460   6.955%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.581608/  0.951651, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.11 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.2357%\n",
      "layer   2  Sparsity: 71.6855%\n",
      "layer   3  Sparsity: 75.8653%\n",
      "total_backward_count 1928630 real_backward_count 133841   6.940%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.577825/  0.939041, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.46 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 94.2141%\n",
      "layer   2  Sparsity: 71.9011%\n",
      "layer   3  Sparsity: 76.4138%\n",
      "total_backward_count 1938420 real_backward_count 134184   6.922%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.562358/  0.961321, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.39 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 94.2436%\n",
      "layer   2  Sparsity: 71.6972%\n",
      "layer   3  Sparsity: 76.1178%\n",
      "total_backward_count 1948210 real_backward_count 134550   6.906%\n",
      "lif layer 1 self.abs_max_v: 11506.5\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.586620/  0.957923, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.69 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 94.2374%\n",
      "layer   2  Sparsity: 71.6322%\n",
      "layer   3  Sparsity: 75.8156%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49874ce698840418f224cc6f3842a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.58662</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>0.95792</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-sweep-319</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gm6vzct1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gm6vzct1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251126_213234-gm6vzct1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h81ia2dv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_015838-h81ia2dv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h81ia2dv' target=\"_blank\">gentle-sweep-322</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h81ia2dv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h81ia2dv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_015846_777', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 169.0\n",
      "lif layer 1 self.abs_max_v: 169.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 430.0\n",
      "lif layer 2 self.abs_max_v: 430.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "fc layer 1 self.abs_max_out: 227.0\n",
      "lif layer 1 self.abs_max_v: 227.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "lif layer 2 self.abs_max_v: 676.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "lif layer 1 self.abs_max_v: 299.0\n",
      "fc layer 2 self.abs_max_out: 540.0\n",
      "lif layer 2 self.abs_max_v: 794.0\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "fc layer 1 self.abs_max_out: 280.0\n",
      "lif layer 1 self.abs_max_v: 313.5\n",
      "fc layer 1 self.abs_max_out: 458.0\n",
      "lif layer 1 self.abs_max_v: 458.0\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "lif layer 1 self.abs_max_v: 528.5\n",
      "fc layer 2 self.abs_max_out: 551.0\n",
      "lif layer 2 self.abs_max_v: 944.0\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "fc layer 2 self.abs_max_out: 612.0\n",
      "lif layer 1 self.abs_max_v: 544.0\n",
      "fc layer 2 self.abs_max_out: 661.0\n",
      "lif layer 1 self.abs_max_v: 603.5\n",
      "lif layer 1 self.abs_max_v: 688.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 1 self.abs_max_out: 510.0\n",
      "lif layer 2 self.abs_max_v: 986.0\n",
      "fc layer 1 self.abs_max_out: 553.0\n",
      "fc layer 1 self.abs_max_out: 564.0\n",
      "fc layer 1 self.abs_max_out: 610.0\n",
      "fc layer 1 self.abs_max_out: 676.0\n",
      "lif layer 1 self.abs_max_v: 824.5\n",
      "fc layer 2 self.abs_max_out: 668.0\n",
      "fc layer 1 self.abs_max_out: 1062.0\n",
      "lif layer 1 self.abs_max_v: 1122.5\n",
      "lif layer 1 self.abs_max_v: 1123.5\n",
      "lif layer 2 self.abs_max_v: 1066.5\n",
      "lif layer 2 self.abs_max_v: 1103.0\n",
      "fc layer 2 self.abs_max_out: 673.0\n",
      "lif layer 1 self.abs_max_v: 1224.5\n",
      "lif layer 2 self.abs_max_v: 1121.0\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "lif layer 2 self.abs_max_v: 1144.5\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 512.0\n",
      "fc layer 2 self.abs_max_out: 674.0\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "fc layer 2 self.abs_max_out: 934.0\n",
      "lif layer 2 self.abs_max_v: 1162.5\n",
      "fc layer 1 self.abs_max_out: 1160.0\n",
      "fc layer 1 self.abs_max_out: 1287.0\n",
      "lif layer 1 self.abs_max_v: 1287.0\n",
      "fc layer 1 self.abs_max_out: 1501.0\n",
      "lif layer 1 self.abs_max_v: 1501.0\n",
      "lif layer 2 self.abs_max_v: 1242.5\n",
      "lif layer 1 self.abs_max_v: 1598.0\n",
      "lif layer 1 self.abs_max_v: 1829.0\n",
      "lif layer 1 self.abs_max_v: 2025.5\n",
      "lif layer 2 self.abs_max_v: 1343.0\n",
      "lif layer 2 self.abs_max_v: 1345.0\n",
      "lif layer 2 self.abs_max_v: 1348.0\n",
      "lif layer 2 self.abs_max_v: 1374.5\n",
      "lif layer 2 self.abs_max_v: 1391.0\n",
      "lif layer 2 self.abs_max_v: 1436.5\n",
      "lif layer 2 self.abs_max_v: 1528.5\n",
      "lif layer 2 self.abs_max_v: 1531.5\n",
      "lif layer 1 self.abs_max_v: 2084.0\n",
      "fc layer 1 self.abs_max_out: 1641.0\n",
      "lif layer 1 self.abs_max_v: 2435.0\n",
      "fc layer 2 self.abs_max_out: 958.0\n",
      "fc layer 2 self.abs_max_out: 988.0\n",
      "lif layer 2 self.abs_max_v: 1601.5\n",
      "lif layer 2 self.abs_max_v: 1624.0\n",
      "lif layer 2 self.abs_max_v: 1738.0\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "lif layer 2 self.abs_max_v: 1749.5\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 2 self.abs_max_out: 1037.0\n",
      "lif layer 2 self.abs_max_v: 1802.0\n",
      "lif layer 2 self.abs_max_v: 1834.0\n",
      "lif layer 2 self.abs_max_v: 1865.0\n",
      "lif layer 2 self.abs_max_v: 1895.5\n",
      "lif layer 2 self.abs_max_v: 1975.0\n",
      "lif layer 2 self.abs_max_v: 1995.5\n",
      "fc layer 1 self.abs_max_out: 1658.0\n",
      "lif layer 1 self.abs_max_v: 2580.5\n",
      "lif layer 1 self.abs_max_v: 2660.5\n",
      "fc layer 1 self.abs_max_out: 1756.0\n",
      "fc layer 2 self.abs_max_out: 1063.0\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "lif layer 1 self.abs_max_v: 2724.0\n",
      "fc layer 2 self.abs_max_out: 1067.0\n",
      "fc layer 1 self.abs_max_out: 1803.0\n",
      "lif layer 1 self.abs_max_v: 2784.0\n",
      "lif layer 1 self.abs_max_v: 2788.0\n",
      "lif layer 1 self.abs_max_v: 2996.0\n",
      "lif layer 1 self.abs_max_v: 3214.0\n",
      "fc layer 2 self.abs_max_out: 1093.0\n",
      "fc layer 1 self.abs_max_out: 1880.0\n",
      "lif layer 1 self.abs_max_v: 3269.5\n",
      "lif layer 1 self.abs_max_v: 3276.5\n",
      "fc layer 2 self.abs_max_out: 1145.0\n",
      "lif layer 1 self.abs_max_v: 3312.5\n",
      "lif layer 2 self.abs_max_v: 2027.5\n",
      "fc layer 3 self.abs_max_out: 632.0\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "fc layer 3 self.abs_max_out: 657.0\n",
      "fc layer 1 self.abs_max_out: 1886.0\n",
      "fc layer 2 self.abs_max_out: 1188.0\n",
      "fc layer 2 self.abs_max_out: 1224.0\n",
      "fc layer 3 self.abs_max_out: 658.0\n",
      "lif layer 2 self.abs_max_v: 2107.0\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "lif layer 2 self.abs_max_v: 2135.5\n",
      "lif layer 2 self.abs_max_v: 2247.0\n",
      "lif layer 2 self.abs_max_v: 2278.5\n",
      "fc layer 1 self.abs_max_out: 1901.0\n",
      "fc layer 2 self.abs_max_out: 1258.0\n",
      "fc layer 1 self.abs_max_out: 2006.0\n",
      "lif layer 1 self.abs_max_v: 3507.5\n",
      "fc layer 1 self.abs_max_out: 2041.0\n",
      "lif layer 1 self.abs_max_v: 3710.5\n",
      "lif layer 1 self.abs_max_v: 3711.5\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "fc layer 2 self.abs_max_out: 1263.0\n",
      "fc layer 1 self.abs_max_out: 2134.0\n",
      "lif layer 1 self.abs_max_v: 3892.0\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "fc layer 2 self.abs_max_out: 1318.0\n",
      "fc layer 2 self.abs_max_out: 1346.0\n",
      "lif layer 2 self.abs_max_v: 2306.0\n",
      "fc layer 1 self.abs_max_out: 2152.0\n",
      "fc layer 2 self.abs_max_out: 1361.0\n",
      "fc layer 1 self.abs_max_out: 2253.0\n",
      "lif layer 1 self.abs_max_v: 3904.0\n",
      "lif layer 1 self.abs_max_v: 4110.0\n",
      "fc layer 1 self.abs_max_out: 2313.0\n",
      "fc layer 1 self.abs_max_out: 2389.0\n",
      "lif layer 1 self.abs_max_v: 4258.5\n",
      "lif layer 1 self.abs_max_v: 4274.5\n",
      "lif layer 1 self.abs_max_v: 4447.5\n",
      "lif layer 1 self.abs_max_v: 4530.0\n",
      "fc layer 1 self.abs_max_out: 2490.0\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.346197/  1.953283, val:  30.00%, val_best:  30.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1679%\n",
      "layer   2  Sparsity: 68.2785%\n",
      "layer   3  Sparsity: 56.0085%\n",
      "total_backward_count 9790 real_backward_count 1230  12.564%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 749.0\n",
      "fc layer 3 self.abs_max_out: 763.0\n",
      "fc layer 1 self.abs_max_out: 2511.0\n",
      "lif layer 1 self.abs_max_v: 4537.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "fc layer 3 self.abs_max_out: 863.0\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "fc layer 1 self.abs_max_out: 2520.0\n",
      "lif layer 1 self.abs_max_v: 4540.0\n",
      "fc layer 3 self.abs_max_out: 961.0\n",
      "fc layer 1 self.abs_max_out: 2545.0\n",
      "fc layer 1 self.abs_max_out: 2641.0\n",
      "fc layer 1 self.abs_max_out: 2710.0\n",
      "lif layer 2 self.abs_max_v: 2348.5\n",
      "lif layer 1 self.abs_max_v: 4703.0\n",
      "lif layer 2 self.abs_max_v: 2456.5\n",
      "lif layer 2 self.abs_max_v: 2482.5\n",
      "fc layer 1 self.abs_max_out: 2738.0\n",
      "fc layer 1 self.abs_max_out: 3135.0\n",
      "fc layer 1 self.abs_max_out: 3214.0\n",
      "lif layer 2 self.abs_max_v: 2507.0\n",
      "lif layer 2 self.abs_max_v: 2511.5\n",
      "lif layer 2 self.abs_max_v: 2555.0\n",
      "fc layer 2 self.abs_max_out: 1385.0\n",
      "lif layer 1 self.abs_max_v: 4799.0\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "lif layer 2 self.abs_max_v: 2605.5\n",
      "lif layer 2 self.abs_max_v: 2631.5\n",
      "fc layer 2 self.abs_max_out: 1394.0\n",
      "fc layer 2 self.abs_max_out: 1401.0\n",
      "fc layer 2 self.abs_max_out: 1492.0\n",
      "fc layer 2 self.abs_max_out: 1511.0\n",
      "lif layer 2 self.abs_max_v: 2693.5\n",
      "lif layer 2 self.abs_max_v: 2697.0\n",
      "lif layer 2 self.abs_max_v: 2757.5\n",
      "fc layer 2 self.abs_max_out: 1533.0\n",
      "fc layer 2 self.abs_max_out: 1585.0\n",
      "lif layer 2 self.abs_max_v: 2885.0\n",
      "fc layer 2 self.abs_max_out: 1609.0\n",
      "fc layer 2 self.abs_max_out: 1638.0\n",
      "fc layer 2 self.abs_max_out: 1717.0\n",
      "lif layer 2 self.abs_max_v: 2996.5\n",
      "lif layer 2 self.abs_max_v: 2998.5\n",
      "lif layer 1 self.abs_max_v: 5132.0\n",
      "lif layer 1 self.abs_max_v: 5204.0\n",
      "lif layer 1 self.abs_max_v: 5382.5\n",
      "lif layer 1 self.abs_max_v: 5530.5\n",
      "lif layer 1 self.abs_max_v: 5629.5\n",
      "lif layer 1 self.abs_max_v: 5727.5\n",
      "lif layer 2 self.abs_max_v: 3002.5\n",
      "lif layer 2 self.abs_max_v: 3007.5\n",
      "lif layer 2 self.abs_max_v: 3010.5\n",
      "lif layer 2 self.abs_max_v: 3056.0\n",
      "lif layer 2 self.abs_max_v: 3058.0\n",
      "lif layer 2 self.abs_max_v: 3170.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.196493/  1.834762, val:  29.58%, val_best:  30.00%, tr:  99.49%, tr_best:  99.59%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1943%\n",
      "layer   2  Sparsity: 70.0839%\n",
      "layer   3  Sparsity: 60.1674%\n",
      "total_backward_count 19580 real_backward_count 2420  12.360%\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "fc layer 1 self.abs_max_out: 3323.0\n",
      "fc layer 3 self.abs_max_out: 982.0\n",
      "fc layer 3 self.abs_max_out: 984.0\n",
      "fc layer 2 self.abs_max_out: 1846.0\n",
      "lif layer 2 self.abs_max_v: 3188.5\n",
      "lif layer 2 self.abs_max_v: 3294.5\n",
      "fc layer 2 self.abs_max_out: 1940.0\n",
      "lif layer 2 self.abs_max_v: 3319.5\n",
      "lif layer 1 self.abs_max_v: 6017.0\n",
      "lif layer 1 self.abs_max_v: 6115.5\n",
      "lif layer 1 self.abs_max_v: 6187.0\n",
      "fc layer 1 self.abs_max_out: 3381.0\n",
      "lif layer 1 self.abs_max_v: 6474.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.139401/  1.658168, val:  42.50%, val_best:  42.50%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.2017%\n",
      "layer   2  Sparsity: 71.9791%\n",
      "layer   3  Sparsity: 60.7614%\n",
      "total_backward_count 29370 real_backward_count 3568  12.148%\n",
      "lif layer 2 self.abs_max_v: 3337.5\n",
      "lif layer 2 self.abs_max_v: 3368.0\n",
      "lif layer 2 self.abs_max_v: 3389.5\n",
      "fc layer 2 self.abs_max_out: 1963.0\n",
      "lif layer 2 self.abs_max_v: 3390.0\n",
      "lif layer 2 self.abs_max_v: 3405.0\n",
      "lif layer 2 self.abs_max_v: 3418.5\n",
      "lif layer 2 self.abs_max_v: 3480.0\n",
      "lif layer 2 self.abs_max_v: 3591.5\n",
      "lif layer 2 self.abs_max_v: 3680.0\n",
      "fc layer 1 self.abs_max_out: 3645.0\n",
      "lif layer 1 self.abs_max_v: 6492.0\n",
      "lif layer 1 self.abs_max_v: 6843.0\n",
      "fc layer 3 self.abs_max_out: 987.0\n",
      "fc layer 1 self.abs_max_out: 3738.0\n",
      "lif layer 1 self.abs_max_v: 7058.5\n",
      "lif layer 1 self.abs_max_v: 7190.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.111786/  1.701893, val:  41.67%, val_best:  42.50%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1697%\n",
      "layer   2  Sparsity: 73.5893%\n",
      "layer   3  Sparsity: 63.8777%\n",
      "total_backward_count 39160 real_backward_count 4758  12.150%\n",
      "fc layer 1 self.abs_max_out: 3783.0\n",
      "fc layer 1 self.abs_max_out: 3823.0\n",
      "fc layer 1 self.abs_max_out: 3951.0\n",
      "fc layer 1 self.abs_max_out: 4047.0\n",
      "lif layer 1 self.abs_max_v: 7306.0\n",
      "lif layer 1 self.abs_max_v: 7624.0\n",
      "lif layer 1 self.abs_max_v: 7764.0\n",
      "fc layer 1 self.abs_max_out: 4095.0\n",
      "lif layer 1 self.abs_max_v: 7977.0\n",
      "fc layer 3 self.abs_max_out: 1010.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.125787/  1.604025, val:  49.17%, val_best:  49.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1733%\n",
      "layer   2  Sparsity: 74.6122%\n",
      "layer   3  Sparsity: 64.7259%\n",
      "total_backward_count 48950 real_backward_count 5888  12.029%\n",
      "fc layer 1 self.abs_max_out: 4340.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.088891/  1.588934, val:  55.83%, val_best:  55.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1652%\n",
      "layer   2  Sparsity: 75.4159%\n",
      "layer   3  Sparsity: 64.1241%\n",
      "total_backward_count 58740 real_backward_count 7009  11.932%\n",
      "fc layer 2 self.abs_max_out: 1997.0\n",
      "lif layer 1 self.abs_max_v: 8121.0\n",
      "lif layer 1 self.abs_max_v: 8330.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.060745/  1.639459, val:  47.92%, val_best:  55.83%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1641%\n",
      "layer   2  Sparsity: 74.9467%\n",
      "layer   3  Sparsity: 65.1284%\n",
      "total_backward_count 68530 real_backward_count 8122  11.852%\n",
      "fc layer 3 self.abs_max_out: 1031.0\n",
      "lif layer 2 self.abs_max_v: 3694.5\n",
      "fc layer 1 self.abs_max_out: 4542.0\n",
      "lif layer 1 self.abs_max_v: 8356.5\n",
      "fc layer 3 self.abs_max_out: 1045.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.055768/  1.499830, val:  58.75%, val_best:  58.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1827%\n",
      "layer   2  Sparsity: 75.0658%\n",
      "layer   3  Sparsity: 66.8186%\n",
      "total_backward_count 78320 real_backward_count 9183  11.725%\n",
      "fc layer 2 self.abs_max_out: 2002.0\n",
      "fc layer 2 self.abs_max_out: 2016.0\n",
      "fc layer 2 self.abs_max_out: 2092.0\n",
      "lif layer 2 self.abs_max_v: 3771.5\n",
      "lif layer 2 self.abs_max_v: 3897.0\n",
      "lif layer 2 self.abs_max_v: 3909.5\n",
      "lif layer 1 self.abs_max_v: 8392.0\n",
      "lif layer 1 self.abs_max_v: 8507.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.052070/  1.552046, val:  48.33%, val_best:  58.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1991%\n",
      "layer   2  Sparsity: 74.6071%\n",
      "layer   3  Sparsity: 66.5790%\n",
      "total_backward_count 88110 real_backward_count 10286  11.674%\n",
      "lif layer 2 self.abs_max_v: 3985.5\n",
      "fc layer 2 self.abs_max_out: 2137.0\n",
      "fc layer 2 self.abs_max_out: 2138.0\n",
      "fc layer 1 self.abs_max_out: 4806.0\n",
      "fc layer 1 self.abs_max_out: 4845.0\n",
      "lif layer 1 self.abs_max_v: 8635.0\n",
      "lif layer 1 self.abs_max_v: 9068.5\n",
      "lif layer 2 self.abs_max_v: 4020.5\n",
      "fc layer 3 self.abs_max_out: 1082.0\n",
      "fc layer 2 self.abs_max_out: 2247.0\n",
      "lif layer 2 self.abs_max_v: 4033.0\n",
      "lif layer 2 self.abs_max_v: 4149.0\n",
      "lif layer 2 self.abs_max_v: 4235.5\n",
      "lif layer 2 self.abs_max_v: 4245.0\n",
      "fc layer 1 self.abs_max_out: 4849.0\n",
      "lif layer 1 self.abs_max_v: 9275.0\n",
      "lif layer 1 self.abs_max_v: 9380.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.043688/  1.563579, val:  54.58%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1867%\n",
      "layer   2  Sparsity: 74.8203%\n",
      "layer   3  Sparsity: 67.1224%\n",
      "total_backward_count 97900 real_backward_count 11344  11.587%\n",
      "fc layer 3 self.abs_max_out: 1083.0\n",
      "fc layer 1 self.abs_max_out: 4896.0\n",
      "fc layer 1 self.abs_max_out: 5175.0\n",
      "lif layer 1 self.abs_max_v: 9478.0\n",
      "lif layer 1 self.abs_max_v: 9669.0\n",
      "lif layer 1 self.abs_max_v: 9987.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.035582/  1.567560, val:  45.83%, val_best:  58.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1793%\n",
      "layer   2  Sparsity: 74.5005%\n",
      "layer   3  Sparsity: 68.1378%\n",
      "total_backward_count 107690 real_backward_count 12402  11.516%\n",
      "fc layer 1 self.abs_max_out: 5211.0\n",
      "fc layer 1 self.abs_max_out: 5427.0\n",
      "lif layer 1 self.abs_max_v: 10263.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.038276/  1.466544, val:  53.75%, val_best:  58.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1420%\n",
      "layer   2  Sparsity: 75.0900%\n",
      "layer   3  Sparsity: 68.2578%\n",
      "total_backward_count 117480 real_backward_count 13484  11.478%\n",
      "fc layer 3 self.abs_max_out: 1091.0\n",
      "fc layer 3 self.abs_max_out: 1093.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.998672/  1.512626, val:  41.25%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.1682%\n",
      "layer   2  Sparsity: 74.7212%\n",
      "layer   3  Sparsity: 68.0224%\n",
      "total_backward_count 127270 real_backward_count 14492  11.387%\n",
      "fc layer 2 self.abs_max_out: 2399.0\n",
      "lif layer 2 self.abs_max_v: 4270.5\n",
      "lif layer 2 self.abs_max_v: 4382.5\n",
      "lif layer 2 self.abs_max_v: 4541.5\n",
      "fc layer 2 self.abs_max_out: 2400.0\n",
      "lif layer 2 self.abs_max_v: 4596.5\n",
      "lif layer 2 self.abs_max_v: 4697.5\n",
      "fc layer 2 self.abs_max_out: 2481.0\n",
      "lif layer 2 self.abs_max_v: 4830.0\n",
      "fc layer 3 self.abs_max_out: 1102.0\n",
      "fc layer 2 self.abs_max_out: 2497.0\n",
      "fc layer 2 self.abs_max_out: 2575.0\n",
      "fc layer 1 self.abs_max_out: 5493.0\n",
      "fc layer 1 self.abs_max_out: 5550.0\n",
      "lif layer 1 self.abs_max_v: 10652.0\n",
      "lif layer 1 self.abs_max_v: 10784.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.024545/  1.578080, val:  44.58%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1902%\n",
      "layer   2  Sparsity: 74.0900%\n",
      "layer   3  Sparsity: 68.0678%\n",
      "total_backward_count 137060 real_backward_count 15491  11.302%\n",
      "lif layer 2 self.abs_max_v: 4874.5\n",
      "lif layer 2 self.abs_max_v: 4882.5\n",
      "fc layer 2 self.abs_max_out: 2590.0\n",
      "fc layer 2 self.abs_max_out: 2596.0\n",
      "fc layer 2 self.abs_max_out: 2631.0\n",
      "lif layer 2 self.abs_max_v: 4911.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.012166/  1.540957, val:  48.33%, val_best:  58.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1583%\n",
      "layer   2  Sparsity: 74.5357%\n",
      "layer   3  Sparsity: 67.7798%\n",
      "total_backward_count 146850 real_backward_count 16483  11.224%\n",
      "fc layer 1 self.abs_max_out: 5601.0\n",
      "lif layer 1 self.abs_max_v: 10815.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.968475/  1.405087, val:  55.83%, val_best:  58.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1620%\n",
      "layer   2  Sparsity: 74.5385%\n",
      "layer   3  Sparsity: 68.0612%\n",
      "total_backward_count 156640 real_backward_count 17519  11.184%\n",
      "fc layer 3 self.abs_max_out: 1125.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.968280/  1.406281, val:  53.75%, val_best:  58.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1804%\n",
      "layer   2  Sparsity: 74.5158%\n",
      "layer   3  Sparsity: 68.5583%\n",
      "total_backward_count 166430 real_backward_count 18506  11.119%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.000595/  1.441809, val:  57.08%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1415%\n",
      "layer   2  Sparsity: 74.3890%\n",
      "layer   3  Sparsity: 69.4778%\n",
      "total_backward_count 176220 real_backward_count 19537  11.087%\n",
      "fc layer 1 self.abs_max_out: 5660.0\n",
      "fc layer 1 self.abs_max_out: 5768.0\n",
      "lif layer 1 self.abs_max_v: 11009.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.984995/  1.376636, val:  57.50%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1565%\n",
      "layer   2  Sparsity: 74.3970%\n",
      "layer   3  Sparsity: 69.8125%\n",
      "total_backward_count 186010 real_backward_count 20579  11.063%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.982594/  1.531036, val:  42.92%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1578%\n",
      "layer   2  Sparsity: 74.8548%\n",
      "layer   3  Sparsity: 69.3801%\n",
      "total_backward_count 195800 real_backward_count 21551  11.007%\n",
      "fc layer 2 self.abs_max_out: 2738.0\n",
      "fc layer 1 self.abs_max_out: 5875.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.963815/  1.533314, val:  50.83%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1615%\n",
      "layer   2  Sparsity: 74.4727%\n",
      "layer   3  Sparsity: 68.6089%\n",
      "total_backward_count 205590 real_backward_count 22522  10.955%\n",
      "fc layer 1 self.abs_max_out: 5952.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.960681/  1.412129, val:  61.25%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1782%\n",
      "layer   2  Sparsity: 74.5024%\n",
      "layer   3  Sparsity: 68.1486%\n",
      "total_backward_count 215380 real_backward_count 23497  10.910%\n",
      "lif layer 1 self.abs_max_v: 11167.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.939843/  1.382162, val:  58.75%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1680%\n",
      "layer   2  Sparsity: 74.7183%\n",
      "layer   3  Sparsity: 68.1427%\n",
      "total_backward_count 225170 real_backward_count 24469  10.867%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.928456/  1.422139, val:  51.25%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1293%\n",
      "layer   2  Sparsity: 74.0274%\n",
      "layer   3  Sparsity: 67.1210%\n",
      "total_backward_count 234960 real_backward_count 25493  10.850%\n",
      "lif layer 2 self.abs_max_v: 4944.5\n",
      "fc layer 1 self.abs_max_out: 5993.0\n",
      "fc layer 1 self.abs_max_out: 6131.0\n",
      "lif layer 1 self.abs_max_v: 11181.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.905327/  1.406975, val:  63.33%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1850%\n",
      "layer   2  Sparsity: 74.1184%\n",
      "layer   3  Sparsity: 67.5620%\n",
      "total_backward_count 244750 real_backward_count 26460  10.811%\n",
      "lif layer 2 self.abs_max_v: 4959.5\n",
      "lif layer 2 self.abs_max_v: 5032.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.906895/  1.316995, val:  63.33%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1619%\n",
      "layer   2  Sparsity: 73.8790%\n",
      "layer   3  Sparsity: 68.0292%\n",
      "total_backward_count 254540 real_backward_count 27481  10.796%\n",
      "fc layer 2 self.abs_max_out: 2902.0\n",
      "lif layer 2 self.abs_max_v: 5138.5\n",
      "lif layer 2 self.abs_max_v: 5213.5\n",
      "lif layer 2 self.abs_max_v: 5391.0\n",
      "lif layer 2 self.abs_max_v: 5419.5\n",
      "lif layer 2 self.abs_max_v: 5450.0\n",
      "lif layer 2 self.abs_max_v: 5474.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.887560/  1.305228, val:  64.17%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1682%\n",
      "layer   2  Sparsity: 73.6660%\n",
      "layer   3  Sparsity: 67.6511%\n",
      "total_backward_count 264330 real_backward_count 28453  10.764%\n",
      "fc layer 3 self.abs_max_out: 1159.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.901201/  1.318708, val:  65.83%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1252%\n",
      "layer   2  Sparsity: 73.3028%\n",
      "layer   3  Sparsity: 67.1179%\n",
      "total_backward_count 274120 real_backward_count 29412  10.730%\n",
      "fc layer 3 self.abs_max_out: 1178.0\n",
      "lif layer 1 self.abs_max_v: 11251.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.845500/  1.390399, val:  52.50%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1603%\n",
      "layer   2  Sparsity: 73.1736%\n",
      "layer   3  Sparsity: 67.4653%\n",
      "total_backward_count 283910 real_backward_count 30326  10.682%\n",
      "fc layer 2 self.abs_max_out: 3037.0\n",
      "lif layer 2 self.abs_max_v: 5492.5\n",
      "lif layer 2 self.abs_max_v: 5512.0\n",
      "fc layer 2 self.abs_max_out: 3053.0\n",
      "lif layer 2 self.abs_max_v: 5553.5\n",
      "lif layer 2 self.abs_max_v: 5684.0\n",
      "lif layer 2 self.abs_max_v: 5689.0\n",
      "lif layer 2 self.abs_max_v: 5769.5\n",
      "lif layer 1 self.abs_max_v: 11274.5\n",
      "lif layer 1 self.abs_max_v: 11750.5\n",
      "lif layer 1 self.abs_max_v: 11915.5\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.870364/  1.384156, val:  55.00%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1619%\n",
      "layer   2  Sparsity: 73.6129%\n",
      "layer   3  Sparsity: 68.2048%\n",
      "total_backward_count 293700 real_backward_count 31264  10.645%\n",
      "fc layer 1 self.abs_max_out: 6238.0\n",
      "fc layer 3 self.abs_max_out: 1193.0\n",
      "fc layer 3 self.abs_max_out: 1204.0\n",
      "fc layer 1 self.abs_max_out: 6439.0\n",
      "fc layer 1 self.abs_max_out: 6592.0\n",
      "lif layer 1 self.abs_max_v: 12151.0\n",
      "lif layer 1 self.abs_max_v: 12569.5\n",
      "fc layer 1 self.abs_max_out: 6776.0\n",
      "lif layer 1 self.abs_max_v: 13061.0\n",
      "lif layer 1 self.abs_max_v: 13230.5\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.886717/  1.316493, val:  61.67%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1988%\n",
      "layer   2  Sparsity: 73.6666%\n",
      "layer   3  Sparsity: 68.1310%\n",
      "total_backward_count 303490 real_backward_count 32203  10.611%\n",
      "fc layer 1 self.abs_max_out: 6856.0\n",
      "lif layer 1 self.abs_max_v: 13257.5\n",
      "lif layer 1 self.abs_max_v: 13391.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.873058/  1.447060, val:  50.00%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1458%\n",
      "layer   2  Sparsity: 74.0108%\n",
      "layer   3  Sparsity: 68.2037%\n",
      "total_backward_count 313280 real_backward_count 33173  10.589%\n",
      "lif layer 2 self.abs_max_v: 5780.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.873951/  1.356110, val:  61.25%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1582%\n",
      "layer   2  Sparsity: 73.8917%\n",
      "layer   3  Sparsity: 68.4124%\n",
      "total_backward_count 323070 real_backward_count 34075  10.547%\n",
      "fc layer 3 self.abs_max_out: 1253.0\n",
      "fc layer 3 self.abs_max_out: 1264.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.876873/  1.402704, val:  54.58%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1906%\n",
      "layer   2  Sparsity: 74.5976%\n",
      "layer   3  Sparsity: 68.8685%\n",
      "total_backward_count 332860 real_backward_count 35038  10.526%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.850251/  1.375456, val:  52.50%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1625%\n",
      "layer   2  Sparsity: 74.2949%\n",
      "layer   3  Sparsity: 68.8708%\n",
      "total_backward_count 342650 real_backward_count 35954  10.493%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.843461/  1.360840, val:  53.75%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1549%\n",
      "layer   2  Sparsity: 73.5504%\n",
      "layer   3  Sparsity: 69.3644%\n",
      "total_backward_count 352440 real_backward_count 36845  10.454%\n",
      "fc layer 1 self.abs_max_out: 6869.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.845583/  1.307414, val:  60.83%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.2049%\n",
      "layer   2  Sparsity: 73.1345%\n",
      "layer   3  Sparsity: 68.7231%\n",
      "total_backward_count 362230 real_backward_count 37749  10.421%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.856642/  1.385859, val:  60.00%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1945%\n",
      "layer   2  Sparsity: 72.9022%\n",
      "layer   3  Sparsity: 69.3438%\n",
      "total_backward_count 372020 real_backward_count 38598  10.375%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.880498/  1.421806, val:  47.92%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1228%\n",
      "layer   2  Sparsity: 72.9285%\n",
      "layer   3  Sparsity: 69.0171%\n",
      "total_backward_count 381810 real_backward_count 39529  10.353%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.824457/  1.318712, val:  57.50%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1815%\n",
      "layer   2  Sparsity: 73.2367%\n",
      "layer   3  Sparsity: 68.2981%\n",
      "total_backward_count 391600 real_backward_count 40444  10.328%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.830111/  1.245027, val:  63.33%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1516%\n",
      "layer   2  Sparsity: 73.5360%\n",
      "layer   3  Sparsity: 67.9353%\n",
      "total_backward_count 401390 real_backward_count 41340  10.299%\n",
      "fc layer 1 self.abs_max_out: 7093.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.811352/  1.224123, val:  71.67%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1646%\n",
      "layer   2  Sparsity: 73.4642%\n",
      "layer   3  Sparsity: 69.2774%\n",
      "total_backward_count 411180 real_backward_count 42252  10.276%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.809207/  1.279694, val:  69.58%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.2020%\n",
      "layer   2  Sparsity: 73.6347%\n",
      "layer   3  Sparsity: 69.4512%\n",
      "total_backward_count 420970 real_backward_count 43133  10.246%\n",
      "fc layer 1 self.abs_max_out: 7145.0\n",
      "fc layer 1 self.abs_max_out: 7687.0\n",
      "lif layer 1 self.abs_max_v: 14132.0\n",
      "fc layer 1 self.abs_max_out: 7869.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.821661/  1.284961, val:  62.08%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.2087%\n",
      "layer   2  Sparsity: 73.0646%\n",
      "layer   3  Sparsity: 70.0521%\n",
      "total_backward_count 430760 real_backward_count 44024  10.220%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.816465/  1.281093, val:  57.08%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.2222%\n",
      "layer   2  Sparsity: 72.9323%\n",
      "layer   3  Sparsity: 70.7018%\n",
      "total_backward_count 440550 real_backward_count 44922  10.197%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.831222/  1.289576, val:  56.25%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1441%\n",
      "layer   2  Sparsity: 73.3385%\n",
      "layer   3  Sparsity: 70.7972%\n",
      "total_backward_count 450340 real_backward_count 45786  10.167%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.832573/  1.345446, val:  50.00%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1444%\n",
      "layer   2  Sparsity: 74.0023%\n",
      "layer   3  Sparsity: 69.9259%\n",
      "total_backward_count 460130 real_backward_count 46661  10.141%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.807884/  1.460745, val:  48.33%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.2059%\n",
      "layer   2  Sparsity: 74.4176%\n",
      "layer   3  Sparsity: 69.4506%\n",
      "total_backward_count 469920 real_backward_count 47551  10.119%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.824880/  1.336730, val:  55.42%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1310%\n",
      "layer   2  Sparsity: 74.6032%\n",
      "layer   3  Sparsity: 70.2963%\n",
      "total_backward_count 479710 real_backward_count 48437  10.097%\n",
      "fc layer 1 self.abs_max_out: 8072.0\n",
      "lif layer 1 self.abs_max_v: 14168.5\n",
      "lif layer 1 self.abs_max_v: 14506.5\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.835917/  1.269474, val:  60.42%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1557%\n",
      "layer   2  Sparsity: 74.4754%\n",
      "layer   3  Sparsity: 70.6177%\n",
      "total_backward_count 489500 real_backward_count 49362  10.084%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.805570/  1.261845, val:  61.25%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1863%\n",
      "layer   2  Sparsity: 74.1845%\n",
      "layer   3  Sparsity: 70.9302%\n",
      "total_backward_count 499290 real_backward_count 50234  10.061%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.770496/  1.171706, val:  70.83%, val_best:  71.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1421%\n",
      "layer   2  Sparsity: 74.6517%\n",
      "layer   3  Sparsity: 69.9881%\n",
      "total_backward_count 509080 real_backward_count 51082  10.034%\n",
      "fc layer 1 self.abs_max_out: 8557.0\n",
      "lif layer 1 self.abs_max_v: 14875.0\n",
      "lif layer 1 self.abs_max_v: 15449.0\n",
      "lif layer 1 self.abs_max_v: 16076.5\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.775282/  1.169901, val:  75.42%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1775%\n",
      "layer   2  Sparsity: 74.7716%\n",
      "layer   3  Sparsity: 70.0715%\n",
      "total_backward_count 518870 real_backward_count 51943  10.011%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.781503/  1.229137, val:  66.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1640%\n",
      "layer   2  Sparsity: 74.6747%\n",
      "layer   3  Sparsity: 70.1087%\n",
      "total_backward_count 528660 real_backward_count 52793   9.986%\n",
      "fc layer 1 self.abs_max_out: 8731.0\n",
      "lif layer 1 self.abs_max_v: 16078.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.750659/  1.202477, val:  64.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1822%\n",
      "layer   2  Sparsity: 74.5509%\n",
      "layer   3  Sparsity: 69.2786%\n",
      "total_backward_count 538450 real_backward_count 53626   9.959%\n",
      "fc layer 3 self.abs_max_out: 1268.0\n",
      "fc layer 1 self.abs_max_out: 8908.0\n",
      "lif layer 1 self.abs_max_v: 16633.0\n",
      "lif layer 1 self.abs_max_v: 16730.5\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.765844/  1.249515, val:  63.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1852%\n",
      "layer   2  Sparsity: 74.4630%\n",
      "layer   3  Sparsity: 70.1320%\n",
      "total_backward_count 548240 real_backward_count 54513   9.943%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.765659/  1.225129, val:  69.58%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1427%\n",
      "layer   2  Sparsity: 74.1339%\n",
      "layer   3  Sparsity: 71.0344%\n",
      "total_backward_count 558030 real_backward_count 55404   9.928%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.775591/  1.152467, val:  73.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1863%\n",
      "layer   2  Sparsity: 74.7057%\n",
      "layer   3  Sparsity: 70.9166%\n",
      "total_backward_count 567820 real_backward_count 56244   9.905%\n",
      "lif layer 1 self.abs_max_v: 16738.5\n",
      "fc layer 1 self.abs_max_out: 8976.0\n",
      "lif layer 1 self.abs_max_v: 17345.5\n",
      "lif layer 1 self.abs_max_v: 17433.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.761479/  1.183083, val:  76.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1794%\n",
      "layer   2  Sparsity: 74.6176%\n",
      "layer   3  Sparsity: 70.6516%\n",
      "total_backward_count 577610 real_backward_count 57053   9.877%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.750901/  1.219437, val:  57.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1795%\n",
      "layer   2  Sparsity: 74.6165%\n",
      "layer   3  Sparsity: 71.5209%\n",
      "total_backward_count 587400 real_backward_count 57835   9.846%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.771487/  1.199050, val:  62.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1833%\n",
      "layer   2  Sparsity: 74.4219%\n",
      "layer   3  Sparsity: 70.6044%\n",
      "total_backward_count 597190 real_backward_count 58647   9.820%\n",
      "fc layer 3 self.abs_max_out: 1276.0\n",
      "fc layer 3 self.abs_max_out: 1334.0\n",
      "fc layer 3 self.abs_max_out: 1342.0\n",
      "fc layer 3 self.abs_max_out: 1366.0\n",
      "fc layer 3 self.abs_max_out: 1379.0\n",
      "fc layer 3 self.abs_max_out: 1408.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.773612/  1.134190, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1476%\n",
      "layer   2  Sparsity: 74.4824%\n",
      "layer   3  Sparsity: 71.0854%\n",
      "total_backward_count 606980 real_backward_count 59509   9.804%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.775358/  1.261148, val:  61.67%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1718%\n",
      "layer   2  Sparsity: 74.5624%\n",
      "layer   3  Sparsity: 71.6526%\n",
      "total_backward_count 616770 real_backward_count 60329   9.781%\n",
      "fc layer 1 self.abs_max_out: 9134.0\n",
      "lif layer 1 self.abs_max_v: 17680.5\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.763991/  1.216715, val:  60.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1959%\n",
      "layer   2  Sparsity: 74.4836%\n",
      "layer   3  Sparsity: 71.6543%\n",
      "total_backward_count 626560 real_backward_count 61101   9.752%\n",
      "fc layer 1 self.abs_max_out: 9272.0\n",
      "lif layer 1 self.abs_max_v: 17950.0\n",
      "lif layer 1 self.abs_max_v: 17951.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.765907/  1.321592, val:  51.67%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1828%\n",
      "layer   2  Sparsity: 74.3911%\n",
      "layer   3  Sparsity: 70.4756%\n",
      "total_backward_count 636350 real_backward_count 61914   9.730%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.753048/  1.171887, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1752%\n",
      "layer   2  Sparsity: 74.5963%\n",
      "layer   3  Sparsity: 70.8288%\n",
      "total_backward_count 646140 real_backward_count 62699   9.704%\n",
      "fc layer 2 self.abs_max_out: 3073.0\n",
      "fc layer 2 self.abs_max_out: 3195.0\n",
      "lif layer 2 self.abs_max_v: 5876.0\n",
      "fc layer 1 self.abs_max_out: 9424.0\n",
      "lif layer 1 self.abs_max_v: 18258.5\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.773399/  1.176181, val:  67.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1604%\n",
      "layer   2  Sparsity: 73.8893%\n",
      "layer   3  Sparsity: 71.2524%\n",
      "total_backward_count 655930 real_backward_count 63481   9.678%\n",
      "lif layer 2 self.abs_max_v: 5902.0\n",
      "lif layer 2 self.abs_max_v: 5955.5\n",
      "lif layer 2 self.abs_max_v: 6031.0\n",
      "fc layer 2 self.abs_max_out: 3352.0\n",
      "lif layer 2 self.abs_max_v: 6327.5\n",
      "fc layer 1 self.abs_max_out: 9430.0\n",
      "fc layer 1 self.abs_max_out: 9625.0\n",
      "lif layer 1 self.abs_max_v: 18654.5\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.755861/  1.147825, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1455%\n",
      "layer   2  Sparsity: 73.5928%\n",
      "layer   3  Sparsity: 70.3119%\n",
      "total_backward_count 665720 real_backward_count 64253   9.652%\n",
      "lif layer 2 self.abs_max_v: 6391.0\n",
      "lif layer 2 self.abs_max_v: 6393.0\n",
      "fc layer 1 self.abs_max_out: 9743.0\n",
      "lif layer 1 self.abs_max_v: 18882.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.728859/  1.092492, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1535%\n",
      "layer   2  Sparsity: 73.9622%\n",
      "layer   3  Sparsity: 69.8400%\n",
      "total_backward_count 675510 real_backward_count 65044   9.629%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.717204/  1.201415, val:  56.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.1614%\n",
      "layer   2  Sparsity: 74.1485%\n",
      "layer   3  Sparsity: 70.3637%\n",
      "total_backward_count 685300 real_backward_count 65764   9.596%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.736524/  1.132869, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1793%\n",
      "layer   2  Sparsity: 74.0486%\n",
      "layer   3  Sparsity: 70.6847%\n",
      "total_backward_count 695090 real_backward_count 66545   9.574%\n",
      "fc layer 2 self.abs_max_out: 3502.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.725669/  1.196771, val:  65.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1607%\n",
      "layer   2  Sparsity: 74.0676%\n",
      "layer   3  Sparsity: 71.2200%\n",
      "total_backward_count 704880 real_backward_count 67298   9.547%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.745622/  1.132947, val:  78.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1674%\n",
      "layer   2  Sparsity: 73.8516%\n",
      "layer   3  Sparsity: 71.8429%\n",
      "total_backward_count 714670 real_backward_count 68052   9.522%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.717591/  1.215147, val:  64.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1635%\n",
      "layer   2  Sparsity: 73.8510%\n",
      "layer   3  Sparsity: 72.1541%\n",
      "total_backward_count 724460 real_backward_count 68764   9.492%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.731878/  1.134274, val:  68.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1784%\n",
      "layer   2  Sparsity: 73.9459%\n",
      "layer   3  Sparsity: 71.5975%\n",
      "total_backward_count 734250 real_backward_count 69512   9.467%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.702669/  1.100888, val:  76.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1616%\n",
      "layer   2  Sparsity: 73.9518%\n",
      "layer   3  Sparsity: 70.6988%\n",
      "total_backward_count 744040 real_backward_count 70289   9.447%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.697321/  1.110389, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1841%\n",
      "layer   2  Sparsity: 73.8196%\n",
      "layer   3  Sparsity: 70.9938%\n",
      "total_backward_count 753830 real_backward_count 71075   9.429%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.671243/  1.142174, val:  63.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1841%\n",
      "layer   2  Sparsity: 73.6774%\n",
      "layer   3  Sparsity: 70.6091%\n",
      "total_backward_count 763620 real_backward_count 71808   9.404%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.670054/  1.083922, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1962%\n",
      "layer   2  Sparsity: 73.7804%\n",
      "layer   3  Sparsity: 71.3797%\n",
      "total_backward_count 773410 real_backward_count 72489   9.373%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.663406/  1.121226, val:  66.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1625%\n",
      "layer   2  Sparsity: 73.6657%\n",
      "layer   3  Sparsity: 71.3724%\n",
      "total_backward_count 783200 real_backward_count 73189   9.345%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.710193/  1.105031, val:  68.75%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1443%\n",
      "layer   2  Sparsity: 73.3968%\n",
      "layer   3  Sparsity: 71.7364%\n",
      "total_backward_count 792990 real_backward_count 73925   9.322%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.676138/  1.155856, val:  63.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1715%\n",
      "layer   2  Sparsity: 73.4337%\n",
      "layer   3  Sparsity: 71.6500%\n",
      "total_backward_count 802780 real_backward_count 74615   9.295%\n",
      "fc layer 2 self.abs_max_out: 3543.0\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.698476/  1.127368, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1608%\n",
      "layer   2  Sparsity: 73.9267%\n",
      "layer   3  Sparsity: 71.6256%\n",
      "total_backward_count 812570 real_backward_count 75366   9.275%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.688147/  1.148858, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1380%\n",
      "layer   2  Sparsity: 74.4820%\n",
      "layer   3  Sparsity: 71.4030%\n",
      "total_backward_count 822360 real_backward_count 76105   9.254%\n",
      "fc layer 2 self.abs_max_out: 3727.0\n",
      "lif layer 2 self.abs_max_v: 6427.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.682492/  1.123996, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1624%\n",
      "layer   2  Sparsity: 73.7671%\n",
      "layer   3  Sparsity: 71.2552%\n",
      "total_backward_count 832150 real_backward_count 76831   9.233%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.671415/  1.122434, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1830%\n",
      "layer   2  Sparsity: 73.7066%\n",
      "layer   3  Sparsity: 71.6626%\n",
      "total_backward_count 841940 real_backward_count 77545   9.210%\n",
      "lif layer 2 self.abs_max_v: 6517.5\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.686821/  1.074313, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.1581%\n",
      "layer   2  Sparsity: 73.6587%\n",
      "layer   3  Sparsity: 71.5841%\n",
      "total_backward_count 851730 real_backward_count 78259   9.188%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.714407/  1.098900, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1174%\n",
      "layer   2  Sparsity: 74.0430%\n",
      "layer   3  Sparsity: 71.7989%\n",
      "total_backward_count 861520 real_backward_count 78960   9.165%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.706034/  1.123362, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1391%\n",
      "layer   2  Sparsity: 73.9609%\n",
      "layer   3  Sparsity: 71.3539%\n",
      "total_backward_count 871310 real_backward_count 79672   9.144%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.707242/  1.109637, val:  76.67%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1782%\n",
      "layer   2  Sparsity: 74.6284%\n",
      "layer   3  Sparsity: 71.4232%\n",
      "total_backward_count 881100 real_backward_count 80362   9.121%\n",
      "fc layer 3 self.abs_max_out: 1426.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.688491/  1.195615, val:  67.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1678%\n",
      "layer   2  Sparsity: 74.4830%\n",
      "layer   3  Sparsity: 72.1049%\n",
      "total_backward_count 890890 real_backward_count 81087   9.102%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.663756/  1.056050, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1370%\n",
      "layer   2  Sparsity: 74.8674%\n",
      "layer   3  Sparsity: 71.2878%\n",
      "total_backward_count 900680 real_backward_count 81751   9.077%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.664486/  1.110656, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1855%\n",
      "layer   2  Sparsity: 74.2578%\n",
      "layer   3  Sparsity: 70.8165%\n",
      "total_backward_count 910470 real_backward_count 82446   9.055%\n",
      "lif layer 1 self.abs_max_v: 18926.5\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.688166/  1.110168, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1776%\n",
      "layer   2  Sparsity: 73.4539%\n",
      "layer   3  Sparsity: 70.8146%\n",
      "total_backward_count 920260 real_backward_count 83129   9.033%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.674386/  1.077613, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1912%\n",
      "layer   2  Sparsity: 73.4617%\n",
      "layer   3  Sparsity: 71.2326%\n",
      "total_backward_count 930050 real_backward_count 83820   9.012%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.679391/  1.041389, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1924%\n",
      "layer   2  Sparsity: 73.8233%\n",
      "layer   3  Sparsity: 72.0844%\n",
      "total_backward_count 939840 real_backward_count 84499   8.991%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.679516/  1.089747, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1348%\n",
      "layer   2  Sparsity: 73.8945%\n",
      "layer   3  Sparsity: 71.7170%\n",
      "total_backward_count 949630 real_backward_count 85186   8.970%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.686606/  1.078852, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1767%\n",
      "layer   2  Sparsity: 73.7342%\n",
      "layer   3  Sparsity: 71.3800%\n",
      "total_backward_count 959420 real_backward_count 85851   8.948%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.676657/  1.253124, val:  52.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1789%\n",
      "layer   2  Sparsity: 74.3286%\n",
      "layer   3  Sparsity: 71.4051%\n",
      "total_backward_count 969210 real_backward_count 86492   8.924%\n",
      "lif layer 2 self.abs_max_v: 6634.0\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.698521/  1.084131, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1670%\n",
      "layer   2  Sparsity: 74.2227%\n",
      "layer   3  Sparsity: 70.7488%\n",
      "total_backward_count 979000 real_backward_count 87166   8.904%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.655278/  1.069882, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1654%\n",
      "layer   2  Sparsity: 74.4067%\n",
      "layer   3  Sparsity: 70.8854%\n",
      "total_backward_count 988790 real_backward_count 87843   8.884%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.644945/  1.098640, val:  72.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1567%\n",
      "layer   2  Sparsity: 74.2519%\n",
      "layer   3  Sparsity: 71.4976%\n",
      "total_backward_count 998580 real_backward_count 88515   8.864%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.637152/  1.056898, val:  72.50%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.2166%\n",
      "layer   2  Sparsity: 74.2271%\n",
      "layer   3  Sparsity: 71.4151%\n",
      "total_backward_count 1008370 real_backward_count 89149   8.841%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.640903/  1.149025, val:  65.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1656%\n",
      "layer   2  Sparsity: 74.4499%\n",
      "layer   3  Sparsity: 70.4309%\n",
      "total_backward_count 1018160 real_backward_count 89807   8.821%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.636145/  1.033858, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1945%\n",
      "layer   2  Sparsity: 74.3489%\n",
      "layer   3  Sparsity: 70.9206%\n",
      "total_backward_count 1027950 real_backward_count 90445   8.799%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.668213/  1.047892, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1702%\n",
      "layer   2  Sparsity: 73.9847%\n",
      "layer   3  Sparsity: 71.9846%\n",
      "total_backward_count 1037740 real_backward_count 91106   8.779%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.655340/  1.057898, val:  75.00%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1713%\n",
      "layer   2  Sparsity: 73.8166%\n",
      "layer   3  Sparsity: 71.6145%\n",
      "total_backward_count 1047530 real_backward_count 91770   8.761%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.621332/  1.104696, val:  66.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1732%\n",
      "layer   2  Sparsity: 74.1479%\n",
      "layer   3  Sparsity: 71.7495%\n",
      "total_backward_count 1057320 real_backward_count 92393   8.738%\n",
      "lif layer 2 self.abs_max_v: 6739.0\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.647850/  1.028067, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1616%\n",
      "layer   2  Sparsity: 74.0050%\n",
      "layer   3  Sparsity: 71.1602%\n",
      "total_backward_count 1067110 real_backward_count 93045   8.719%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.618965/  1.053324, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1278%\n",
      "layer   2  Sparsity: 74.1546%\n",
      "layer   3  Sparsity: 70.7425%\n",
      "total_backward_count 1076900 real_backward_count 93710   8.702%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.634923/  1.046187, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1947%\n",
      "layer   2  Sparsity: 74.2253%\n",
      "layer   3  Sparsity: 70.3087%\n",
      "total_backward_count 1086690 real_backward_count 94348   8.682%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.615332/  1.064063, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1907%\n",
      "layer   2  Sparsity: 74.0693%\n",
      "layer   3  Sparsity: 70.4285%\n",
      "total_backward_count 1096480 real_backward_count 94935   8.658%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.615471/  1.117114, val:  63.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1894%\n",
      "layer   2  Sparsity: 74.2447%\n",
      "layer   3  Sparsity: 70.4962%\n",
      "total_backward_count 1106270 real_backward_count 95592   8.641%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.622798/  1.082382, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1130%\n",
      "layer   2  Sparsity: 74.0005%\n",
      "layer   3  Sparsity: 71.1033%\n",
      "total_backward_count 1116060 real_backward_count 96228   8.622%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.625216/  1.116740, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1757%\n",
      "layer   2  Sparsity: 74.2747%\n",
      "layer   3  Sparsity: 71.2994%\n",
      "total_backward_count 1125850 real_backward_count 96846   8.602%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.608290/  1.055397, val:  71.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1758%\n",
      "layer   2  Sparsity: 74.3644%\n",
      "layer   3  Sparsity: 70.5063%\n",
      "total_backward_count 1135640 real_backward_count 97485   8.584%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.607218/  1.045315, val:  72.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1669%\n",
      "layer   2  Sparsity: 74.2304%\n",
      "layer   3  Sparsity: 70.5242%\n",
      "total_backward_count 1145430 real_backward_count 98096   8.564%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.595326/  0.987380, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1913%\n",
      "layer   2  Sparsity: 74.2276%\n",
      "layer   3  Sparsity: 70.5784%\n",
      "total_backward_count 1155220 real_backward_count 98724   8.546%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.598682/  1.110473, val:  65.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1497%\n",
      "layer   2  Sparsity: 74.2996%\n",
      "layer   3  Sparsity: 71.0482%\n",
      "total_backward_count 1165010 real_backward_count 99344   8.527%\n",
      "fc layer 3 self.abs_max_out: 1467.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.588529/  1.009162, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1591%\n",
      "layer   2  Sparsity: 74.7106%\n",
      "layer   3  Sparsity: 70.9720%\n",
      "total_backward_count 1174800 real_backward_count 99933   8.506%\n",
      "fc layer 3 self.abs_max_out: 1485.0\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.589674/  1.026990, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1548%\n",
      "layer   2  Sparsity: 74.8584%\n",
      "layer   3  Sparsity: 70.8815%\n",
      "total_backward_count 1184590 real_backward_count 100564   8.489%\n",
      "fc layer 1 self.abs_max_out: 9804.0\n",
      "fc layer 1 self.abs_max_out: 9840.0\n",
      "lif layer 1 self.abs_max_v: 19227.0\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.591134/  1.093957, val:  65.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1827%\n",
      "layer   2  Sparsity: 75.0418%\n",
      "layer   3  Sparsity: 70.9347%\n",
      "total_backward_count 1194380 real_backward_count 101173   8.471%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.588572/  1.004474, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1568%\n",
      "layer   2  Sparsity: 75.0450%\n",
      "layer   3  Sparsity: 71.1803%\n",
      "total_backward_count 1204170 real_backward_count 101788   8.453%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.584048/  0.969314, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1591%\n",
      "layer   2  Sparsity: 74.4378%\n",
      "layer   3  Sparsity: 71.6400%\n",
      "total_backward_count 1213960 real_backward_count 102391   8.434%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.593506/  1.005458, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1848%\n",
      "layer   2  Sparsity: 74.7591%\n",
      "layer   3  Sparsity: 72.2762%\n",
      "total_backward_count 1223750 real_backward_count 102974   8.415%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.599426/  1.052680, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1358%\n",
      "layer   2  Sparsity: 75.1784%\n",
      "layer   3  Sparsity: 71.5212%\n",
      "total_backward_count 1233540 real_backward_count 103589   8.398%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.605869/  1.220060, val:  61.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1933%\n",
      "layer   2  Sparsity: 75.2776%\n",
      "layer   3  Sparsity: 71.5958%\n",
      "total_backward_count 1243330 real_backward_count 104217   8.382%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.585886/  1.150372, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1893%\n",
      "layer   2  Sparsity: 75.5993%\n",
      "layer   3  Sparsity: 71.5616%\n",
      "total_backward_count 1253120 real_backward_count 104772   8.361%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.599153/  1.053724, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1518%\n",
      "layer   2  Sparsity: 75.5649%\n",
      "layer   3  Sparsity: 71.5868%\n",
      "total_backward_count 1262910 real_backward_count 105355   8.342%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.589625/  1.001379, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1632%\n",
      "layer   2  Sparsity: 75.9040%\n",
      "layer   3  Sparsity: 71.4390%\n",
      "total_backward_count 1272700 real_backward_count 105953   8.325%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.591355/  1.067662, val:  67.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1536%\n",
      "layer   2  Sparsity: 75.6779%\n",
      "layer   3  Sparsity: 71.8537%\n",
      "total_backward_count 1282490 real_backward_count 106544   8.308%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.579462/  1.091056, val:  70.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1572%\n",
      "layer   2  Sparsity: 75.4109%\n",
      "layer   3  Sparsity: 72.0941%\n",
      "total_backward_count 1292280 real_backward_count 107098   8.288%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.581165/  1.012615, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1819%\n",
      "layer   2  Sparsity: 75.3764%\n",
      "layer   3  Sparsity: 72.4612%\n",
      "total_backward_count 1302070 real_backward_count 107667   8.269%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.607061/  1.089028, val:  74.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1733%\n",
      "layer   2  Sparsity: 75.2950%\n",
      "layer   3  Sparsity: 72.4477%\n",
      "total_backward_count 1311860 real_backward_count 108249   8.252%\n",
      "fc layer 3 self.abs_max_out: 1495.0\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.587965/  1.076540, val:  79.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1710%\n",
      "layer   2  Sparsity: 75.1498%\n",
      "layer   3  Sparsity: 72.0555%\n",
      "total_backward_count 1321650 real_backward_count 108848   8.236%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.584575/  1.019830, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1422%\n",
      "layer   2  Sparsity: 75.0434%\n",
      "layer   3  Sparsity: 72.1131%\n",
      "total_backward_count 1331440 real_backward_count 109432   8.219%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.583660/  0.971892, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1693%\n",
      "layer   2  Sparsity: 75.1178%\n",
      "layer   3  Sparsity: 72.2373%\n",
      "total_backward_count 1341230 real_backward_count 110051   8.205%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.572059/  0.978724, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1942%\n",
      "layer   2  Sparsity: 75.3836%\n",
      "layer   3  Sparsity: 72.0334%\n",
      "total_backward_count 1351020 real_backward_count 110607   8.187%\n",
      "fc layer 3 self.abs_max_out: 1503.0\n",
      "fc layer 3 self.abs_max_out: 1516.0\n",
      "fc layer 3 self.abs_max_out: 1537.0\n",
      "fc layer 3 self.abs_max_out: 1568.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.569162/  0.999780, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1497%\n",
      "layer   2  Sparsity: 75.6614%\n",
      "layer   3  Sparsity: 72.3155%\n",
      "total_backward_count 1360810 real_backward_count 111169   8.169%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.585542/  1.086068, val:  65.83%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1803%\n",
      "layer   2  Sparsity: 75.5889%\n",
      "layer   3  Sparsity: 72.6786%\n",
      "total_backward_count 1370600 real_backward_count 111772   8.155%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.590387/  1.044735, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1310%\n",
      "layer   2  Sparsity: 75.4198%\n",
      "layer   3  Sparsity: 72.3515%\n",
      "total_backward_count 1380390 real_backward_count 112356   8.139%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.573025/  0.995944, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1615%\n",
      "layer   2  Sparsity: 75.6938%\n",
      "layer   3  Sparsity: 72.4598%\n",
      "total_backward_count 1390180 real_backward_count 112906   8.122%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.584286/  1.032572, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1524%\n",
      "layer   2  Sparsity: 76.0048%\n",
      "layer   3  Sparsity: 73.1646%\n",
      "total_backward_count 1399970 real_backward_count 113463   8.105%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.582109/  0.952324, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1454%\n",
      "layer   2  Sparsity: 76.1060%\n",
      "layer   3  Sparsity: 72.3502%\n",
      "total_backward_count 1409760 real_backward_count 114057   8.091%\n",
      "fc layer 1 self.abs_max_out: 10747.0\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.560721/  1.042527, val:  72.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1798%\n",
      "layer   2  Sparsity: 76.0553%\n",
      "layer   3  Sparsity: 71.9757%\n",
      "total_backward_count 1419550 real_backward_count 114645   8.076%\n",
      "fc layer 1 self.abs_max_out: 11441.0\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.571332/  1.047908, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1935%\n",
      "layer   2  Sparsity: 75.9732%\n",
      "layer   3  Sparsity: 72.8016%\n",
      "total_backward_count 1429340 real_backward_count 115240   8.062%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.582370/  1.090270, val:  65.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1863%\n",
      "layer   2  Sparsity: 75.8645%\n",
      "layer   3  Sparsity: 73.0064%\n",
      "total_backward_count 1439130 real_backward_count 115808   8.047%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.571293/  1.158778, val:  61.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1665%\n",
      "layer   2  Sparsity: 75.9327%\n",
      "layer   3  Sparsity: 73.0367%\n",
      "total_backward_count 1448920 real_backward_count 116363   8.031%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.579336/  1.000867, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1771%\n",
      "layer   2  Sparsity: 76.4421%\n",
      "layer   3  Sparsity: 73.1913%\n",
      "total_backward_count 1458710 real_backward_count 116967   8.019%\n",
      "fc layer 3 self.abs_max_out: 1641.0\n",
      "fc layer 3 self.abs_max_out: 1652.0\n",
      "fc layer 3 self.abs_max_out: 1744.0\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.558091/  1.011924, val:  74.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1640%\n",
      "layer   2  Sparsity: 76.5629%\n",
      "layer   3  Sparsity: 73.3752%\n",
      "total_backward_count 1468500 real_backward_count 117530   8.003%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.570285/  0.951248, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.2224%\n",
      "layer   2  Sparsity: 76.5902%\n",
      "layer   3  Sparsity: 72.6546%\n",
      "total_backward_count 1478290 real_backward_count 118075   7.987%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.568090/  1.003555, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1779%\n",
      "layer   2  Sparsity: 76.4109%\n",
      "layer   3  Sparsity: 72.5943%\n",
      "total_backward_count 1488080 real_backward_count 118630   7.972%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.551201/  1.027738, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1822%\n",
      "layer   2  Sparsity: 76.2640%\n",
      "layer   3  Sparsity: 72.3878%\n",
      "total_backward_count 1497870 real_backward_count 119173   7.956%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.566857/  0.940865, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1484%\n",
      "layer   2  Sparsity: 75.9991%\n",
      "layer   3  Sparsity: 73.4515%\n",
      "total_backward_count 1507660 real_backward_count 119753   7.943%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.537964/  0.914804, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1613%\n",
      "layer   2  Sparsity: 76.0925%\n",
      "layer   3  Sparsity: 72.9453%\n",
      "total_backward_count 1517450 real_backward_count 120281   7.927%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.532551/  1.007516, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1933%\n",
      "layer   2  Sparsity: 76.0443%\n",
      "layer   3  Sparsity: 72.5193%\n",
      "total_backward_count 1527240 real_backward_count 120778   7.908%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.518208/  0.921106, val:  83.75%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1567%\n",
      "layer   2  Sparsity: 76.1097%\n",
      "layer   3  Sparsity: 72.7375%\n",
      "total_backward_count 1537030 real_backward_count 121255   7.889%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.520304/  0.961673, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1586%\n",
      "layer   2  Sparsity: 76.0318%\n",
      "layer   3  Sparsity: 72.1680%\n",
      "total_backward_count 1546820 real_backward_count 121775   7.873%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.532569/  0.931687, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1903%\n",
      "layer   2  Sparsity: 75.6382%\n",
      "layer   3  Sparsity: 72.4350%\n",
      "total_backward_count 1556610 real_backward_count 122280   7.856%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.508913/  0.946255, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1932%\n",
      "layer   2  Sparsity: 75.6902%\n",
      "layer   3  Sparsity: 72.8569%\n",
      "total_backward_count 1566400 real_backward_count 122705   7.834%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.522006/  0.949740, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1961%\n",
      "layer   2  Sparsity: 75.7013%\n",
      "layer   3  Sparsity: 72.1991%\n",
      "total_backward_count 1576190 real_backward_count 123211   7.817%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.526089/  0.941638, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1677%\n",
      "layer   2  Sparsity: 75.9301%\n",
      "layer   3  Sparsity: 72.1366%\n",
      "total_backward_count 1585980 real_backward_count 123739   7.802%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.507260/  0.979275, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1615%\n",
      "layer   2  Sparsity: 76.2992%\n",
      "layer   3  Sparsity: 72.0721%\n",
      "total_backward_count 1595770 real_backward_count 124240   7.786%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.508540/  0.960829, val:  75.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1600%\n",
      "layer   2  Sparsity: 76.0402%\n",
      "layer   3  Sparsity: 72.5579%\n",
      "total_backward_count 1605560 real_backward_count 124721   7.768%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.518084/  1.023561, val:  72.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1598%\n",
      "layer   2  Sparsity: 75.8922%\n",
      "layer   3  Sparsity: 72.4795%\n",
      "total_backward_count 1615350 real_backward_count 125256   7.754%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.518483/  0.990659, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1744%\n",
      "layer   2  Sparsity: 75.8509%\n",
      "layer   3  Sparsity: 72.5783%\n",
      "total_backward_count 1625140 real_backward_count 125794   7.741%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.523084/  1.001533, val:  74.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1926%\n",
      "layer   2  Sparsity: 75.9791%\n",
      "layer   3  Sparsity: 71.9532%\n",
      "total_backward_count 1634930 real_backward_count 126274   7.724%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.524749/  0.971664, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1411%\n",
      "layer   2  Sparsity: 75.6711%\n",
      "layer   3  Sparsity: 72.3551%\n",
      "total_backward_count 1644720 real_backward_count 126809   7.710%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.542463/  0.957585, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.2017%\n",
      "layer   2  Sparsity: 75.7254%\n",
      "layer   3  Sparsity: 72.6469%\n",
      "total_backward_count 1654510 real_backward_count 127301   7.694%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.529182/  0.948699, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1728%\n",
      "layer   2  Sparsity: 75.6922%\n",
      "layer   3  Sparsity: 72.5103%\n",
      "total_backward_count 1664300 real_backward_count 127778   7.678%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.521769/  1.027678, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1749%\n",
      "layer   2  Sparsity: 75.6995%\n",
      "layer   3  Sparsity: 72.3865%\n",
      "total_backward_count 1674090 real_backward_count 128298   7.664%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.529545/  1.022221, val:  76.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1366%\n",
      "layer   2  Sparsity: 75.4607%\n",
      "layer   3  Sparsity: 72.4849%\n",
      "total_backward_count 1683880 real_backward_count 128785   7.648%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.525190/  0.989011, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1629%\n",
      "layer   2  Sparsity: 75.3101%\n",
      "layer   3  Sparsity: 71.9752%\n",
      "total_backward_count 1693670 real_backward_count 129300   7.634%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.517942/  0.927810, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 90.1484%\n",
      "layer   2  Sparsity: 75.4441%\n",
      "layer   3  Sparsity: 71.9144%\n",
      "total_backward_count 1703460 real_backward_count 129770   7.618%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.517420/  0.986042, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1448%\n",
      "layer   2  Sparsity: 75.3900%\n",
      "layer   3  Sparsity: 71.8814%\n",
      "total_backward_count 1713250 real_backward_count 130249   7.602%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.519561/  0.910448, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1311%\n",
      "layer   2  Sparsity: 75.7409%\n",
      "layer   3  Sparsity: 72.0399%\n",
      "total_backward_count 1723040 real_backward_count 130746   7.588%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.520860/  0.982139, val:  72.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1643%\n",
      "layer   2  Sparsity: 75.7029%\n",
      "layer   3  Sparsity: 71.7323%\n",
      "total_backward_count 1732830 real_backward_count 131247   7.574%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.519143/  0.932342, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1719%\n",
      "layer   2  Sparsity: 75.4231%\n",
      "layer   3  Sparsity: 71.9113%\n",
      "total_backward_count 1742620 real_backward_count 131675   7.556%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.514409/  1.023480, val:  74.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1533%\n",
      "layer   2  Sparsity: 75.1905%\n",
      "layer   3  Sparsity: 72.1240%\n",
      "total_backward_count 1752410 real_backward_count 132183   7.543%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.525356/  0.907452, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1818%\n",
      "layer   2  Sparsity: 75.3122%\n",
      "layer   3  Sparsity: 72.2941%\n",
      "total_backward_count 1762200 real_backward_count 132717   7.531%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.509091/  0.926238, val:  81.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1457%\n",
      "layer   2  Sparsity: 75.4325%\n",
      "layer   3  Sparsity: 72.0790%\n",
      "total_backward_count 1771990 real_backward_count 133177   7.516%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.512661/  0.948829, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1732%\n",
      "layer   2  Sparsity: 75.5174%\n",
      "layer   3  Sparsity: 72.2323%\n",
      "total_backward_count 1781780 real_backward_count 133621   7.499%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.514009/  0.930479, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1295%\n",
      "layer   2  Sparsity: 75.4644%\n",
      "layer   3  Sparsity: 72.3501%\n",
      "total_backward_count 1791570 real_backward_count 134080   7.484%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.507678/  0.954657, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.1601%\n",
      "layer   2  Sparsity: 75.5144%\n",
      "layer   3  Sparsity: 72.5541%\n",
      "total_backward_count 1801360 real_backward_count 134522   7.468%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.518187/  0.921556, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.1740%\n",
      "layer   2  Sparsity: 75.8342%\n",
      "layer   3  Sparsity: 72.7283%\n",
      "total_backward_count 1811150 real_backward_count 134969   7.452%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.513628/  0.940689, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1434%\n",
      "layer   2  Sparsity: 75.8025%\n",
      "layer   3  Sparsity: 72.9892%\n",
      "total_backward_count 1820940 real_backward_count 135433   7.438%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.512148/  0.964561, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1480%\n",
      "layer   2  Sparsity: 75.5606%\n",
      "layer   3  Sparsity: 72.5272%\n",
      "total_backward_count 1830730 real_backward_count 135923   7.425%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.501919/  0.981795, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1674%\n",
      "layer   2  Sparsity: 76.0558%\n",
      "layer   3  Sparsity: 73.1000%\n",
      "total_backward_count 1840520 real_backward_count 136430   7.413%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.514193/  1.004202, val:  74.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1581%\n",
      "layer   2  Sparsity: 75.5744%\n",
      "layer   3  Sparsity: 73.0309%\n",
      "total_backward_count 1850310 real_backward_count 136886   7.398%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.511904/  0.980056, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1335%\n",
      "layer   2  Sparsity: 75.7437%\n",
      "layer   3  Sparsity: 72.1842%\n",
      "total_backward_count 1860100 real_backward_count 137373   7.385%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.505330/  0.951141, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1395%\n",
      "layer   2  Sparsity: 75.9887%\n",
      "layer   3  Sparsity: 72.4049%\n",
      "total_backward_count 1869890 real_backward_count 137816   7.370%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.518254/  1.059189, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1622%\n",
      "layer   2  Sparsity: 76.0494%\n",
      "layer   3  Sparsity: 72.6898%\n",
      "total_backward_count 1879680 real_backward_count 138254   7.355%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.526746/  0.937293, val:  83.33%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.1595%\n",
      "layer   2  Sparsity: 75.9806%\n",
      "layer   3  Sparsity: 72.5001%\n",
      "total_backward_count 1889470 real_backward_count 138708   7.341%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.509933/  1.049094, val:  75.42%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.1847%\n",
      "layer   2  Sparsity: 75.7773%\n",
      "layer   3  Sparsity: 72.6001%\n",
      "total_backward_count 1899260 real_backward_count 139141   7.326%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.492838/  0.931037, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1689%\n",
      "layer   2  Sparsity: 75.6987%\n",
      "layer   3  Sparsity: 73.0482%\n",
      "total_backward_count 1909050 real_backward_count 139587   7.312%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.503816/  1.018122, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1236%\n",
      "layer   2  Sparsity: 75.7678%\n",
      "layer   3  Sparsity: 72.8622%\n",
      "total_backward_count 1918840 real_backward_count 140020   7.297%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.497341/  0.978063, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1861%\n",
      "layer   2  Sparsity: 75.6881%\n",
      "layer   3  Sparsity: 73.0781%\n",
      "total_backward_count 1928630 real_backward_count 140502   7.285%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.512659/  0.965496, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.2336%\n",
      "layer   2  Sparsity: 75.5819%\n",
      "layer   3  Sparsity: 72.8858%\n",
      "total_backward_count 1938420 real_backward_count 140941   7.271%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.515351/  0.964749, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1546%\n",
      "layer   2  Sparsity: 75.8074%\n",
      "layer   3  Sparsity: 72.4442%\n",
      "total_backward_count 1948210 real_backward_count 141406   7.258%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.506018/  0.951662, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.1758%\n",
      "layer   2  Sparsity: 75.9605%\n",
      "layer   3  Sparsity: 72.5673%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c010363c5448a4bcfb90db1b0a1a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.50602</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.83333</td></tr><tr><td>val_loss</td><td>0.95166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-322</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h81ia2dv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h81ia2dv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_015838-h81ia2dv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: diu31hql with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_061902-diu31hql</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/diu31hql' target=\"_blank\">young-sweep-325</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/diu31hql' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/diu31hql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_061910_599', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 219.0\n",
      "lif layer 1 self.abs_max_v: 219.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 295.0\n",
      "lif layer 1 self.abs_max_v: 364.5\n",
      "fc layer 2 self.abs_max_out: 193.0\n",
      "lif layer 2 self.abs_max_v: 193.0\n",
      "fc layer 1 self.abs_max_out: 323.0\n",
      "lif layer 1 self.abs_max_v: 503.0\n",
      "fc layer 2 self.abs_max_out: 282.0\n",
      "lif layer 2 self.abs_max_v: 316.5\n",
      "fc layer 1 self.abs_max_out: 352.0\n",
      "lif layer 2 self.abs_max_v: 318.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 1 self.abs_max_out: 365.0\n",
      "fc layer 2 self.abs_max_out: 467.0\n",
      "lif layer 2 self.abs_max_v: 538.5\n",
      "fc layer 3 self.abs_max_out: 169.0\n",
      "fc layer 1 self.abs_max_out: 526.0\n",
      "lif layer 1 self.abs_max_v: 575.5\n",
      "fc layer 1 self.abs_max_out: 690.0\n",
      "lif layer 1 self.abs_max_v: 833.0\n",
      "fc layer 2 self.abs_max_out: 641.0\n",
      "lif layer 2 self.abs_max_v: 839.5\n",
      "lif layer 1 self.abs_max_v: 967.5\n",
      "lif layer 2 self.abs_max_v: 920.5\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "fc layer 1 self.abs_max_out: 692.0\n",
      "lif layer 2 self.abs_max_v: 1053.5\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "lif layer 2 self.abs_max_v: 1107.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 1 self.abs_max_out: 906.0\n",
      "lif layer 1 self.abs_max_v: 1020.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 1 self.abs_max_out: 1450.0\n",
      "lif layer 1 self.abs_max_v: 1450.0\n",
      "lif layer 1 self.abs_max_v: 1471.5\n",
      "fc layer 2 self.abs_max_out: 1154.0\n",
      "lif layer 2 self.abs_max_v: 1170.5\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "lif layer 2 self.abs_max_v: 1250.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "fc layer 1 self.abs_max_out: 1539.0\n",
      "lif layer 1 self.abs_max_v: 1539.0\n",
      "lif layer 2 self.abs_max_v: 1462.5\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "fc layer 1 self.abs_max_out: 1587.0\n",
      "lif layer 1 self.abs_max_v: 1587.0\n",
      "fc layer 2 self.abs_max_out: 1351.0\n",
      "lif layer 2 self.abs_max_v: 1500.0\n",
      "lif layer 2 self.abs_max_v: 1580.0\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "fc layer 2 self.abs_max_out: 1469.0\n",
      "lif layer 2 self.abs_max_v: 1621.0\n",
      "lif layer 2 self.abs_max_v: 1809.5\n",
      "fc layer 2 self.abs_max_out: 1517.0\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "fc layer 3 self.abs_max_out: 662.0\n",
      "lif layer 2 self.abs_max_v: 1875.5\n",
      "fc layer 1 self.abs_max_out: 1706.0\n",
      "lif layer 1 self.abs_max_v: 1706.0\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "fc layer 1 self.abs_max_out: 1838.0\n",
      "lif layer 1 self.abs_max_v: 1838.0\n",
      "fc layer 2 self.abs_max_out: 1637.0\n",
      "lif layer 2 self.abs_max_v: 1882.5\n",
      "lif layer 2 self.abs_max_v: 1919.5\n",
      "fc layer 1 self.abs_max_out: 2328.0\n",
      "lif layer 1 self.abs_max_v: 2328.0\n",
      "lif layer 2 self.abs_max_v: 1960.5\n",
      "fc layer 2 self.abs_max_out: 1733.0\n",
      "lif layer 2 self.abs_max_v: 2161.5\n",
      "lif layer 2 self.abs_max_v: 2176.5\n",
      "lif layer 2 self.abs_max_v: 2289.5\n",
      "lif layer 2 self.abs_max_v: 2366.0\n",
      "fc layer 2 self.abs_max_out: 1904.0\n",
      "fc layer 1 self.abs_max_out: 2431.0\n",
      "lif layer 1 self.abs_max_v: 2431.0\n",
      "lif layer 1 self.abs_max_v: 2690.5\n",
      "fc layer 2 self.abs_max_out: 1972.0\n",
      "lif layer 2 self.abs_max_v: 2403.5\n",
      "lif layer 2 self.abs_max_v: 2461.0\n",
      "fc layer 2 self.abs_max_out: 2001.0\n",
      "fc layer 2 self.abs_max_out: 2038.0\n",
      "lif layer 2 self.abs_max_v: 2763.0\n",
      "fc layer 3 self.abs_max_out: 782.0\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 833.0\n",
      "fc layer 3 self.abs_max_out: 838.0\n",
      "lif layer 2 self.abs_max_v: 2769.5\n",
      "fc layer 2 self.abs_max_out: 2098.0\n",
      "lif layer 2 self.abs_max_v: 2780.5\n",
      "lif layer 2 self.abs_max_v: 2854.0\n",
      "fc layer 2 self.abs_max_out: 2112.0\n",
      "fc layer 2 self.abs_max_out: 2215.0\n",
      "fc layer 3 self.abs_max_out: 867.0\n",
      "fc layer 2 self.abs_max_out: 2334.0\n",
      "lif layer 2 self.abs_max_v: 2899.5\n",
      "lif layer 2 self.abs_max_v: 2954.0\n",
      "lif layer 2 self.abs_max_v: 3255.0\n",
      "lif layer 1 self.abs_max_v: 2778.0\n",
      "fc layer 2 self.abs_max_out: 2407.0\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "fc layer 1 self.abs_max_out: 2565.0\n",
      "lif layer 1 self.abs_max_v: 2869.5\n",
      "lif layer 2 self.abs_max_v: 3598.0\n",
      "lif layer 1 self.abs_max_v: 3156.0\n",
      "fc layer 2 self.abs_max_out: 2563.0\n",
      "fc layer 1 self.abs_max_out: 2789.0\n",
      "lif layer 1 self.abs_max_v: 3324.0\n",
      "fc layer 2 self.abs_max_out: 2670.0\n",
      "fc layer 2 self.abs_max_out: 2690.0\n",
      "fc layer 2 self.abs_max_out: 2722.0\n",
      "fc layer 3 self.abs_max_out: 869.0\n",
      "fc layer 3 self.abs_max_out: 928.0\n",
      "fc layer 3 self.abs_max_out: 1029.0\n",
      "fc layer 3 self.abs_max_out: 1093.0\n",
      "fc layer 3 self.abs_max_out: 1138.0\n",
      "fc layer 2 self.abs_max_out: 2792.0\n",
      "lif layer 2 self.abs_max_v: 3646.0\n",
      "lif layer 1 self.abs_max_v: 3422.0\n",
      "lif layer 1 self.abs_max_v: 4149.0\n",
      "fc layer 1 self.abs_max_out: 3099.0\n",
      "lif layer 2 self.abs_max_v: 3710.5\n",
      "lif layer 2 self.abs_max_v: 4086.5\n",
      "fc layer 3 self.abs_max_out: 1270.0\n",
      "lif layer 2 self.abs_max_v: 4160.5\n",
      "lif layer 2 self.abs_max_v: 4234.5\n",
      "lif layer 2 self.abs_max_v: 4337.5\n",
      "fc layer 2 self.abs_max_out: 2897.0\n",
      "fc layer 2 self.abs_max_out: 2898.0\n",
      "fc layer 2 self.abs_max_out: 2966.0\n",
      "fc layer 2 self.abs_max_out: 3250.0\n",
      "fc layer 1 self.abs_max_out: 3148.0\n",
      "fc layer 1 self.abs_max_out: 3182.0\n",
      "fc layer 1 self.abs_max_out: 3692.0\n",
      "lif layer 2 self.abs_max_v: 4352.5\n",
      "fc layer 2 self.abs_max_out: 3604.0\n",
      "lif layer 2 self.abs_max_v: 4433.0\n",
      "lif layer 2 self.abs_max_v: 4462.0\n",
      "lif layer 2 self.abs_max_v: 4603.5\n",
      "lif layer 2 self.abs_max_v: 4891.5\n",
      "lif layer 2 self.abs_max_v: 4917.5\n",
      "lif layer 2 self.abs_max_v: 5018.0\n",
      "lif layer 2 self.abs_max_v: 5092.5\n",
      "lif layer 2 self.abs_max_v: 5268.5\n",
      "lif layer 1 self.abs_max_v: 4581.5\n",
      "lif layer 1 self.abs_max_v: 5061.0\n",
      "lif layer 1 self.abs_max_v: 5381.5\n",
      "lif layer 1 self.abs_max_v: 5722.0\n",
      "lif layer 2 self.abs_max_v: 5499.0\n",
      "fc layer 2 self.abs_max_out: 3629.0\n",
      "lif layer 2 self.abs_max_v: 5672.0\n",
      "lif layer 2 self.abs_max_v: 5690.5\n",
      "lif layer 2 self.abs_max_v: 5708.5\n",
      "lif layer 2 self.abs_max_v: 5726.0\n",
      "lif layer 2 self.abs_max_v: 5775.0\n",
      "lif layer 2 self.abs_max_v: 5936.5\n",
      "lif layer 2 self.abs_max_v: 5973.5\n",
      "lif layer 2 self.abs_max_v: 5984.5\n",
      "lif layer 2 self.abs_max_v: 6022.5\n",
      "lif layer 2 self.abs_max_v: 6179.5\n",
      "lif layer 2 self.abs_max_v: 6514.0\n",
      "lif layer 2 self.abs_max_v: 6551.0\n",
      "fc layer 2 self.abs_max_out: 3641.0\n",
      "fc layer 2 self.abs_max_out: 3792.0\n",
      "fc layer 1 self.abs_max_out: 3695.0\n",
      "fc layer 1 self.abs_max_out: 4245.0\n",
      "fc layer 1 self.abs_max_out: 4519.0\n",
      "fc layer 1 self.abs_max_out: 5026.0\n",
      "fc layer 2 self.abs_max_out: 4013.0\n",
      "fc layer 2 self.abs_max_out: 4090.0\n",
      "fc layer 2 self.abs_max_out: 4111.0\n",
      "lif layer 2 self.abs_max_v: 6705.5\n",
      "lif layer 2 self.abs_max_v: 6995.0\n",
      "lif layer 2 self.abs_max_v: 7326.5\n",
      "lif layer 1 self.abs_max_v: 6295.0\n",
      "lif layer 1 self.abs_max_v: 6931.5\n",
      "fc layer 2 self.abs_max_out: 4131.0\n",
      "fc layer 2 self.abs_max_out: 4236.0\n",
      "fc layer 2 self.abs_max_out: 4281.0\n",
      "lif layer 2 self.abs_max_v: 7412.5\n",
      "lif layer 2 self.abs_max_v: 7413.5\n",
      "lif layer 2 self.abs_max_v: 7577.0\n",
      "fc layer 2 self.abs_max_out: 4334.0\n",
      "lif layer 2 self.abs_max_v: 7680.5\n",
      "fc layer 2 self.abs_max_out: 4609.0\n",
      "fc layer 2 self.abs_max_out: 4868.0\n",
      "lif layer 1 self.abs_max_v: 8137.0\n",
      "lif layer 1 self.abs_max_v: 8593.5\n",
      "fc layer 1 self.abs_max_out: 5405.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.535496/  1.882613, val:  34.17%, val_best:  34.17%, tr:  98.67%, tr_best:  98.67%, epoch time: 80.95 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.0348%\n",
      "layer   3  Sparsity: 70.9588%\n",
      "total_backward_count 9790 real_backward_count 1803  18.417%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 7744.5\n",
      "lif layer 2 self.abs_max_v: 7823.5\n",
      "fc layer 2 self.abs_max_out: 4986.0\n",
      "lif layer 2 self.abs_max_v: 8510.0\n",
      "fc layer 3 self.abs_max_out: 1284.0\n",
      "fc layer 2 self.abs_max_out: 5006.0\n",
      "fc layer 2 self.abs_max_out: 5187.0\n",
      "lif layer 2 self.abs_max_v: 9006.5\n",
      "lif layer 1 self.abs_max_v: 9083.0\n",
      "fc layer 1 self.abs_max_out: 5523.0\n",
      "fc layer 1 self.abs_max_out: 6213.0\n",
      "lif layer 1 self.abs_max_v: 10319.5\n",
      "fc layer 2 self.abs_max_out: 5294.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.440995/  1.914026, val:  38.75%, val_best:  38.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 73.1552%\n",
      "layer   3  Sparsity: 70.6838%\n",
      "total_backward_count 19580 real_backward_count 3300  16.854%\n",
      "fc layer 2 self.abs_max_out: 5312.0\n",
      "fc layer 3 self.abs_max_out: 1311.0\n",
      "fc layer 2 self.abs_max_out: 5398.0\n",
      "lif layer 2 self.abs_max_v: 9486.5\n",
      "fc layer 2 self.abs_max_out: 5537.0\n",
      "lif layer 2 self.abs_max_v: 9494.0\n",
      "fc layer 2 self.abs_max_out: 5620.0\n",
      "fc layer 3 self.abs_max_out: 1329.0\n",
      "fc layer 3 self.abs_max_out: 1354.0\n",
      "fc layer 3 self.abs_max_out: 1366.0\n",
      "fc layer 3 self.abs_max_out: 1383.0\n",
      "fc layer 2 self.abs_max_out: 5791.0\n",
      "lif layer 1 self.abs_max_v: 10902.0\n",
      "lif layer 1 self.abs_max_v: 10907.0\n",
      "fc layer 1 self.abs_max_out: 6857.0\n",
      "lif layer 1 self.abs_max_v: 11814.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.355685/  1.852213, val:  33.33%, val_best:  38.75%, tr:  99.28%, tr_best:  99.49%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 74.5003%\n",
      "layer   3  Sparsity: 69.3578%\n",
      "total_backward_count 29370 real_backward_count 4810  16.377%\n",
      "fc layer 2 self.abs_max_out: 5812.0\n",
      "fc layer 2 self.abs_max_out: 5916.0\n",
      "fc layer 2 self.abs_max_out: 6007.0\n",
      "fc layer 3 self.abs_max_out: 1399.0\n",
      "fc layer 1 self.abs_max_out: 7679.0\n",
      "lif layer 1 self.abs_max_v: 13274.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.318006/  1.836470, val:  37.92%, val_best:  38.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 74.4295%\n",
      "layer   3  Sparsity: 68.3800%\n",
      "total_backward_count 39160 real_backward_count 6283  16.044%\n",
      "fc layer 2 self.abs_max_out: 6211.0\n",
      "fc layer 3 self.abs_max_out: 1421.0\n",
      "fc layer 2 self.abs_max_out: 6639.0\n",
      "fc layer 3 self.abs_max_out: 1533.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.266102/  1.784325, val:  37.08%, val_best:  38.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 74.8665%\n",
      "layer   3  Sparsity: 68.7487%\n",
      "total_backward_count 48950 real_backward_count 7732  15.796%\n",
      "fc layer 1 self.abs_max_out: 7770.0\n",
      "lif layer 1 self.abs_max_v: 13626.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.237871/  1.826977, val:  44.17%, val_best:  44.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 75.4161%\n",
      "layer   3  Sparsity: 68.8997%\n",
      "total_backward_count 58740 real_backward_count 9137  15.555%\n",
      "fc layer 3 self.abs_max_out: 1576.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.213678/  1.791784, val:  45.00%, val_best:  45.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 75.7451%\n",
      "layer   3  Sparsity: 69.3034%\n",
      "total_backward_count 68530 real_backward_count 10579  15.437%\n",
      "fc layer 3 self.abs_max_out: 1585.0\n",
      "fc layer 2 self.abs_max_out: 6694.0\n",
      "lif layer 1 self.abs_max_v: 13730.0\n",
      "lif layer 1 self.abs_max_v: 14521.0\n",
      "fc layer 1 self.abs_max_out: 7779.0\n",
      "fc layer 1 self.abs_max_out: 8096.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.206826/  1.702333, val:  49.17%, val_best:  49.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 75.7507%\n",
      "layer   3  Sparsity: 68.5400%\n",
      "total_backward_count 78320 real_backward_count 11915  15.213%\n",
      "fc layer 3 self.abs_max_out: 1598.0\n",
      "fc layer 3 self.abs_max_out: 1707.0\n",
      "fc layer 3 self.abs_max_out: 1785.0\n",
      "fc layer 2 self.abs_max_out: 6777.0\n",
      "lif layer 2 self.abs_max_v: 9495.0\n",
      "lif layer 1 self.abs_max_v: 14604.5\n",
      "fc layer 1 self.abs_max_out: 8531.0\n",
      "lif layer 1 self.abs_max_v: 14913.5\n",
      "fc layer 1 self.abs_max_out: 8666.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.197346/  1.581179, val:  57.50%, val_best:  57.50%, tr:  99.49%, tr_best:  99.80%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 75.2136%\n",
      "layer   3  Sparsity: 70.1945%\n",
      "total_backward_count 88110 real_backward_count 13372  15.176%\n",
      "fc layer 1 self.abs_max_out: 9136.0\n",
      "lif layer 1 self.abs_max_v: 15139.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.167798/  1.661547, val:  50.42%, val_best:  57.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 75.0860%\n",
      "layer   3  Sparsity: 71.3071%\n",
      "total_backward_count 97900 real_backward_count 14796  15.113%\n",
      "fc layer 2 self.abs_max_out: 7151.0\n",
      "lif layer 1 self.abs_max_v: 15590.0\n",
      "fc layer 1 self.abs_max_out: 9454.0\n",
      "lif layer 1 self.abs_max_v: 16331.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.170799/  1.700297, val:  46.67%, val_best:  57.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 74.5591%\n",
      "layer   3  Sparsity: 71.8545%\n",
      "total_backward_count 107690 real_backward_count 16176  15.021%\n",
      "fc layer 3 self.abs_max_out: 1815.0\n",
      "lif layer 2 self.abs_max_v: 9499.5\n",
      "lif layer 2 self.abs_max_v: 9683.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.125779/  1.614841, val:  44.17%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 73.5956%\n",
      "layer   3  Sparsity: 70.2170%\n",
      "total_backward_count 117480 real_backward_count 17556  14.944%\n",
      "fc layer 3 self.abs_max_out: 1834.0\n",
      "fc layer 1 self.abs_max_out: 10075.0\n",
      "lif layer 1 self.abs_max_v: 17699.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.083580/  1.621236, val:  46.67%, val_best:  57.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 74.3592%\n",
      "layer   3  Sparsity: 71.8226%\n",
      "total_backward_count 127270 real_backward_count 18794  14.767%\n",
      "fc layer 3 self.abs_max_out: 1856.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.090482/  1.604446, val:  51.67%, val_best:  57.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 73.6665%\n",
      "layer   3  Sparsity: 70.0492%\n",
      "total_backward_count 137060 real_backward_count 20052  14.630%\n",
      "fc layer 2 self.abs_max_out: 7232.0\n",
      "fc layer 3 self.abs_max_out: 1886.0\n",
      "fc layer 3 self.abs_max_out: 1892.0\n",
      "fc layer 3 self.abs_max_out: 1925.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.069412/  1.534986, val:  57.50%, val_best:  57.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.48 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 73.6063%\n",
      "layer   3  Sparsity: 70.6173%\n",
      "total_backward_count 146850 real_backward_count 21289  14.497%\n",
      "fc layer 3 self.abs_max_out: 2029.0\n",
      "fc layer 3 self.abs_max_out: 2043.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.037032/  1.555149, val:  58.75%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 73.3993%\n",
      "layer   3  Sparsity: 70.1573%\n",
      "total_backward_count 156640 real_backward_count 22518  14.376%\n",
      "fc layer 2 self.abs_max_out: 7326.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.990302/  1.417524, val:  68.75%, val_best:  68.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 81.08 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 73.0911%\n",
      "layer   3  Sparsity: 69.7896%\n",
      "total_backward_count 166430 real_backward_count 23705  14.243%\n",
      "fc layer 3 self.abs_max_out: 2061.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.972375/  1.433768, val:  68.75%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 73.2374%\n",
      "layer   3  Sparsity: 70.9884%\n",
      "total_backward_count 176220 real_backward_count 24892  14.126%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.012931/  1.571082, val:  52.08%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 73.0251%\n",
      "layer   3  Sparsity: 70.9503%\n",
      "total_backward_count 186010 real_backward_count 26072  14.016%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.011593/  1.493378, val:  55.00%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.9626%\n",
      "layer   3  Sparsity: 72.7541%\n",
      "total_backward_count 195800 real_backward_count 27229  13.907%\n",
      "lif layer 2 self.abs_max_v: 9882.0\n",
      "lif layer 2 self.abs_max_v: 9889.0\n",
      "fc layer 1 self.abs_max_out: 10219.0\n",
      "lif layer 1 self.abs_max_v: 17702.5\n",
      "fc layer 1 self.abs_max_out: 10556.0\n",
      "lif layer 1 self.abs_max_v: 18445.5\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.998337/  1.427673, val:  52.92%, val_best:  68.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.5775%\n",
      "layer   3  Sparsity: 71.8774%\n",
      "total_backward_count 205590 real_backward_count 28386  13.807%\n",
      "fc layer 1 self.abs_max_out: 10942.0\n",
      "lif layer 1 self.abs_max_v: 19231.5\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.982650/  1.505005, val:  50.83%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.8005%\n",
      "layer   3  Sparsity: 71.2573%\n",
      "total_backward_count 215380 real_backward_count 29527  13.709%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.961951/  1.485203, val:  65.42%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.3863%\n",
      "layer   3  Sparsity: 70.4927%\n",
      "total_backward_count 225170 real_backward_count 30662  13.617%\n",
      "fc layer 3 self.abs_max_out: 2105.0\n",
      "fc layer 3 self.abs_max_out: 2176.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.968725/  1.367880, val:  73.75%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.46 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.7345%\n",
      "layer   3  Sparsity: 71.9221%\n",
      "total_backward_count 234960 real_backward_count 31697  13.490%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.974321/  1.431254, val:  70.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.1248%\n",
      "layer   3  Sparsity: 73.4491%\n",
      "total_backward_count 244750 real_backward_count 32765  13.387%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.976626/  1.378076, val:  72.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.1212%\n",
      "layer   3  Sparsity: 74.4026%\n",
      "total_backward_count 254540 real_backward_count 33846  13.297%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.002154/  1.444073, val:  61.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.5502%\n",
      "layer   3  Sparsity: 74.4134%\n",
      "total_backward_count 264330 real_backward_count 34871  13.192%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.966313/  1.426296, val:  70.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.1730%\n",
      "layer   3  Sparsity: 73.0139%\n",
      "total_backward_count 274120 real_backward_count 35877  13.088%\n",
      "fc layer 1 self.abs_max_out: 11130.0\n",
      "lif layer 1 self.abs_max_v: 19724.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.949343/  1.430395, val:  65.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.8733%\n",
      "layer   3  Sparsity: 72.7037%\n",
      "total_backward_count 283910 real_backward_count 36848  12.979%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.980906/  1.374799, val:  73.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.9595%\n",
      "layer   3  Sparsity: 72.5629%\n",
      "total_backward_count 293700 real_backward_count 37825  12.879%\n",
      "lif layer 2 self.abs_max_v: 9951.5\n",
      "fc layer 1 self.abs_max_out: 11429.0\n",
      "lif layer 1 self.abs_max_v: 20353.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.937009/  1.333214, val:  71.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.3159%\n",
      "layer   3  Sparsity: 72.5017%\n",
      "total_backward_count 303490 real_backward_count 38778  12.777%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.913810/  1.415854, val:  56.25%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.5253%\n",
      "layer   3  Sparsity: 72.6419%\n",
      "total_backward_count 313280 real_backward_count 39673  12.664%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.913878/  1.331965, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 72.4067%\n",
      "layer   3  Sparsity: 72.7041%\n",
      "total_backward_count 323070 real_backward_count 40606  12.569%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.903524/  1.383493, val:  62.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.9406%\n",
      "layer   3  Sparsity: 72.7038%\n",
      "total_backward_count 332860 real_backward_count 41495  12.466%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.891042/  1.340878, val:  74.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7475%\n",
      "layer   3  Sparsity: 72.3051%\n",
      "total_backward_count 342650 real_backward_count 42406  12.376%\n",
      "fc layer 1 self.abs_max_out: 11602.0\n",
      "lif layer 1 self.abs_max_v: 20749.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.921178/  1.306871, val:  78.33%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6067%\n",
      "layer   3  Sparsity: 73.5836%\n",
      "total_backward_count 352440 real_backward_count 43346  12.299%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.910658/  1.351837, val:  72.92%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5985%\n",
      "layer   3  Sparsity: 73.9954%\n",
      "total_backward_count 362230 real_backward_count 44226  12.209%\n",
      "fc layer 1 self.abs_max_out: 11608.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.916090/  1.432635, val:  63.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7716%\n",
      "layer   3  Sparsity: 73.9145%\n",
      "total_backward_count 372020 real_backward_count 45041  12.107%\n",
      "fc layer 1 self.abs_max_out: 11702.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.884957/  1.345758, val:  72.92%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6732%\n",
      "layer   3  Sparsity: 73.7087%\n",
      "total_backward_count 381810 real_backward_count 45930  12.030%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.890289/  1.337772, val:  71.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6806%\n",
      "layer   3  Sparsity: 73.9491%\n",
      "total_backward_count 391600 real_backward_count 46772  11.944%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.919343/  1.372496, val:  73.33%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3759%\n",
      "layer   3  Sparsity: 73.4835%\n",
      "total_backward_count 401390 real_backward_count 47603  11.860%\n",
      "fc layer 1 self.abs_max_out: 11812.0\n",
      "lif layer 1 self.abs_max_v: 20759.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.921536/  1.308988, val:  77.50%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5681%\n",
      "layer   3  Sparsity: 74.8790%\n",
      "total_backward_count 411180 real_backward_count 48423  11.777%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.892275/  1.366864, val:  65.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6783%\n",
      "layer   3  Sparsity: 74.8698%\n",
      "total_backward_count 420970 real_backward_count 49251  11.699%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.913739/  1.346412, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4811%\n",
      "layer   3  Sparsity: 75.2661%\n",
      "total_backward_count 430760 real_backward_count 50048  11.619%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.910907/  1.297126, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4331%\n",
      "layer   3  Sparsity: 74.7381%\n",
      "total_backward_count 440550 real_backward_count 50889  11.551%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.899409/  1.294487, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6308%\n",
      "layer   3  Sparsity: 76.1112%\n",
      "total_backward_count 450340 real_backward_count 51652  11.470%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.896985/  1.240922, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4212%\n",
      "layer   3  Sparsity: 75.0218%\n",
      "total_backward_count 460130 real_backward_count 52428  11.394%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.849298/  1.316341, val:  65.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2420%\n",
      "layer   3  Sparsity: 73.9904%\n",
      "total_backward_count 469920 real_backward_count 53164  11.313%\n",
      "fc layer 1 self.abs_max_out: 12247.0\n",
      "lif layer 1 self.abs_max_v: 21629.5\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.890108/  1.281311, val:  77.92%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3944%\n",
      "layer   3  Sparsity: 74.9811%\n",
      "total_backward_count 479710 real_backward_count 53928  11.242%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.904729/  1.304518, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4639%\n",
      "layer   3  Sparsity: 76.0169%\n",
      "total_backward_count 489500 real_backward_count 54683  11.171%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.938823/  1.294229, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4851%\n",
      "layer   3  Sparsity: 76.3774%\n",
      "total_backward_count 499290 real_backward_count 55452  11.106%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.918509/  1.312086, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.8749%\n",
      "layer   3  Sparsity: 76.5595%\n",
      "total_backward_count 509080 real_backward_count 56160  11.032%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.925965/  1.320245, val:  76.67%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.9038%\n",
      "layer   3  Sparsity: 76.5753%\n",
      "total_backward_count 518870 real_backward_count 56870  10.960%\n",
      "lif layer 2 self.abs_max_v: 9996.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.900177/  1.235843, val:  80.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.8112%\n",
      "layer   3  Sparsity: 75.7649%\n",
      "total_backward_count 528660 real_backward_count 57639  10.903%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.875539/  1.308185, val:  74.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3471%\n",
      "layer   3  Sparsity: 75.6544%\n",
      "total_backward_count 538450 real_backward_count 58372  10.841%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.916038/  1.284929, val:  78.75%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3195%\n",
      "layer   3  Sparsity: 75.5995%\n",
      "total_backward_count 548240 real_backward_count 59132  10.786%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.904197/  1.327029, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5966%\n",
      "layer   3  Sparsity: 75.0792%\n",
      "total_backward_count 558030 real_backward_count 59855  10.726%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.891798/  1.292977, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.9224%\n",
      "layer   3  Sparsity: 75.8868%\n",
      "total_backward_count 567820 real_backward_count 60589  10.670%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.878295/  1.299960, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.9450%\n",
      "layer   3  Sparsity: 76.7452%\n",
      "total_backward_count 577610 real_backward_count 61291  10.611%\n",
      "fc layer 1 self.abs_max_out: 12522.0\n",
      "lif layer 1 self.abs_max_v: 22188.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.911056/  1.370049, val:  70.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5534%\n",
      "layer   3  Sparsity: 76.6504%\n",
      "total_backward_count 587400 real_backward_count 61961  10.548%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.913345/  1.343824, val:  70.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5362%\n",
      "layer   3  Sparsity: 76.9916%\n",
      "total_backward_count 597190 real_backward_count 62625  10.487%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.892759/  1.224525, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7135%\n",
      "layer   3  Sparsity: 75.3091%\n",
      "total_backward_count 606980 real_backward_count 63339  10.435%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.866189/  1.322433, val:  71.67%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4544%\n",
      "layer   3  Sparsity: 76.0694%\n",
      "total_backward_count 616770 real_backward_count 63978  10.373%\n",
      "lif layer 2 self.abs_max_v: 10018.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.901904/  1.332589, val:  71.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6579%\n",
      "layer   3  Sparsity: 76.9820%\n",
      "total_backward_count 626560 real_backward_count 64653  10.319%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.909096/  1.321342, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4601%\n",
      "layer   3  Sparsity: 76.9980%\n",
      "total_backward_count 636350 real_backward_count 65344  10.269%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.887612/  1.317766, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4573%\n",
      "layer   3  Sparsity: 76.2950%\n",
      "total_backward_count 646140 real_backward_count 65995  10.214%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.887688/  1.333690, val:  67.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6381%\n",
      "layer   3  Sparsity: 75.5758%\n",
      "total_backward_count 655930 real_backward_count 66649  10.161%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.902302/  1.316098, val:  77.92%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6136%\n",
      "layer   3  Sparsity: 76.6177%\n",
      "total_backward_count 665720 real_backward_count 67324  10.113%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.880976/  1.276250, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4449%\n",
      "layer   3  Sparsity: 76.3653%\n",
      "total_backward_count 675510 real_backward_count 68013  10.068%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.893389/  1.290759, val:  80.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4838%\n",
      "layer   3  Sparsity: 76.5067%\n",
      "total_backward_count 685300 real_backward_count 68715  10.027%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.869664/  1.274055, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3708%\n",
      "layer   3  Sparsity: 76.3801%\n",
      "total_backward_count 695090 real_backward_count 69336   9.975%\n",
      "fc layer 3 self.abs_max_out: 2187.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.880315/  1.317195, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6803%\n",
      "layer   3  Sparsity: 76.9866%\n",
      "total_backward_count 704880 real_backward_count 69988   9.929%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.867474/  1.188512, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4659%\n",
      "layer   3  Sparsity: 76.6663%\n",
      "total_backward_count 714670 real_backward_count 70610   9.880%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.850377/  1.376406, val:  66.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3607%\n",
      "layer   3  Sparsity: 78.0448%\n",
      "total_backward_count 724460 real_backward_count 71215   9.830%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.883931/  1.303847, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4717%\n",
      "layer   3  Sparsity: 77.3265%\n",
      "total_backward_count 734250 real_backward_count 71919   9.795%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.916518/  1.205757, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7390%\n",
      "layer   3  Sparsity: 77.2011%\n",
      "total_backward_count 744040 real_backward_count 72562   9.752%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.869415/  1.211248, val:  83.33%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7688%\n",
      "layer   3  Sparsity: 77.7966%\n",
      "total_backward_count 753830 real_backward_count 73172   9.707%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.866640/  1.293576, val:  73.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5693%\n",
      "layer   3  Sparsity: 76.7180%\n",
      "total_backward_count 763620 real_backward_count 73815   9.666%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.858171/  1.319283, val:  68.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5473%\n",
      "layer   3  Sparsity: 77.7861%\n",
      "total_backward_count 773410 real_backward_count 74413   9.621%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.848737/  1.289357, val:  77.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3053%\n",
      "layer   3  Sparsity: 76.8838%\n",
      "total_backward_count 783200 real_backward_count 75032   9.580%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.891821/  1.289791, val:  78.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4300%\n",
      "layer   3  Sparsity: 77.1854%\n",
      "total_backward_count 792990 real_backward_count 75674   9.543%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.868290/  1.304757, val:  63.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4957%\n",
      "layer   3  Sparsity: 77.1834%\n",
      "total_backward_count 802780 real_backward_count 76257   9.499%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.861885/  1.309825, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3879%\n",
      "layer   3  Sparsity: 76.9080%\n",
      "total_backward_count 812570 real_backward_count 76913   9.465%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.876162/  1.339112, val:  72.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.1840%\n",
      "layer   3  Sparsity: 77.4856%\n",
      "total_backward_count 822360 real_backward_count 77552   9.430%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.900603/  1.264867, val:  77.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2515%\n",
      "layer   3  Sparsity: 76.8246%\n",
      "total_backward_count 832150 real_backward_count 78198   9.397%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.872699/  1.294566, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3239%\n",
      "layer   3  Sparsity: 77.0988%\n",
      "total_backward_count 841940 real_backward_count 78830   9.363%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.859449/  1.256202, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2958%\n",
      "layer   3  Sparsity: 77.4205%\n",
      "total_backward_count 851730 real_backward_count 79437   9.327%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.858548/  1.252311, val:  75.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3470%\n",
      "layer   3  Sparsity: 77.0779%\n",
      "total_backward_count 861520 real_backward_count 80049   9.292%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.860132/  1.267568, val:  75.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4849%\n",
      "layer   3  Sparsity: 77.0503%\n",
      "total_backward_count 871310 real_backward_count 80656   9.257%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.867178/  1.240142, val:  77.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3763%\n",
      "layer   3  Sparsity: 76.7698%\n",
      "total_backward_count 881100 real_backward_count 81252   9.222%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.853501/  1.204301, val:  75.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3287%\n",
      "layer   3  Sparsity: 76.0708%\n",
      "total_backward_count 890890 real_backward_count 81865   9.189%\n",
      "fc layer 1 self.abs_max_out: 12553.0\n",
      "lif layer 1 self.abs_max_v: 22567.0\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.787836/  1.181765, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2597%\n",
      "layer   3  Sparsity: 76.1622%\n",
      "total_backward_count 900680 real_backward_count 82402   9.149%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.797469/  1.231760, val:  77.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4273%\n",
      "layer   3  Sparsity: 76.1847%\n",
      "total_backward_count 910470 real_backward_count 83018   9.118%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.833820/  1.257308, val:  78.75%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6165%\n",
      "layer   3  Sparsity: 76.9810%\n",
      "total_backward_count 920260 real_backward_count 83641   9.089%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.819287/  1.283550, val:  74.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7789%\n",
      "layer   3  Sparsity: 76.1406%\n",
      "total_backward_count 930050 real_backward_count 84236   9.057%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.796694/  1.209290, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5459%\n",
      "layer   3  Sparsity: 76.8712%\n",
      "total_backward_count 939840 real_backward_count 84834   9.026%\n",
      "fc layer 3 self.abs_max_out: 2271.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.800431/  1.288793, val:  73.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6441%\n",
      "layer   3  Sparsity: 75.8457%\n",
      "total_backward_count 949630 real_backward_count 85429   8.996%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.822765/  1.245918, val:  72.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4783%\n",
      "layer   3  Sparsity: 76.0992%\n",
      "total_backward_count 959420 real_backward_count 86023   8.966%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.818193/  1.279755, val:  75.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5846%\n",
      "layer   3  Sparsity: 76.8135%\n",
      "total_backward_count 969210 real_backward_count 86586   8.934%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.815445/  1.231465, val:  80.42%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3988%\n",
      "layer   3  Sparsity: 77.1189%\n",
      "total_backward_count 979000 real_backward_count 87145   8.901%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.841169/  1.222133, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.1311%\n",
      "layer   3  Sparsity: 77.9544%\n",
      "total_backward_count 988790 real_backward_count 87652   8.865%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.833305/  1.240622, val:  78.75%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5344%\n",
      "layer   3  Sparsity: 77.6308%\n",
      "total_backward_count 998580 real_backward_count 88225   8.835%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.846135/  1.272648, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5426%\n",
      "layer   3  Sparsity: 77.8349%\n",
      "total_backward_count 1008370 real_backward_count 88762   8.803%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.827374/  1.241929, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6563%\n",
      "layer   3  Sparsity: 78.6756%\n",
      "total_backward_count 1018160 real_backward_count 89260   8.767%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.851421/  1.243025, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7010%\n",
      "layer   3  Sparsity: 78.6114%\n",
      "total_backward_count 1027950 real_backward_count 89818   8.738%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.840735/  1.210268, val:  79.17%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4669%\n",
      "layer   3  Sparsity: 78.3881%\n",
      "total_backward_count 1037740 real_backward_count 90360   8.707%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.798905/  1.226239, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4006%\n",
      "layer   3  Sparsity: 77.5859%\n",
      "total_backward_count 1047530 real_backward_count 90868   8.675%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.830502/  1.231183, val:  77.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4288%\n",
      "layer   3  Sparsity: 77.8637%\n",
      "total_backward_count 1057320 real_backward_count 91476   8.652%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.839170/  1.275192, val:  74.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4513%\n",
      "layer   3  Sparsity: 77.9623%\n",
      "total_backward_count 1067110 real_backward_count 92034   8.625%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.812600/  1.255265, val:  75.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3810%\n",
      "layer   3  Sparsity: 77.8626%\n",
      "total_backward_count 1076900 real_backward_count 92536   8.593%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.815789/  1.206869, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4207%\n",
      "layer   3  Sparsity: 77.8074%\n",
      "total_backward_count 1086690 real_backward_count 93106   8.568%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.857736/  1.283106, val:  75.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5575%\n",
      "layer   3  Sparsity: 78.8895%\n",
      "total_backward_count 1096480 real_backward_count 93616   8.538%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.855614/  1.228049, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5498%\n",
      "layer   3  Sparsity: 78.0071%\n",
      "total_backward_count 1106270 real_backward_count 94150   8.511%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.835635/  1.238790, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5995%\n",
      "layer   3  Sparsity: 78.1614%\n",
      "total_backward_count 1116060 real_backward_count 94657   8.481%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.835868/  1.304627, val:  73.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3290%\n",
      "layer   3  Sparsity: 77.7650%\n",
      "total_backward_count 1125850 real_backward_count 95196   8.455%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.822936/  1.228869, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5703%\n",
      "layer   3  Sparsity: 78.2963%\n",
      "total_backward_count 1135640 real_backward_count 95695   8.427%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.836838/  1.239147, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2037%\n",
      "layer   3  Sparsity: 77.6391%\n",
      "total_backward_count 1145430 real_backward_count 96241   8.402%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.805924/  1.205136, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4431%\n",
      "layer   3  Sparsity: 77.8755%\n",
      "total_backward_count 1155220 real_backward_count 96753   8.375%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.816038/  1.200366, val:  73.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4441%\n",
      "layer   3  Sparsity: 77.5994%\n",
      "total_backward_count 1165010 real_backward_count 97255   8.348%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.790473/  1.240055, val:  76.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4969%\n",
      "layer   3  Sparsity: 77.9119%\n",
      "total_backward_count 1174800 real_backward_count 97833   8.328%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.795481/  1.238820, val:  74.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.1358%\n",
      "layer   3  Sparsity: 77.7091%\n",
      "total_backward_count 1184590 real_backward_count 98390   8.306%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.808748/  1.239664, val:  73.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7073%\n",
      "layer   3  Sparsity: 78.6178%\n",
      "total_backward_count 1194380 real_backward_count 98934   8.283%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.814461/  1.179416, val:  82.92%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5370%\n",
      "layer   3  Sparsity: 78.5508%\n",
      "total_backward_count 1204170 real_backward_count 99468   8.260%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.825738/  1.200734, val:  77.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4658%\n",
      "layer   3  Sparsity: 77.5352%\n",
      "total_backward_count 1213960 real_backward_count 100016   8.239%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.797955/  1.147316, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5255%\n",
      "layer   3  Sparsity: 77.9591%\n",
      "total_backward_count 1223750 real_backward_count 100534   8.215%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.823744/  1.191293, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6942%\n",
      "layer   3  Sparsity: 77.3866%\n",
      "total_backward_count 1233540 real_backward_count 101024   8.190%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.827769/  1.193549, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6689%\n",
      "layer   3  Sparsity: 76.9345%\n",
      "total_backward_count 1243330 real_backward_count 101488   8.163%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.780830/  1.161375, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5864%\n",
      "layer   3  Sparsity: 76.9254%\n",
      "total_backward_count 1253120 real_backward_count 101950   8.136%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.805500/  1.207431, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5899%\n",
      "layer   3  Sparsity: 78.0603%\n",
      "total_backward_count 1262910 real_backward_count 102475   8.114%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.828884/  1.184797, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6804%\n",
      "layer   3  Sparsity: 79.5841%\n",
      "total_backward_count 1272700 real_backward_count 102980   8.091%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.814591/  1.241296, val:  76.25%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4708%\n",
      "layer   3  Sparsity: 78.2218%\n",
      "total_backward_count 1282490 real_backward_count 103475   8.068%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.811315/  1.181665, val:  82.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4317%\n",
      "layer   3  Sparsity: 77.7677%\n",
      "total_backward_count 1292280 real_backward_count 104002   8.048%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.811637/  1.245584, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3980%\n",
      "layer   3  Sparsity: 78.3381%\n",
      "total_backward_count 1302070 real_backward_count 104506   8.026%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.810680/  1.259985, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.1819%\n",
      "layer   3  Sparsity: 78.4988%\n",
      "total_backward_count 1311860 real_backward_count 105058   8.008%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.836882/  1.307631, val:  67.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.0411%\n",
      "layer   3  Sparsity: 78.1547%\n",
      "total_backward_count 1321650 real_backward_count 105574   7.988%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.856049/  1.280840, val:  74.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3127%\n",
      "layer   3  Sparsity: 78.9354%\n",
      "total_backward_count 1331440 real_backward_count 106122   7.970%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.836992/  1.234468, val:  72.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2544%\n",
      "layer   3  Sparsity: 78.9537%\n",
      "total_backward_count 1341230 real_backward_count 106636   7.951%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.844668/  1.251548, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2094%\n",
      "layer   3  Sparsity: 79.1206%\n",
      "total_backward_count 1351020 real_backward_count 107154   7.931%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.836280/  1.226839, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3396%\n",
      "layer   3  Sparsity: 78.8919%\n",
      "total_backward_count 1360810 real_backward_count 107651   7.911%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.853389/  1.209788, val:  77.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3176%\n",
      "layer   3  Sparsity: 78.5139%\n",
      "total_backward_count 1370600 real_backward_count 108182   7.893%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.814918/  1.178719, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4122%\n",
      "layer   3  Sparsity: 79.2127%\n",
      "total_backward_count 1380390 real_backward_count 108647   7.871%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.828557/  1.215906, val:  82.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5578%\n",
      "layer   3  Sparsity: 78.2655%\n",
      "total_backward_count 1390180 real_backward_count 109164   7.853%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.809158/  1.242953, val:  75.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6130%\n",
      "layer   3  Sparsity: 78.7317%\n",
      "total_backward_count 1399970 real_backward_count 109636   7.831%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.828398/  1.240087, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3411%\n",
      "layer   3  Sparsity: 79.0597%\n",
      "total_backward_count 1409760 real_backward_count 110153   7.814%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.824564/  1.226137, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3201%\n",
      "layer   3  Sparsity: 79.0090%\n",
      "total_backward_count 1419550 real_backward_count 110644   7.794%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.846044/  1.247784, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.1974%\n",
      "layer   3  Sparsity: 79.7436%\n",
      "total_backward_count 1429340 real_backward_count 111110   7.774%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.832213/  1.209402, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.0736%\n",
      "layer   3  Sparsity: 78.6403%\n",
      "total_backward_count 1439130 real_backward_count 111609   7.755%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.795375/  1.353110, val:  65.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.1484%\n",
      "layer   3  Sparsity: 78.1386%\n",
      "total_backward_count 1448920 real_backward_count 112064   7.734%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.809599/  1.223753, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.1985%\n",
      "layer   3  Sparsity: 78.3402%\n",
      "total_backward_count 1458710 real_backward_count 112514   7.713%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.800235/  1.251427, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 70.8145%\n",
      "layer   3  Sparsity: 78.7144%\n",
      "total_backward_count 1468500 real_backward_count 112981   7.694%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.807209/  1.284867, val:  72.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2792%\n",
      "layer   3  Sparsity: 77.9062%\n",
      "total_backward_count 1478290 real_backward_count 113466   7.675%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.824254/  1.230748, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5600%\n",
      "layer   3  Sparsity: 79.7774%\n",
      "total_backward_count 1488080 real_backward_count 113916   7.655%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.805817/  1.274683, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.49 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6046%\n",
      "layer   3  Sparsity: 78.9098%\n",
      "total_backward_count 1497870 real_backward_count 114351   7.634%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.819863/  1.225717, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5240%\n",
      "layer   3  Sparsity: 79.4563%\n",
      "total_backward_count 1507660 real_backward_count 114843   7.617%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.812962/  1.217613, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.23 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5931%\n",
      "layer   3  Sparsity: 79.4490%\n",
      "total_backward_count 1517450 real_backward_count 115293   7.598%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.820132/  1.290063, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.8808%\n",
      "layer   3  Sparsity: 78.8549%\n",
      "total_backward_count 1527240 real_backward_count 115767   7.580%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.857493/  1.200120, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3708%\n",
      "layer   3  Sparsity: 79.8570%\n",
      "total_backward_count 1537030 real_backward_count 116236   7.562%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.833282/  1.217633, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5856%\n",
      "layer   3  Sparsity: 79.1147%\n",
      "total_backward_count 1546820 real_backward_count 116725   7.546%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.816057/  1.226282, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3445%\n",
      "layer   3  Sparsity: 79.0985%\n",
      "total_backward_count 1556610 real_backward_count 117213   7.530%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.822013/  1.219794, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5351%\n",
      "layer   3  Sparsity: 79.4539%\n",
      "total_backward_count 1566400 real_backward_count 117664   7.512%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.815228/  1.180233, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5398%\n",
      "layer   3  Sparsity: 78.3359%\n",
      "total_backward_count 1576190 real_backward_count 118119   7.494%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.835394/  1.240027, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4431%\n",
      "layer   3  Sparsity: 79.5307%\n",
      "total_backward_count 1585980 real_backward_count 118613   7.479%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.845401/  1.260474, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4665%\n",
      "layer   3  Sparsity: 79.4599%\n",
      "total_backward_count 1595770 real_backward_count 119124   7.465%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.813301/  1.266879, val:  73.75%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3636%\n",
      "layer   3  Sparsity: 79.7769%\n",
      "total_backward_count 1605560 real_backward_count 119581   7.448%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.843802/  1.267181, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4858%\n",
      "layer   3  Sparsity: 80.0829%\n",
      "total_backward_count 1615350 real_backward_count 120086   7.434%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.854907/  1.283053, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4030%\n",
      "layer   3  Sparsity: 81.0737%\n",
      "total_backward_count 1625140 real_backward_count 120563   7.419%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.837107/  1.200709, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2833%\n",
      "layer   3  Sparsity: 80.0871%\n",
      "total_backward_count 1634930 real_backward_count 121023   7.402%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.803683/  1.259064, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3909%\n",
      "layer   3  Sparsity: 78.8507%\n",
      "total_backward_count 1644720 real_backward_count 121473   7.386%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.807051/  1.238452, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6136%\n",
      "layer   3  Sparsity: 79.5198%\n",
      "total_backward_count 1654510 real_backward_count 121948   7.371%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.825688/  1.244650, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.2310%\n",
      "layer   3  Sparsity: 78.5845%\n",
      "total_backward_count 1664300 real_backward_count 122386   7.354%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.822103/  1.259241, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5955%\n",
      "layer   3  Sparsity: 78.6739%\n",
      "total_backward_count 1674090 real_backward_count 122865   7.339%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.811335/  1.228048, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3940%\n",
      "layer   3  Sparsity: 80.4971%\n",
      "total_backward_count 1683880 real_backward_count 123318   7.323%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.823575/  1.182023, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4590%\n",
      "layer   3  Sparsity: 79.7475%\n",
      "total_backward_count 1693670 real_backward_count 123780   7.308%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.809145/  1.188638, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5271%\n",
      "layer   3  Sparsity: 79.6201%\n",
      "total_backward_count 1703460 real_backward_count 124224   7.292%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.804495/  1.193042, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6395%\n",
      "layer   3  Sparsity: 79.1946%\n",
      "total_backward_count 1713250 real_backward_count 124649   7.276%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.788201/  1.162975, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6224%\n",
      "layer   3  Sparsity: 78.3047%\n",
      "total_backward_count 1723040 real_backward_count 125033   7.257%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.775115/  1.173092, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3964%\n",
      "layer   3  Sparsity: 78.2927%\n",
      "total_backward_count 1732830 real_backward_count 125504   7.243%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.805742/  1.221056, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3148%\n",
      "layer   3  Sparsity: 78.8981%\n",
      "total_backward_count 1742620 real_backward_count 126018   7.232%\n",
      "fc layer 3 self.abs_max_out: 2389.0\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.801918/  1.249507, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.29 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3317%\n",
      "layer   3  Sparsity: 79.4002%\n",
      "total_backward_count 1752410 real_backward_count 126472   7.217%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.803392/  1.149913, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.57 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4910%\n",
      "layer   3  Sparsity: 79.6379%\n",
      "total_backward_count 1762200 real_backward_count 126937   7.203%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.800032/  1.217306, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4979%\n",
      "layer   3  Sparsity: 78.8977%\n",
      "total_backward_count 1771990 real_backward_count 127381   7.189%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.780578/  1.193902, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6036%\n",
      "layer   3  Sparsity: 78.6764%\n",
      "total_backward_count 1781780 real_backward_count 127838   7.175%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.761727/  1.150932, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4608%\n",
      "layer   3  Sparsity: 77.5311%\n",
      "total_backward_count 1791570 real_backward_count 128283   7.160%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.769730/  1.233175, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4836%\n",
      "layer   3  Sparsity: 78.0267%\n",
      "total_backward_count 1801360 real_backward_count 128737   7.147%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.792909/  1.202235, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4810%\n",
      "layer   3  Sparsity: 78.8646%\n",
      "total_backward_count 1811150 real_backward_count 129147   7.131%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.781471/  1.219797, val:  76.25%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3322%\n",
      "layer   3  Sparsity: 78.4063%\n",
      "total_backward_count 1820940 real_backward_count 129614   7.118%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.775765/  1.188241, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3104%\n",
      "layer   3  Sparsity: 79.3466%\n",
      "total_backward_count 1830730 real_backward_count 130088   7.106%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.751313/  1.209528, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.8187%\n",
      "layer   3  Sparsity: 79.4315%\n",
      "total_backward_count 1840520 real_backward_count 130527   7.092%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.761178/  1.193940, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.8082%\n",
      "layer   3  Sparsity: 79.3260%\n",
      "total_backward_count 1850310 real_backward_count 130921   7.076%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.797903/  1.190122, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5945%\n",
      "layer   3  Sparsity: 80.2188%\n",
      "total_backward_count 1860100 real_backward_count 131327   7.060%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.821296/  1.240733, val:  76.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5694%\n",
      "layer   3  Sparsity: 80.0342%\n",
      "total_backward_count 1869890 real_backward_count 131685   7.042%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.808174/  1.256151, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.3800%\n",
      "layer   3  Sparsity: 80.1573%\n",
      "total_backward_count 1879680 real_backward_count 132113   7.028%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.806411/  1.236775, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.7922%\n",
      "layer   3  Sparsity: 80.2610%\n",
      "total_backward_count 1889470 real_backward_count 132498   7.012%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.791269/  1.254351, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.5730%\n",
      "layer   3  Sparsity: 79.8012%\n",
      "total_backward_count 1899260 real_backward_count 132887   6.997%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.797343/  1.194396, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4718%\n",
      "layer   3  Sparsity: 78.7771%\n",
      "total_backward_count 1909050 real_backward_count 133294   6.982%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.788088/  1.214390, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.4437%\n",
      "layer   3  Sparsity: 79.5765%\n",
      "total_backward_count 1918840 real_backward_count 133723   6.969%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.813123/  1.249411, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6182%\n",
      "layer   3  Sparsity: 81.2877%\n",
      "total_backward_count 1928630 real_backward_count 134137   6.955%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.826874/  1.203606, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.8138%\n",
      "layer   3  Sparsity: 80.3992%\n",
      "total_backward_count 1938420 real_backward_count 134564   6.942%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.798871/  1.209902, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6257%\n",
      "layer   3  Sparsity: 79.6709%\n",
      "total_backward_count 1948210 real_backward_count 134954   6.927%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.799985/  1.176076, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 71.6831%\n",
      "layer   3  Sparsity: 79.7849%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2707b555ef14461d9ad6b226c233cb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñá‚ñá‚ñà‚ñà‚ñÖ‚ñá‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.79998</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.79167</td></tr><tr><td>val_loss</td><td>1.17608</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-sweep-325</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/diu31hql' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/diu31hql</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_061902-diu31hql/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kyil0pii with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_104447-kyil0pii</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kyil0pii' target=\"_blank\">electric-sweep-328</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kyil0pii' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kyil0pii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_104456_457', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 391.0\n",
      "lif layer 1 self.abs_max_v: 391.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 712.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "fc layer 1 self.abs_max_out: 510.0\n",
      "lif layer 1 self.abs_max_v: 525.5\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "lif layer 2 self.abs_max_v: 1078.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "lif layer 1 self.abs_max_v: 687.5\n",
      "fc layer 2 self.abs_max_out: 870.0\n",
      "lif layer 2 self.abs_max_v: 1340.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 1 self.abs_max_out: 604.0\n",
      "lif layer 1 self.abs_max_v: 726.5\n",
      "fc layer 2 self.abs_max_out: 926.0\n",
      "lif layer 2 self.abs_max_v: 1421.0\n",
      "fc layer 1 self.abs_max_out: 912.0\n",
      "lif layer 1 self.abs_max_v: 912.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "lif layer 2 self.abs_max_v: 1516.5\n",
      "fc layer 1 self.abs_max_out: 1085.0\n",
      "lif layer 1 self.abs_max_v: 1085.0\n",
      "lif layer 2 self.abs_max_v: 1664.5\n",
      "fc layer 1 self.abs_max_out: 1258.0\n",
      "lif layer 1 self.abs_max_v: 1258.0\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "lif layer 1 self.abs_max_v: 1423.0\n",
      "lif layer 1 self.abs_max_v: 1525.0\n",
      "fc layer 3 self.abs_max_out: 550.0\n",
      "fc layer 1 self.abs_max_out: 1282.0\n",
      "lif layer 1 self.abs_max_v: 1573.5\n",
      "fc layer 2 self.abs_max_out: 1205.0\n",
      "lif layer 1 self.abs_max_v: 1599.0\n",
      "lif layer 1 self.abs_max_v: 1648.5\n",
      "lif layer 2 self.abs_max_v: 1690.5\n",
      "fc layer 1 self.abs_max_out: 1319.0\n",
      "lif layer 1 self.abs_max_v: 1743.5\n",
      "fc layer 1 self.abs_max_out: 1551.0\n",
      "lif layer 1 self.abs_max_v: 1834.0\n",
      "lif layer 2 self.abs_max_v: 1722.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 2 self.abs_max_out: 1298.0\n",
      "lif layer 2 self.abs_max_v: 1926.5\n",
      "fc layer 2 self.abs_max_out: 1301.0\n",
      "fc layer 1 self.abs_max_out: 1580.0\n",
      "fc layer 1 self.abs_max_out: 2331.0\n",
      "lif layer 1 self.abs_max_v: 2331.0\n",
      "fc layer 1 self.abs_max_out: 2792.0\n",
      "lif layer 1 self.abs_max_v: 2947.5\n",
      "lif layer 2 self.abs_max_v: 2010.0\n",
      "lif layer 2 self.abs_max_v: 2059.0\n",
      "lif layer 1 self.abs_max_v: 3018.0\n",
      "lif layer 1 self.abs_max_v: 3105.0\n",
      "lif layer 2 self.abs_max_v: 2111.5\n",
      "fc layer 1 self.abs_max_out: 2876.0\n",
      "fc layer 2 self.abs_max_out: 1399.0\n",
      "fc layer 2 self.abs_max_out: 1418.0\n",
      "lif layer 1 self.abs_max_v: 3406.0\n",
      "lif layer 2 self.abs_max_v: 2316.0\n",
      "fc layer 3 self.abs_max_out: 633.0\n",
      "fc layer 2 self.abs_max_out: 1433.0\n",
      "fc layer 2 self.abs_max_out: 1703.0\n",
      "lif layer 2 self.abs_max_v: 2365.0\n",
      "lif layer 2 self.abs_max_v: 2386.5\n",
      "lif layer 2 self.abs_max_v: 2589.0\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "fc layer 2 self.abs_max_out: 1719.0\n",
      "fc layer 2 self.abs_max_out: 1847.0\n",
      "lif layer 2 self.abs_max_v: 2717.0\n",
      "lif layer 2 self.abs_max_v: 2832.5\n",
      "fc layer 3 self.abs_max_out: 661.0\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "lif layer 1 self.abs_max_v: 3488.5\n",
      "lif layer 1 self.abs_max_v: 3880.5\n",
      "fc layer 1 self.abs_max_out: 3138.0\n",
      "lif layer 1 self.abs_max_v: 3932.5\n",
      "fc layer 1 self.abs_max_out: 3146.0\n",
      "fc layer 2 self.abs_max_out: 1895.0\n",
      "lif layer 1 self.abs_max_v: 4192.5\n",
      "lif layer 1 self.abs_max_v: 4480.5\n",
      "fc layer 3 self.abs_max_out: 681.0\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "lif layer 2 self.abs_max_v: 2934.0\n",
      "lif layer 2 self.abs_max_v: 2997.0\n",
      "lif layer 2 self.abs_max_v: 3114.5\n",
      "fc layer 2 self.abs_max_out: 1908.0\n",
      "fc layer 3 self.abs_max_out: 740.0\n",
      "fc layer 3 self.abs_max_out: 803.0\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "fc layer 3 self.abs_max_out: 943.0\n",
      "fc layer 3 self.abs_max_out: 1004.0\n",
      "fc layer 2 self.abs_max_out: 1915.0\n",
      "lif layer 2 self.abs_max_v: 3115.0\n",
      "fc layer 2 self.abs_max_out: 1927.0\n",
      "lif layer 2 self.abs_max_v: 3382.5\n",
      "lif layer 1 self.abs_max_v: 4630.0\n",
      "lif layer 1 self.abs_max_v: 4933.0\n",
      "fc layer 1 self.abs_max_out: 3724.0\n",
      "fc layer 1 self.abs_max_out: 3753.0\n",
      "lif layer 1 self.abs_max_v: 4999.0\n",
      "fc layer 1 self.abs_max_out: 3852.0\n",
      "lif layer 1 self.abs_max_v: 5716.5\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "fc layer 2 self.abs_max_out: 2110.0\n",
      "fc layer 2 self.abs_max_out: 2133.0\n",
      "lif layer 2 self.abs_max_v: 3519.0\n",
      "fc layer 2 self.abs_max_out: 2137.0\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "lif layer 1 self.abs_max_v: 5888.5\n",
      "fc layer 3 self.abs_max_out: 1019.0\n",
      "lif layer 1 self.abs_max_v: 6034.5\n",
      "lif layer 1 self.abs_max_v: 6226.5\n",
      "fc layer 1 self.abs_max_out: 4286.0\n",
      "lif layer 1 self.abs_max_v: 7262.5\n",
      "lif layer 1 self.abs_max_v: 7575.5\n",
      "lif layer 1 self.abs_max_v: 7878.0\n",
      "fc layer 3 self.abs_max_out: 1021.0\n",
      "fc layer 1 self.abs_max_out: 4375.0\n",
      "fc layer 1 self.abs_max_out: 4651.0\n",
      "lif layer 2 self.abs_max_v: 3549.0\n",
      "lif layer 2 self.abs_max_v: 3703.5\n",
      "lif layer 2 self.abs_max_v: 3975.5\n",
      "lif layer 2 self.abs_max_v: 4047.0\n",
      "fc layer 2 self.abs_max_out: 2356.0\n",
      "lif layer 2 self.abs_max_v: 4379.5\n",
      "fc layer 2 self.abs_max_out: 2386.0\n",
      "fc layer 3 self.abs_max_out: 1022.0\n",
      "fc layer 3 self.abs_max_out: 1138.0\n",
      "fc layer 3 self.abs_max_out: 1165.0\n",
      "fc layer 3 self.abs_max_out: 1213.0\n",
      "fc layer 3 self.abs_max_out: 1250.0\n",
      "fc layer 1 self.abs_max_out: 4796.0\n",
      "lif layer 1 self.abs_max_v: 8091.5\n",
      "lif layer 1 self.abs_max_v: 8623.0\n",
      "fc layer 1 self.abs_max_out: 5027.0\n",
      "lif layer 1 self.abs_max_v: 9239.5\n",
      "fc layer 3 self.abs_max_out: 1258.0\n",
      "fc layer 3 self.abs_max_out: 1270.0\n",
      "fc layer 1 self.abs_max_out: 5164.0\n",
      "fc layer 3 self.abs_max_out: 1288.0\n",
      "fc layer 2 self.abs_max_out: 2423.0\n",
      "fc layer 1 self.abs_max_out: 5200.0\n",
      "lif layer 1 self.abs_max_v: 9607.5\n",
      "lif layer 1 self.abs_max_v: 9721.0\n",
      "fc layer 3 self.abs_max_out: 1289.0\n",
      "fc layer 3 self.abs_max_out: 1351.0\n",
      "fc layer 3 self.abs_max_out: 1385.0\n",
      "fc layer 2 self.abs_max_out: 2651.0\n",
      "lif layer 1 self.abs_max_v: 9800.5\n",
      "lif layer 1 self.abs_max_v: 9929.5\n",
      "fc layer 1 self.abs_max_out: 5215.0\n",
      "fc layer 1 self.abs_max_out: 5465.0\n",
      "fc layer 1 self.abs_max_out: 5583.0\n",
      "lif layer 1 self.abs_max_v: 9945.5\n",
      "lif layer 1 self.abs_max_v: 10325.5\n",
      "lif layer 1 self.abs_max_v: 10614.0\n",
      "lif layer 2 self.abs_max_v: 4385.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.419160/  1.857085, val:  28.75%, val_best:  28.75%, tr:  99.18%, tr_best:  99.18%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.4142%\n",
      "layer   3  Sparsity: 62.6196%\n",
      "total_backward_count 9790 real_backward_count 1348  13.769%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1415.0\n",
      "fc layer 3 self.abs_max_out: 1430.0\n",
      "fc layer 1 self.abs_max_out: 5610.0\n",
      "fc layer 1 self.abs_max_out: 5773.0\n",
      "fc layer 2 self.abs_max_out: 2702.0\n",
      "lif layer 1 self.abs_max_v: 10636.0\n",
      "lif layer 1 self.abs_max_v: 10722.0\n",
      "fc layer 3 self.abs_max_out: 1460.0\n",
      "fc layer 1 self.abs_max_out: 5822.0\n",
      "fc layer 1 self.abs_max_out: 5823.0\n",
      "lif layer 1 self.abs_max_v: 10795.5\n",
      "fc layer 1 self.abs_max_out: 5962.0\n",
      "fc layer 1 self.abs_max_out: 6008.0\n",
      "fc layer 1 self.abs_max_out: 6218.0\n",
      "fc layer 3 self.abs_max_out: 1508.0\n",
      "fc layer 3 self.abs_max_out: 1510.0\n",
      "fc layer 3 self.abs_max_out: 1677.0\n",
      "fc layer 3 self.abs_max_out: 1684.0\n",
      "fc layer 1 self.abs_max_out: 6555.0\n",
      "fc layer 1 self.abs_max_out: 6607.0\n",
      "lif layer 1 self.abs_max_v: 11729.5\n",
      "lif layer 1 self.abs_max_v: 12277.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.286695/  1.681748, val:  42.08%, val_best:  42.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.6758%\n",
      "layer   3  Sparsity: 65.7943%\n",
      "total_backward_count 19580 real_backward_count 2588  13.218%\n",
      "lif layer 2 self.abs_max_v: 4407.5\n",
      "lif layer 2 self.abs_max_v: 4874.0\n",
      "lif layer 1 self.abs_max_v: 12536.0\n",
      "fc layer 1 self.abs_max_out: 6622.0\n",
      "fc layer 2 self.abs_max_out: 2813.0\n",
      "fc layer 2 self.abs_max_out: 2980.0\n",
      "fc layer 1 self.abs_max_out: 6834.0\n",
      "fc layer 3 self.abs_max_out: 1781.0\n",
      "lif layer 2 self.abs_max_v: 5009.5\n",
      "lif layer 2 self.abs_max_v: 5014.5\n",
      "lif layer 2 self.abs_max_v: 5187.5\n",
      "fc layer 1 self.abs_max_out: 6848.0\n",
      "fc layer 1 self.abs_max_out: 6922.0\n",
      "lif layer 1 self.abs_max_v: 13174.5\n",
      "fc layer 1 self.abs_max_out: 7104.0\n",
      "fc layer 2 self.abs_max_out: 3063.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.180728/  1.639123, val:  45.83%, val_best:  45.83%, tr:  99.59%, tr_best:  99.69%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.3738%\n",
      "layer   3  Sparsity: 64.1915%\n",
      "total_backward_count 29370 real_backward_count 3823  13.017%\n",
      "fc layer 2 self.abs_max_out: 3069.0\n",
      "fc layer 2 self.abs_max_out: 3105.0\n",
      "fc layer 2 self.abs_max_out: 3282.0\n",
      "fc layer 2 self.abs_max_out: 3403.0\n",
      "fc layer 1 self.abs_max_out: 7485.0\n",
      "lif layer 2 self.abs_max_v: 5244.5\n",
      "lif layer 2 self.abs_max_v: 5430.0\n",
      "lif layer 2 self.abs_max_v: 5514.0\n",
      "lif layer 2 self.abs_max_v: 5573.5\n",
      "lif layer 2 self.abs_max_v: 5807.0\n",
      "lif layer 2 self.abs_max_v: 5956.0\n",
      "lif layer 1 self.abs_max_v: 13392.5\n",
      "lif layer 1 self.abs_max_v: 13473.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.118576/  1.659904, val:  42.50%, val_best:  45.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.7745%\n",
      "layer   3  Sparsity: 64.2529%\n",
      "total_backward_count 39160 real_backward_count 5025  12.832%\n",
      "fc layer 1 self.abs_max_out: 7610.0\n",
      "lif layer 1 self.abs_max_v: 13636.5\n",
      "fc layer 2 self.abs_max_out: 3440.0\n",
      "lif layer 2 self.abs_max_v: 6088.5\n",
      "lif layer 1 self.abs_max_v: 14085.5\n",
      "fc layer 1 self.abs_max_out: 7639.0\n",
      "fc layer 1 self.abs_max_out: 7750.0\n",
      "lif layer 1 self.abs_max_v: 14211.5\n",
      "fc layer 1 self.abs_max_out: 8236.0\n",
      "lif layer 1 self.abs_max_v: 14267.0\n",
      "lif layer 1 self.abs_max_v: 14481.0\n",
      "lif layer 1 self.abs_max_v: 15073.5\n",
      "lif layer 1 self.abs_max_v: 15325.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.098692/  1.612008, val:  45.00%, val_best:  45.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.7150%\n",
      "layer   3  Sparsity: 65.7811%\n",
      "total_backward_count 48950 real_backward_count 6205  12.676%\n",
      "fc layer 2 self.abs_max_out: 3597.0\n",
      "fc layer 3 self.abs_max_out: 1809.0\n",
      "fc layer 3 self.abs_max_out: 1812.0\n",
      "fc layer 3 self.abs_max_out: 1818.0\n",
      "fc layer 3 self.abs_max_out: 1950.0\n",
      "fc layer 1 self.abs_max_out: 8488.0\n",
      "fc layer 1 self.abs_max_out: 8855.0\n",
      "lif layer 1 self.abs_max_v: 16095.5\n",
      "lif layer 1 self.abs_max_v: 16600.0\n",
      "lif layer 1 self.abs_max_v: 16759.0\n",
      "lif layer 1 self.abs_max_v: 17079.5\n",
      "fc layer 1 self.abs_max_out: 8872.0\n",
      "lif layer 1 self.abs_max_v: 17412.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.060441/  1.578053, val:  52.92%, val_best:  52.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0939%\n",
      "layer   3  Sparsity: 66.8773%\n",
      "total_backward_count 58740 real_backward_count 7405  12.606%\n",
      "lif layer 2 self.abs_max_v: 6148.0\n",
      "fc layer 3 self.abs_max_out: 1951.0\n",
      "fc layer 3 self.abs_max_out: 1983.0\n",
      "fc layer 3 self.abs_max_out: 2003.0\n",
      "fc layer 3 self.abs_max_out: 2012.0\n",
      "fc layer 3 self.abs_max_out: 2018.0\n",
      "fc layer 2 self.abs_max_out: 3618.0\n",
      "fc layer 2 self.abs_max_out: 3631.0\n",
      "fc layer 2 self.abs_max_out: 3716.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.014148/  1.559768, val:  51.67%, val_best:  52.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2257%\n",
      "layer   3  Sparsity: 65.8617%\n",
      "total_backward_count 68530 real_backward_count 8584  12.526%\n",
      "fc layer 2 self.abs_max_out: 3941.0\n",
      "fc layer 3 self.abs_max_out: 2026.0\n",
      "fc layer 2 self.abs_max_out: 4080.0\n",
      "fc layer 1 self.abs_max_out: 9152.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  0.981773/  1.536764, val:  52.08%, val_best:  52.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2313%\n",
      "layer   3  Sparsity: 65.4616%\n",
      "total_backward_count 78320 real_backward_count 9722  12.413%\n",
      "fc layer 1 self.abs_max_out: 9897.0\n",
      "lif layer 1 self.abs_max_v: 17787.5\n",
      "fc layer 1 self.abs_max_out: 9908.0\n",
      "lif layer 1 self.abs_max_v: 18690.0\n",
      "fc layer 1 self.abs_max_out: 10296.0\n",
      "lif layer 1 self.abs_max_v: 19641.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  0.968347/  1.397858, val:  55.42%, val_best:  55.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.3164%\n",
      "layer   3  Sparsity: 65.4814%\n",
      "total_backward_count 88110 real_backward_count 10841  12.304%\n",
      "lif layer 2 self.abs_max_v: 6208.5\n",
      "lif layer 2 self.abs_max_v: 6412.5\n",
      "lif layer 2 self.abs_max_v: 6430.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  0.915985/  1.520386, val:  47.50%, val_best:  55.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7413%\n",
      "layer   3  Sparsity: 65.6289%\n",
      "total_backward_count 97900 real_backward_count 11980  12.237%\n",
      "fc layer 3 self.abs_max_out: 2032.0\n",
      "lif layer 2 self.abs_max_v: 6703.5\n",
      "lif layer 2 self.abs_max_v: 7012.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.920972/  1.536579, val:  46.25%, val_best:  55.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9430%\n",
      "layer   3  Sparsity: 66.1897%\n",
      "total_backward_count 107690 real_backward_count 13078  12.144%\n",
      "fc layer 3 self.abs_max_out: 2075.0\n",
      "fc layer 3 self.abs_max_out: 2107.0\n",
      "fc layer 3 self.abs_max_out: 2140.0\n",
      "fc layer 2 self.abs_max_out: 4288.0\n",
      "lif layer 2 self.abs_max_v: 7450.5\n",
      "lif layer 2 self.abs_max_v: 7463.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.915207/  1.429627, val:  49.58%, val_best:  55.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6614%\n",
      "layer   3  Sparsity: 66.5473%\n",
      "total_backward_count 117480 real_backward_count 14171  12.062%\n",
      "fc layer 3 self.abs_max_out: 2192.0\n",
      "fc layer 3 self.abs_max_out: 2265.0\n",
      "fc layer 2 self.abs_max_out: 4500.0\n",
      "lif layer 2 self.abs_max_v: 8001.0\n",
      "fc layer 3 self.abs_max_out: 2296.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.870851/  1.421613, val:  49.17%, val_best:  55.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.7807%\n",
      "layer   3  Sparsity: 66.2802%\n",
      "total_backward_count 127270 real_backward_count 15200  11.943%\n",
      "fc layer 3 self.abs_max_out: 2328.0\n",
      "fc layer 3 self.abs_max_out: 2429.0\n",
      "fc layer 3 self.abs_max_out: 2505.0\n",
      "fc layer 3 self.abs_max_out: 2533.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.827760/  1.543275, val:  42.92%, val_best:  55.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.9075%\n",
      "layer   3  Sparsity: 65.0078%\n",
      "total_backward_count 137060 real_backward_count 16215  11.831%\n",
      "fc layer 3 self.abs_max_out: 2699.0\n",
      "fc layer 3 self.abs_max_out: 2728.0\n",
      "fc layer 3 self.abs_max_out: 2847.0\n",
      "fc layer 3 self.abs_max_out: 2848.0\n",
      "fc layer 3 self.abs_max_out: 2989.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.809107/  1.351925, val:  54.17%, val_best:  55.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.3267%\n",
      "layer   3  Sparsity: 64.7550%\n",
      "total_backward_count 146850 real_backward_count 17242  11.741%\n",
      "fc layer 1 self.abs_max_out: 10300.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.787994/  1.312352, val:  54.17%, val_best:  55.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4070%\n",
      "layer   3  Sparsity: 62.6600%\n",
      "total_backward_count 156640 real_backward_count 18273  11.666%\n",
      "fc layer 1 self.abs_max_out: 10469.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.772329/  1.290784, val:  61.67%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4726%\n",
      "layer   3  Sparsity: 64.7229%\n",
      "total_backward_count 166430 real_backward_count 19283  11.586%\n",
      "fc layer 2 self.abs_max_out: 4543.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.766226/  1.444035, val:  49.17%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.8197%\n",
      "layer   3  Sparsity: 64.1185%\n",
      "total_backward_count 176220 real_backward_count 20290  11.514%\n",
      "fc layer 1 self.abs_max_out: 10563.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.717724/  1.321310, val:  50.42%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.6341%\n",
      "layer   3  Sparsity: 63.6527%\n",
      "total_backward_count 186010 real_backward_count 21295  11.448%\n",
      "fc layer 1 self.abs_max_out: 10627.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.702429/  1.412299, val:  53.33%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.8151%\n",
      "layer   3  Sparsity: 63.8673%\n",
      "total_backward_count 195800 real_backward_count 22212  11.344%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.687996/  1.325373, val:  54.17%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.0177%\n",
      "layer   3  Sparsity: 64.1808%\n",
      "total_backward_count 205590 real_backward_count 23146  11.258%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.680979/  1.357952, val:  53.75%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.3673%\n",
      "layer   3  Sparsity: 63.9949%\n",
      "total_backward_count 215380 real_backward_count 24103  11.191%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.686205/  1.262375, val:  52.08%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8165%\n",
      "layer   3  Sparsity: 65.4638%\n",
      "total_backward_count 225170 real_backward_count 25041  11.121%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.687573/  1.295771, val:  59.58%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.5676%\n",
      "layer   3  Sparsity: 66.3150%\n",
      "total_backward_count 234960 real_backward_count 26010  11.070%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.648594/  1.214751, val:  62.50%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.4017%\n",
      "layer   3  Sparsity: 65.2125%\n",
      "total_backward_count 244750 real_backward_count 26902  10.992%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.634175/  1.133361, val:  66.67%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.9408%\n",
      "layer   3  Sparsity: 64.8635%\n",
      "total_backward_count 254540 real_backward_count 27850  10.941%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.622290/  1.223830, val:  57.92%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.3747%\n",
      "layer   3  Sparsity: 65.9910%\n",
      "total_backward_count 264330 real_backward_count 28738  10.872%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.577466/  1.145571, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.7550%\n",
      "layer   3  Sparsity: 64.6654%\n",
      "total_backward_count 274120 real_backward_count 29576  10.789%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.582326/  1.163552, val:  67.08%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8750%\n",
      "layer   3  Sparsity: 63.4374%\n",
      "total_backward_count 283910 real_backward_count 30393  10.705%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.565415/  1.256902, val:  55.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.2754%\n",
      "layer   3  Sparsity: 64.6291%\n",
      "total_backward_count 293700 real_backward_count 31215  10.628%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.576160/  1.172565, val:  69.17%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8490%\n",
      "layer   3  Sparsity: 65.7437%\n",
      "total_backward_count 303490 real_backward_count 32044  10.559%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.579072/  1.413511, val:  54.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.9204%\n",
      "layer   3  Sparsity: 65.6734%\n",
      "total_backward_count 313280 real_backward_count 32853  10.487%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.566924/  1.213140, val:  63.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.9279%\n",
      "layer   3  Sparsity: 65.7799%\n",
      "total_backward_count 323070 real_backward_count 33669  10.422%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.585608/  1.268653, val:  60.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.7080%\n",
      "layer   3  Sparsity: 66.2068%\n",
      "total_backward_count 332860 real_backward_count 34463  10.354%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.563271/  1.205496, val:  58.33%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.7150%\n",
      "layer   3  Sparsity: 66.8450%\n",
      "total_backward_count 342650 real_backward_count 35242  10.285%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.550710/  1.135694, val:  68.75%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.0766%\n",
      "layer   3  Sparsity: 67.3247%\n",
      "total_backward_count 352440 real_backward_count 36055  10.230%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.543231/  1.259739, val:  62.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.1786%\n",
      "layer   3  Sparsity: 67.8679%\n",
      "total_backward_count 362230 real_backward_count 36786  10.155%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.535283/  1.224031, val:  62.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.4919%\n",
      "layer   3  Sparsity: 67.8503%\n",
      "total_backward_count 372020 real_backward_count 37474  10.073%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.557561/  1.271172, val:  60.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.4641%\n",
      "layer   3  Sparsity: 67.8872%\n",
      "total_backward_count 381810 real_backward_count 38203  10.006%\n",
      "fc layer 1 self.abs_max_out: 10871.0\n",
      "lif layer 1 self.abs_max_v: 19879.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.530599/  1.295239, val:  61.67%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.6376%\n",
      "layer   3  Sparsity: 67.7117%\n",
      "total_backward_count 391600 real_backward_count 38896   9.933%\n",
      "fc layer 1 self.abs_max_out: 10914.0\n",
      "lif layer 1 self.abs_max_v: 19911.5\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.528336/  1.182366, val:  65.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.2299%\n",
      "layer   3  Sparsity: 67.7494%\n",
      "total_backward_count 401390 real_backward_count 39630   9.873%\n",
      "lif layer 2 self.abs_max_v: 8246.5\n",
      "lif layer 2 self.abs_max_v: 8352.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.541128/  1.185358, val:  65.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.2738%\n",
      "layer   3  Sparsity: 67.3894%\n",
      "total_backward_count 411180 real_backward_count 40338   9.810%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.499792/  1.093499, val:  72.08%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.0912%\n",
      "layer   3  Sparsity: 66.4459%\n",
      "total_backward_count 420970 real_backward_count 41011   9.742%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.494464/  1.155666, val:  64.17%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.4517%\n",
      "layer   3  Sparsity: 66.3856%\n",
      "total_backward_count 430760 real_backward_count 41642   9.667%\n",
      "fc layer 3 self.abs_max_out: 2993.0\n",
      "fc layer 3 self.abs_max_out: 3054.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.480243/  1.190088, val:  67.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.5976%\n",
      "layer   3  Sparsity: 67.4461%\n",
      "total_backward_count 440550 real_backward_count 42292   9.600%\n",
      "fc layer 3 self.abs_max_out: 3072.0\n",
      "fc layer 3 self.abs_max_out: 3093.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.484139/  1.097417, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8963%\n",
      "layer   3  Sparsity: 67.7333%\n",
      "total_backward_count 450340 real_backward_count 42932   9.533%\n",
      "fc layer 3 self.abs_max_out: 3154.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.457567/  1.114528, val:  70.42%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.0289%\n",
      "layer   3  Sparsity: 68.5984%\n",
      "total_backward_count 460130 real_backward_count 43497   9.453%\n",
      "fc layer 3 self.abs_max_out: 3187.0\n",
      "fc layer 1 self.abs_max_out: 11034.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.484725/  1.255914, val:  56.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.6252%\n",
      "layer   3  Sparsity: 67.9584%\n",
      "total_backward_count 469920 real_backward_count 44102   9.385%\n",
      "lif layer 1 self.abs_max_v: 20008.5\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.480194/  1.034842, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8192%\n",
      "layer   3  Sparsity: 68.2048%\n",
      "total_backward_count 479710 real_backward_count 44716   9.321%\n",
      "fc layer 1 self.abs_max_out: 11215.0\n",
      "fc layer 1 self.abs_max_out: 11245.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.467052/  1.067294, val:  67.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.7561%\n",
      "layer   3  Sparsity: 68.8664%\n",
      "total_backward_count 489500 real_backward_count 45288   9.252%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.469541/  1.311442, val:  65.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.0055%\n",
      "layer   3  Sparsity: 68.5635%\n",
      "total_backward_count 499290 real_backward_count 45902   9.193%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.456562/  1.040777, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8683%\n",
      "layer   3  Sparsity: 68.5943%\n",
      "total_backward_count 509080 real_backward_count 46477   9.130%\n",
      "lif layer 1 self.abs_max_v: 20080.5\n",
      "lif layer 1 self.abs_max_v: 20366.5\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.484499/  1.226442, val:  60.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.4983%\n",
      "layer   3  Sparsity: 69.4092%\n",
      "total_backward_count 518870 real_backward_count 47053   9.068%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.474105/  1.169725, val:  60.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.0408%\n",
      "layer   3  Sparsity: 67.0683%\n",
      "total_backward_count 528660 real_backward_count 47649   9.013%\n",
      "fc layer 1 self.abs_max_out: 11289.0\n",
      "lif layer 1 self.abs_max_v: 21446.0\n",
      "lif layer 1 self.abs_max_v: 21762.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.481122/  1.118147, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.3070%\n",
      "layer   3  Sparsity: 66.7479%\n",
      "total_backward_count 538450 real_backward_count 48260   8.963%\n",
      "fc layer 1 self.abs_max_out: 11363.0\n",
      "lif layer 1 self.abs_max_v: 21908.5\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.471235/  1.078497, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.0945%\n",
      "layer   3  Sparsity: 67.4100%\n",
      "total_backward_count 548240 real_backward_count 48838   8.908%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.464652/  1.222814, val:  63.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.7882%\n",
      "layer   3  Sparsity: 66.3159%\n",
      "total_backward_count 558030 real_backward_count 49395   8.852%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.441549/  1.101445, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2221%\n",
      "layer   3  Sparsity: 66.2341%\n",
      "total_backward_count 567820 real_backward_count 49921   8.792%\n",
      "fc layer 1 self.abs_max_out: 11400.0\n",
      "lif layer 1 self.abs_max_v: 21982.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.444728/  1.026062, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4108%\n",
      "layer   3  Sparsity: 67.2863%\n",
      "total_backward_count 577610 real_backward_count 50480   8.739%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.426838/  1.097509, val:  67.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4631%\n",
      "layer   3  Sparsity: 68.2764%\n",
      "total_backward_count 587400 real_backward_count 50955   8.675%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.425085/  1.123128, val:  65.42%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.1111%\n",
      "layer   3  Sparsity: 67.8334%\n",
      "total_backward_count 597190 real_backward_count 51485   8.621%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.426687/  1.085344, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.9701%\n",
      "layer   3  Sparsity: 67.9483%\n",
      "total_backward_count 606980 real_backward_count 51981   8.564%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.451539/  1.103452, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.6273%\n",
      "layer   3  Sparsity: 68.1015%\n",
      "total_backward_count 616770 real_backward_count 52534   8.518%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.434703/  1.106870, val:  69.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.0887%\n",
      "layer   3  Sparsity: 68.3278%\n",
      "total_backward_count 626560 real_backward_count 53020   8.462%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.433059/  1.123553, val:  67.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.1992%\n",
      "layer   3  Sparsity: 67.9080%\n",
      "total_backward_count 636350 real_backward_count 53570   8.418%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.414777/  1.020180, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.3745%\n",
      "layer   3  Sparsity: 68.4107%\n",
      "total_backward_count 646140 real_backward_count 54054   8.366%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.417291/  1.064452, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.5660%\n",
      "layer   3  Sparsity: 68.5598%\n",
      "total_backward_count 655930 real_backward_count 54552   8.317%\n",
      "fc layer 3 self.abs_max_out: 3247.0\n",
      "fc layer 1 self.abs_max_out: 11436.0\n",
      "lif layer 1 self.abs_max_v: 22053.0\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.417975/  1.121873, val:  69.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.4132%\n",
      "layer   3  Sparsity: 67.7289%\n",
      "total_backward_count 665720 real_backward_count 55017   8.264%\n",
      "fc layer 3 self.abs_max_out: 3339.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.401679/  1.009343, val:  72.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.6139%\n",
      "layer   3  Sparsity: 68.6607%\n",
      "total_backward_count 675510 real_backward_count 55454   8.209%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.392401/  1.188710, val:  62.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.1694%\n",
      "layer   3  Sparsity: 68.3304%\n",
      "total_backward_count 685300 real_backward_count 55863   8.152%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.400768/  1.046706, val:  70.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.3645%\n",
      "layer   3  Sparsity: 69.2284%\n",
      "total_backward_count 695090 real_backward_count 56340   8.105%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.408017/  1.016794, val:  74.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.3779%\n",
      "layer   3  Sparsity: 68.2301%\n",
      "total_backward_count 704880 real_backward_count 56772   8.054%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.400290/  0.992024, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.8896%\n",
      "layer   3  Sparsity: 68.5912%\n",
      "total_backward_count 714670 real_backward_count 57212   8.005%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.397213/  1.135046, val:  66.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.8614%\n",
      "layer   3  Sparsity: 68.2548%\n",
      "total_backward_count 724460 real_backward_count 57661   7.959%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.383913/  1.064340, val:  70.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.8977%\n",
      "layer   3  Sparsity: 68.1243%\n",
      "total_backward_count 734250 real_backward_count 58101   7.913%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.370958/  1.005033, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.5187%\n",
      "layer   3  Sparsity: 68.9011%\n",
      "total_backward_count 744040 real_backward_count 58484   7.860%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.371886/  1.081999, val:  66.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.5634%\n",
      "layer   3  Sparsity: 68.5504%\n",
      "total_backward_count 753830 real_backward_count 58900   7.813%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.372551/  1.100136, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.3276%\n",
      "layer   3  Sparsity: 68.4889%\n",
      "total_backward_count 763620 real_backward_count 59322   7.769%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.382637/  1.200042, val:  70.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.6596%\n",
      "layer   3  Sparsity: 68.7296%\n",
      "total_backward_count 773410 real_backward_count 59760   7.727%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.375813/  1.032254, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.7083%\n",
      "layer   3  Sparsity: 69.4540%\n",
      "total_backward_count 783200 real_backward_count 60176   7.683%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.374512/  1.000289, val:  72.08%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9546%\n",
      "layer   3  Sparsity: 69.3771%\n",
      "total_backward_count 792990 real_backward_count 60594   7.641%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.369502/  1.099144, val:  67.08%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8819%\n",
      "layer   3  Sparsity: 69.1594%\n",
      "total_backward_count 802780 real_backward_count 60990   7.597%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.362056/  1.010855, val:  74.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8180%\n",
      "layer   3  Sparsity: 68.7433%\n",
      "total_backward_count 812570 real_backward_count 61370   7.553%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.366283/  1.038874, val:  74.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.7723%\n",
      "layer   3  Sparsity: 69.2379%\n",
      "total_backward_count 822360 real_backward_count 61772   7.512%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.372029/  1.013749, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0911%\n",
      "layer   3  Sparsity: 69.6836%\n",
      "total_backward_count 832150 real_backward_count 62165   7.470%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.361698/  1.053476, val:  73.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5049%\n",
      "layer   3  Sparsity: 71.0295%\n",
      "total_backward_count 841940 real_backward_count 62546   7.429%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.370108/  1.063109, val:  74.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5746%\n",
      "layer   3  Sparsity: 71.1898%\n",
      "total_backward_count 851730 real_backward_count 62912   7.386%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.360642/  1.063318, val:  72.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5776%\n",
      "layer   3  Sparsity: 70.8425%\n",
      "total_backward_count 861520 real_backward_count 63275   7.345%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.354997/  1.001610, val:  74.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2095%\n",
      "layer   3  Sparsity: 70.1895%\n",
      "total_backward_count 871310 real_backward_count 63633   7.303%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.359508/  1.049604, val:  71.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1320%\n",
      "layer   3  Sparsity: 70.6249%\n",
      "total_backward_count 881100 real_backward_count 64009   7.265%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.352680/  1.021842, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0379%\n",
      "layer   3  Sparsity: 69.5658%\n",
      "total_backward_count 890890 real_backward_count 64381   7.227%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.338090/  0.998485, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.6526%\n",
      "layer   3  Sparsity: 70.5247%\n",
      "total_backward_count 900680 real_backward_count 64697   7.183%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.335756/  0.969604, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.5726%\n",
      "layer   3  Sparsity: 70.3547%\n",
      "total_backward_count 910470 real_backward_count 65033   7.143%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.349354/  0.990120, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4545%\n",
      "layer   3  Sparsity: 69.7535%\n",
      "total_backward_count 920260 real_backward_count 65392   7.106%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.360167/  1.092312, val:  70.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.5239%\n",
      "layer   3  Sparsity: 70.3661%\n",
      "total_backward_count 930050 real_backward_count 65774   7.072%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.343536/  1.037760, val:  74.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1510%\n",
      "layer   3  Sparsity: 71.8125%\n",
      "total_backward_count 939840 real_backward_count 66078   7.031%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.343428/  1.139807, val:  68.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9438%\n",
      "layer   3  Sparsity: 71.2837%\n",
      "total_backward_count 949630 real_backward_count 66399   6.992%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.363031/  1.021009, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.7558%\n",
      "layer   3  Sparsity: 71.0901%\n",
      "total_backward_count 959420 real_backward_count 66726   6.955%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.346858/  1.000602, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4398%\n",
      "layer   3  Sparsity: 70.5776%\n",
      "total_backward_count 969210 real_backward_count 67070   6.920%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.354147/  1.024240, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.6837%\n",
      "layer   3  Sparsity: 70.2155%\n",
      "total_backward_count 979000 real_backward_count 67421   6.887%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.348273/  0.977840, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.6420%\n",
      "layer   3  Sparsity: 70.4524%\n",
      "total_backward_count 988790 real_backward_count 67747   6.852%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.329795/  1.013077, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2932%\n",
      "layer   3  Sparsity: 70.8141%\n",
      "total_backward_count 998580 real_backward_count 68041   6.814%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.361245/  1.080879, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.7146%\n",
      "layer   3  Sparsity: 70.5533%\n",
      "total_backward_count 1008370 real_backward_count 68383   6.782%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.351284/  1.115764, val:  65.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1226%\n",
      "layer   3  Sparsity: 71.1072%\n",
      "total_backward_count 1018160 real_backward_count 68668   6.744%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.367263/  1.032953, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8806%\n",
      "layer   3  Sparsity: 69.8519%\n",
      "total_backward_count 1027950 real_backward_count 69009   6.713%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.351868/  1.040576, val:  72.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9906%\n",
      "layer   3  Sparsity: 70.5863%\n",
      "total_backward_count 1037740 real_backward_count 69319   6.680%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.322504/  1.009886, val:  74.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1824%\n",
      "layer   3  Sparsity: 70.0800%\n",
      "total_backward_count 1047530 real_backward_count 69603   6.644%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.329800/  1.058864, val:  73.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.6379%\n",
      "layer   3  Sparsity: 70.6378%\n",
      "total_backward_count 1057320 real_backward_count 69940   6.615%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.335278/  1.001751, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4208%\n",
      "layer   3  Sparsity: 71.1129%\n",
      "total_backward_count 1067110 real_backward_count 70237   6.582%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.341945/  1.021437, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8329%\n",
      "layer   3  Sparsity: 72.1992%\n",
      "total_backward_count 1076900 real_backward_count 70526   6.549%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.333372/  1.073012, val:  72.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.6358%\n",
      "layer   3  Sparsity: 72.2895%\n",
      "total_backward_count 1086690 real_backward_count 70799   6.515%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.357361/  1.152079, val:  61.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4497%\n",
      "layer   3  Sparsity: 71.2179%\n",
      "total_backward_count 1096480 real_backward_count 71099   6.484%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.339999/  1.010239, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8139%\n",
      "layer   3  Sparsity: 72.0866%\n",
      "total_backward_count 1106270 real_backward_count 71396   6.454%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.334532/  1.011135, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1312%\n",
      "layer   3  Sparsity: 71.9269%\n",
      "total_backward_count 1116060 real_backward_count 71657   6.421%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.343580/  1.029238, val:  73.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9875%\n",
      "layer   3  Sparsity: 71.1211%\n",
      "total_backward_count 1125850 real_backward_count 71942   6.390%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.354871/  1.068638, val:  73.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9112%\n",
      "layer   3  Sparsity: 71.4682%\n",
      "total_backward_count 1135640 real_backward_count 72233   6.361%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.347081/  0.998035, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0825%\n",
      "layer   3  Sparsity: 71.7749%\n",
      "total_backward_count 1145430 real_backward_count 72468   6.327%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.319748/  0.972874, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0445%\n",
      "layer   3  Sparsity: 70.5901%\n",
      "total_backward_count 1155220 real_backward_count 72711   6.294%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.313441/  1.046480, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1467%\n",
      "layer   3  Sparsity: 70.6961%\n",
      "total_backward_count 1165010 real_backward_count 72942   6.261%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.320946/  1.083589, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1538%\n",
      "layer   3  Sparsity: 71.8467%\n",
      "total_backward_count 1174800 real_backward_count 73190   6.230%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.311746/  1.020692, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0651%\n",
      "layer   3  Sparsity: 71.3352%\n",
      "total_backward_count 1184590 real_backward_count 73427   6.199%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.311785/  1.010015, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1078%\n",
      "layer   3  Sparsity: 71.2219%\n",
      "total_backward_count 1194380 real_backward_count 73654   6.167%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.315479/  1.024566, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8731%\n",
      "layer   3  Sparsity: 71.6334%\n",
      "total_backward_count 1204170 real_backward_count 73881   6.135%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.303329/  0.992108, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1480%\n",
      "layer   3  Sparsity: 71.2738%\n",
      "total_backward_count 1213960 real_backward_count 74097   6.104%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.309300/  0.953634, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9038%\n",
      "layer   3  Sparsity: 71.0346%\n",
      "total_backward_count 1223750 real_backward_count 74336   6.074%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.323160/  1.008949, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9491%\n",
      "layer   3  Sparsity: 72.3865%\n",
      "total_backward_count 1233540 real_backward_count 74599   6.048%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.329995/  1.028274, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9181%\n",
      "layer   3  Sparsity: 71.3749%\n",
      "total_backward_count 1243330 real_backward_count 74835   6.019%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.323612/  0.989872, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0443%\n",
      "layer   3  Sparsity: 71.6198%\n",
      "total_backward_count 1253120 real_backward_count 75066   5.990%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.306051/  1.026518, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9126%\n",
      "layer   3  Sparsity: 70.9438%\n",
      "total_backward_count 1262910 real_backward_count 75280   5.961%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.313032/  1.006916, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4767%\n",
      "layer   3  Sparsity: 70.4971%\n",
      "total_backward_count 1272700 real_backward_count 75482   5.931%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.317709/  1.036368, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8282%\n",
      "layer   3  Sparsity: 70.7194%\n",
      "total_backward_count 1282490 real_backward_count 75713   5.904%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.304539/  0.996309, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1914%\n",
      "layer   3  Sparsity: 71.2828%\n",
      "total_backward_count 1292280 real_backward_count 75939   5.876%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.294366/  0.983135, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2110%\n",
      "layer   3  Sparsity: 71.9659%\n",
      "total_backward_count 1302070 real_backward_count 76132   5.847%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.282110/  1.022684, val:  74.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.3666%\n",
      "layer   3  Sparsity: 71.8736%\n",
      "total_backward_count 1311860 real_backward_count 76308   5.817%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.289740/  1.029075, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9874%\n",
      "layer   3  Sparsity: 71.2518%\n",
      "total_backward_count 1321650 real_backward_count 76515   5.789%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.297237/  0.979698, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2004%\n",
      "layer   3  Sparsity: 70.0646%\n",
      "total_backward_count 1331440 real_backward_count 76774   5.766%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.266451/  0.966228, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.3320%\n",
      "layer   3  Sparsity: 70.4913%\n",
      "total_backward_count 1341230 real_backward_count 76938   5.736%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.280666/  1.065166, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.4691%\n",
      "layer   3  Sparsity: 70.2746%\n",
      "total_backward_count 1351020 real_backward_count 77122   5.708%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.287495/  1.020732, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1570%\n",
      "layer   3  Sparsity: 70.5925%\n",
      "total_backward_count 1360810 real_backward_count 77330   5.683%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.288774/  1.071344, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7355%\n",
      "layer   3  Sparsity: 70.5436%\n",
      "total_backward_count 1370600 real_backward_count 77563   5.659%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.290833/  1.014222, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0065%\n",
      "layer   3  Sparsity: 70.5331%\n",
      "total_backward_count 1380390 real_backward_count 77790   5.635%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.272138/  1.014457, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9977%\n",
      "layer   3  Sparsity: 70.7638%\n",
      "total_backward_count 1390180 real_backward_count 77979   5.609%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.277208/  1.086446, val:  70.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0353%\n",
      "layer   3  Sparsity: 70.7636%\n",
      "total_backward_count 1399970 real_backward_count 78176   5.584%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.264560/  1.064977, val:  72.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9801%\n",
      "layer   3  Sparsity: 70.9621%\n",
      "total_backward_count 1409760 real_backward_count 78323   5.556%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.286013/  1.007317, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6853%\n",
      "layer   3  Sparsity: 70.7824%\n",
      "total_backward_count 1419550 real_backward_count 78534   5.532%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.275754/  1.011497, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7809%\n",
      "layer   3  Sparsity: 70.4066%\n",
      "total_backward_count 1429340 real_backward_count 78743   5.509%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.265631/  1.030210, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7402%\n",
      "layer   3  Sparsity: 71.2466%\n",
      "total_backward_count 1439130 real_backward_count 78924   5.484%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.249807/  1.042635, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7858%\n",
      "layer   3  Sparsity: 71.7968%\n",
      "total_backward_count 1448920 real_backward_count 79071   5.457%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.263962/  1.012498, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6893%\n",
      "layer   3  Sparsity: 71.7478%\n",
      "total_backward_count 1458710 real_backward_count 79227   5.431%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.274272/  1.035398, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.4932%\n",
      "layer   3  Sparsity: 71.0627%\n",
      "total_backward_count 1468500 real_backward_count 79404   5.407%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.267020/  0.992682, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5865%\n",
      "layer   3  Sparsity: 71.3690%\n",
      "total_backward_count 1478290 real_backward_count 79584   5.384%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.259291/  1.006826, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.4977%\n",
      "layer   3  Sparsity: 71.1452%\n",
      "total_backward_count 1488080 real_backward_count 79750   5.359%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.272512/  1.030353, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6548%\n",
      "layer   3  Sparsity: 71.1724%\n",
      "total_backward_count 1497870 real_backward_count 79914   5.335%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.255597/  0.990396, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6423%\n",
      "layer   3  Sparsity: 71.8357%\n",
      "total_backward_count 1507660 real_backward_count 80083   5.312%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.258727/  1.060307, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7075%\n",
      "layer   3  Sparsity: 71.6164%\n",
      "total_backward_count 1517450 real_backward_count 80243   5.288%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.272089/  1.038733, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6324%\n",
      "layer   3  Sparsity: 70.9362%\n",
      "total_backward_count 1527240 real_backward_count 80395   5.264%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.259496/  1.030426, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6478%\n",
      "layer   3  Sparsity: 71.1128%\n",
      "total_backward_count 1537030 real_backward_count 80530   5.239%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.250574/  1.046664, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1361%\n",
      "layer   3  Sparsity: 71.2826%\n",
      "total_backward_count 1546820 real_backward_count 80663   5.215%\n",
      "fc layer 1 self.abs_max_out: 11594.0\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.254717/  0.999455, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7993%\n",
      "layer   3  Sparsity: 71.0081%\n",
      "total_backward_count 1556610 real_backward_count 80804   5.191%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.266775/  1.023189, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6529%\n",
      "layer   3  Sparsity: 71.2396%\n",
      "total_backward_count 1566400 real_backward_count 80951   5.168%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.258023/  0.955520, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.4645%\n",
      "layer   3  Sparsity: 70.6963%\n",
      "total_backward_count 1576190 real_backward_count 81084   5.144%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.256355/  0.999304, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.4435%\n",
      "layer   3  Sparsity: 71.3594%\n",
      "total_backward_count 1585980 real_backward_count 81223   5.121%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.240790/  1.013599, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5482%\n",
      "layer   3  Sparsity: 71.5539%\n",
      "total_backward_count 1595770 real_backward_count 81342   5.097%\n",
      "fc layer 1 self.abs_max_out: 11811.0\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.251146/  0.987344, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7174%\n",
      "layer   3  Sparsity: 72.0529%\n",
      "total_backward_count 1605560 real_backward_count 81485   5.075%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.263968/  1.015696, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5848%\n",
      "layer   3  Sparsity: 70.9374%\n",
      "total_backward_count 1615350 real_backward_count 81618   5.053%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.250589/  1.006927, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.8094%\n",
      "layer   3  Sparsity: 71.4322%\n",
      "total_backward_count 1625140 real_backward_count 81728   5.029%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.248308/  1.059367, val:  75.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9258%\n",
      "layer   3  Sparsity: 71.7127%\n",
      "total_backward_count 1634930 real_backward_count 81868   5.007%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.252205/  1.073594, val:  72.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7598%\n",
      "layer   3  Sparsity: 71.6908%\n",
      "total_backward_count 1644720 real_backward_count 81994   4.985%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.247957/  0.983034, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7558%\n",
      "layer   3  Sparsity: 71.1203%\n",
      "total_backward_count 1654510 real_backward_count 82125   4.964%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.230850/  0.990646, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0050%\n",
      "layer   3  Sparsity: 72.2597%\n",
      "total_backward_count 1664300 real_backward_count 82247   4.942%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.228571/  1.012939, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.8603%\n",
      "layer   3  Sparsity: 72.7130%\n",
      "total_backward_count 1674090 real_backward_count 82353   4.919%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.253357/  1.042185, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6671%\n",
      "layer   3  Sparsity: 71.5062%\n",
      "total_backward_count 1683880 real_backward_count 82519   4.901%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.242736/  0.992406, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9565%\n",
      "layer   3  Sparsity: 71.3603%\n",
      "total_backward_count 1693670 real_backward_count 82659   4.880%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.240564/  1.022399, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1519%\n",
      "layer   3  Sparsity: 71.6389%\n",
      "total_backward_count 1703460 real_backward_count 82781   4.860%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.248657/  0.993698, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0685%\n",
      "layer   3  Sparsity: 72.0487%\n",
      "total_backward_count 1713250 real_backward_count 82914   4.840%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.235018/  1.000042, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.8146%\n",
      "layer   3  Sparsity: 71.5686%\n",
      "total_backward_count 1723040 real_backward_count 83022   4.818%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.250106/  0.980814, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5566%\n",
      "layer   3  Sparsity: 71.7805%\n",
      "total_backward_count 1732830 real_backward_count 83140   4.798%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.231144/  1.056017, val:  75.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9020%\n",
      "layer   3  Sparsity: 72.1039%\n",
      "total_backward_count 1742620 real_backward_count 83219   4.776%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.239794/  1.035871, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9583%\n",
      "layer   3  Sparsity: 71.6537%\n",
      "total_backward_count 1752410 real_backward_count 83312   4.754%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.234485/  1.018448, val:  77.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0409%\n",
      "layer   3  Sparsity: 71.4273%\n",
      "total_backward_count 1762200 real_backward_count 83421   4.734%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.230521/  1.006101, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1085%\n",
      "layer   3  Sparsity: 71.2415%\n",
      "total_backward_count 1771990 real_backward_count 83517   4.713%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.224684/  0.989844, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9326%\n",
      "layer   3  Sparsity: 71.5836%\n",
      "total_backward_count 1781780 real_backward_count 83609   4.692%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.237981/  0.951971, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6429%\n",
      "layer   3  Sparsity: 71.9547%\n",
      "total_backward_count 1791570 real_backward_count 83722   4.673%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.250261/  0.974833, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2284%\n",
      "layer   3  Sparsity: 71.1921%\n",
      "total_backward_count 1801360 real_backward_count 83857   4.655%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.245697/  0.986347, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6370%\n",
      "layer   3  Sparsity: 70.8054%\n",
      "total_backward_count 1811150 real_backward_count 83967   4.636%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.244098/  1.039010, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.8408%\n",
      "layer   3  Sparsity: 71.0461%\n",
      "total_backward_count 1820940 real_backward_count 84082   4.618%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.230878/  0.965992, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6502%\n",
      "layer   3  Sparsity: 70.6649%\n",
      "total_backward_count 1830730 real_backward_count 84196   4.599%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.236074/  0.979246, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0468%\n",
      "layer   3  Sparsity: 70.7853%\n",
      "total_backward_count 1840520 real_backward_count 84299   4.580%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.238227/  1.028115, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0116%\n",
      "layer   3  Sparsity: 71.1169%\n",
      "total_backward_count 1850310 real_backward_count 84408   4.562%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.220776/  1.043448, val:  74.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1251%\n",
      "layer   3  Sparsity: 71.3902%\n",
      "total_backward_count 1860100 real_backward_count 84495   4.542%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.229270/  0.992833, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.2737%\n",
      "layer   3  Sparsity: 71.8401%\n",
      "total_backward_count 1869890 real_backward_count 84580   4.523%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.218612/  0.978186, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.3708%\n",
      "layer   3  Sparsity: 71.6666%\n",
      "total_backward_count 1879680 real_backward_count 84678   4.505%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.231401/  1.006017, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.2115%\n",
      "layer   3  Sparsity: 71.3724%\n",
      "total_backward_count 1889470 real_backward_count 84792   4.488%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.234333/  0.999375, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.8842%\n",
      "layer   3  Sparsity: 71.6047%\n",
      "total_backward_count 1899260 real_backward_count 84927   4.472%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.226747/  1.030226, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.8469%\n",
      "layer   3  Sparsity: 71.8935%\n",
      "total_backward_count 1909050 real_backward_count 85034   4.454%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.223325/  1.030621, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9006%\n",
      "layer   3  Sparsity: 72.7307%\n",
      "total_backward_count 1918840 real_backward_count 85113   4.436%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.242719/  1.024630, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9196%\n",
      "layer   3  Sparsity: 71.5927%\n",
      "total_backward_count 1928630 real_backward_count 85200   4.418%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.241886/  0.998716, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0425%\n",
      "layer   3  Sparsity: 71.7687%\n",
      "total_backward_count 1938420 real_backward_count 85307   4.401%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.229206/  1.031858, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1217%\n",
      "layer   3  Sparsity: 71.4950%\n",
      "total_backward_count 1948210 real_backward_count 85410   4.384%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.232898/  0.993779, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1440%\n",
      "layer   3  Sparsity: 70.8644%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f0fff1542d4826974476a9cdf45860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.2329</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>0.99378</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">electric-sweep-328</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kyil0pii' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kyil0pii</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_104447-kyil0pii/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ksd11usi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_150713-ksd11usi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksd11usi' target=\"_blank\">playful-sweep-331</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksd11usi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksd11usi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_150722_740', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 173.0\n",
      "lif layer 1 self.abs_max_v: 173.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 67.0\n",
      "lif layer 2 self.abs_max_v: 67.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 207.0\n",
      "lif layer 1 self.abs_max_v: 268.0\n",
      "fc layer 2 self.abs_max_out: 238.0\n",
      "lif layer 2 self.abs_max_v: 245.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "lif layer 1 self.abs_max_v: 269.0\n",
      "fc layer 2 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 322.5\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "fc layer 1 self.abs_max_out: 248.0\n",
      "lif layer 1 self.abs_max_v: 272.5\n",
      "fc layer 2 self.abs_max_out: 258.0\n",
      "lif layer 1 self.abs_max_v: 314.0\n",
      "fc layer 2 self.abs_max_out: 314.0\n",
      "lif layer 2 self.abs_max_v: 401.0\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "lif layer 1 self.abs_max_v: 329.0\n",
      "lif layer 2 self.abs_max_v: 402.5\n",
      "fc layer 1 self.abs_max_out: 306.0\n",
      "lif layer 1 self.abs_max_v: 405.5\n",
      "lif layer 2 self.abs_max_v: 415.5\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "fc layer 1 self.abs_max_out: 372.0\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "fc layer 2 self.abs_max_out: 345.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 2 self.abs_max_out: 358.0\n",
      "lif layer 2 self.abs_max_v: 492.5\n",
      "fc layer 1 self.abs_max_out: 425.0\n",
      "lif layer 1 self.abs_max_v: 448.5\n",
      "fc layer 2 self.abs_max_out: 375.0\n",
      "lif layer 1 self.abs_max_v: 482.0\n",
      "lif layer 2 self.abs_max_v: 506.5\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "lif layer 1 self.abs_max_v: 544.0\n",
      "fc layer 2 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 662.5\n",
      "fc layer 1 self.abs_max_out: 523.0\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "fc layer 1 self.abs_max_out: 599.0\n",
      "lif layer 1 self.abs_max_v: 599.0\n",
      "fc layer 3 self.abs_max_out: 219.0\n",
      "fc layer 1 self.abs_max_out: 631.0\n",
      "lif layer 1 self.abs_max_v: 631.0\n",
      "fc layer 2 self.abs_max_out: 424.0\n",
      "fc layer 2 self.abs_max_out: 443.0\n",
      "fc layer 2 self.abs_max_out: 444.0\n",
      "lif layer 2 self.abs_max_v: 665.5\n",
      "fc layer 2 self.abs_max_out: 542.0\n",
      "lif layer 2 self.abs_max_v: 722.5\n",
      "lif layer 1 self.abs_max_v: 692.5\n",
      "lif layer 2 self.abs_max_v: 739.5\n",
      "lif layer 2 self.abs_max_v: 775.0\n",
      "lif layer 2 self.abs_max_v: 797.5\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "lif layer 2 self.abs_max_v: 807.0\n",
      "fc layer 2 self.abs_max_out: 605.0\n",
      "lif layer 2 self.abs_max_v: 925.0\n",
      "lif layer 1 self.abs_max_v: 700.0\n",
      "lif layer 1 self.abs_max_v: 710.5\n",
      "fc layer 1 self.abs_max_out: 650.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 1 self.abs_max_out: 771.0\n",
      "lif layer 1 self.abs_max_v: 771.0\n",
      "lif layer 1 self.abs_max_v: 802.0\n",
      "lif layer 1 self.abs_max_v: 876.5\n",
      "fc layer 1 self.abs_max_out: 779.0\n",
      "fc layer 1 self.abs_max_out: 827.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 2 self.abs_max_out: 622.0\n",
      "lif layer 2 self.abs_max_v: 1000.5\n",
      "fc layer 1 self.abs_max_out: 897.0\n",
      "lif layer 1 self.abs_max_v: 897.0\n",
      "lif layer 2 self.abs_max_v: 1006.0\n",
      "lif layer 2 self.abs_max_v: 1046.5\n",
      "fc layer 3 self.abs_max_out: 260.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "lif layer 2 self.abs_max_v: 1056.5\n",
      "lif layer 2 self.abs_max_v: 1098.5\n",
      "fc layer 2 self.abs_max_out: 645.0\n",
      "lif layer 2 self.abs_max_v: 1123.0\n",
      "fc layer 1 self.abs_max_out: 902.0\n",
      "lif layer 1 self.abs_max_v: 902.0\n",
      "lif layer 1 self.abs_max_v: 906.0\n",
      "lif layer 1 self.abs_max_v: 919.0\n",
      "lif layer 1 self.abs_max_v: 955.5\n",
      "fc layer 2 self.abs_max_out: 675.0\n",
      "fc layer 2 self.abs_max_out: 705.0\n",
      "lif layer 2 self.abs_max_v: 1180.5\n",
      "fc layer 1 self.abs_max_out: 916.0\n",
      "fc layer 1 self.abs_max_out: 937.0\n",
      "lif layer 1 self.abs_max_v: 1052.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 2 self.abs_max_out: 717.0\n",
      "lif layer 1 self.abs_max_v: 1340.0\n",
      "fc layer 1 self.abs_max_out: 957.0\n",
      "fc layer 1 self.abs_max_out: 965.0\n",
      "fc layer 1 self.abs_max_out: 1081.0\n",
      "fc layer 2 self.abs_max_out: 739.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "fc layer 1 self.abs_max_out: 1108.0\n",
      "fc layer 1 self.abs_max_out: 1222.0\n",
      "fc layer 2 self.abs_max_out: 788.0\n",
      "fc layer 1 self.abs_max_out: 1269.0\n",
      "fc layer 2 self.abs_max_out: 842.0\n",
      "fc layer 1 self.abs_max_out: 1282.0\n",
      "lif layer 1 self.abs_max_v: 1407.5\n",
      "fc layer 2 self.abs_max_out: 849.0\n",
      "fc layer 1 self.abs_max_out: 1313.0\n",
      "lif layer 1 self.abs_max_v: 1453.5\n",
      "lif layer 1 self.abs_max_v: 1589.0\n",
      "fc layer 1 self.abs_max_out: 1351.0\n",
      "lif layer 2 self.abs_max_v: 1187.0\n",
      "fc layer 2 self.abs_max_out: 854.0\n",
      "fc layer 2 self.abs_max_out: 869.0\n",
      "fc layer 2 self.abs_max_out: 875.0\n",
      "fc layer 2 self.abs_max_out: 883.0\n",
      "fc layer 1 self.abs_max_out: 1364.0\n",
      "fc layer 1 self.abs_max_out: 1477.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "lif layer 1 self.abs_max_v: 1732.5\n",
      "lif layer 1 self.abs_max_v: 1788.5\n",
      "lif layer 1 self.abs_max_v: 2027.5\n",
      "lif layer 1 self.abs_max_v: 2114.0\n",
      "lif layer 1 self.abs_max_v: 2217.0\n",
      "lif layer 1 self.abs_max_v: 2223.5\n",
      "lif layer 2 self.abs_max_v: 1187.5\n",
      "fc layer 2 self.abs_max_out: 914.0\n",
      "fc layer 2 self.abs_max_out: 924.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.875105/  2.028286, val:  37.92%, val_best:  37.92%, tr:  94.48%, tr_best:  94.48%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4098%\n",
      "layer   2  Sparsity: 81.0733%\n",
      "layer   3  Sparsity: 79.6384%\n",
      "total_backward_count 9790 real_backward_count 2550  26.047%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 933.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 2 self.abs_max_out: 941.0\n",
      "fc layer 2 self.abs_max_out: 960.0\n",
      "fc layer 2 self.abs_max_out: 970.0\n",
      "fc layer 2 self.abs_max_out: 977.0\n",
      "fc layer 2 self.abs_max_out: 982.0\n",
      "lif layer 2 self.abs_max_v: 1190.0\n",
      "fc layer 1 self.abs_max_out: 1570.0\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "lif layer 2 self.abs_max_v: 1278.0\n",
      "lif layer 2 self.abs_max_v: 1297.0\n",
      "lif layer 2 self.abs_max_v: 1419.0\n",
      "fc layer 1 self.abs_max_out: 1672.0\n",
      "fc layer 2 self.abs_max_out: 1015.0\n",
      "fc layer 1 self.abs_max_out: 1702.0\n",
      "lif layer 1 self.abs_max_v: 2280.0\n",
      "lif layer 1 self.abs_max_v: 2351.5\n",
      "lif layer 1 self.abs_max_v: 2510.0\n",
      "lif layer 2 self.abs_max_v: 1420.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.800974/  1.976099, val:  45.00%, val_best:  45.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4265%\n",
      "layer   2  Sparsity: 81.0469%\n",
      "layer   3  Sparsity: 77.5993%\n",
      "total_backward_count 19580 real_backward_count 4257  21.742%\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "lif layer 2 self.abs_max_v: 1422.0\n",
      "lif layer 2 self.abs_max_v: 1430.0\n",
      "fc layer 1 self.abs_max_out: 1740.0\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "fc layer 1 self.abs_max_out: 1767.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.761325/  1.960907, val:  47.50%, val_best:  47.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4111%\n",
      "layer   2  Sparsity: 80.1518%\n",
      "layer   3  Sparsity: 76.0437%\n",
      "total_backward_count 29370 real_backward_count 5844  19.898%\n",
      "fc layer 2 self.abs_max_out: 1044.0\n",
      "fc layer 1 self.abs_max_out: 1796.0\n",
      "fc layer 1 self.abs_max_out: 1820.0\n",
      "fc layer 1 self.abs_max_out: 1896.0\n",
      "fc layer 1 self.abs_max_out: 1936.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 3 self.abs_max_out: 414.0\n",
      "lif layer 1 self.abs_max_v: 2593.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.743276/  1.944078, val:  45.83%, val_best:  47.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4183%\n",
      "layer   2  Sparsity: 80.3559%\n",
      "layer   3  Sparsity: 75.7169%\n",
      "total_backward_count 39160 real_backward_count 7366  18.810%\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 1 self.abs_max_v: 2651.5\n",
      "lif layer 1 self.abs_max_v: 2678.0\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.732125/  1.930873, val:  41.25%, val_best:  47.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4279%\n",
      "layer   2  Sparsity: 80.0778%\n",
      "layer   3  Sparsity: 74.8473%\n",
      "total_backward_count 48950 real_backward_count 8813  18.004%\n",
      "lif layer 1 self.abs_max_v: 2974.0\n",
      "fc layer 1 self.abs_max_out: 1952.0\n",
      "lif layer 1 self.abs_max_v: 3074.5\n",
      "lif layer 1 self.abs_max_v: 3184.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.717569/  1.936699, val:  55.83%, val_best:  55.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4096%\n",
      "layer   2  Sparsity: 79.5758%\n",
      "layer   3  Sparsity: 75.2336%\n",
      "total_backward_count 58740 real_backward_count 10246  17.443%\n",
      "fc layer 2 self.abs_max_out: 1120.0\n",
      "fc layer 1 self.abs_max_out: 1991.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.699749/  1.888192, val:  50.83%, val_best:  55.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4430%\n",
      "layer   2  Sparsity: 79.4067%\n",
      "layer   3  Sparsity: 74.8247%\n",
      "total_backward_count 68530 real_backward_count 11631  16.972%\n",
      "lif layer 2 self.abs_max_v: 1487.0\n",
      "lif layer 2 self.abs_max_v: 1628.0\n",
      "lif layer 1 self.abs_max_v: 3224.0\n",
      "lif layer 1 self.abs_max_v: 3256.0\n",
      "lif layer 2 self.abs_max_v: 1629.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.675076/  1.876017, val:  53.75%, val_best:  55.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4064%\n",
      "layer   2  Sparsity: 79.3631%\n",
      "layer   3  Sparsity: 74.9345%\n",
      "total_backward_count 78320 real_backward_count 12957  16.544%\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "lif layer 2 self.abs_max_v: 1704.5\n",
      "lif layer 2 self.abs_max_v: 1738.5\n",
      "lif layer 1 self.abs_max_v: 3277.5\n",
      "fc layer 1 self.abs_max_out: 2067.0\n",
      "fc layer 1 self.abs_max_out: 2083.0\n",
      "lif layer 1 self.abs_max_v: 3528.5\n",
      "lif layer 1 self.abs_max_v: 3564.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.665873/  1.858317, val:  55.00%, val_best:  55.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4253%\n",
      "layer   2  Sparsity: 79.7222%\n",
      "layer   3  Sparsity: 75.0936%\n",
      "total_backward_count 88110 real_backward_count 14318  16.250%\n",
      "lif layer 1 self.abs_max_v: 3620.0\n",
      "fc layer 1 self.abs_max_out: 2090.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.627967/  1.838551, val:  53.75%, val_best:  55.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4007%\n",
      "layer   2  Sparsity: 79.4692%\n",
      "layer   3  Sparsity: 74.9960%\n",
      "total_backward_count 97900 real_backward_count 15616  15.951%\n",
      "fc layer 2 self.abs_max_out: 1125.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.624040/  1.868892, val:  43.75%, val_best:  55.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4091%\n",
      "layer   2  Sparsity: 79.2874%\n",
      "layer   3  Sparsity: 75.1050%\n",
      "total_backward_count 107690 real_backward_count 16924  15.715%\n",
      "fc layer 2 self.abs_max_out: 1143.0\n",
      "lif layer 2 self.abs_max_v: 1741.0\n",
      "lif layer 2 self.abs_max_v: 1819.0\n",
      "fc layer 2 self.abs_max_out: 1155.0\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 1 self.abs_max_out: 2100.0\n",
      "fc layer 2 self.abs_max_out: 1263.0\n",
      "lif layer 2 self.abs_max_v: 1850.5\n",
      "fc layer 1 self.abs_max_out: 2113.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.620757/  1.830996, val:  58.33%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4155%\n",
      "layer   2  Sparsity: 79.0761%\n",
      "layer   3  Sparsity: 74.4988%\n",
      "total_backward_count 117480 real_backward_count 18239  15.525%\n",
      "lif layer 2 self.abs_max_v: 1885.0\n",
      "fc layer 1 self.abs_max_out: 2117.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "fc layer 1 self.abs_max_out: 2125.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.600023/  1.811551, val:  45.83%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4339%\n",
      "layer   2  Sparsity: 79.2600%\n",
      "layer   3  Sparsity: 74.5894%\n",
      "total_backward_count 127270 real_backward_count 19471  15.299%\n",
      "lif layer 1 self.abs_max_v: 3647.0\n",
      "fc layer 1 self.abs_max_out: 2154.0\n",
      "fc layer 1 self.abs_max_out: 2208.0\n",
      "lif layer 2 self.abs_max_v: 1920.5\n",
      "lif layer 2 self.abs_max_v: 1941.5\n",
      "lif layer 2 self.abs_max_v: 1976.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.610611/  1.853539, val:  41.25%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4351%\n",
      "layer   2  Sparsity: 79.1467%\n",
      "layer   3  Sparsity: 75.0991%\n",
      "total_backward_count 137060 real_backward_count 20749  15.139%\n",
      "lif layer 2 self.abs_max_v: 2085.0\n",
      "fc layer 1 self.abs_max_out: 2221.0\n",
      "lif layer 1 self.abs_max_v: 3647.5\n",
      "fc layer 1 self.abs_max_out: 2246.0\n",
      "lif layer 1 self.abs_max_v: 3750.5\n",
      "lif layer 1 self.abs_max_v: 3850.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.602462/  1.818220, val:  55.42%, val_best:  58.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4418%\n",
      "layer   2  Sparsity: 79.0361%\n",
      "layer   3  Sparsity: 74.9353%\n",
      "total_backward_count 146850 real_backward_count 21955  14.951%\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "lif layer 1 self.abs_max_v: 3896.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.588438/  1.792786, val:  48.75%, val_best:  58.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3960%\n",
      "layer   2  Sparsity: 78.7927%\n",
      "layer   3  Sparsity: 75.2829%\n",
      "total_backward_count 156640 real_backward_count 23175  14.795%\n",
      "lif layer 1 self.abs_max_v: 3899.0\n",
      "fc layer 1 self.abs_max_out: 2247.0\n",
      "fc layer 1 self.abs_max_out: 2420.0\n",
      "lif layer 1 self.abs_max_v: 4076.5\n",
      "lif layer 1 self.abs_max_v: 4153.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.562004/  1.786255, val:  61.67%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4159%\n",
      "layer   2  Sparsity: 78.8054%\n",
      "layer   3  Sparsity: 75.1269%\n",
      "total_backward_count 166430 real_backward_count 24313  14.609%\n",
      "fc layer 1 self.abs_max_out: 2448.0\n",
      "lif layer 1 self.abs_max_v: 4208.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.573058/  1.769570, val:  70.42%, val_best:  70.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4304%\n",
      "layer   2  Sparsity: 79.0508%\n",
      "layer   3  Sparsity: 75.0394%\n",
      "total_backward_count 176220 real_backward_count 25543  14.495%\n",
      "lif layer 1 self.abs_max_v: 4325.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.572672/  1.774781, val:  46.67%, val_best:  70.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4033%\n",
      "layer   2  Sparsity: 79.2451%\n",
      "layer   3  Sparsity: 75.2463%\n",
      "total_backward_count 186010 real_backward_count 26732  14.371%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.558426/  1.790097, val:  48.75%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4266%\n",
      "layer   2  Sparsity: 79.2518%\n",
      "layer   3  Sparsity: 75.3482%\n",
      "total_backward_count 195800 real_backward_count 27905  14.252%\n",
      "fc layer 1 self.abs_max_out: 2475.0\n",
      "fc layer 1 self.abs_max_out: 2510.0\n",
      "lif layer 1 self.abs_max_v: 4334.5\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.559207/  1.805936, val:  46.67%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4127%\n",
      "layer   2  Sparsity: 79.3615%\n",
      "layer   3  Sparsity: 76.0446%\n",
      "total_backward_count 205590 real_backward_count 29003  14.107%\n",
      "fc layer 2 self.abs_max_out: 1374.0\n",
      "lif layer 1 self.abs_max_v: 4479.5\n",
      "fc layer 1 self.abs_max_out: 2536.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.564525/  1.781876, val:  58.75%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.3847%\n",
      "layer   2  Sparsity: 78.9941%\n",
      "layer   3  Sparsity: 76.6415%\n",
      "total_backward_count 215380 real_backward_count 30150  13.999%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.565203/  1.734651, val:  66.67%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.3983%\n",
      "layer   2  Sparsity: 78.6313%\n",
      "layer   3  Sparsity: 76.2228%\n",
      "total_backward_count 225170 real_backward_count 31332  13.915%\n",
      "fc layer 1 self.abs_max_out: 2579.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.560694/  1.757429, val:  65.00%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4327%\n",
      "layer   2  Sparsity: 78.9010%\n",
      "layer   3  Sparsity: 75.5135%\n",
      "total_backward_count 234960 real_backward_count 32445  13.809%\n",
      "fc layer 1 self.abs_max_out: 2590.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.552824/  1.746956, val:  74.17%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4031%\n",
      "layer   2  Sparsity: 78.3690%\n",
      "layer   3  Sparsity: 75.5922%\n",
      "total_backward_count 244750 real_backward_count 33496  13.686%\n",
      "fc layer 1 self.abs_max_out: 2696.0\n",
      "lif layer 1 self.abs_max_v: 4486.5\n",
      "lif layer 1 self.abs_max_v: 4574.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.535940/  1.718069, val:  72.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4096%\n",
      "layer   2  Sparsity: 78.5810%\n",
      "layer   3  Sparsity: 75.2771%\n",
      "total_backward_count 254540 real_backward_count 34603  13.594%\n",
      "lif layer 1 self.abs_max_v: 4641.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.519928/  1.719820, val:  61.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4024%\n",
      "layer   2  Sparsity: 78.4254%\n",
      "layer   3  Sparsity: 75.1046%\n",
      "total_backward_count 264330 real_backward_count 35631  13.480%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.521846/  1.692752, val:  69.58%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4290%\n",
      "layer   2  Sparsity: 78.7648%\n",
      "layer   3  Sparsity: 75.4940%\n",
      "total_backward_count 274120 real_backward_count 36718  13.395%\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 2 self.abs_max_out: 1424.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.489735/  1.719863, val:  63.33%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4442%\n",
      "layer   2  Sparsity: 78.9375%\n",
      "layer   3  Sparsity: 75.4577%\n",
      "total_backward_count 283910 real_backward_count 37748  13.296%\n",
      "lif layer 1 self.abs_max_v: 4708.5\n",
      "fc layer 1 self.abs_max_out: 2755.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.510346/  1.669108, val:  59.58%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4386%\n",
      "layer   2  Sparsity: 78.6972%\n",
      "layer   3  Sparsity: 75.6714%\n",
      "total_backward_count 293700 real_backward_count 38807  13.213%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.492746/  1.668855, val:  64.58%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4296%\n",
      "layer   2  Sparsity: 78.5457%\n",
      "layer   3  Sparsity: 75.5755%\n",
      "total_backward_count 303490 real_backward_count 39831  13.124%\n",
      "fc layer 1 self.abs_max_out: 2788.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.500445/  1.728160, val:  67.50%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4422%\n",
      "layer   2  Sparsity: 78.2394%\n",
      "layer   3  Sparsity: 75.5030%\n",
      "total_backward_count 313280 real_backward_count 40838  13.036%\n",
      "fc layer 1 self.abs_max_out: 2885.0\n",
      "lif layer 1 self.abs_max_v: 4785.0\n",
      "lif layer 1 self.abs_max_v: 4941.5\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.504157/  1.710579, val:  66.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4058%\n",
      "layer   2  Sparsity: 78.5529%\n",
      "layer   3  Sparsity: 75.6073%\n",
      "total_backward_count 323070 real_backward_count 41807  12.941%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.496688/  1.680357, val:  67.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4344%\n",
      "layer   2  Sparsity: 79.2471%\n",
      "layer   3  Sparsity: 75.1159%\n",
      "total_backward_count 332860 real_backward_count 42801  12.859%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.454761/  1.674785, val:  62.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4424%\n",
      "layer   2  Sparsity: 79.0279%\n",
      "layer   3  Sparsity: 75.1391%\n",
      "total_backward_count 342650 real_backward_count 43755  12.770%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.465927/  1.662132, val:  72.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4390%\n",
      "layer   2  Sparsity: 78.8198%\n",
      "layer   3  Sparsity: 75.4169%\n",
      "total_backward_count 352440 real_backward_count 44663  12.673%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.467046/  1.642992, val:  74.58%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4105%\n",
      "layer   2  Sparsity: 78.9626%\n",
      "layer   3  Sparsity: 75.4272%\n",
      "total_backward_count 362230 real_backward_count 45577  12.582%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.441123/  1.673664, val:  62.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4198%\n",
      "layer   2  Sparsity: 79.0027%\n",
      "layer   3  Sparsity: 75.8150%\n",
      "total_backward_count 372020 real_backward_count 46451  12.486%\n",
      "lif layer 1 self.abs_max_v: 5004.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.450800/  1.660668, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4397%\n",
      "layer   2  Sparsity: 78.8425%\n",
      "layer   3  Sparsity: 75.8743%\n",
      "total_backward_count 381810 real_backward_count 47424  12.421%\n",
      "lif layer 1 self.abs_max_v: 5099.5\n",
      "fc layer 1 self.abs_max_out: 2928.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.453840/  1.657645, val:  68.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4415%\n",
      "layer   2  Sparsity: 78.6091%\n",
      "layer   3  Sparsity: 75.8111%\n",
      "total_backward_count 391600 real_backward_count 48337  12.343%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.442951/  1.638110, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3986%\n",
      "layer   2  Sparsity: 78.9800%\n",
      "layer   3  Sparsity: 75.6212%\n",
      "total_backward_count 401390 real_backward_count 49249  12.270%\n",
      "lif layer 1 self.abs_max_v: 5127.5\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.428887/  1.608105, val:  67.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4302%\n",
      "layer   2  Sparsity: 78.6396%\n",
      "layer   3  Sparsity: 75.4610%\n",
      "total_backward_count 411180 real_backward_count 50176  12.203%\n",
      "fc layer 2 self.abs_max_out: 1430.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.424004/  1.657632, val:  61.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4280%\n",
      "layer   2  Sparsity: 78.6735%\n",
      "layer   3  Sparsity: 75.6084%\n",
      "total_backward_count 420970 real_backward_count 51093  12.137%\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.406106/  1.599202, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4282%\n",
      "layer   2  Sparsity: 78.6684%\n",
      "layer   3  Sparsity: 75.3135%\n",
      "total_backward_count 430760 real_backward_count 51968  12.064%\n",
      "lif layer 1 self.abs_max_v: 5196.5\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "fc layer 1 self.abs_max_out: 2984.0\n",
      "lif layer 1 self.abs_max_v: 5252.5\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.411575/  1.620847, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4333%\n",
      "layer   2  Sparsity: 78.2896%\n",
      "layer   3  Sparsity: 75.1078%\n",
      "total_backward_count 440550 real_backward_count 52798  11.985%\n",
      "lif layer 1 self.abs_max_v: 5366.5\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.399127/  1.594721, val:  74.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4373%\n",
      "layer   2  Sparsity: 78.1733%\n",
      "layer   3  Sparsity: 74.9416%\n",
      "total_backward_count 450340 real_backward_count 53662  11.916%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.391782/  1.627690, val:  68.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3834%\n",
      "layer   2  Sparsity: 78.3058%\n",
      "layer   3  Sparsity: 74.7087%\n",
      "total_backward_count 460130 real_backward_count 54513  11.847%\n",
      "lif layer 1 self.abs_max_v: 5396.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.392966/  1.623093, val:  65.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4273%\n",
      "layer   2  Sparsity: 78.3824%\n",
      "layer   3  Sparsity: 74.9913%\n",
      "total_backward_count 469920 real_backward_count 55311  11.770%\n",
      "fc layer 3 self.abs_max_out: 613.0\n",
      "fc layer 1 self.abs_max_out: 2987.0\n",
      "lif layer 1 self.abs_max_v: 5491.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.385038/  1.602121, val:  86.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3900%\n",
      "layer   2  Sparsity: 78.4607%\n",
      "layer   3  Sparsity: 74.5522%\n",
      "total_backward_count 479710 real_backward_count 56148  11.705%\n",
      "lif layer 1 self.abs_max_v: 5606.5\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.381518/  1.608879, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4131%\n",
      "layer   2  Sparsity: 78.4530%\n",
      "layer   3  Sparsity: 73.9621%\n",
      "total_backward_count 489500 real_backward_count 56987  11.642%\n",
      "fc layer 1 self.abs_max_out: 3042.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.383284/  1.598578, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4121%\n",
      "layer   2  Sparsity: 77.9975%\n",
      "layer   3  Sparsity: 74.4282%\n",
      "total_backward_count 499290 real_backward_count 57799  11.576%\n",
      "fc layer 2 self.abs_max_out: 1576.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.375741/  1.589682, val:  73.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4411%\n",
      "layer   2  Sparsity: 78.3858%\n",
      "layer   3  Sparsity: 74.7311%\n",
      "total_backward_count 509080 real_backward_count 58531  11.497%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.397683/  1.593292, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4021%\n",
      "layer   2  Sparsity: 78.2975%\n",
      "layer   3  Sparsity: 74.7153%\n",
      "total_backward_count 518870 real_backward_count 59326  11.434%\n",
      "lif layer 2 self.abs_max_v: 2131.0\n",
      "lif layer 2 self.abs_max_v: 2230.5\n",
      "lif layer 2 self.abs_max_v: 2268.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.380362/  1.590339, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4021%\n",
      "layer   2  Sparsity: 78.4993%\n",
      "layer   3  Sparsity: 74.8906%\n",
      "total_backward_count 528660 real_backward_count 60101  11.369%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.366472/  1.578987, val:  85.83%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4110%\n",
      "layer   2  Sparsity: 78.2304%\n",
      "layer   3  Sparsity: 75.2759%\n",
      "total_backward_count 538450 real_backward_count 60808  11.293%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.389007/  1.594307, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4140%\n",
      "layer   2  Sparsity: 78.2279%\n",
      "layer   3  Sparsity: 74.8328%\n",
      "total_backward_count 548240 real_backward_count 61578  11.232%\n",
      "lif layer 1 self.abs_max_v: 5621.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.369970/  1.597353, val:  75.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4122%\n",
      "layer   2  Sparsity: 78.2046%\n",
      "layer   3  Sparsity: 75.0528%\n",
      "total_backward_count 558030 real_backward_count 62308  11.166%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.371486/  1.573808, val:  76.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4437%\n",
      "layer   2  Sparsity: 78.2738%\n",
      "layer   3  Sparsity: 75.4131%\n",
      "total_backward_count 567820 real_backward_count 62992  11.094%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.365141/  1.552590, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4138%\n",
      "layer   2  Sparsity: 78.4060%\n",
      "layer   3  Sparsity: 75.3228%\n",
      "total_backward_count 577610 real_backward_count 63709  11.030%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.360924/  1.562750, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4375%\n",
      "layer   2  Sparsity: 78.2429%\n",
      "layer   3  Sparsity: 74.7456%\n",
      "total_backward_count 587400 real_backward_count 64422  10.967%\n",
      "lif layer 1 self.abs_max_v: 5706.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.358348/  1.565737, val:  78.33%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3844%\n",
      "layer   2  Sparsity: 77.9868%\n",
      "layer   3  Sparsity: 74.9246%\n",
      "total_backward_count 597190 real_backward_count 65146  10.909%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.347198/  1.529954, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4058%\n",
      "layer   2  Sparsity: 78.1192%\n",
      "layer   3  Sparsity: 75.4562%\n",
      "total_backward_count 606980 real_backward_count 65893  10.856%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.335743/  1.557340, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4089%\n",
      "layer   2  Sparsity: 78.5431%\n",
      "layer   3  Sparsity: 75.5461%\n",
      "total_backward_count 616770 real_backward_count 66624  10.802%\n",
      "fc layer 3 self.abs_max_out: 641.0\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.341565/  1.540029, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4043%\n",
      "layer   2  Sparsity: 78.2031%\n",
      "layer   3  Sparsity: 75.7374%\n",
      "total_backward_count 626560 real_backward_count 67278  10.738%\n",
      "fc layer 1 self.abs_max_out: 3074.0\n",
      "lif layer 1 self.abs_max_v: 5717.5\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.341188/  1.564752, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4359%\n",
      "layer   2  Sparsity: 78.1754%\n",
      "layer   3  Sparsity: 75.6069%\n",
      "total_backward_count 636350 real_backward_count 67946  10.677%\n",
      "fc layer 1 self.abs_max_out: 3082.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.346054/  1.550314, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4260%\n",
      "layer   2  Sparsity: 78.3610%\n",
      "layer   3  Sparsity: 74.7838%\n",
      "total_backward_count 646140 real_backward_count 68618  10.620%\n",
      "fc layer 1 self.abs_max_out: 3139.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.349705/  1.564400, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4253%\n",
      "layer   2  Sparsity: 78.5565%\n",
      "layer   3  Sparsity: 74.8570%\n",
      "total_backward_count 655930 real_backward_count 69349  10.573%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.343007/  1.538440, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4118%\n",
      "layer   2  Sparsity: 78.1043%\n",
      "layer   3  Sparsity: 75.1445%\n",
      "total_backward_count 665720 real_backward_count 70025  10.519%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.329898/  1.540276, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4058%\n",
      "layer   2  Sparsity: 78.2009%\n",
      "layer   3  Sparsity: 75.1847%\n",
      "total_backward_count 675510 real_backward_count 70716  10.469%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.328476/  1.554298, val:  78.33%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4306%\n",
      "layer   2  Sparsity: 78.0457%\n",
      "layer   3  Sparsity: 75.2055%\n",
      "total_backward_count 685300 real_backward_count 71357  10.413%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.335652/  1.542631, val:  75.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4474%\n",
      "layer   2  Sparsity: 78.0218%\n",
      "layer   3  Sparsity: 75.4402%\n",
      "total_backward_count 695090 real_backward_count 71991  10.357%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.333719/  1.557725, val:  67.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4140%\n",
      "layer   2  Sparsity: 77.6109%\n",
      "layer   3  Sparsity: 75.2151%\n",
      "total_backward_count 704880 real_backward_count 72629  10.304%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.320164/  1.536898, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4231%\n",
      "layer   2  Sparsity: 78.1951%\n",
      "layer   3  Sparsity: 75.4773%\n",
      "total_backward_count 714670 real_backward_count 73238  10.248%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.323079/  1.535291, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4289%\n",
      "layer   2  Sparsity: 78.3130%\n",
      "layer   3  Sparsity: 75.2299%\n",
      "total_backward_count 724460 real_backward_count 73870  10.197%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.334021/  1.537782, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4205%\n",
      "layer   2  Sparsity: 78.3300%\n",
      "layer   3  Sparsity: 74.9419%\n",
      "total_backward_count 734250 real_backward_count 74498  10.146%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.336659/  1.524132, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4036%\n",
      "layer   2  Sparsity: 78.1920%\n",
      "layer   3  Sparsity: 74.4745%\n",
      "total_backward_count 744040 real_backward_count 75075  10.090%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.299107/  1.495236, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4512%\n",
      "layer   2  Sparsity: 78.3311%\n",
      "layer   3  Sparsity: 74.4519%\n",
      "total_backward_count 753830 real_backward_count 75653  10.036%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.299638/  1.531581, val:  71.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4035%\n",
      "layer   2  Sparsity: 78.3189%\n",
      "layer   3  Sparsity: 74.8934%\n",
      "total_backward_count 763620 real_backward_count 76250   9.985%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.302948/  1.521361, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3926%\n",
      "layer   2  Sparsity: 78.1300%\n",
      "layer   3  Sparsity: 74.5074%\n",
      "total_backward_count 773410 real_backward_count 76860   9.938%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.298998/  1.514508, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4131%\n",
      "layer   2  Sparsity: 77.9579%\n",
      "layer   3  Sparsity: 74.7912%\n",
      "total_backward_count 783200 real_backward_count 77465   9.891%\n",
      "lif layer 1 self.abs_max_v: 5754.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.305049/  1.518574, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4234%\n",
      "layer   2  Sparsity: 78.0494%\n",
      "layer   3  Sparsity: 75.3523%\n",
      "total_backward_count 792990 real_backward_count 78062   9.844%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.310669/  1.524126, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4023%\n",
      "layer   2  Sparsity: 77.9845%\n",
      "layer   3  Sparsity: 75.0315%\n",
      "total_backward_count 802780 real_backward_count 78680   9.801%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.308473/  1.519091, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4335%\n",
      "layer   2  Sparsity: 78.0625%\n",
      "layer   3  Sparsity: 74.9604%\n",
      "total_backward_count 812570 real_backward_count 79300   9.759%\n",
      "lif layer 2 self.abs_max_v: 2282.5\n",
      "lif layer 2 self.abs_max_v: 2283.5\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.303828/  1.522546, val:  81.25%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4206%\n",
      "layer   2  Sparsity: 77.9014%\n",
      "layer   3  Sparsity: 75.0185%\n",
      "total_backward_count 822360 real_backward_count 79879   9.713%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.288645/  1.479849, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4365%\n",
      "layer   2  Sparsity: 77.9476%\n",
      "layer   3  Sparsity: 74.8777%\n",
      "total_backward_count 832150 real_backward_count 80449   9.668%\n",
      "lif layer 2 self.abs_max_v: 2342.0\n",
      "fc layer 1 self.abs_max_out: 3145.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.287537/  1.506970, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4425%\n",
      "layer   2  Sparsity: 78.3319%\n",
      "layer   3  Sparsity: 75.4268%\n",
      "total_backward_count 841940 real_backward_count 80989   9.619%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.293624/  1.526382, val:  75.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4266%\n",
      "layer   2  Sparsity: 78.2135%\n",
      "layer   3  Sparsity: 75.4705%\n",
      "total_backward_count 851730 real_backward_count 81574   9.577%\n",
      "fc layer 1 self.abs_max_out: 3172.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.275005/  1.495201, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4199%\n",
      "layer   2  Sparsity: 78.1237%\n",
      "layer   3  Sparsity: 75.0146%\n",
      "total_backward_count 861520 real_backward_count 82090   9.529%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.267369/  1.486256, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4391%\n",
      "layer   2  Sparsity: 78.2445%\n",
      "layer   3  Sparsity: 74.9331%\n",
      "total_backward_count 871310 real_backward_count 82621   9.482%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.263657/  1.463194, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4073%\n",
      "layer   2  Sparsity: 78.2327%\n",
      "layer   3  Sparsity: 74.8884%\n",
      "total_backward_count 881100 real_backward_count 83163   9.439%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.259342/  1.469904, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3952%\n",
      "layer   2  Sparsity: 78.3449%\n",
      "layer   3  Sparsity: 74.8260%\n",
      "total_backward_count 890890 real_backward_count 83720   9.397%\n",
      "fc layer 2 self.abs_max_out: 1677.0\n",
      "lif layer 1 self.abs_max_v: 5779.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.251397/  1.463425, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4245%\n",
      "layer   2  Sparsity: 78.3895%\n",
      "layer   3  Sparsity: 74.6575%\n",
      "total_backward_count 900680 real_backward_count 84253   9.354%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.255051/  1.460040, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3937%\n",
      "layer   2  Sparsity: 78.3242%\n",
      "layer   3  Sparsity: 74.8062%\n",
      "total_backward_count 910470 real_backward_count 84777   9.311%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.239261/  1.464967, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4352%\n",
      "layer   2  Sparsity: 78.3564%\n",
      "layer   3  Sparsity: 75.1026%\n",
      "total_backward_count 920260 real_backward_count 85286   9.268%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.236817/  1.467719, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4309%\n",
      "layer   2  Sparsity: 78.2490%\n",
      "layer   3  Sparsity: 74.8328%\n",
      "total_backward_count 930050 real_backward_count 85820   9.227%\n",
      "fc layer 2 self.abs_max_out: 1682.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.218678/  1.435661, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4067%\n",
      "layer   2  Sparsity: 78.2569%\n",
      "layer   3  Sparsity: 74.7279%\n",
      "total_backward_count 939840 real_backward_count 86319   9.184%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.230410/  1.452941, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4243%\n",
      "layer   2  Sparsity: 78.5479%\n",
      "layer   3  Sparsity: 75.1790%\n",
      "total_backward_count 949630 real_backward_count 86826   9.143%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.223709/  1.441069, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4203%\n",
      "layer   2  Sparsity: 78.5285%\n",
      "layer   3  Sparsity: 74.9873%\n",
      "total_backward_count 959420 real_backward_count 87313   9.101%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.231313/  1.460927, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4100%\n",
      "layer   2  Sparsity: 78.3645%\n",
      "layer   3  Sparsity: 74.8707%\n",
      "total_backward_count 969210 real_backward_count 87839   9.063%\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.236903/  1.451448, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4213%\n",
      "layer   2  Sparsity: 78.2741%\n",
      "layer   3  Sparsity: 74.6641%\n",
      "total_backward_count 979000 real_backward_count 88288   9.018%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.234400/  1.458294, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4383%\n",
      "layer   2  Sparsity: 78.3566%\n",
      "layer   3  Sparsity: 74.8044%\n",
      "total_backward_count 988790 real_backward_count 88752   8.976%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.239457/  1.452553, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4295%\n",
      "layer   2  Sparsity: 78.2580%\n",
      "layer   3  Sparsity: 74.9623%\n",
      "total_backward_count 998580 real_backward_count 89252   8.938%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.227899/  1.455138, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4183%\n",
      "layer   2  Sparsity: 78.1516%\n",
      "layer   3  Sparsity: 74.9285%\n",
      "total_backward_count 1008370 real_backward_count 89702   8.896%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.212877/  1.443988, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4105%\n",
      "layer   2  Sparsity: 78.1226%\n",
      "layer   3  Sparsity: 75.0217%\n",
      "total_backward_count 1018160 real_backward_count 90164   8.856%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.229400/  1.437505, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4295%\n",
      "layer   2  Sparsity: 78.1666%\n",
      "layer   3  Sparsity: 74.7909%\n",
      "total_backward_count 1027950 real_backward_count 90636   8.817%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.225637/  1.423628, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4211%\n",
      "layer   2  Sparsity: 78.4032%\n",
      "layer   3  Sparsity: 74.7674%\n",
      "total_backward_count 1037740 real_backward_count 91093   8.778%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.214898/  1.452491, val:  82.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4122%\n",
      "layer   2  Sparsity: 78.2360%\n",
      "layer   3  Sparsity: 74.5783%\n",
      "total_backward_count 1047530 real_backward_count 91526   8.737%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.226098/  1.451598, val:  82.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4060%\n",
      "layer   2  Sparsity: 78.5022%\n",
      "layer   3  Sparsity: 74.5039%\n",
      "total_backward_count 1057320 real_backward_count 91985   8.700%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.227298/  1.425214, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4038%\n",
      "layer   2  Sparsity: 78.3479%\n",
      "layer   3  Sparsity: 74.8035%\n",
      "total_backward_count 1067110 real_backward_count 92442   8.663%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.210486/  1.430390, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4342%\n",
      "layer   2  Sparsity: 78.2008%\n",
      "layer   3  Sparsity: 74.8597%\n",
      "total_backward_count 1076900 real_backward_count 92865   8.623%\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "fc layer 3 self.abs_max_out: 680.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.195158/  1.410603, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4254%\n",
      "layer   2  Sparsity: 78.1009%\n",
      "layer   3  Sparsity: 74.8126%\n",
      "total_backward_count 1086690 real_backward_count 93322   8.588%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.188619/  1.420731, val:  78.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3925%\n",
      "layer   2  Sparsity: 78.2407%\n",
      "layer   3  Sparsity: 74.9138%\n",
      "total_backward_count 1096480 real_backward_count 93713   8.547%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.182405/  1.431286, val:  84.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4145%\n",
      "layer   2  Sparsity: 78.3281%\n",
      "layer   3  Sparsity: 75.0133%\n",
      "total_backward_count 1106270 real_backward_count 94165   8.512%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.189084/  1.419181, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4171%\n",
      "layer   2  Sparsity: 78.2312%\n",
      "layer   3  Sparsity: 75.0742%\n",
      "total_backward_count 1116060 real_backward_count 94596   8.476%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.189423/  1.447986, val:  81.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4533%\n",
      "layer   2  Sparsity: 78.2975%\n",
      "layer   3  Sparsity: 75.2850%\n",
      "total_backward_count 1125850 real_backward_count 95008   8.439%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.188912/  1.414043, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4231%\n",
      "layer   2  Sparsity: 78.1932%\n",
      "layer   3  Sparsity: 75.1736%\n",
      "total_backward_count 1135640 real_backward_count 95400   8.401%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.185446/  1.416091, val:  84.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4469%\n",
      "layer   2  Sparsity: 78.2587%\n",
      "layer   3  Sparsity: 75.0511%\n",
      "total_backward_count 1145430 real_backward_count 95764   8.361%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.182642/  1.407209, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4207%\n",
      "layer   2  Sparsity: 78.3366%\n",
      "layer   3  Sparsity: 75.0044%\n",
      "total_backward_count 1155220 real_backward_count 96155   8.324%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.187436/  1.400124, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4199%\n",
      "layer   2  Sparsity: 78.4484%\n",
      "layer   3  Sparsity: 74.7407%\n",
      "total_backward_count 1165010 real_backward_count 96557   8.288%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.177509/  1.415790, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4436%\n",
      "layer   2  Sparsity: 78.5718%\n",
      "layer   3  Sparsity: 74.4608%\n",
      "total_backward_count 1174800 real_backward_count 97016   8.258%\n",
      "fc layer 1 self.abs_max_out: 3184.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.168476/  1.409897, val:  83.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4155%\n",
      "layer   2  Sparsity: 78.4473%\n",
      "layer   3  Sparsity: 74.4505%\n",
      "total_backward_count 1184590 real_backward_count 97409   8.223%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.161862/  1.394622, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4225%\n",
      "layer   2  Sparsity: 78.3511%\n",
      "layer   3  Sparsity: 74.6838%\n",
      "total_backward_count 1194380 real_backward_count 97748   8.184%\n",
      "fc layer 1 self.abs_max_out: 3185.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.163589/  1.405905, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4406%\n",
      "layer   2  Sparsity: 78.3512%\n",
      "layer   3  Sparsity: 74.7017%\n",
      "total_backward_count 1204170 real_backward_count 98133   8.149%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.152837/  1.383595, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4446%\n",
      "layer   2  Sparsity: 78.3335%\n",
      "layer   3  Sparsity: 74.7598%\n",
      "total_backward_count 1213960 real_backward_count 98496   8.114%\n",
      "fc layer 1 self.abs_max_out: 3187.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.151008/  1.384311, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4179%\n",
      "layer   2  Sparsity: 78.3383%\n",
      "layer   3  Sparsity: 74.8848%\n",
      "total_backward_count 1223750 real_backward_count 98881   8.080%\n",
      "fc layer 1 self.abs_max_out: 3191.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.155482/  1.389147, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4157%\n",
      "layer   2  Sparsity: 78.3382%\n",
      "layer   3  Sparsity: 74.8626%\n",
      "total_backward_count 1233540 real_backward_count 99245   8.046%\n",
      "lif layer 1 self.abs_max_v: 5789.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.160329/  1.390234, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4251%\n",
      "layer   2  Sparsity: 78.1836%\n",
      "layer   3  Sparsity: 74.7511%\n",
      "total_backward_count 1243330 real_backward_count 99651   8.015%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.149129/  1.407148, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4053%\n",
      "layer   2  Sparsity: 78.2944%\n",
      "layer   3  Sparsity: 74.8184%\n",
      "total_backward_count 1253120 real_backward_count 100021   7.982%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.149240/  1.394842, val:  83.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4130%\n",
      "layer   2  Sparsity: 78.4502%\n",
      "layer   3  Sparsity: 74.6806%\n",
      "total_backward_count 1262910 real_backward_count 100385   7.949%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.149017/  1.379474, val:  85.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4444%\n",
      "layer   2  Sparsity: 78.4483%\n",
      "layer   3  Sparsity: 74.4559%\n",
      "total_backward_count 1272700 real_backward_count 100760   7.917%\n",
      "fc layer 1 self.abs_max_out: 3224.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.144009/  1.367461, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4358%\n",
      "layer   2  Sparsity: 78.3639%\n",
      "layer   3  Sparsity: 74.9036%\n",
      "total_backward_count 1282490 real_backward_count 101085   7.882%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.132370/  1.369033, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4411%\n",
      "layer   2  Sparsity: 78.4043%\n",
      "layer   3  Sparsity: 74.7137%\n",
      "total_backward_count 1292280 real_backward_count 101477   7.853%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.123224/  1.391635, val:  84.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4484%\n",
      "layer   2  Sparsity: 78.3239%\n",
      "layer   3  Sparsity: 74.8386%\n",
      "total_backward_count 1302070 real_backward_count 101826   7.820%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.129075/  1.395837, val:  81.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4045%\n",
      "layer   2  Sparsity: 78.3426%\n",
      "layer   3  Sparsity: 74.6054%\n",
      "total_backward_count 1311860 real_backward_count 102185   7.789%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.132168/  1.370859, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4363%\n",
      "layer   2  Sparsity: 78.3634%\n",
      "layer   3  Sparsity: 74.6520%\n",
      "total_backward_count 1321650 real_backward_count 102520   7.757%\n",
      "fc layer 1 self.abs_max_out: 3274.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.137286/  1.355244, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4087%\n",
      "layer   2  Sparsity: 78.3705%\n",
      "layer   3  Sparsity: 74.6666%\n",
      "total_backward_count 1331440 real_backward_count 102879   7.727%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.120568/  1.356479, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4312%\n",
      "layer   2  Sparsity: 78.3479%\n",
      "layer   3  Sparsity: 74.6523%\n",
      "total_backward_count 1341230 real_backward_count 103219   7.696%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.118653/  1.350919, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3950%\n",
      "layer   2  Sparsity: 78.4532%\n",
      "layer   3  Sparsity: 74.8490%\n",
      "total_backward_count 1351020 real_backward_count 103551   7.665%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.120757/  1.378023, val:  83.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4080%\n",
      "layer   2  Sparsity: 78.1199%\n",
      "layer   3  Sparsity: 74.6358%\n",
      "total_backward_count 1360810 real_backward_count 103894   7.635%\n",
      "fc layer 1 self.abs_max_out: 3320.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.128545/  1.354254, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4386%\n",
      "layer   2  Sparsity: 78.1991%\n",
      "layer   3  Sparsity: 74.9212%\n",
      "total_backward_count 1370600 real_backward_count 104214   7.604%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.115054/  1.350358, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4407%\n",
      "layer   2  Sparsity: 78.2424%\n",
      "layer   3  Sparsity: 74.8261%\n",
      "total_backward_count 1380390 real_backward_count 104561   7.575%\n",
      "fc layer 3 self.abs_max_out: 681.0\n",
      "lif layer 2 self.abs_max_v: 2439.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.100072/  1.329311, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3922%\n",
      "layer   2  Sparsity: 78.2000%\n",
      "layer   3  Sparsity: 74.7046%\n",
      "total_backward_count 1390180 real_backward_count 104933   7.548%\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "fc layer 2 self.abs_max_out: 1705.0\n",
      "fc layer 1 self.abs_max_out: 3384.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.117374/  1.342075, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4434%\n",
      "layer   2  Sparsity: 78.2547%\n",
      "layer   3  Sparsity: 74.5119%\n",
      "total_backward_count 1399970 real_backward_count 105276   7.520%\n",
      "lif layer 2 self.abs_max_v: 2495.5\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.111593/  1.370449, val:  84.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4624%\n",
      "layer   2  Sparsity: 78.2559%\n",
      "layer   3  Sparsity: 74.6293%\n",
      "total_backward_count 1409760 real_backward_count 105658   7.495%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.113216/  1.352218, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4423%\n",
      "layer   2  Sparsity: 78.1066%\n",
      "layer   3  Sparsity: 74.8012%\n",
      "total_backward_count 1419550 real_backward_count 106002   7.467%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.109314/  1.351601, val:  83.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3944%\n",
      "layer   2  Sparsity: 78.0504%\n",
      "layer   3  Sparsity: 74.6842%\n",
      "total_backward_count 1429340 real_backward_count 106345   7.440%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.116721/  1.354827, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4338%\n",
      "layer   2  Sparsity: 78.1411%\n",
      "layer   3  Sparsity: 74.7765%\n",
      "total_backward_count 1439130 real_backward_count 106681   7.413%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.099015/  1.345997, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4333%\n",
      "layer   2  Sparsity: 78.4195%\n",
      "layer   3  Sparsity: 75.0765%\n",
      "total_backward_count 1448920 real_backward_count 106982   7.384%\n",
      "fc layer 3 self.abs_max_out: 692.0\n",
      "fc layer 1 self.abs_max_out: 3403.0\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.099914/  1.327774, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4583%\n",
      "layer   2  Sparsity: 78.5705%\n",
      "layer   3  Sparsity: 75.2475%\n",
      "total_backward_count 1458710 real_backward_count 107319   7.357%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.092073/  1.330098, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4245%\n",
      "layer   2  Sparsity: 78.4606%\n",
      "layer   3  Sparsity: 74.8986%\n",
      "total_backward_count 1468500 real_backward_count 107631   7.329%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.076412/  1.333411, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4161%\n",
      "layer   2  Sparsity: 78.3278%\n",
      "layer   3  Sparsity: 74.9057%\n",
      "total_backward_count 1478290 real_backward_count 107909   7.300%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.094433/  1.342900, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3983%\n",
      "layer   2  Sparsity: 78.2533%\n",
      "layer   3  Sparsity: 75.1961%\n",
      "total_backward_count 1488080 real_backward_count 108208   7.272%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.087628/  1.342096, val:  83.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4212%\n",
      "layer   2  Sparsity: 78.2635%\n",
      "layer   3  Sparsity: 74.7374%\n",
      "total_backward_count 1497870 real_backward_count 108510   7.244%\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "fc layer 3 self.abs_max_out: 706.0\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.087590/  1.317089, val:  85.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3666%\n",
      "layer   2  Sparsity: 78.2407%\n",
      "layer   3  Sparsity: 74.8961%\n",
      "total_backward_count 1507660 real_backward_count 108805   7.217%\n",
      "fc layer 2 self.abs_max_out: 1725.0\n",
      "fc layer 2 self.abs_max_out: 1749.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.072641/  1.332941, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4073%\n",
      "layer   2  Sparsity: 78.3868%\n",
      "layer   3  Sparsity: 75.1693%\n",
      "total_backward_count 1517450 real_backward_count 109100   7.190%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.075302/  1.328296, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4079%\n",
      "layer   2  Sparsity: 78.4274%\n",
      "layer   3  Sparsity: 75.4658%\n",
      "total_backward_count 1527240 real_backward_count 109361   7.161%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.087274/  1.335084, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3996%\n",
      "layer   2  Sparsity: 78.4848%\n",
      "layer   3  Sparsity: 75.3343%\n",
      "total_backward_count 1537030 real_backward_count 109670   7.135%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.082462/  1.338905, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4295%\n",
      "layer   2  Sparsity: 78.6055%\n",
      "layer   3  Sparsity: 75.4460%\n",
      "total_backward_count 1546820 real_backward_count 109955   7.108%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.087466/  1.342816, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4072%\n",
      "layer   2  Sparsity: 78.5422%\n",
      "layer   3  Sparsity: 75.2711%\n",
      "total_backward_count 1556610 real_backward_count 110257   7.083%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.086116/  1.330750, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4244%\n",
      "layer   2  Sparsity: 78.5157%\n",
      "layer   3  Sparsity: 75.3332%\n",
      "total_backward_count 1566400 real_backward_count 110528   7.056%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.078391/  1.329657, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4170%\n",
      "layer   2  Sparsity: 78.6557%\n",
      "layer   3  Sparsity: 75.4026%\n",
      "total_backward_count 1576190 real_backward_count 110829   7.031%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.095618/  1.323830, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4047%\n",
      "layer   2  Sparsity: 78.7520%\n",
      "layer   3  Sparsity: 75.4626%\n",
      "total_backward_count 1585980 real_backward_count 111127   7.007%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.090813/  1.326014, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3967%\n",
      "layer   2  Sparsity: 78.4587%\n",
      "layer   3  Sparsity: 75.2378%\n",
      "total_backward_count 1595770 real_backward_count 111431   6.983%\n",
      "lif layer 1 self.abs_max_v: 5798.0\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.090252/  1.342224, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3874%\n",
      "layer   2  Sparsity: 78.3742%\n",
      "layer   3  Sparsity: 75.1612%\n",
      "total_backward_count 1605560 real_backward_count 111746   6.960%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.083655/  1.331716, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3977%\n",
      "layer   2  Sparsity: 78.3006%\n",
      "layer   3  Sparsity: 75.5147%\n",
      "total_backward_count 1615350 real_backward_count 112023   6.935%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.082538/  1.330673, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4131%\n",
      "layer   2  Sparsity: 78.3831%\n",
      "layer   3  Sparsity: 75.7134%\n",
      "total_backward_count 1625140 real_backward_count 112295   6.910%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.084553/  1.334518, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4187%\n",
      "layer   2  Sparsity: 78.4747%\n",
      "layer   3  Sparsity: 75.4833%\n",
      "total_backward_count 1634930 real_backward_count 112560   6.885%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.077453/  1.332413, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4348%\n",
      "layer   2  Sparsity: 78.4640%\n",
      "layer   3  Sparsity: 75.4281%\n",
      "total_backward_count 1644720 real_backward_count 112815   6.859%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.082716/  1.340011, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4141%\n",
      "layer   2  Sparsity: 78.2995%\n",
      "layer   3  Sparsity: 75.3098%\n",
      "total_backward_count 1654510 real_backward_count 113082   6.835%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.085034/  1.319647, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4240%\n",
      "layer   2  Sparsity: 78.3645%\n",
      "layer   3  Sparsity: 75.4268%\n",
      "total_backward_count 1664300 real_backward_count 113359   6.811%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.089735/  1.329924, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.3953%\n",
      "layer   2  Sparsity: 78.4779%\n",
      "layer   3  Sparsity: 75.3702%\n",
      "total_backward_count 1674090 real_backward_count 113626   6.787%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.066202/  1.323381, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 65.32 seconds, 1.09 minutes\n",
      "layer   1  Sparsity: 91.3960%\n",
      "layer   2  Sparsity: 78.4030%\n",
      "layer   3  Sparsity: 75.6857%\n",
      "total_backward_count 1683880 real_backward_count 113896   6.764%\n",
      "lif layer 1 self.abs_max_v: 5903.5\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.068254/  1.322482, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.14 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.4390%\n",
      "layer   2  Sparsity: 78.2030%\n",
      "layer   3  Sparsity: 75.8312%\n",
      "total_backward_count 1693670 real_backward_count 114138   6.739%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.070758/  1.308999, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.09 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.4398%\n",
      "layer   2  Sparsity: 78.2002%\n",
      "layer   3  Sparsity: 75.8836%\n",
      "total_backward_count 1703460 real_backward_count 114387   6.715%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.067751/  1.310758, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4384%\n",
      "layer   2  Sparsity: 78.4140%\n",
      "layer   3  Sparsity: 75.6975%\n",
      "total_backward_count 1713250 real_backward_count 114627   6.691%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.069020/  1.307326, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3849%\n",
      "layer   2  Sparsity: 78.4783%\n",
      "layer   3  Sparsity: 76.0240%\n",
      "total_backward_count 1723040 real_backward_count 114862   6.666%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.054381/  1.301983, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4237%\n",
      "layer   2  Sparsity: 78.5394%\n",
      "layer   3  Sparsity: 76.1837%\n",
      "total_backward_count 1732830 real_backward_count 115093   6.642%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.059433/  1.302675, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4470%\n",
      "layer   2  Sparsity: 78.5458%\n",
      "layer   3  Sparsity: 76.0359%\n",
      "total_backward_count 1742620 real_backward_count 115324   6.618%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.048962/  1.307898, val:  80.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4380%\n",
      "layer   2  Sparsity: 78.4917%\n",
      "layer   3  Sparsity: 75.7979%\n",
      "total_backward_count 1752410 real_backward_count 115559   6.594%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.056989/  1.315018, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4220%\n",
      "layer   2  Sparsity: 78.5639%\n",
      "layer   3  Sparsity: 75.6612%\n",
      "total_backward_count 1762200 real_backward_count 115788   6.571%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.051634/  1.302249, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4345%\n",
      "layer   2  Sparsity: 78.4975%\n",
      "layer   3  Sparsity: 75.6451%\n",
      "total_backward_count 1771990 real_backward_count 116029   6.548%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.041387/  1.289966, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3929%\n",
      "layer   2  Sparsity: 78.6166%\n",
      "layer   3  Sparsity: 75.7023%\n",
      "total_backward_count 1781780 real_backward_count 116296   6.527%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.037217/  1.281807, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4145%\n",
      "layer   2  Sparsity: 78.6333%\n",
      "layer   3  Sparsity: 75.8616%\n",
      "total_backward_count 1791570 real_backward_count 116547   6.505%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.044363/  1.282981, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.3950%\n",
      "layer   2  Sparsity: 78.4441%\n",
      "layer   3  Sparsity: 75.8922%\n",
      "total_backward_count 1801360 real_backward_count 116778   6.483%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.036610/  1.299594, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4251%\n",
      "layer   2  Sparsity: 78.5868%\n",
      "layer   3  Sparsity: 75.6408%\n",
      "total_backward_count 1811150 real_backward_count 117006   6.460%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.037484/  1.286155, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4008%\n",
      "layer   2  Sparsity: 78.5740%\n",
      "layer   3  Sparsity: 75.8046%\n",
      "total_backward_count 1820940 real_backward_count 117287   6.441%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.046485/  1.300625, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4282%\n",
      "layer   2  Sparsity: 78.6421%\n",
      "layer   3  Sparsity: 75.9846%\n",
      "total_backward_count 1830730 real_backward_count 117523   6.419%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.042006/  1.297288, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4104%\n",
      "layer   2  Sparsity: 78.5535%\n",
      "layer   3  Sparsity: 76.2303%\n",
      "total_backward_count 1840520 real_backward_count 117761   6.398%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.052324/  1.291147, val:  85.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4139%\n",
      "layer   2  Sparsity: 78.5654%\n",
      "layer   3  Sparsity: 76.3996%\n",
      "total_backward_count 1850310 real_backward_count 117982   6.376%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.044940/  1.302798, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4280%\n",
      "layer   2  Sparsity: 78.3928%\n",
      "layer   3  Sparsity: 75.8215%\n",
      "total_backward_count 1860100 real_backward_count 118211   6.355%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.038946/  1.289225, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4231%\n",
      "layer   2  Sparsity: 78.4630%\n",
      "layer   3  Sparsity: 75.8027%\n",
      "total_backward_count 1869890 real_backward_count 118464   6.335%\n",
      "fc layer 3 self.abs_max_out: 740.0\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.040169/  1.288359, val:  84.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4021%\n",
      "layer   2  Sparsity: 78.2929%\n",
      "layer   3  Sparsity: 75.9409%\n",
      "total_backward_count 1879680 real_backward_count 118713   6.316%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.036349/  1.277898, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4051%\n",
      "layer   2  Sparsity: 78.4281%\n",
      "layer   3  Sparsity: 76.1306%\n",
      "total_backward_count 1889470 real_backward_count 118943   6.295%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.030520/  1.259581, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4207%\n",
      "layer   2  Sparsity: 78.4629%\n",
      "layer   3  Sparsity: 76.3642%\n",
      "total_backward_count 1899260 real_backward_count 119183   6.275%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.019177/  1.280460, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.4109%\n",
      "layer   2  Sparsity: 78.4501%\n",
      "layer   3  Sparsity: 76.1121%\n",
      "total_backward_count 1909050 real_backward_count 119396   6.254%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.026360/  1.290042, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.23 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.4405%\n",
      "layer   2  Sparsity: 78.4811%\n",
      "layer   3  Sparsity: 76.0132%\n",
      "total_backward_count 1918840 real_backward_count 119594   6.233%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.034642/  1.275926, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.69 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.4300%\n",
      "layer   2  Sparsity: 78.4971%\n",
      "layer   3  Sparsity: 76.1168%\n",
      "total_backward_count 1928630 real_backward_count 119795   6.211%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.031781/  1.282707, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.84 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.3972%\n",
      "layer   2  Sparsity: 78.4105%\n",
      "layer   3  Sparsity: 76.2784%\n",
      "total_backward_count 1938420 real_backward_count 119979   6.190%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.024720/  1.290001, val:  84.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.55 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.4313%\n",
      "layer   2  Sparsity: 78.4234%\n",
      "layer   3  Sparsity: 76.4037%\n",
      "total_backward_count 1948210 real_backward_count 120216   6.171%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.028276/  1.301990, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.19 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.4178%\n",
      "layer   2  Sparsity: 78.5035%\n",
      "layer   3  Sparsity: 76.6188%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69539da21a6a4f6a8f21146ca1f7acd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.02828</td></tr><tr><td>val_acc_best</td><td>0.92083</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>1.30199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-331</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksd11usi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksd11usi</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_150713-ksd11usi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x8uoseeu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251127_192929-x8uoseeu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8uoseeu' target=\"_blank\">floral-sweep-334</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8uoseeu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8uoseeu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251127_192937_974', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 3, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 893.0\n",
      "lif layer 1 self.abs_max_v: 893.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1071.0\n",
      "lif layer 1 self.abs_max_v: 1376.5\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "fc layer 1 self.abs_max_out: 1129.0\n",
      "lif layer 1 self.abs_max_v: 1790.0\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 342.0\n",
      "fc layer 2 self.abs_max_out: 402.0\n",
      "lif layer 2 self.abs_max_v: 435.5\n",
      "fc layer 1 self.abs_max_out: 1249.0\n",
      "lif layer 2 self.abs_max_v: 492.5\n",
      "fc layer 1 self.abs_max_out: 2133.0\n",
      "lif layer 1 self.abs_max_v: 2133.0\n",
      "fc layer 2 self.abs_max_out: 546.0\n",
      "lif layer 2 self.abs_max_v: 729.0\n",
      "fc layer 1 self.abs_max_out: 3298.0\n",
      "lif layer 1 self.abs_max_v: 3298.0\n",
      "fc layer 2 self.abs_max_out: 637.0\n",
      "lif layer 2 self.abs_max_v: 875.0\n",
      "lif layer 2 self.abs_max_v: 1008.5\n",
      "fc layer 1 self.abs_max_out: 3873.0\n",
      "lif layer 1 self.abs_max_v: 3873.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "lif layer 2 self.abs_max_v: 1169.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 2 self.abs_max_out: 938.0\n",
      "fc layer 2 self.abs_max_out: 946.0\n",
      "lif layer 2 self.abs_max_v: 1174.0\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "lif layer 2 self.abs_max_v: 1291.5\n",
      "fc layer 2 self.abs_max_out: 1250.0\n",
      "lif layer 2 self.abs_max_v: 1605.5\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "lif layer 2 self.abs_max_v: 1666.0\n",
      "lif layer 2 self.abs_max_v: 1844.5\n",
      "fc layer 1 self.abs_max_out: 4713.0\n",
      "lif layer 1 self.abs_max_v: 4713.0\n",
      "lif layer 2 self.abs_max_v: 1909.0\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "lif layer 2 self.abs_max_v: 1910.5\n",
      "fc layer 1 self.abs_max_out: 4938.0\n",
      "lif layer 1 self.abs_max_v: 4938.0\n",
      "lif layer 2 self.abs_max_v: 1949.5\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "lif layer 2 self.abs_max_v: 2091.5\n",
      "fc layer 2 self.abs_max_out: 1440.0\n",
      "lif layer 2 self.abs_max_v: 2190.0\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "fc layer 2 self.abs_max_out: 1527.0\n",
      "fc layer 1 self.abs_max_out: 5718.0\n",
      "lif layer 1 self.abs_max_v: 6088.0\n",
      "fc layer 2 self.abs_max_out: 1596.0\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "lif layer 2 self.abs_max_v: 2278.0\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "fc layer 2 self.abs_max_out: 2038.0\n",
      "lif layer 2 self.abs_max_v: 2401.0\n",
      "fc layer 2 self.abs_max_out: 2063.0\n",
      "lif layer 2 self.abs_max_v: 2408.0\n",
      "fc layer 1 self.abs_max_out: 6324.0\n",
      "lif layer 1 self.abs_max_v: 6324.0\n",
      "fc layer 2 self.abs_max_out: 2212.0\n",
      "fc layer 2 self.abs_max_out: 2274.0\n",
      "fc layer 2 self.abs_max_out: 2316.0\n",
      "lif layer 2 self.abs_max_v: 2437.0\n",
      "lif layer 2 self.abs_max_v: 2601.0\n",
      "lif layer 2 self.abs_max_v: 2857.0\n",
      "lif layer 1 self.abs_max_v: 8060.0\n",
      "fc layer 2 self.abs_max_out: 2339.0\n",
      "lif layer 2 self.abs_max_v: 3031.5\n",
      "fc layer 2 self.abs_max_out: 2485.0\n",
      "fc layer 2 self.abs_max_out: 2716.0\n",
      "fc layer 2 self.abs_max_out: 2818.0\n",
      "lif layer 2 self.abs_max_v: 3168.0\n",
      "fc layer 3 self.abs_max_out: 508.0\n",
      "lif layer 2 self.abs_max_v: 3183.5\n",
      "lif layer 2 self.abs_max_v: 3276.0\n",
      "fc layer 2 self.abs_max_out: 2822.0\n",
      "fc layer 2 self.abs_max_out: 2874.0\n",
      "fc layer 2 self.abs_max_out: 2995.0\n",
      "fc layer 1 self.abs_max_out: 6635.0\n",
      "fc layer 2 self.abs_max_out: 3097.0\n",
      "fc layer 1 self.abs_max_out: 7637.0\n",
      "fc layer 1 self.abs_max_out: 8173.0\n",
      "lif layer 1 self.abs_max_v: 8499.0\n",
      "fc layer 1 self.abs_max_out: 8213.0\n",
      "fc layer 1 self.abs_max_out: 8760.0\n",
      "lif layer 1 self.abs_max_v: 8760.0\n",
      "lif layer 1 self.abs_max_v: 8888.5\n",
      "lif layer 1 self.abs_max_v: 9697.0\n",
      "fc layer 2 self.abs_max_out: 3125.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "lif layer 2 self.abs_max_v: 3389.0\n",
      "fc layer 2 self.abs_max_out: 3295.0\n",
      "fc layer 2 self.abs_max_out: 3493.0\n",
      "lif layer 2 self.abs_max_v: 3493.0\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "lif layer 2 self.abs_max_v: 3501.5\n",
      "lif layer 2 self.abs_max_v: 3685.5\n",
      "lif layer 1 self.abs_max_v: 9911.0\n",
      "lif layer 1 self.abs_max_v: 10245.0\n",
      "lif layer 1 self.abs_max_v: 10654.5\n",
      "lif layer 1 self.abs_max_v: 10869.5\n",
      "fc layer 1 self.abs_max_out: 8950.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "lif layer 2 self.abs_max_v: 3790.5\n",
      "lif layer 2 self.abs_max_v: 3952.5\n",
      "lif layer 2 self.abs_max_v: 3987.5\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "fc layer 3 self.abs_max_out: 686.0\n",
      "fc layer 1 self.abs_max_out: 9036.0\n",
      "fc layer 1 self.abs_max_out: 9283.0\n",
      "lif layer 1 self.abs_max_v: 11608.5\n",
      "lif layer 1 self.abs_max_v: 11819.5\n",
      "fc layer 1 self.abs_max_out: 9523.0\n",
      "fc layer 2 self.abs_max_out: 3516.0\n",
      "lif layer 1 self.abs_max_v: 12883.5\n",
      "fc layer 2 self.abs_max_out: 3564.0\n",
      "fc layer 2 self.abs_max_out: 3572.0\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "fc layer 2 self.abs_max_out: 3606.0\n",
      "lif layer 2 self.abs_max_v: 4135.5\n",
      "lif layer 2 self.abs_max_v: 4302.5\n",
      "lif layer 2 self.abs_max_v: 4620.5\n",
      "lif layer 2 self.abs_max_v: 4766.0\n",
      "lif layer 2 self.abs_max_v: 4877.0\n",
      "fc layer 1 self.abs_max_out: 9775.0\n",
      "lif layer 1 self.abs_max_v: 13326.0\n",
      "fc layer 1 self.abs_max_out: 10040.0\n",
      "lif layer 1 self.abs_max_v: 15324.0\n",
      "fc layer 1 self.abs_max_out: 10183.0\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "fc layer 1 self.abs_max_out: 10510.0\n",
      "fc layer 1 self.abs_max_out: 11447.0\n",
      "lif layer 1 self.abs_max_v: 15418.5\n",
      "lif layer 1 self.abs_max_v: 16018.5\n",
      "fc layer 1 self.abs_max_out: 11463.0\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "fc layer 2 self.abs_max_out: 3802.0\n",
      "fc layer 1 self.abs_max_out: 11470.0\n",
      "fc layer 1 self.abs_max_out: 11670.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  2.094491/  2.173147, val:  29.17%, val_best:  29.17%, tr:  93.56%, tr_best:  93.56%, epoch time: 87.03 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.1070%\n",
      "layer   2  Sparsity: 85.1194%\n",
      "layer   3  Sparsity: 93.7377%\n",
      "total_backward_count 9790 real_backward_count 2703  27.610%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 16397.0\n",
      "fc layer 2 self.abs_max_out: 3853.0\n",
      "lif layer 1 self.abs_max_v: 16627.5\n",
      "lif layer 1 self.abs_max_v: 16884.0\n",
      "lif layer 1 self.abs_max_v: 16916.0\n",
      "lif layer 1 self.abs_max_v: 17482.5\n",
      "lif layer 1 self.abs_max_v: 18255.5\n",
      "lif layer 1 self.abs_max_v: 18461.5\n",
      "lif layer 1 self.abs_max_v: 18620.0\n",
      "lif layer 1 self.abs_max_v: 19497.0\n",
      "fc layer 1 self.abs_max_out: 11945.0\n",
      "fc layer 1 self.abs_max_out: 12714.0\n",
      "lif layer 2 self.abs_max_v: 5006.0\n",
      "lif layer 2 self.abs_max_v: 5062.5\n",
      "lif layer 1 self.abs_max_v: 19912.5\n",
      "lif layer 1 self.abs_max_v: 20840.5\n",
      "lif layer 2 self.abs_max_v: 5092.0\n",
      "lif layer 2 self.abs_max_v: 5430.0\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "lif layer 2 self.abs_max_v: 5633.0\n",
      "fc layer 2 self.abs_max_out: 3932.0\n",
      "fc layer 2 self.abs_max_out: 3934.0\n",
      "fc layer 1 self.abs_max_out: 12847.0\n",
      "fc layer 1 self.abs_max_out: 13589.0\n",
      "lif layer 1 self.abs_max_v: 22148.0\n",
      "fc layer 2 self.abs_max_out: 3963.0\n",
      "fc layer 1 self.abs_max_out: 13638.0\n",
      "fc layer 1 self.abs_max_out: 14758.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  2.071749/  2.145445, val:  42.92%, val_best:  42.92%, tr:  96.94%, tr_best:  96.94%, epoch time: 86.71 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.1035%\n",
      "layer   2  Sparsity: 86.0567%\n",
      "layer   3  Sparsity: 92.9015%\n",
      "total_backward_count 19580 real_backward_count 4947  25.266%\n",
      "lif layer 2 self.abs_max_v: 5643.5\n",
      "lif layer 2 self.abs_max_v: 5772.0\n",
      "fc layer 2 self.abs_max_out: 4149.0\n",
      "lif layer 1 self.abs_max_v: 22159.0\n",
      "fc layer 3 self.abs_max_out: 972.0\n",
      "fc layer 3 self.abs_max_out: 1028.0\n",
      "fc layer 2 self.abs_max_out: 4197.0\n",
      "fc layer 2 self.abs_max_out: 4246.0\n",
      "lif layer 1 self.abs_max_v: 23193.5\n",
      "lif layer 1 self.abs_max_v: 23321.0\n",
      "lif layer 2 self.abs_max_v: 5784.0\n",
      "lif layer 2 self.abs_max_v: 6156.0\n",
      "lif layer 2 self.abs_max_v: 6542.0\n",
      "fc layer 3 self.abs_max_out: 1055.0\n",
      "fc layer 1 self.abs_max_out: 14883.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  2.030355/  2.153629, val:  32.50%, val_best:  42.92%, tr:  97.14%, tr_best:  97.14%, epoch time: 86.66 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.1305%\n",
      "layer   2  Sparsity: 84.5551%\n",
      "layer   3  Sparsity: 92.0426%\n",
      "total_backward_count 29370 real_backward_count 7159  24.375%\n",
      "lif layer 1 self.abs_max_v: 24199.0\n",
      "fc layer 2 self.abs_max_out: 4327.0\n",
      "fc layer 2 self.abs_max_out: 4354.0\n",
      "lif layer 1 self.abs_max_v: 24394.5\n",
      "lif layer 1 self.abs_max_v: 25353.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  2.043149/  2.164488, val:  35.83%, val_best:  42.92%, tr:  96.32%, tr_best:  97.14%, epoch time: 87.18 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.0889%\n",
      "layer   2  Sparsity: 84.7801%\n",
      "layer   3  Sparsity: 92.2572%\n",
      "total_backward_count 39160 real_backward_count 9311  23.777%\n",
      "lif layer 1 self.abs_max_v: 25453.5\n",
      "lif layer 1 self.abs_max_v: 26722.0\n",
      "lif layer 1 self.abs_max_v: 27251.5\n",
      "lif layer 1 self.abs_max_v: 27671.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  2.050475/  2.150822, val:  39.17%, val_best:  42.92%, tr:  96.42%, tr_best:  97.14%, epoch time: 87.15 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.0596%\n",
      "layer   2  Sparsity: 85.9808%\n",
      "layer   3  Sparsity: 92.3553%\n",
      "total_backward_count 48950 real_backward_count 11580  23.657%\n",
      "fc layer 1 self.abs_max_out: 14951.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  2.062706/  2.179568, val:  47.92%, val_best:  47.92%, tr:  93.97%, tr_best:  97.14%, epoch time: 87.28 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.1070%\n",
      "layer   2  Sparsity: 87.1969%\n",
      "layer   3  Sparsity: 93.0716%\n",
      "total_backward_count 58740 real_backward_count 13994  23.824%\n",
      "fc layer 1 self.abs_max_out: 15020.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  2.052143/  2.177342, val:  37.50%, val_best:  47.92%, tr:  95.20%, tr_best:  97.14%, epoch time: 87.14 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.1050%\n",
      "layer   2  Sparsity: 87.7071%\n",
      "layer   3  Sparsity: 92.7994%\n",
      "total_backward_count 68530 real_backward_count 16410  23.946%\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  2.035016/  2.127311, val:  50.00%, val_best:  50.00%, tr:  96.12%, tr_best:  97.14%, epoch time: 87.60 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 88.0964%\n",
      "layer   2  Sparsity: 87.2350%\n",
      "layer   3  Sparsity: 92.1260%\n",
      "total_backward_count 78320 real_backward_count 18756  23.948%\n",
      "fc layer 1 self.abs_max_out: 16291.0\n",
      "lif layer 1 self.abs_max_v: 27946.5\n",
      "lif layer 1 self.abs_max_v: 28498.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  2.054612/  2.157088, val:  37.50%, val_best:  50.00%, tr:  95.30%, tr_best:  97.14%, epoch time: 86.55 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.1135%\n",
      "layer   2  Sparsity: 87.3896%\n",
      "layer   3  Sparsity: 92.7559%\n",
      "total_backward_count 88110 real_backward_count 21200  24.061%\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  2.043816/  2.157861, val:  42.50%, val_best:  50.00%, tr:  95.40%, tr_best:  97.14%, epoch time: 86.82 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.1027%\n",
      "layer   2  Sparsity: 87.5642%\n",
      "layer   3  Sparsity: 91.9778%\n",
      "total_backward_count 97900 real_backward_count 23609  24.115%\n",
      "fc layer 3 self.abs_max_out: 1133.0\n",
      "lif layer 1 self.abs_max_v: 28517.0\n",
      "lif layer 1 self.abs_max_v: 30378.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.984565/  2.131026, val:  30.00%, val_best:  50.00%, tr:  97.55%, tr_best:  97.55%, epoch time: 86.97 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.0750%\n",
      "layer   2  Sparsity: 86.9038%\n",
      "layer   3  Sparsity: 89.9525%\n",
      "total_backward_count 107690 real_backward_count 25841  23.996%\n",
      "fc layer 3 self.abs_max_out: 1168.0\n",
      "fc layer 3 self.abs_max_out: 1172.0\n",
      "fc layer 3 self.abs_max_out: 1193.0\n",
      "fc layer 3 self.abs_max_out: 1217.0\n",
      "fc layer 2 self.abs_max_out: 4515.0\n",
      "fc layer 3 self.abs_max_out: 1220.0\n",
      "fc layer 3 self.abs_max_out: 1239.0\n",
      "fc layer 3 self.abs_max_out: 1287.0\n",
      "fc layer 1 self.abs_max_out: 16701.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.959809/  2.111763, val:  48.33%, val_best:  50.00%, tr:  96.73%, tr_best:  97.55%, epoch time: 87.23 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.0510%\n",
      "layer   2  Sparsity: 86.9943%\n",
      "layer   3  Sparsity: 89.6144%\n",
      "total_backward_count 117480 real_backward_count 27994  23.829%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.973474/  2.167394, val:  27.08%, val_best:  50.00%, tr:  96.63%, tr_best:  97.55%, epoch time: 87.71 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 88.0774%\n",
      "layer   2  Sparsity: 86.8862%\n",
      "layer   3  Sparsity: 90.7253%\n",
      "total_backward_count 127270 real_backward_count 30181  23.714%\n",
      "fc layer 1 self.abs_max_out: 16768.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.990663/  2.156369, val:  35.00%, val_best:  50.00%, tr:  97.34%, tr_best:  97.55%, epoch time: 86.75 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.0477%\n",
      "layer   2  Sparsity: 86.8360%\n",
      "layer   3  Sparsity: 90.3877%\n",
      "total_backward_count 137060 real_backward_count 32360  23.610%\n",
      "fc layer 2 self.abs_max_out: 4557.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.970641/  2.113571, val:  35.42%, val_best:  50.00%, tr:  98.37%, tr_best:  98.37%, epoch time: 86.70 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0424%\n",
      "layer   2  Sparsity: 86.6926%\n",
      "layer   3  Sparsity: 89.4917%\n",
      "total_backward_count 146850 real_backward_count 34348  23.390%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.993083/  2.133858, val:  42.50%, val_best:  50.00%, tr:  96.22%, tr_best:  98.37%, epoch time: 87.14 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.0804%\n",
      "layer   2  Sparsity: 87.2477%\n",
      "layer   3  Sparsity: 90.7446%\n",
      "total_backward_count 156640 real_backward_count 36675  23.414%\n",
      "fc layer 1 self.abs_max_out: 17668.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.999074/  2.135826, val:  45.42%, val_best:  50.00%, tr:  97.04%, tr_best:  98.37%, epoch time: 87.43 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 88.0933%\n",
      "layer   2  Sparsity: 87.5810%\n",
      "layer   3  Sparsity: 91.1072%\n",
      "total_backward_count 166430 real_backward_count 38983  23.423%\n",
      "fc layer 1 self.abs_max_out: 18080.0\n",
      "lif layer 1 self.abs_max_v: 30724.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.986019/  2.122205, val:  34.17%, val_best:  50.00%, tr:  96.63%, tr_best:  98.37%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.1244%\n",
      "layer   2  Sparsity: 86.9500%\n",
      "layer   3  Sparsity: 90.3817%\n",
      "total_backward_count 176220 real_backward_count 41244  23.405%\n",
      "fc layer 2 self.abs_max_out: 4598.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.975520/  2.119482, val:  42.50%, val_best:  50.00%, tr:  97.24%, tr_best:  98.37%, epoch time: 86.81 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.0813%\n",
      "layer   2  Sparsity: 86.5489%\n",
      "layer   3  Sparsity: 89.7326%\n",
      "total_backward_count 186010 real_backward_count 43558  23.417%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.968868/  2.150268, val:  40.83%, val_best:  50.00%, tr:  96.83%, tr_best:  98.37%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0615%\n",
      "layer   2  Sparsity: 87.5503%\n",
      "layer   3  Sparsity: 89.8403%\n",
      "total_backward_count 195800 real_backward_count 45815  23.399%\n",
      "fc layer 3 self.abs_max_out: 1542.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.955936/  2.112507, val:  35.83%, val_best:  50.00%, tr:  96.73%, tr_best:  98.37%, epoch time: 84.05 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 88.1100%\n",
      "layer   2  Sparsity: 87.7875%\n",
      "layer   3  Sparsity: 89.7123%\n",
      "total_backward_count 205590 real_backward_count 48102  23.397%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.968439/  2.172345, val:  30.42%, val_best:  50.00%, tr:  97.45%, tr_best:  98.37%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0919%\n",
      "layer   2  Sparsity: 87.6350%\n",
      "layer   3  Sparsity: 90.3456%\n",
      "total_backward_count 215380 real_backward_count 50364  23.384%\n",
      "lif layer 2 self.abs_max_v: 6613.5\n",
      "lif layer 2 self.abs_max_v: 6665.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.962875/  2.095802, val:  41.25%, val_best:  50.00%, tr:  97.04%, tr_best:  98.37%, epoch time: 86.68 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0830%\n",
      "layer   2  Sparsity: 87.2897%\n",
      "layer   3  Sparsity: 88.9556%\n",
      "total_backward_count 225170 real_backward_count 52622  23.370%\n",
      "fc layer 2 self.abs_max_out: 4626.0\n",
      "lif layer 2 self.abs_max_v: 6848.5\n",
      "lif layer 2 self.abs_max_v: 6896.5\n",
      "fc layer 2 self.abs_max_out: 4761.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.861926/  2.049744, val:  47.92%, val_best:  50.00%, tr:  98.47%, tr_best:  98.47%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0835%\n",
      "layer   2  Sparsity: 86.3540%\n",
      "layer   3  Sparsity: 86.5758%\n",
      "total_backward_count 234960 real_backward_count 54620  23.247%\n",
      "fc layer 2 self.abs_max_out: 4791.0\n",
      "fc layer 2 self.abs_max_out: 4876.0\n",
      "lif layer 1 self.abs_max_v: 31094.0\n",
      "lif layer 1 self.abs_max_v: 31465.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.884548/  2.087398, val:  47.92%, val_best:  50.00%, tr:  97.04%, tr_best:  98.47%, epoch time: 84.58 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1036%\n",
      "layer   2  Sparsity: 86.8209%\n",
      "layer   3  Sparsity: 88.0901%\n",
      "total_backward_count 244750 real_backward_count 56660  23.150%\n",
      "lif layer 2 self.abs_max_v: 6984.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.899547/  2.061414, val:  53.75%, val_best:  53.75%, tr:  97.75%, tr_best:  98.47%, epoch time: 86.93 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.0686%\n",
      "layer   2  Sparsity: 86.8052%\n",
      "layer   3  Sparsity: 88.4430%\n",
      "total_backward_count 254540 real_backward_count 58777  23.091%\n",
      "fc layer 3 self.abs_max_out: 1554.0\n",
      "fc layer 2 self.abs_max_out: 4890.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.887557/  2.025034, val:  45.83%, val_best:  53.75%, tr:  98.26%, tr_best:  98.47%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1054%\n",
      "layer   2  Sparsity: 85.9248%\n",
      "layer   3  Sparsity: 87.9606%\n",
      "total_backward_count 264330 real_backward_count 60762  22.987%\n",
      "fc layer 2 self.abs_max_out: 4896.0\n",
      "fc layer 2 self.abs_max_out: 4912.0\n",
      "fc layer 2 self.abs_max_out: 5007.0\n",
      "fc layer 2 self.abs_max_out: 5364.0\n",
      "lif layer 1 self.abs_max_v: 32138.5\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.871307/  2.080541, val:  46.25%, val_best:  53.75%, tr:  98.37%, tr_best:  98.47%, epoch time: 84.77 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0791%\n",
      "layer   2  Sparsity: 86.2701%\n",
      "layer   3  Sparsity: 87.5114%\n",
      "total_backward_count 274120 real_backward_count 62756  22.894%\n",
      "fc layer 3 self.abs_max_out: 1611.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.913033/  2.073478, val:  35.00%, val_best:  53.75%, tr:  96.73%, tr_best:  98.47%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1019%\n",
      "layer   2  Sparsity: 86.2505%\n",
      "layer   3  Sparsity: 88.5177%\n",
      "total_backward_count 283910 real_backward_count 64835  22.836%\n",
      "fc layer 3 self.abs_max_out: 1635.0\n",
      "fc layer 3 self.abs_max_out: 1863.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.881257/  2.035619, val:  48.33%, val_best:  53.75%, tr:  98.47%, tr_best:  98.47%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0846%\n",
      "layer   2  Sparsity: 85.8989%\n",
      "layer   3  Sparsity: 87.5012%\n",
      "total_backward_count 293700 real_backward_count 66897  22.777%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.872953/  2.086464, val:  48.33%, val_best:  53.75%, tr:  99.08%, tr_best:  99.08%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0521%\n",
      "layer   2  Sparsity: 85.8485%\n",
      "layer   3  Sparsity: 88.2458%\n",
      "total_backward_count 303490 real_backward_count 68914  22.707%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.941406/  2.064922, val:  43.75%, val_best:  53.75%, tr:  97.96%, tr_best:  99.08%, epoch time: 84.56 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0614%\n",
      "layer   2  Sparsity: 86.9578%\n",
      "layer   3  Sparsity: 89.8799%\n",
      "total_backward_count 313280 real_backward_count 71013  22.668%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.892115/  2.099355, val:  39.17%, val_best:  53.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 84.70 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0740%\n",
      "layer   2  Sparsity: 86.9518%\n",
      "layer   3  Sparsity: 87.9883%\n",
      "total_backward_count 323070 real_backward_count 73074  22.619%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.909521/  2.106016, val:  33.33%, val_best:  53.75%, tr:  97.55%, tr_best:  99.08%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1101%\n",
      "layer   2  Sparsity: 86.8006%\n",
      "layer   3  Sparsity: 89.0177%\n",
      "total_backward_count 332860 real_backward_count 75205  22.594%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.886066/  2.056151, val:  40.00%, val_best:  53.75%, tr:  97.85%, tr_best:  99.08%, epoch time: 84.62 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0576%\n",
      "layer   2  Sparsity: 86.7879%\n",
      "layer   3  Sparsity: 88.7856%\n",
      "total_backward_count 342650 real_backward_count 77253  22.546%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.841506/  2.098598, val:  37.92%, val_best:  53.75%, tr:  98.47%, tr_best:  99.08%, epoch time: 83.98 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 88.0853%\n",
      "layer   2  Sparsity: 86.0908%\n",
      "layer   3  Sparsity: 87.3735%\n",
      "total_backward_count 352440 real_backward_count 79211  22.475%\n",
      "lif layer 2 self.abs_max_v: 7138.0\n",
      "lif layer 2 self.abs_max_v: 7169.5\n",
      "lif layer 2 self.abs_max_v: 7216.0\n",
      "lif layer 2 self.abs_max_v: 7360.5\n",
      "lif layer 2 self.abs_max_v: 7497.5\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.866486/  2.059061, val:  52.08%, val_best:  53.75%, tr:  97.96%, tr_best:  99.08%, epoch time: 85.10 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0747%\n",
      "layer   2  Sparsity: 87.2007%\n",
      "layer   3  Sparsity: 87.7100%\n",
      "total_backward_count 362230 real_backward_count 81305  22.446%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.908816/  2.121595, val:  31.67%, val_best:  53.75%, tr:  97.55%, tr_best:  99.08%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1014%\n",
      "layer   2  Sparsity: 87.3888%\n",
      "layer   3  Sparsity: 88.4982%\n",
      "total_backward_count 372020 real_backward_count 83328  22.399%\n",
      "lif layer 2 self.abs_max_v: 7617.0\n",
      "lif layer 2 self.abs_max_v: 7717.5\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.880993/  2.036663, val:  35.00%, val_best:  53.75%, tr:  97.24%, tr_best:  99.08%, epoch time: 84.35 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1348%\n",
      "layer   2  Sparsity: 86.6432%\n",
      "layer   3  Sparsity: 87.7665%\n",
      "total_backward_count 381810 real_backward_count 85418  22.372%\n",
      "lif layer 1 self.abs_max_v: 32205.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.854585/  2.057372, val:  29.17%, val_best:  53.75%, tr:  98.26%, tr_best:  99.08%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0938%\n",
      "layer   2  Sparsity: 86.1368%\n",
      "layer   3  Sparsity: 87.2544%\n",
      "total_backward_count 391600 real_backward_count 87434  22.327%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.791220/  2.044320, val:  44.17%, val_best:  53.75%, tr:  98.57%, tr_best:  99.08%, epoch time: 84.91 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1034%\n",
      "layer   2  Sparsity: 85.9604%\n",
      "layer   3  Sparsity: 85.7272%\n",
      "total_backward_count 401390 real_backward_count 89387  22.269%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.890957/  2.117137, val:  47.92%, val_best:  53.75%, tr:  96.83%, tr_best:  99.08%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0839%\n",
      "layer   2  Sparsity: 86.8027%\n",
      "layer   3  Sparsity: 88.8182%\n",
      "total_backward_count 411180 real_backward_count 91528  22.260%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.923826/  2.088792, val:  40.00%, val_best:  53.75%, tr:  96.94%, tr_best:  99.08%, epoch time: 86.06 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0776%\n",
      "layer   2  Sparsity: 86.7183%\n",
      "layer   3  Sparsity: 88.9099%\n",
      "total_backward_count 420970 real_backward_count 93651  22.246%\n",
      "lif layer 2 self.abs_max_v: 7813.5\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.884426/  2.079671, val:  40.42%, val_best:  53.75%, tr:  97.14%, tr_best:  99.08%, epoch time: 85.60 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1395%\n",
      "layer   2  Sparsity: 86.5526%\n",
      "layer   3  Sparsity: 87.8094%\n",
      "total_backward_count 430760 real_backward_count 95693  22.215%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.881249/  2.062024, val:  41.25%, val_best:  53.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1165%\n",
      "layer   2  Sparsity: 86.5512%\n",
      "layer   3  Sparsity: 87.7669%\n",
      "total_backward_count 440550 real_backward_count 97692  22.175%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.877950/  2.053551, val:  42.08%, val_best:  53.75%, tr:  97.75%, tr_best:  99.08%, epoch time: 85.64 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1188%\n",
      "layer   2  Sparsity: 86.7310%\n",
      "layer   3  Sparsity: 88.6851%\n",
      "total_backward_count 450340 real_backward_count 99762  22.153%\n",
      "fc layer 1 self.abs_max_out: 18182.0\n",
      "fc layer 1 self.abs_max_out: 18371.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.852524/  2.044270, val:  44.58%, val_best:  53.75%, tr:  98.47%, tr_best:  99.08%, epoch time: 84.79 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1061%\n",
      "layer   2  Sparsity: 86.0649%\n",
      "layer   3  Sparsity: 87.6062%\n",
      "total_backward_count 460130 real_backward_count 101661  22.094%\n",
      "lif layer 1 self.abs_max_v: 32593.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.833823/  2.064836, val:  32.92%, val_best:  53.75%, tr:  98.37%, tr_best:  99.08%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0885%\n",
      "layer   2  Sparsity: 85.1911%\n",
      "layer   3  Sparsity: 86.6496%\n",
      "total_backward_count 469920 real_backward_count 103531  22.032%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.851268/  2.055796, val:  45.42%, val_best:  53.75%, tr:  98.98%, tr_best:  99.08%, epoch time: 85.75 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1191%\n",
      "layer   2  Sparsity: 85.3951%\n",
      "layer   3  Sparsity: 87.0132%\n",
      "total_backward_count 479710 real_backward_count 105450  21.982%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.866276/  2.025597, val:  47.92%, val_best:  53.75%, tr:  98.06%, tr_best:  99.08%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0720%\n",
      "layer   2  Sparsity: 86.0495%\n",
      "layer   3  Sparsity: 87.4132%\n",
      "total_backward_count 489500 real_backward_count 107479  21.957%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.868391/  2.068974, val:  35.00%, val_best:  53.75%, tr:  97.65%, tr_best:  99.08%, epoch time: 85.44 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0894%\n",
      "layer   2  Sparsity: 86.1240%\n",
      "layer   3  Sparsity: 87.7320%\n",
      "total_backward_count 499290 real_backward_count 109577  21.947%\n",
      "fc layer 1 self.abs_max_out: 19376.0\n",
      "fc layer 1 self.abs_max_out: 20077.0\n",
      "lif layer 1 self.abs_max_v: 33884.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.902559/  2.097458, val:  32.08%, val_best:  53.75%, tr:  97.45%, tr_best:  99.08%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0537%\n",
      "layer   2  Sparsity: 86.8368%\n",
      "layer   3  Sparsity: 88.6978%\n",
      "total_backward_count 509080 real_backward_count 111687  21.939%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.868057/  2.053228, val:  43.75%, val_best:  53.75%, tr:  97.45%, tr_best:  99.08%, epoch time: 84.88 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1012%\n",
      "layer   2  Sparsity: 86.5901%\n",
      "layer   3  Sparsity: 87.3174%\n",
      "total_backward_count 518870 real_backward_count 113799  21.932%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.866986/  2.080237, val:  50.83%, val_best:  53.75%, tr:  97.85%, tr_best:  99.08%, epoch time: 85.71 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0892%\n",
      "layer   2  Sparsity: 86.6475%\n",
      "layer   3  Sparsity: 87.7410%\n",
      "total_backward_count 528660 real_backward_count 115771  21.899%\n",
      "lif layer 1 self.abs_max_v: 33971.0\n",
      "lif layer 1 self.abs_max_v: 34069.5\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.873731/  2.051710, val:  51.67%, val_best:  53.75%, tr:  97.75%, tr_best:  99.08%, epoch time: 84.78 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0856%\n",
      "layer   2  Sparsity: 86.9386%\n",
      "layer   3  Sparsity: 87.5317%\n",
      "total_backward_count 538450 real_backward_count 117866  21.890%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.863348/  2.082143, val:  35.42%, val_best:  53.75%, tr:  97.96%, tr_best:  99.08%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1085%\n",
      "layer   2  Sparsity: 86.2798%\n",
      "layer   3  Sparsity: 88.4676%\n",
      "total_backward_count 548240 real_backward_count 119916  21.873%\n",
      "fc layer 1 self.abs_max_out: 20173.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.887218/  2.080010, val:  37.92%, val_best:  53.75%, tr:  97.85%, tr_best:  99.08%, epoch time: 86.03 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1391%\n",
      "layer   2  Sparsity: 86.0195%\n",
      "layer   3  Sparsity: 88.7366%\n",
      "total_backward_count 558030 real_backward_count 121931  21.850%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.891831/  2.039172, val:  51.25%, val_best:  53.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 84.95 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0999%\n",
      "layer   2  Sparsity: 85.2313%\n",
      "layer   3  Sparsity: 88.1534%\n",
      "total_backward_count 567820 real_backward_count 123941  21.828%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.850679/  1.966427, val:  49.17%, val_best:  53.75%, tr:  98.77%, tr_best:  99.08%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0772%\n",
      "layer   2  Sparsity: 85.4648%\n",
      "layer   3  Sparsity: 86.3165%\n",
      "total_backward_count 577610 real_backward_count 125768  21.774%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.846638/  2.050053, val:  44.58%, val_best:  53.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0666%\n",
      "layer   2  Sparsity: 85.6066%\n",
      "layer   3  Sparsity: 87.6206%\n",
      "total_backward_count 587400 real_backward_count 127690  21.738%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.920826/  2.119603, val:  40.00%, val_best:  53.75%, tr:  96.42%, tr_best:  99.08%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1295%\n",
      "layer   2  Sparsity: 86.2272%\n",
      "layer   3  Sparsity: 89.1577%\n",
      "total_backward_count 597190 real_backward_count 129809  21.737%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.953530/  2.084108, val:  56.25%, val_best:  56.25%, tr:  96.02%, tr_best:  99.08%, epoch time: 85.27 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0719%\n",
      "layer   2  Sparsity: 86.1392%\n",
      "layer   3  Sparsity: 89.7718%\n",
      "total_backward_count 606980 real_backward_count 132013  21.749%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.900519/  2.068441, val:  43.33%, val_best:  56.25%, tr:  97.85%, tr_best:  99.08%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0458%\n",
      "layer   2  Sparsity: 85.9726%\n",
      "layer   3  Sparsity: 88.4379%\n",
      "total_backward_count 616770 real_backward_count 133981  21.723%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.908311/  2.071555, val:  47.92%, val_best:  56.25%, tr:  97.55%, tr_best:  99.08%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0940%\n",
      "layer   2  Sparsity: 86.4015%\n",
      "layer   3  Sparsity: 88.9119%\n",
      "total_backward_count 626560 real_backward_count 135976  21.702%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.918666/  2.059370, val:  45.42%, val_best:  56.25%, tr:  96.73%, tr_best:  99.08%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0634%\n",
      "layer   2  Sparsity: 86.4050%\n",
      "layer   3  Sparsity: 89.3196%\n",
      "total_backward_count 636350 real_backward_count 138127  21.706%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.897403/  2.088469, val:  35.42%, val_best:  56.25%, tr:  97.55%, tr_best:  99.08%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0786%\n",
      "layer   2  Sparsity: 85.2833%\n",
      "layer   3  Sparsity: 88.6466%\n",
      "total_backward_count 646140 real_backward_count 140137  21.688%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.915831/  2.086271, val:  44.58%, val_best:  56.25%, tr:  95.91%, tr_best:  99.08%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0843%\n",
      "layer   2  Sparsity: 85.4911%\n",
      "layer   3  Sparsity: 89.3793%\n",
      "total_backward_count 655930 real_backward_count 142264  21.689%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.912306/  2.062394, val:  45.42%, val_best:  56.25%, tr:  97.24%, tr_best:  99.08%, epoch time: 85.69 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0850%\n",
      "layer   2  Sparsity: 85.1591%\n",
      "layer   3  Sparsity: 88.9980%\n",
      "total_backward_count 665720 real_backward_count 144269  21.671%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.923469/  2.043599, val:  58.75%, val_best:  58.75%, tr:  97.04%, tr_best:  99.08%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0663%\n",
      "layer   2  Sparsity: 85.9812%\n",
      "layer   3  Sparsity: 89.3284%\n",
      "total_backward_count 675510 real_backward_count 146363  21.667%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.919344/  2.102612, val:  43.75%, val_best:  58.75%, tr:  98.37%, tr_best:  99.08%, epoch time: 85.44 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1388%\n",
      "layer   2  Sparsity: 85.7582%\n",
      "layer   3  Sparsity: 89.3452%\n",
      "total_backward_count 685300 real_backward_count 148292  21.639%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.939018/  2.130721, val:  48.33%, val_best:  58.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1062%\n",
      "layer   2  Sparsity: 85.4525%\n",
      "layer   3  Sparsity: 89.7767%\n",
      "total_backward_count 695090 real_backward_count 150335  21.628%\n",
      "lif layer 1 self.abs_max_v: 34532.0\n",
      "lif layer 1 self.abs_max_v: 34626.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.946201/  2.125432, val:  40.00%, val_best:  58.75%, tr:  97.75%, tr_best:  99.08%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0793%\n",
      "layer   2  Sparsity: 85.6982%\n",
      "layer   3  Sparsity: 89.6774%\n",
      "total_backward_count 704880 real_backward_count 152352  21.614%\n",
      "fc layer 1 self.abs_max_out: 21218.0\n",
      "lif layer 1 self.abs_max_v: 37711.0\n",
      "lif layer 1 self.abs_max_v: 37884.0\n",
      "lif layer 1 self.abs_max_v: 38266.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.932888/  2.059221, val:  47.50%, val_best:  58.75%, tr:  98.37%, tr_best:  99.08%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0835%\n",
      "layer   2  Sparsity: 85.9565%\n",
      "layer   3  Sparsity: 89.0186%\n",
      "total_backward_count 714670 real_backward_count 154294  21.590%\n",
      "fc layer 1 self.abs_max_out: 21248.0\n",
      "lif layer 1 self.abs_max_v: 38337.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.927822/  2.120422, val:  33.33%, val_best:  58.75%, tr:  98.16%, tr_best:  99.08%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1203%\n",
      "layer   2  Sparsity: 85.6860%\n",
      "layer   3  Sparsity: 89.6341%\n",
      "total_backward_count 724460 real_backward_count 156275  21.571%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.980380/  2.127723, val:  49.17%, val_best:  58.75%, tr:  97.14%, tr_best:  99.08%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0511%\n",
      "layer   2  Sparsity: 85.6240%\n",
      "layer   3  Sparsity: 90.9198%\n",
      "total_backward_count 734250 real_backward_count 158461  21.581%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.996863/  2.075803, val:  45.42%, val_best:  58.75%, tr:  96.02%, tr_best:  99.08%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0799%\n",
      "layer   2  Sparsity: 86.0575%\n",
      "layer   3  Sparsity: 90.8155%\n",
      "total_backward_count 744040 real_backward_count 160757  21.606%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.938439/  2.053959, val:  58.33%, val_best:  58.75%, tr:  98.16%, tr_best:  99.08%, epoch time: 85.18 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0815%\n",
      "layer   2  Sparsity: 86.2565%\n",
      "layer   3  Sparsity: 88.9215%\n",
      "total_backward_count 753830 real_backward_count 162841  21.602%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.934449/  2.118142, val:  27.92%, val_best:  58.75%, tr:  97.85%, tr_best:  99.08%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1349%\n",
      "layer   2  Sparsity: 85.5427%\n",
      "layer   3  Sparsity: 89.4109%\n",
      "total_backward_count 763620 real_backward_count 164825  21.585%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.884605/  2.068056, val:  42.92%, val_best:  58.75%, tr:  97.55%, tr_best:  99.08%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0379%\n",
      "layer   2  Sparsity: 86.2875%\n",
      "layer   3  Sparsity: 88.3050%\n",
      "total_backward_count 773410 real_backward_count 166840  21.572%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.910146/  2.079179, val:  40.42%, val_best:  58.75%, tr:  97.04%, tr_best:  99.08%, epoch time: 85.27 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0967%\n",
      "layer   2  Sparsity: 86.6498%\n",
      "layer   3  Sparsity: 88.9041%\n",
      "total_backward_count 783200 real_backward_count 168984  21.576%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.880727/  2.049154, val:  43.33%, val_best:  58.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0908%\n",
      "layer   2  Sparsity: 86.7984%\n",
      "layer   3  Sparsity: 88.1918%\n",
      "total_backward_count 792990 real_backward_count 171147  21.582%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.930595/  2.139764, val:  25.83%, val_best:  58.75%, tr:  95.81%, tr_best:  99.08%, epoch time: 84.98 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1067%\n",
      "layer   2  Sparsity: 85.9387%\n",
      "layer   3  Sparsity: 89.2720%\n",
      "total_backward_count 802780 real_backward_count 173284  21.585%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.914339/  2.036585, val:  49.17%, val_best:  58.75%, tr:  96.42%, tr_best:  99.08%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0824%\n",
      "layer   2  Sparsity: 86.0472%\n",
      "layer   3  Sparsity: 88.1196%\n",
      "total_backward_count 812570 real_backward_count 175455  21.593%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.872926/  2.110015, val:  37.08%, val_best:  58.75%, tr:  97.65%, tr_best:  99.08%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1032%\n",
      "layer   2  Sparsity: 86.2096%\n",
      "layer   3  Sparsity: 87.4285%\n",
      "total_backward_count 822360 real_backward_count 177556  21.591%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.866179/  2.072772, val:  32.08%, val_best:  58.75%, tr:  98.16%, tr_best:  99.08%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0834%\n",
      "layer   2  Sparsity: 85.5873%\n",
      "layer   3  Sparsity: 87.4722%\n",
      "total_backward_count 832150 real_backward_count 179598  21.582%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.886179/  2.073400, val:  32.92%, val_best:  58.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.51 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1282%\n",
      "layer   2  Sparsity: 86.8868%\n",
      "layer   3  Sparsity: 87.8588%\n",
      "total_backward_count 841940 real_backward_count 181757  21.588%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.868702/  2.083762, val:  41.67%, val_best:  58.75%, tr:  96.32%, tr_best:  99.08%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1293%\n",
      "layer   2  Sparsity: 87.0395%\n",
      "layer   3  Sparsity: 87.9162%\n",
      "total_backward_count 851730 real_backward_count 183963  21.599%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.891445/  2.069019, val:  45.00%, val_best:  58.75%, tr:  97.65%, tr_best:  99.08%, epoch time: 85.29 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0861%\n",
      "layer   2  Sparsity: 86.7473%\n",
      "layer   3  Sparsity: 88.1157%\n",
      "total_backward_count 861520 real_backward_count 186159  21.608%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.855322/  2.046755, val:  46.67%, val_best:  58.75%, tr:  96.63%, tr_best:  99.08%, epoch time: 84.74 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0857%\n",
      "layer   2  Sparsity: 87.3192%\n",
      "layer   3  Sparsity: 87.5389%\n",
      "total_backward_count 871310 real_backward_count 188314  21.613%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.847818/  2.083071, val:  36.25%, val_best:  58.75%, tr:  96.02%, tr_best:  99.08%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0935%\n",
      "layer   2  Sparsity: 87.4642%\n",
      "layer   3  Sparsity: 86.4655%\n",
      "total_backward_count 881100 real_backward_count 190527  21.624%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.856015/  2.078419, val:  44.58%, val_best:  58.75%, tr:  96.83%, tr_best:  99.08%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1467%\n",
      "layer   2  Sparsity: 88.3154%\n",
      "layer   3  Sparsity: 87.3589%\n",
      "total_backward_count 890890 real_backward_count 192814  21.643%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.856023/  2.018795, val:  53.33%, val_best:  58.75%, tr:  96.42%, tr_best:  99.08%, epoch time: 86.17 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0908%\n",
      "layer   2  Sparsity: 87.8255%\n",
      "layer   3  Sparsity: 86.9980%\n",
      "total_backward_count 900680 real_backward_count 194932  21.643%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.864056/  2.038604, val:  35.00%, val_best:  58.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 86.60 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.1105%\n",
      "layer   2  Sparsity: 87.6556%\n",
      "layer   3  Sparsity: 87.2237%\n",
      "total_backward_count 910470 real_backward_count 197157  21.654%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.859018/  2.076010, val:  42.92%, val_best:  58.75%, tr:  96.53%, tr_best:  99.08%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0825%\n",
      "layer   2  Sparsity: 87.4063%\n",
      "layer   3  Sparsity: 87.7862%\n",
      "total_backward_count 920260 real_backward_count 199304  21.657%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.924298/  2.048708, val:  48.33%, val_best:  58.75%, tr:  96.42%, tr_best:  99.08%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0629%\n",
      "layer   2  Sparsity: 88.1095%\n",
      "layer   3  Sparsity: 89.0985%\n",
      "total_backward_count 930050 real_backward_count 201601  21.676%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.899144/  2.070283, val:  53.75%, val_best:  58.75%, tr:  96.63%, tr_best:  99.08%, epoch time: 86.47 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0852%\n",
      "layer   2  Sparsity: 88.2081%\n",
      "layer   3  Sparsity: 88.3242%\n",
      "total_backward_count 939840 real_backward_count 203758  21.680%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.877213/  2.117595, val:  44.58%, val_best:  58.75%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.50 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1151%\n",
      "layer   2  Sparsity: 87.3497%\n",
      "layer   3  Sparsity: 88.1517%\n",
      "total_backward_count 949630 real_backward_count 205845  21.676%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.926661/  2.040220, val:  60.00%, val_best:  60.00%, tr:  95.51%, tr_best:  99.08%, epoch time: 85.97 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0723%\n",
      "layer   2  Sparsity: 88.2926%\n",
      "layer   3  Sparsity: 88.7475%\n",
      "total_backward_count 959420 real_backward_count 208311  21.712%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.874118/  2.091067, val:  31.67%, val_best:  60.00%, tr:  97.04%, tr_best:  99.08%, epoch time: 86.23 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0601%\n",
      "layer   2  Sparsity: 87.6270%\n",
      "layer   3  Sparsity: 87.8401%\n",
      "total_backward_count 969210 real_backward_count 210490  21.718%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.871969/  2.029829, val:  52.08%, val_best:  60.00%, tr:  96.42%, tr_best:  99.08%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0784%\n",
      "layer   2  Sparsity: 87.7496%\n",
      "layer   3  Sparsity: 87.7162%\n",
      "total_backward_count 979000 real_backward_count 212695  21.726%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.918579/  2.078031, val:  35.83%, val_best:  60.00%, tr:  94.08%, tr_best:  99.08%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1088%\n",
      "layer   2  Sparsity: 87.9346%\n",
      "layer   3  Sparsity: 88.9128%\n",
      "total_backward_count 988790 real_backward_count 215117  21.756%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.906645/  2.066244, val:  49.17%, val_best:  60.00%, tr:  95.61%, tr_best:  99.08%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1318%\n",
      "layer   2  Sparsity: 87.8966%\n",
      "layer   3  Sparsity: 87.9777%\n",
      "total_backward_count 998580 real_backward_count 217425  21.773%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.902301/  2.097416, val:  32.08%, val_best:  60.00%, tr:  96.42%, tr_best:  99.08%, epoch time: 85.50 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1216%\n",
      "layer   2  Sparsity: 87.8194%\n",
      "layer   3  Sparsity: 87.9548%\n",
      "total_backward_count 1008370 real_backward_count 219638  21.781%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.918132/  2.034366, val:  44.17%, val_best:  60.00%, tr:  96.53%, tr_best:  99.08%, epoch time: 84.56 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0355%\n",
      "layer   2  Sparsity: 87.7320%\n",
      "layer   3  Sparsity: 88.0501%\n",
      "total_backward_count 1018160 real_backward_count 221965  21.801%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.895932/  2.063664, val:  50.83%, val_best:  60.00%, tr:  95.10%, tr_best:  99.08%, epoch time: 85.42 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0801%\n",
      "layer   2  Sparsity: 88.0886%\n",
      "layer   3  Sparsity: 88.2333%\n",
      "total_backward_count 1027950 real_backward_count 224317  21.822%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.918154/  2.091230, val:  40.83%, val_best:  60.00%, tr:  96.42%, tr_best:  99.08%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0421%\n",
      "layer   2  Sparsity: 87.7680%\n",
      "layer   3  Sparsity: 88.1480%\n",
      "total_backward_count 1037740 real_backward_count 226634  21.839%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.886496/  2.080027, val:  24.58%, val_best:  60.00%, tr:  96.94%, tr_best:  99.08%, epoch time: 86.01 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1218%\n",
      "layer   2  Sparsity: 87.4428%\n",
      "layer   3  Sparsity: 87.9989%\n",
      "total_backward_count 1047530 real_backward_count 228739  21.836%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.908319/  2.038300, val:  46.67%, val_best:  60.00%, tr:  96.73%, tr_best:  99.08%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0608%\n",
      "layer   2  Sparsity: 87.1457%\n",
      "layer   3  Sparsity: 87.0305%\n",
      "total_backward_count 1057320 real_backward_count 230915  21.840%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.862817/  2.061000, val:  44.58%, val_best:  60.00%, tr:  97.65%, tr_best:  99.08%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1057%\n",
      "layer   2  Sparsity: 87.0062%\n",
      "layer   3  Sparsity: 86.6676%\n",
      "total_backward_count 1067110 real_backward_count 233024  21.837%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.846630/  2.035284, val:  42.92%, val_best:  60.00%, tr:  96.73%, tr_best:  99.08%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0919%\n",
      "layer   2  Sparsity: 87.3628%\n",
      "layer   3  Sparsity: 86.1044%\n",
      "total_backward_count 1076900 real_backward_count 235187  21.839%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.844337/  2.048156, val:  32.08%, val_best:  60.00%, tr:  97.75%, tr_best:  99.08%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0796%\n",
      "layer   2  Sparsity: 87.2824%\n",
      "layer   3  Sparsity: 86.4004%\n",
      "total_backward_count 1086690 real_backward_count 237364  21.843%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.872473/  2.047338, val:  42.50%, val_best:  60.00%, tr:  96.22%, tr_best:  99.08%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0776%\n",
      "layer   2  Sparsity: 87.9228%\n",
      "layer   3  Sparsity: 87.3253%\n",
      "total_backward_count 1096480 real_backward_count 239610  21.853%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.843120/  2.000439, val:  61.67%, val_best:  61.67%, tr:  95.40%, tr_best:  99.08%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0773%\n",
      "layer   2  Sparsity: 87.7652%\n",
      "layer   3  Sparsity: 86.4331%\n",
      "total_backward_count 1106270 real_backward_count 241791  21.856%\n",
      "fc layer 3 self.abs_max_out: 1893.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.864450/  2.056656, val:  37.08%, val_best:  61.67%, tr:  95.91%, tr_best:  99.08%, epoch time: 85.99 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0859%\n",
      "layer   2  Sparsity: 87.8095%\n",
      "layer   3  Sparsity: 87.3680%\n",
      "total_backward_count 1116060 real_backward_count 244052  21.867%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.883358/  2.093725, val:  29.17%, val_best:  61.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0992%\n",
      "layer   2  Sparsity: 88.2981%\n",
      "layer   3  Sparsity: 87.7011%\n",
      "total_backward_count 1125850 real_backward_count 246295  21.876%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.858957/  2.078290, val:  41.67%, val_best:  61.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 86.40 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.1139%\n",
      "layer   2  Sparsity: 88.0453%\n",
      "layer   3  Sparsity: 86.9389%\n",
      "total_backward_count 1135640 real_backward_count 248540  21.885%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.816097/  2.019746, val:  51.25%, val_best:  61.67%, tr:  97.45%, tr_best:  99.08%, epoch time: 86.24 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0763%\n",
      "layer   2  Sparsity: 87.1696%\n",
      "layer   3  Sparsity: 85.2692%\n",
      "total_backward_count 1145430 real_backward_count 250645  21.882%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.783560/  1.990343, val:  54.58%, val_best:  61.67%, tr:  97.55%, tr_best:  99.08%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1202%\n",
      "layer   2  Sparsity: 87.4298%\n",
      "layer   3  Sparsity: 85.1758%\n",
      "total_backward_count 1155220 real_backward_count 252787  21.882%\n",
      "fc layer 3 self.abs_max_out: 1906.0\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.831310/  2.038976, val:  45.83%, val_best:  61.67%, tr:  95.30%, tr_best:  99.08%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0817%\n",
      "layer   2  Sparsity: 87.5569%\n",
      "layer   3  Sparsity: 86.4462%\n",
      "total_backward_count 1165010 real_backward_count 255011  21.889%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.871914/  2.018635, val:  51.25%, val_best:  61.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 85.47 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0741%\n",
      "layer   2  Sparsity: 87.3472%\n",
      "layer   3  Sparsity: 87.3206%\n",
      "total_backward_count 1174800 real_backward_count 257259  21.898%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.814422/  1.981220, val:  48.75%, val_best:  61.67%, tr:  97.14%, tr_best:  99.08%, epoch time: 85.09 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1188%\n",
      "layer   2  Sparsity: 87.1885%\n",
      "layer   3  Sparsity: 85.9683%\n",
      "total_backward_count 1184590 real_backward_count 259342  21.893%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.828985/  2.085123, val:  34.58%, val_best:  61.67%, tr:  96.63%, tr_best:  99.08%, epoch time: 85.41 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0822%\n",
      "layer   2  Sparsity: 86.3426%\n",
      "layer   3  Sparsity: 86.9747%\n",
      "total_backward_count 1194380 real_backward_count 261466  21.891%\n",
      "fc layer 3 self.abs_max_out: 1958.0\n",
      "fc layer 3 self.abs_max_out: 1966.0\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.867211/  2.040403, val:  40.00%, val_best:  61.67%, tr:  97.14%, tr_best:  99.08%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1142%\n",
      "layer   2  Sparsity: 86.7452%\n",
      "layer   3  Sparsity: 87.5297%\n",
      "total_backward_count 1204170 real_backward_count 263686  21.898%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.869906/  2.070130, val:  42.50%, val_best:  61.67%, tr:  96.32%, tr_best:  99.08%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1205%\n",
      "layer   2  Sparsity: 87.3309%\n",
      "layer   3  Sparsity: 87.9337%\n",
      "total_backward_count 1213960 real_backward_count 265851  21.899%\n",
      "fc layer 3 self.abs_max_out: 2430.0\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.873802/  2.063845, val:  47.92%, val_best:  61.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 85.81 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1355%\n",
      "layer   2  Sparsity: 87.1759%\n",
      "layer   3  Sparsity: 87.9916%\n",
      "total_backward_count 1223750 real_backward_count 268153  21.912%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.865810/  2.050310, val:  44.58%, val_best:  61.67%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.69 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1146%\n",
      "layer   2  Sparsity: 87.2819%\n",
      "layer   3  Sparsity: 87.8539%\n",
      "total_backward_count 1233540 real_backward_count 270292  21.912%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.857647/  2.066745, val:  38.33%, val_best:  61.67%, tr:  97.45%, tr_best:  99.08%, epoch time: 85.81 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1295%\n",
      "layer   2  Sparsity: 86.5114%\n",
      "layer   3  Sparsity: 87.2957%\n",
      "total_backward_count 1243330 real_backward_count 272356  21.905%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.868322/  2.052529, val:  52.08%, val_best:  61.67%, tr:  97.55%, tr_best:  99.08%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0587%\n",
      "layer   2  Sparsity: 86.7308%\n",
      "layer   3  Sparsity: 87.1924%\n",
      "total_backward_count 1253120 real_backward_count 274366  21.895%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.905815/  2.064647, val:  44.58%, val_best:  61.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 86.01 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0963%\n",
      "layer   2  Sparsity: 87.1426%\n",
      "layer   3  Sparsity: 88.3285%\n",
      "total_backward_count 1262910 real_backward_count 276524  21.896%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.895276/  2.034824, val:  47.92%, val_best:  61.67%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0972%\n",
      "layer   2  Sparsity: 86.8051%\n",
      "layer   3  Sparsity: 87.9155%\n",
      "total_backward_count 1272700 real_backward_count 278538  21.886%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.852120/  2.110060, val:  35.83%, val_best:  61.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1151%\n",
      "layer   2  Sparsity: 86.8458%\n",
      "layer   3  Sparsity: 87.5852%\n",
      "total_backward_count 1282490 real_backward_count 280625  21.881%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.868808/  2.032243, val:  50.83%, val_best:  61.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 85.27 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0706%\n",
      "layer   2  Sparsity: 87.0241%\n",
      "layer   3  Sparsity: 87.2832%\n",
      "total_backward_count 1292280 real_backward_count 282823  21.886%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.846214/  2.067597, val:  42.50%, val_best:  61.67%, tr:  97.65%, tr_best:  99.08%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0982%\n",
      "layer   2  Sparsity: 87.8600%\n",
      "layer   3  Sparsity: 88.0701%\n",
      "total_backward_count 1302070 real_backward_count 284983  21.887%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.813054/  2.030807, val:  39.58%, val_best:  61.67%, tr:  97.45%, tr_best:  99.08%, epoch time: 86.44 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.1130%\n",
      "layer   2  Sparsity: 87.2822%\n",
      "layer   3  Sparsity: 85.9748%\n",
      "total_backward_count 1311860 real_backward_count 287138  21.888%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.815612/  2.053986, val:  41.67%, val_best:  61.67%, tr:  98.26%, tr_best:  99.08%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1135%\n",
      "layer   2  Sparsity: 87.3472%\n",
      "layer   3  Sparsity: 86.9711%\n",
      "total_backward_count 1321650 real_backward_count 289269  21.887%\n",
      "lif layer 2 self.abs_max_v: 7833.5\n",
      "lif layer 2 self.abs_max_v: 7986.0\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.890676/  2.024681, val:  50.42%, val_best:  61.67%, tr:  95.71%, tr_best:  99.08%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0815%\n",
      "layer   2  Sparsity: 87.0064%\n",
      "layer   3  Sparsity: 88.1787%\n",
      "total_backward_count 1331440 real_backward_count 291531  21.896%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.878853/  2.058019, val:  43.75%, val_best:  61.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0260%\n",
      "layer   2  Sparsity: 87.0411%\n",
      "layer   3  Sparsity: 87.6532%\n",
      "total_backward_count 1341230 real_backward_count 293660  21.895%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.889625/  2.031739, val:  36.25%, val_best:  61.67%, tr:  97.65%, tr_best:  99.08%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0641%\n",
      "layer   2  Sparsity: 86.9807%\n",
      "layer   3  Sparsity: 87.3842%\n",
      "total_backward_count 1351020 real_backward_count 295757  21.891%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.877914/  2.060067, val:  49.17%, val_best:  61.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0917%\n",
      "layer   2  Sparsity: 86.6614%\n",
      "layer   3  Sparsity: 87.7043%\n",
      "total_backward_count 1360810 real_backward_count 297923  21.893%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.833360/  1.974638, val:  45.83%, val_best:  61.67%, tr:  97.34%, tr_best:  99.08%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0844%\n",
      "layer   2  Sparsity: 86.8036%\n",
      "layer   3  Sparsity: 86.2048%\n",
      "total_backward_count 1370600 real_backward_count 300004  21.889%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.837803/  2.069838, val:  42.08%, val_best:  61.67%, tr:  97.75%, tr_best:  99.08%, epoch time: 86.00 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0782%\n",
      "layer   2  Sparsity: 86.4918%\n",
      "layer   3  Sparsity: 87.6831%\n",
      "total_backward_count 1380390 real_backward_count 302063  21.882%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.874485/  2.018999, val:  52.50%, val_best:  61.67%, tr:  98.16%, tr_best:  99.08%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0943%\n",
      "layer   2  Sparsity: 86.2107%\n",
      "layer   3  Sparsity: 87.2221%\n",
      "total_backward_count 1390180 real_backward_count 304167  21.880%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.857067/  2.000949, val:  44.58%, val_best:  61.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 87.13 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 88.1096%\n",
      "layer   2  Sparsity: 86.4713%\n",
      "layer   3  Sparsity: 87.0909%\n",
      "total_backward_count 1399970 real_backward_count 306284  21.878%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.885833/  2.054492, val:  50.00%, val_best:  61.67%, tr:  97.65%, tr_best:  99.08%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0893%\n",
      "layer   2  Sparsity: 86.2237%\n",
      "layer   3  Sparsity: 88.7508%\n",
      "total_backward_count 1409760 real_backward_count 308457  21.880%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.893071/  2.070399, val:  43.75%, val_best:  61.67%, tr:  95.20%, tr_best:  99.08%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0396%\n",
      "layer   2  Sparsity: 86.9457%\n",
      "layer   3  Sparsity: 88.4555%\n",
      "total_backward_count 1419550 real_backward_count 310767  21.892%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.898302/  2.058343, val:  32.50%, val_best:  61.67%, tr:  95.40%, tr_best:  99.08%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1088%\n",
      "layer   2  Sparsity: 87.2120%\n",
      "layer   3  Sparsity: 87.7469%\n",
      "total_backward_count 1429340 real_backward_count 313067  21.903%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.922145/  2.082748, val:  46.25%, val_best:  61.67%, tr:  94.89%, tr_best:  99.08%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0748%\n",
      "layer   2  Sparsity: 87.5375%\n",
      "layer   3  Sparsity: 89.0783%\n",
      "total_backward_count 1439130 real_backward_count 315521  21.924%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.913571/  2.118292, val:  36.67%, val_best:  61.67%, tr:  94.48%, tr_best:  99.08%, epoch time: 85.85 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0495%\n",
      "layer   2  Sparsity: 88.1872%\n",
      "layer   3  Sparsity: 89.4127%\n",
      "total_backward_count 1448920 real_backward_count 317907  21.941%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.924294/  2.052837, val:  49.58%, val_best:  61.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1248%\n",
      "layer   2  Sparsity: 87.8446%\n",
      "layer   3  Sparsity: 89.2170%\n",
      "total_backward_count 1458710 real_backward_count 320289  21.957%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.886264/  2.029212, val:  50.42%, val_best:  61.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 84.73 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1040%\n",
      "layer   2  Sparsity: 87.5600%\n",
      "layer   3  Sparsity: 88.5061%\n",
      "total_backward_count 1468500 real_backward_count 322560  21.965%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.867867/  2.051662, val:  36.25%, val_best:  61.67%, tr:  96.63%, tr_best:  99.08%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1046%\n",
      "layer   2  Sparsity: 87.6274%\n",
      "layer   3  Sparsity: 87.6665%\n",
      "total_backward_count 1478290 real_backward_count 324737  21.967%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.851592/  2.033255, val:  41.67%, val_best:  61.67%, tr:  96.63%, tr_best:  99.08%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0816%\n",
      "layer   2  Sparsity: 87.6035%\n",
      "layer   3  Sparsity: 87.5532%\n",
      "total_backward_count 1488080 real_backward_count 326986  21.974%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.858564/  2.076473, val:  52.08%, val_best:  61.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0805%\n",
      "layer   2  Sparsity: 87.3002%\n",
      "layer   3  Sparsity: 87.4789%\n",
      "total_backward_count 1497870 real_backward_count 329122  21.973%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.868410/  2.041437, val:  55.42%, val_best:  61.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 85.42 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0750%\n",
      "layer   2  Sparsity: 87.3513%\n",
      "layer   3  Sparsity: 87.5807%\n",
      "total_backward_count 1507660 real_backward_count 331331  21.977%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.907111/  2.027322, val:  52.92%, val_best:  61.67%, tr:  96.12%, tr_best:  99.08%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0786%\n",
      "layer   2  Sparsity: 87.6668%\n",
      "layer   3  Sparsity: 88.5392%\n",
      "total_backward_count 1517450 real_backward_count 333600  21.984%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.907475/  2.058702, val:  49.17%, val_best:  61.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1142%\n",
      "layer   2  Sparsity: 87.5622%\n",
      "layer   3  Sparsity: 88.7231%\n",
      "total_backward_count 1527240 real_backward_count 335931  21.996%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.860572/  2.038391, val:  44.17%, val_best:  61.67%, tr:  96.12%, tr_best:  99.08%, epoch time: 84.44 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0062%\n",
      "layer   2  Sparsity: 87.5589%\n",
      "layer   3  Sparsity: 87.4403%\n",
      "total_backward_count 1537030 real_backward_count 338169  22.001%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.881894/  2.010423, val:  56.25%, val_best:  61.67%, tr:  95.20%, tr_best:  99.08%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1391%\n",
      "layer   2  Sparsity: 88.2100%\n",
      "layer   3  Sparsity: 87.6468%\n",
      "total_backward_count 1546820 real_backward_count 340539  22.015%\n",
      "fc layer 3 self.abs_max_out: 2670.0\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.843904/  2.054380, val:  42.08%, val_best:  61.67%, tr:  95.20%, tr_best:  99.08%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0870%\n",
      "layer   2  Sparsity: 87.9217%\n",
      "layer   3  Sparsity: 87.1478%\n",
      "total_backward_count 1556610 real_backward_count 342877  22.027%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.830124/  2.025241, val:  51.25%, val_best:  61.67%, tr:  96.63%, tr_best:  99.08%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0969%\n",
      "layer   2  Sparsity: 87.8149%\n",
      "layer   3  Sparsity: 86.8527%\n",
      "total_backward_count 1566400 real_backward_count 345186  22.037%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.837937/  2.046840, val:  54.17%, val_best:  61.67%, tr:  95.40%, tr_best:  99.08%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0961%\n",
      "layer   2  Sparsity: 88.1011%\n",
      "layer   3  Sparsity: 87.3037%\n",
      "total_backward_count 1576190 real_backward_count 347505  22.047%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.851539/  2.002522, val:  56.67%, val_best:  61.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 86.10 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0840%\n",
      "layer   2  Sparsity: 87.6543%\n",
      "layer   3  Sparsity: 87.2640%\n",
      "total_backward_count 1585980 real_backward_count 349765  22.054%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.877427/  2.062260, val:  42.92%, val_best:  61.67%, tr:  94.69%, tr_best:  99.08%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0789%\n",
      "layer   2  Sparsity: 87.7399%\n",
      "layer   3  Sparsity: 88.2580%\n",
      "total_backward_count 1595770 real_backward_count 352109  22.065%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.879300/  2.044488, val:  45.42%, val_best:  61.67%, tr:  93.97%, tr_best:  99.08%, epoch time: 85.76 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0720%\n",
      "layer   2  Sparsity: 87.9176%\n",
      "layer   3  Sparsity: 87.3708%\n",
      "total_backward_count 1605560 real_backward_count 354593  22.085%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.804086/  1.982169, val:  43.75%, val_best:  61.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.1361%\n",
      "layer   2  Sparsity: 87.3594%\n",
      "layer   3  Sparsity: 85.4815%\n",
      "total_backward_count 1615350 real_backward_count 356978  22.099%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.762599/  1.995124, val:  48.75%, val_best:  61.67%, tr:  96.12%, tr_best:  99.08%, epoch time: 81.98 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 88.0970%\n",
      "layer   2  Sparsity: 87.1266%\n",
      "layer   3  Sparsity: 84.7431%\n",
      "total_backward_count 1625140 real_backward_count 359111  22.097%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.777230/  2.022702, val:  41.25%, val_best:  61.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0694%\n",
      "layer   2  Sparsity: 87.7674%\n",
      "layer   3  Sparsity: 85.1388%\n",
      "total_backward_count 1634930 real_backward_count 361330  22.101%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.812776/  1.983781, val:  40.00%, val_best:  61.67%, tr:  95.40%, tr_best:  99.08%, epoch time: 84.50 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0712%\n",
      "layer   2  Sparsity: 88.1504%\n",
      "layer   3  Sparsity: 85.6892%\n",
      "total_backward_count 1644720 real_backward_count 363749  22.116%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.825111/  2.044580, val:  43.75%, val_best:  61.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 84.45 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0844%\n",
      "layer   2  Sparsity: 88.5419%\n",
      "layer   3  Sparsity: 86.7476%\n",
      "total_backward_count 1654510 real_backward_count 366030  22.123%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.831356/  2.056425, val:  45.83%, val_best:  61.67%, tr:  96.42%, tr_best:  99.08%, epoch time: 82.99 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 88.1321%\n",
      "layer   2  Sparsity: 88.0970%\n",
      "layer   3  Sparsity: 86.6323%\n",
      "total_backward_count 1664300 real_backward_count 368256  22.127%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.825915/  2.030532, val:  43.75%, val_best:  61.67%, tr:  95.51%, tr_best:  99.08%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0751%\n",
      "layer   2  Sparsity: 87.9169%\n",
      "layer   3  Sparsity: 86.3597%\n",
      "total_backward_count 1674090 real_backward_count 370561  22.135%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.848887/  2.038230, val:  32.08%, val_best:  61.67%, tr:  95.40%, tr_best:  99.08%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0755%\n",
      "layer   2  Sparsity: 88.2276%\n",
      "layer   3  Sparsity: 86.6445%\n",
      "total_backward_count 1683880 real_backward_count 372934  22.147%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.852223/  2.015476, val:  43.75%, val_best:  61.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0505%\n",
      "layer   2  Sparsity: 88.2819%\n",
      "layer   3  Sparsity: 86.6030%\n",
      "total_backward_count 1693670 real_backward_count 375233  22.155%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.862551/  2.045199, val:  45.42%, val_best:  61.67%, tr:  96.63%, tr_best:  99.08%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0843%\n",
      "layer   2  Sparsity: 88.0721%\n",
      "layer   3  Sparsity: 86.7728%\n",
      "total_backward_count 1703460 real_backward_count 377358  22.152%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.839084/  2.002995, val:  54.58%, val_best:  61.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0726%\n",
      "layer   2  Sparsity: 87.4982%\n",
      "layer   3  Sparsity: 86.5974%\n",
      "total_backward_count 1713250 real_backward_count 379554  22.154%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.864137/  2.019523, val:  56.25%, val_best:  61.67%, tr:  94.48%, tr_best:  99.08%, epoch time: 84.50 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.1069%\n",
      "layer   2  Sparsity: 88.1114%\n",
      "layer   3  Sparsity: 86.6317%\n",
      "total_backward_count 1723040 real_backward_count 381928  22.166%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.847406/  2.074882, val:  39.17%, val_best:  61.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0720%\n",
      "layer   2  Sparsity: 87.8352%\n",
      "layer   3  Sparsity: 86.4369%\n",
      "total_backward_count 1732830 real_backward_count 384239  22.174%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.823923/  1.948587, val:  50.42%, val_best:  61.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0873%\n",
      "layer   2  Sparsity: 87.3683%\n",
      "layer   3  Sparsity: 85.3046%\n",
      "total_backward_count 1742620 real_backward_count 386467  22.177%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.812304/  2.026051, val:  45.83%, val_best:  61.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 84.91 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0940%\n",
      "layer   2  Sparsity: 87.3957%\n",
      "layer   3  Sparsity: 85.3952%\n",
      "total_backward_count 1752410 real_backward_count 388562  22.173%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.862716/  2.034230, val:  45.83%, val_best:  61.67%, tr:  96.32%, tr_best:  99.08%, epoch time: 86.51 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0683%\n",
      "layer   2  Sparsity: 87.7260%\n",
      "layer   3  Sparsity: 86.8276%\n",
      "total_backward_count 1762200 real_backward_count 390742  22.174%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.860723/  2.039882, val:  47.50%, val_best:  61.67%, tr:  94.79%, tr_best:  99.08%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0747%\n",
      "layer   2  Sparsity: 87.9867%\n",
      "layer   3  Sparsity: 86.4789%\n",
      "total_backward_count 1771990 real_backward_count 393171  22.188%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.841950/  2.013825, val:  33.33%, val_best:  61.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0610%\n",
      "layer   2  Sparsity: 87.3624%\n",
      "layer   3  Sparsity: 85.7606%\n",
      "total_backward_count 1781780 real_backward_count 395553  22.200%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.788310/  2.006318, val:  47.08%, val_best:  61.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 84.43 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0981%\n",
      "layer   2  Sparsity: 87.4839%\n",
      "layer   3  Sparsity: 84.5650%\n",
      "total_backward_count 1791570 real_backward_count 397754  22.201%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.803134/  2.016168, val:  35.42%, val_best:  61.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 88.0591%\n",
      "layer   2  Sparsity: 87.6733%\n",
      "layer   3  Sparsity: 85.6885%\n",
      "total_backward_count 1801360 real_backward_count 400052  22.208%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.835614/  2.123574, val:  20.83%, val_best:  61.67%, tr:  95.20%, tr_best:  99.08%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0880%\n",
      "layer   2  Sparsity: 87.8694%\n",
      "layer   3  Sparsity: 86.5447%\n",
      "total_backward_count 1811150 real_backward_count 402342  22.215%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.872079/  2.106769, val:  27.50%, val_best:  61.67%, tr:  94.48%, tr_best:  99.08%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0861%\n",
      "layer   2  Sparsity: 88.6735%\n",
      "layer   3  Sparsity: 87.9932%\n",
      "total_backward_count 1820940 real_backward_count 404831  22.232%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.872024/  1.977693, val:  50.00%, val_best:  61.67%, tr:  93.77%, tr_best:  99.08%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0902%\n",
      "layer   2  Sparsity: 88.7406%\n",
      "layer   3  Sparsity: 86.3912%\n",
      "total_backward_count 1830730 real_backward_count 407248  22.245%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.809028/  2.070846, val:  32.50%, val_best:  61.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 86.45 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0938%\n",
      "layer   2  Sparsity: 87.9407%\n",
      "layer   3  Sparsity: 85.5803%\n",
      "total_backward_count 1840520 real_backward_count 409513  22.250%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.820466/  2.011608, val:  52.08%, val_best:  61.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0829%\n",
      "layer   2  Sparsity: 88.1874%\n",
      "layer   3  Sparsity: 85.6840%\n",
      "total_backward_count 1850310 real_backward_count 411692  22.250%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.816724/  2.006398, val:  40.00%, val_best:  61.67%, tr:  95.10%, tr_best:  99.08%, epoch time: 85.86 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0716%\n",
      "layer   2  Sparsity: 88.6103%\n",
      "layer   3  Sparsity: 85.7506%\n",
      "total_backward_count 1860100 real_backward_count 414030  22.258%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.804130/  1.975264, val:  49.17%, val_best:  61.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1181%\n",
      "layer   2  Sparsity: 88.2047%\n",
      "layer   3  Sparsity: 85.9317%\n",
      "total_backward_count 1869890 real_backward_count 416205  22.258%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.821943/  2.036604, val:  30.83%, val_best:  61.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 86.11 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0948%\n",
      "layer   2  Sparsity: 88.0232%\n",
      "layer   3  Sparsity: 84.8296%\n",
      "total_backward_count 1879680 real_backward_count 418394  22.259%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.740613/  1.999897, val:  44.17%, val_best:  61.67%, tr:  97.85%, tr_best:  99.08%, epoch time: 82.00 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 88.1504%\n",
      "layer   2  Sparsity: 87.0628%\n",
      "layer   3  Sparsity: 83.5347%\n",
      "total_backward_count 1889470 real_backward_count 420457  22.253%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.797908/  2.034736, val:  34.58%, val_best:  61.67%, tr:  97.85%, tr_best:  99.08%, epoch time: 85.33 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0870%\n",
      "layer   2  Sparsity: 87.7499%\n",
      "layer   3  Sparsity: 85.1014%\n",
      "total_backward_count 1899260 real_backward_count 422594  22.250%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.808428/  2.077171, val:  31.67%, val_best:  61.67%, tr:  96.12%, tr_best:  99.08%, epoch time: 85.44 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.0826%\n",
      "layer   2  Sparsity: 87.5623%\n",
      "layer   3  Sparsity: 86.3540%\n",
      "total_backward_count 1909050 real_backward_count 424922  22.258%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.885262/  2.052542, val:  47.92%, val_best:  61.67%, tr:  95.40%, tr_best:  99.08%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1175%\n",
      "layer   2  Sparsity: 88.1062%\n",
      "layer   3  Sparsity: 87.3599%\n",
      "total_backward_count 1918840 real_backward_count 427244  22.266%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.901954/  2.099332, val:  37.50%, val_best:  61.67%, tr:  94.59%, tr_best:  99.08%, epoch time: 85.76 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.1180%\n",
      "layer   2  Sparsity: 88.0278%\n",
      "layer   3  Sparsity: 87.5230%\n",
      "total_backward_count 1928630 real_backward_count 429677  22.279%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.862311/  2.021013, val:  40.00%, val_best:  61.67%, tr:  95.71%, tr_best:  99.08%, epoch time: 86.54 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 88.0654%\n",
      "layer   2  Sparsity: 88.2906%\n",
      "layer   3  Sparsity: 86.5282%\n",
      "total_backward_count 1938420 real_backward_count 432001  22.286%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.801077/  2.043336, val:  35.00%, val_best:  61.67%, tr:  95.30%, tr_best:  99.08%, epoch time: 85.51 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.0861%\n",
      "layer   2  Sparsity: 88.3364%\n",
      "layer   3  Sparsity: 85.4395%\n",
      "total_backward_count 1948210 real_backward_count 434427  22.299%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.802788/  2.059890, val:  47.50%, val_best:  61.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 83.39 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 88.0609%\n",
      "layer   2  Sparsity: 88.0535%\n",
      "layer   3  Sparsity: 85.8811%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4fabe4facc4f03aea83ab7d9ba7f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñÖ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñá‚ñá‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÑ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.95812</td></tr><tr><td>tr_epoch_loss</td><td>1.80279</td></tr><tr><td>val_acc_best</td><td>0.61667</td></tr><tr><td>val_acc_now</td><td>0.475</td></tr><tr><td>val_loss</td><td>2.05989</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-sweep-334</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8uoseeu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8uoseeu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251127_192929-x8uoseeu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 83q04p62 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_001523-83q04p62</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/83q04p62' target=\"_blank\">winter-sweep-337</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/83q04p62' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/83q04p62</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251128_001532_267', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 111.0\n",
      "lif layer 1 self.abs_max_v: 111.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 408.0\n",
      "lif layer 2 self.abs_max_v: 408.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 1 self.abs_max_out: 148.0\n",
      "lif layer 1 self.abs_max_v: 184.0\n",
      "fc layer 2 self.abs_max_out: 574.0\n",
      "lif layer 2 self.abs_max_v: 705.0\n",
      "fc layer 1 self.abs_max_out: 162.0\n",
      "lif layer 1 self.abs_max_v: 252.0\n",
      "lif layer 2 self.abs_max_v: 756.5\n",
      "fc layer 1 self.abs_max_out: 174.0\n",
      "lif layer 2 self.abs_max_v: 773.5\n",
      "fc layer 3 self.abs_max_out: 234.0\n",
      "fc layer 1 self.abs_max_out: 203.0\n",
      "lif layer 1 self.abs_max_v: 281.5\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "lif layer 1 self.abs_max_v: 312.0\n",
      "lif layer 2 self.abs_max_v: 823.5\n",
      "fc layer 1 self.abs_max_out: 215.0\n",
      "fc layer 1 self.abs_max_out: 243.0\n",
      "lif layer 1 self.abs_max_v: 348.5\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "fc layer 2 self.abs_max_out: 583.0\n",
      "fc layer 1 self.abs_max_out: 271.0\n",
      "lif layer 1 self.abs_max_v: 358.0\n",
      "fc layer 1 self.abs_max_out: 307.0\n",
      "lif layer 1 self.abs_max_v: 404.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "lif layer 2 self.abs_max_v: 835.0\n",
      "fc layer 2 self.abs_max_out: 598.0\n",
      "fc layer 1 self.abs_max_out: 360.0\n",
      "lif layer 1 self.abs_max_v: 461.0\n",
      "fc layer 2 self.abs_max_out: 616.0\n",
      "lif layer 2 self.abs_max_v: 839.5\n",
      "lif layer 2 self.abs_max_v: 850.0\n",
      "lif layer 1 self.abs_max_v: 479.0\n",
      "lif layer 2 self.abs_max_v: 949.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "lif layer 2 self.abs_max_v: 1046.5\n",
      "lif layer 1 self.abs_max_v: 485.0\n",
      "lif layer 1 self.abs_max_v: 489.5\n",
      "fc layer 2 self.abs_max_out: 645.0\n",
      "lif layer 1 self.abs_max_v: 503.5\n",
      "fc layer 1 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 671.0\n",
      "fc layer 1 self.abs_max_out: 378.0\n",
      "fc layer 1 self.abs_max_out: 400.0\n",
      "lif layer 2 self.abs_max_v: 1092.5\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 1 self.abs_max_out: 445.0\n",
      "lif layer 1 self.abs_max_v: 559.0\n",
      "fc layer 1 self.abs_max_out: 484.0\n",
      "fc layer 1 self.abs_max_out: 562.0\n",
      "lif layer 1 self.abs_max_v: 562.0\n",
      "lif layer 1 self.abs_max_v: 581.0\n",
      "lif layer 1 self.abs_max_v: 590.0\n",
      "lif layer 1 self.abs_max_v: 698.0\n",
      "fc layer 1 self.abs_max_out: 605.0\n",
      "lif layer 2 self.abs_max_v: 1138.5\n",
      "fc layer 2 self.abs_max_out: 683.0\n",
      "fc layer 2 self.abs_max_out: 709.0\n",
      "fc layer 1 self.abs_max_out: 690.0\n",
      "lif layer 1 self.abs_max_v: 779.5\n",
      "lif layer 1 self.abs_max_v: 880.0\n",
      "fc layer 2 self.abs_max_out: 744.0\n",
      "lif layer 2 self.abs_max_v: 1264.0\n",
      "fc layer 2 self.abs_max_out: 824.0\n",
      "lif layer 1 self.abs_max_v: 896.0\n",
      "lif layer 1 self.abs_max_v: 900.0\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "lif layer 1 self.abs_max_v: 903.0\n",
      "lif layer 1 self.abs_max_v: 908.0\n",
      "lif layer 2 self.abs_max_v: 1299.0\n",
      "fc layer 1 self.abs_max_out: 703.0\n",
      "lif layer 2 self.abs_max_v: 1370.0\n",
      "lif layer 1 self.abs_max_v: 923.0\n",
      "fc layer 1 self.abs_max_out: 886.0\n",
      "lif layer 1 self.abs_max_v: 1060.5\n",
      "lif layer 1 self.abs_max_v: 1193.0\n",
      "fc layer 2 self.abs_max_out: 829.0\n",
      "lif layer 2 self.abs_max_v: 1382.0\n",
      "lif layer 2 self.abs_max_v: 1405.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 2 self.abs_max_out: 865.0\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "lif layer 1 self.abs_max_v: 1194.0\n",
      "lif layer 1 self.abs_max_v: 1367.0\n",
      "lif layer 1 self.abs_max_v: 1445.5\n",
      "lif layer 2 self.abs_max_v: 1461.0\n",
      "fc layer 2 self.abs_max_out: 867.0\n",
      "lif layer 2 self.abs_max_v: 1529.5\n",
      "fc layer 1 self.abs_max_out: 944.0\n",
      "fc layer 1 self.abs_max_out: 995.0\n",
      "lif layer 1 self.abs_max_v: 1459.5\n",
      "fc layer 1 self.abs_max_out: 1073.0\n",
      "lif layer 1 self.abs_max_v: 1549.0\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "lif layer 2 self.abs_max_v: 1539.0\n",
      "fc layer 2 self.abs_max_out: 906.0\n",
      "lif layer 2 self.abs_max_v: 1651.0\n",
      "lif layer 2 self.abs_max_v: 1653.5\n",
      "fc layer 2 self.abs_max_out: 908.0\n",
      "fc layer 2 self.abs_max_out: 959.0\n",
      "lif layer 2 self.abs_max_v: 1670.5\n",
      "lif layer 2 self.abs_max_v: 1690.5\n",
      "fc layer 2 self.abs_max_out: 969.0\n",
      "lif layer 2 self.abs_max_v: 1718.5\n",
      "lif layer 2 self.abs_max_v: 1743.5\n",
      "lif layer 2 self.abs_max_v: 1793.0\n",
      "lif layer 2 self.abs_max_v: 1818.5\n",
      "lif layer 2 self.abs_max_v: 1823.5\n",
      "fc layer 2 self.abs_max_out: 972.0\n",
      "fc layer 2 self.abs_max_out: 1031.0\n",
      "fc layer 2 self.abs_max_out: 1059.0\n",
      "lif layer 2 self.abs_max_v: 1894.5\n",
      "lif layer 2 self.abs_max_v: 1907.5\n",
      "lif layer 2 self.abs_max_v: 1973.0\n",
      "fc layer 2 self.abs_max_out: 1084.0\n",
      "fc layer 2 self.abs_max_out: 1125.0\n",
      "lif layer 2 self.abs_max_v: 1998.0\n",
      "lif layer 2 self.abs_max_v: 2009.0\n",
      "lif layer 2 self.abs_max_v: 2038.5\n",
      "fc layer 2 self.abs_max_out: 1226.0\n",
      "lif layer 2 self.abs_max_v: 2127.0\n",
      "lif layer 1 self.abs_max_v: 1645.5\n",
      "lif layer 1 self.abs_max_v: 1746.0\n",
      "fc layer 1 self.abs_max_out: 1097.0\n",
      "lif layer 1 self.abs_max_v: 1921.5\n",
      "lif layer 1 self.abs_max_v: 1954.0\n",
      "lif layer 1 self.abs_max_v: 2048.0\n",
      "fc layer 1 self.abs_max_out: 1196.0\n",
      "lif layer 1 self.abs_max_v: 2135.0\n",
      "lif layer 1 self.abs_max_v: 2137.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.666253/  1.963457, val:  33.33%, val_best:  33.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0472%\n",
      "layer   2  Sparsity: 70.8256%\n",
      "layer   3  Sparsity: 61.4259%\n",
      "total_backward_count 9790 real_backward_count 1673  17.089%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "fc layer 1 self.abs_max_out: 1206.0\n",
      "lif layer 2 self.abs_max_v: 2132.5\n",
      "lif layer 2 self.abs_max_v: 2213.5\n",
      "fc layer 1 self.abs_max_out: 1286.0\n",
      "lif layer 1 self.abs_max_v: 2301.0\n",
      "fc layer 1 self.abs_max_out: 1297.0\n",
      "lif layer 1 self.abs_max_v: 2323.0\n",
      "fc layer 2 self.abs_max_out: 1265.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.581884/  1.935221, val:  33.33%, val_best:  33.33%, tr:  99.08%, tr_best:  99.08%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0561%\n",
      "layer   2  Sparsity: 71.3260%\n",
      "layer   3  Sparsity: 61.3890%\n",
      "total_backward_count 19580 real_backward_count 3040  15.526%\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "fc layer 1 self.abs_max_out: 1314.0\n",
      "fc layer 2 self.abs_max_out: 1369.0\n",
      "lif layer 2 self.abs_max_v: 2242.5\n",
      "lif layer 2 self.abs_max_v: 2303.0\n",
      "lif layer 2 self.abs_max_v: 2331.0\n",
      "lif layer 2 self.abs_max_v: 2359.5\n",
      "fc layer 1 self.abs_max_out: 1351.0\n",
      "lif layer 2 self.abs_max_v: 2361.5\n",
      "lif layer 2 self.abs_max_v: 2367.0\n",
      "lif layer 2 self.abs_max_v: 2382.5\n",
      "fc layer 1 self.abs_max_out: 1499.0\n",
      "fc layer 1 self.abs_max_out: 1544.0\n",
      "lif layer 1 self.abs_max_v: 2651.0\n",
      "fc layer 1 self.abs_max_out: 1599.0\n",
      "lif layer 1 self.abs_max_v: 2924.5\n",
      "lif layer 1 self.abs_max_v: 2987.5\n",
      "fc layer 1 self.abs_max_out: 1706.0\n",
      "lif layer 1 self.abs_max_v: 3179.0\n",
      "lif layer 2 self.abs_max_v: 2452.0\n",
      "lif layer 2 self.abs_max_v: 2503.5\n",
      "fc layer 2 self.abs_max_out: 1379.0\n",
      "lif layer 2 self.abs_max_v: 2507.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.562866/  1.900186, val:  34.58%, val_best:  34.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0637%\n",
      "layer   2  Sparsity: 71.8715%\n",
      "layer   3  Sparsity: 61.6503%\n",
      "total_backward_count 29370 real_backward_count 4402  14.988%\n",
      "lif layer 2 self.abs_max_v: 2543.0\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "fc layer 2 self.abs_max_out: 1427.0\n",
      "fc layer 2 self.abs_max_out: 1449.0\n",
      "fc layer 2 self.abs_max_out: 1484.0\n",
      "lif layer 2 self.abs_max_v: 2643.0\n",
      "lif layer 2 self.abs_max_v: 2687.5\n",
      "lif layer 2 self.abs_max_v: 2717.0\n",
      "fc layer 1 self.abs_max_out: 1787.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.530689/  1.843754, val:  45.00%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0551%\n",
      "layer   2  Sparsity: 72.1976%\n",
      "layer   3  Sparsity: 60.9239%\n",
      "total_backward_count 39160 real_backward_count 5757  14.701%\n",
      "fc layer 2 self.abs_max_out: 1493.0\n",
      "lif layer 2 self.abs_max_v: 2722.5\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "lif layer 2 self.abs_max_v: 2754.5\n",
      "lif layer 2 self.abs_max_v: 2757.5\n",
      "lif layer 2 self.abs_max_v: 2898.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "lif layer 2 self.abs_max_v: 2962.5\n",
      "fc layer 1 self.abs_max_out: 2089.0\n",
      "lif layer 1 self.abs_max_v: 3565.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.489358/  1.818619, val:  42.08%, val_best:  45.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0495%\n",
      "layer   2  Sparsity: 71.2559%\n",
      "layer   3  Sparsity: 60.7948%\n",
      "total_backward_count 48950 real_backward_count 6991  14.282%\n",
      "fc layer 2 self.abs_max_out: 1559.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.501248/  1.850320, val:  51.25%, val_best:  51.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0547%\n",
      "layer   2  Sparsity: 71.0462%\n",
      "layer   3  Sparsity: 63.6684%\n",
      "total_backward_count 58740 real_backward_count 8257  14.057%\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "fc layer 2 self.abs_max_out: 1604.0\n",
      "lif layer 1 self.abs_max_v: 3641.5\n",
      "lif layer 1 self.abs_max_v: 3786.0\n",
      "fc layer 1 self.abs_max_out: 2166.0\n",
      "lif layer 2 self.abs_max_v: 3020.5\n",
      "fc layer 2 self.abs_max_out: 1606.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.533127/  1.851251, val:  44.58%, val_best:  51.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0403%\n",
      "layer   2  Sparsity: 71.4428%\n",
      "layer   3  Sparsity: 64.9226%\n",
      "total_backward_count 68530 real_backward_count 9508  13.874%\n",
      "fc layer 3 self.abs_max_out: 588.0\n",
      "fc layer 2 self.abs_max_out: 1608.0\n",
      "fc layer 2 self.abs_max_out: 1687.0\n",
      "lif layer 2 self.abs_max_v: 3089.5\n",
      "fc layer 3 self.abs_max_out: 595.0\n",
      "fc layer 1 self.abs_max_out: 2190.0\n",
      "lif layer 1 self.abs_max_v: 3838.0\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.514421/  1.753763, val:  48.75%, val_best:  51.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0642%\n",
      "layer   2  Sparsity: 71.6341%\n",
      "layer   3  Sparsity: 65.7031%\n",
      "total_backward_count 78320 real_backward_count 10646  13.593%\n",
      "fc layer 2 self.abs_max_out: 1716.0\n",
      "lif layer 2 self.abs_max_v: 3172.5\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 2 self.abs_max_out: 1909.0\n",
      "lif layer 2 self.abs_max_v: 3189.5\n",
      "lif layer 2 self.abs_max_v: 3239.0\n",
      "lif layer 2 self.abs_max_v: 3407.5\n",
      "lif layer 2 self.abs_max_v: 3470.0\n",
      "fc layer 1 self.abs_max_out: 2422.0\n",
      "lif layer 1 self.abs_max_v: 4098.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.463855/  1.745455, val:  60.00%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0614%\n",
      "layer   2  Sparsity: 71.6500%\n",
      "layer   3  Sparsity: 64.6407%\n",
      "total_backward_count 88110 real_backward_count 11857  13.457%\n",
      "lif layer 2 self.abs_max_v: 3610.0\n",
      "fc layer 1 self.abs_max_out: 2525.0\n",
      "lif layer 1 self.abs_max_v: 4266.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.451872/  1.777250, val:  44.58%, val_best:  60.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0571%\n",
      "layer   2  Sparsity: 70.9262%\n",
      "layer   3  Sparsity: 65.0116%\n",
      "total_backward_count 97900 real_backward_count 12983  13.261%\n",
      "fc layer 3 self.abs_max_out: 606.0\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "fc layer 2 self.abs_max_out: 1931.0\n",
      "lif layer 2 self.abs_max_v: 3724.0\n",
      "lif layer 2 self.abs_max_v: 3749.0\n",
      "lif layer 2 self.abs_max_v: 3774.5\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "fc layer 1 self.abs_max_out: 2564.0\n",
      "lif layer 1 self.abs_max_v: 4350.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.430641/  1.771973, val:  46.25%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0597%\n",
      "layer   2  Sparsity: 70.8287%\n",
      "layer   3  Sparsity: 65.7555%\n",
      "total_backward_count 107690 real_backward_count 14130  13.121%\n",
      "fc layer 2 self.abs_max_out: 1969.0\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 1 self.abs_max_out: 2622.0\n",
      "lif layer 1 self.abs_max_v: 4537.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.429426/  1.731965, val:  47.50%, val_best:  60.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0421%\n",
      "layer   2  Sparsity: 70.1856%\n",
      "layer   3  Sparsity: 65.7135%\n",
      "total_backward_count 117480 real_backward_count 15230  12.964%\n",
      "fc layer 3 self.abs_max_out: 637.0\n",
      "fc layer 1 self.abs_max_out: 2738.0\n",
      "lif layer 1 self.abs_max_v: 4766.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.409859/  1.721147, val:  50.42%, val_best:  60.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0553%\n",
      "layer   2  Sparsity: 70.3651%\n",
      "layer   3  Sparsity: 65.9073%\n",
      "total_backward_count 127270 real_backward_count 16246  12.765%\n",
      "fc layer 1 self.abs_max_out: 2989.0\n",
      "lif layer 1 self.abs_max_v: 5138.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.399207/  1.723971, val:  41.25%, val_best:  60.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0636%\n",
      "layer   2  Sparsity: 70.1235%\n",
      "layer   3  Sparsity: 65.9191%\n",
      "total_backward_count 137060 real_backward_count 17261  12.594%\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "lif layer 1 self.abs_max_v: 5157.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.384662/  1.652674, val:  64.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0397%\n",
      "layer   2  Sparsity: 70.2313%\n",
      "layer   3  Sparsity: 65.6123%\n",
      "total_backward_count 146850 real_backward_count 18297  12.460%\n",
      "fc layer 3 self.abs_max_out: 679.0\n",
      "fc layer 3 self.abs_max_out: 686.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.363948/  1.670584, val:  53.33%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0473%\n",
      "layer   2  Sparsity: 70.3596%\n",
      "layer   3  Sparsity: 65.7139%\n",
      "total_backward_count 156640 real_backward_count 19316  12.331%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.362937/  1.610828, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0579%\n",
      "layer   2  Sparsity: 69.8946%\n",
      "layer   3  Sparsity: 65.1051%\n",
      "total_backward_count 166430 real_backward_count 20306  12.201%\n",
      "fc layer 2 self.abs_max_out: 2069.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.351691/  1.575254, val:  80.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0361%\n",
      "layer   2  Sparsity: 69.5530%\n",
      "layer   3  Sparsity: 65.0026%\n",
      "total_backward_count 176220 real_backward_count 21259  12.064%\n",
      "fc layer 3 self.abs_max_out: 690.0\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.326445/  1.561821, val:  68.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0461%\n",
      "layer   2  Sparsity: 69.5048%\n",
      "layer   3  Sparsity: 64.8267%\n",
      "total_backward_count 186010 real_backward_count 22194  11.932%\n",
      "fc layer 3 self.abs_max_out: 735.0\n",
      "fc layer 1 self.abs_max_out: 2997.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.292590/  1.631528, val:  42.92%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0447%\n",
      "layer   2  Sparsity: 69.5037%\n",
      "layer   3  Sparsity: 64.7846%\n",
      "total_backward_count 195800 real_backward_count 23133  11.815%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.321704/  1.603978, val:  65.42%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0389%\n",
      "layer   2  Sparsity: 69.3290%\n",
      "layer   3  Sparsity: 65.6886%\n",
      "total_backward_count 205590 real_backward_count 24017  11.682%\n",
      "fc layer 1 self.abs_max_out: 3025.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.310667/  1.545317, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0562%\n",
      "layer   2  Sparsity: 69.3813%\n",
      "layer   3  Sparsity: 65.5548%\n",
      "total_backward_count 215380 real_backward_count 24921  11.571%\n",
      "fc layer 1 self.abs_max_out: 3328.0\n",
      "lif layer 1 self.abs_max_v: 5753.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.298017/  1.554159, val:  70.00%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0387%\n",
      "layer   2  Sparsity: 69.7868%\n",
      "layer   3  Sparsity: 65.6672%\n",
      "total_backward_count 225170 real_backward_count 25796  11.456%\n",
      "fc layer 2 self.abs_max_out: 2182.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.284509/  1.517315, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0335%\n",
      "layer   2  Sparsity: 70.0238%\n",
      "layer   3  Sparsity: 66.0484%\n",
      "total_backward_count 234960 real_backward_count 26671  11.351%\n",
      "fc layer 2 self.abs_max_out: 2268.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.307396/  1.581805, val:  67.50%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0490%\n",
      "layer   2  Sparsity: 69.7064%\n",
      "layer   3  Sparsity: 66.8835%\n",
      "total_backward_count 244750 real_backward_count 27505  11.238%\n",
      "fc layer 2 self.abs_max_out: 2308.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.287148/  1.500792, val:  82.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0516%\n",
      "layer   2  Sparsity: 69.5649%\n",
      "layer   3  Sparsity: 66.6445%\n",
      "total_backward_count 254540 real_backward_count 28388  11.153%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.293981/  1.548225, val:  76.67%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0512%\n",
      "layer   2  Sparsity: 69.7057%\n",
      "layer   3  Sparsity: 66.9443%\n",
      "total_backward_count 264330 real_backward_count 29243  11.063%\n",
      "fc layer 1 self.abs_max_out: 3358.0\n",
      "lif layer 1 self.abs_max_v: 5806.5\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.310250/  1.576107, val:  70.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0340%\n",
      "layer   2  Sparsity: 69.5888%\n",
      "layer   3  Sparsity: 66.6347%\n",
      "total_backward_count 274120 real_backward_count 30057  10.965%\n",
      "fc layer 1 self.abs_max_out: 3422.0\n",
      "lif layer 1 self.abs_max_v: 5967.5\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.273051/  1.580194, val:  57.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0414%\n",
      "layer   2  Sparsity: 69.6032%\n",
      "layer   3  Sparsity: 66.4145%\n",
      "total_backward_count 283910 real_backward_count 30849  10.866%\n",
      "fc layer 1 self.abs_max_out: 3452.0\n",
      "lif layer 1 self.abs_max_v: 6041.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.287014/  1.546598, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0482%\n",
      "layer   2  Sparsity: 69.6269%\n",
      "layer   3  Sparsity: 66.2832%\n",
      "total_backward_count 293700 real_backward_count 31632  10.770%\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.297553/  1.518765, val:  75.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0597%\n",
      "layer   2  Sparsity: 69.5438%\n",
      "layer   3  Sparsity: 66.8510%\n",
      "total_backward_count 303490 real_backward_count 32424  10.684%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.291203/  1.526924, val:  76.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0386%\n",
      "layer   2  Sparsity: 69.2393%\n",
      "layer   3  Sparsity: 67.4813%\n",
      "total_backward_count 313280 real_backward_count 33189  10.594%\n",
      "lif layer 2 self.abs_max_v: 3779.5\n",
      "lif layer 2 self.abs_max_v: 3811.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.296249/  1.565726, val:  72.50%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0481%\n",
      "layer   2  Sparsity: 69.3754%\n",
      "layer   3  Sparsity: 67.4529%\n",
      "total_backward_count 323070 real_backward_count 33976  10.517%\n",
      "lif layer 2 self.abs_max_v: 3925.5\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.297973/  1.503207, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0656%\n",
      "layer   2  Sparsity: 69.4342%\n",
      "layer   3  Sparsity: 67.2756%\n",
      "total_backward_count 332860 real_backward_count 34693  10.423%\n",
      "fc layer 2 self.abs_max_out: 2438.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.282184/  1.524017, val:  72.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0533%\n",
      "layer   2  Sparsity: 69.5632%\n",
      "layer   3  Sparsity: 66.6580%\n",
      "total_backward_count 342650 real_backward_count 35355  10.318%\n",
      "fc layer 2 self.abs_max_out: 2526.0\n",
      "fc layer 1 self.abs_max_out: 3560.0\n",
      "lif layer 1 self.abs_max_v: 6273.5\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.265125/  1.518341, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0407%\n",
      "layer   2  Sparsity: 69.6132%\n",
      "layer   3  Sparsity: 66.8005%\n",
      "total_backward_count 352440 real_backward_count 36076  10.236%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.239763/  1.496447, val:  78.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0594%\n",
      "layer   2  Sparsity: 69.4989%\n",
      "layer   3  Sparsity: 67.0615%\n",
      "total_backward_count 362230 real_backward_count 36847  10.172%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.249453/  1.532199, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0586%\n",
      "layer   2  Sparsity: 69.3054%\n",
      "layer   3  Sparsity: 67.2998%\n",
      "total_backward_count 372020 real_backward_count 37507  10.082%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.253798/  1.497210, val:  70.00%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0418%\n",
      "layer   2  Sparsity: 69.1772%\n",
      "layer   3  Sparsity: 67.4649%\n",
      "total_backward_count 381810 real_backward_count 38206  10.007%\n",
      "lif layer 2 self.abs_max_v: 3932.5\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.248502/  1.455760, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0468%\n",
      "layer   2  Sparsity: 69.1273%\n",
      "layer   3  Sparsity: 67.1308%\n",
      "total_backward_count 391600 real_backward_count 38911   9.936%\n",
      "lif layer 2 self.abs_max_v: 4142.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.239380/  1.457427, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0467%\n",
      "layer   2  Sparsity: 68.9313%\n",
      "layer   3  Sparsity: 67.4252%\n",
      "total_backward_count 401390 real_backward_count 39530   9.848%\n",
      "lif layer 1 self.abs_max_v: 6383.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.226580/  1.469524, val:  76.25%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0485%\n",
      "layer   2  Sparsity: 69.0535%\n",
      "layer   3  Sparsity: 67.2668%\n",
      "total_backward_count 411180 real_backward_count 40159   9.767%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.198583/  1.469744, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0590%\n",
      "layer   2  Sparsity: 69.2858%\n",
      "layer   3  Sparsity: 67.6610%\n",
      "total_backward_count 420970 real_backward_count 40881   9.711%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.214945/  1.447625, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0637%\n",
      "layer   2  Sparsity: 69.3890%\n",
      "layer   3  Sparsity: 67.6915%\n",
      "total_backward_count 430760 real_backward_count 41542   9.644%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.208074/  1.451501, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0732%\n",
      "layer   2  Sparsity: 69.4208%\n",
      "layer   3  Sparsity: 67.3336%\n",
      "total_backward_count 440550 real_backward_count 42147   9.567%\n",
      "lif layer 2 self.abs_max_v: 4178.0\n",
      "fc layer 1 self.abs_max_out: 3739.0\n",
      "lif layer 1 self.abs_max_v: 6544.5\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.208927/  1.461144, val:  73.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0453%\n",
      "layer   2  Sparsity: 69.3566%\n",
      "layer   3  Sparsity: 67.7966%\n",
      "total_backward_count 450340 real_backward_count 42812   9.507%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.206430/  1.472770, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0457%\n",
      "layer   2  Sparsity: 69.1117%\n",
      "layer   3  Sparsity: 67.6331%\n",
      "total_backward_count 460130 real_backward_count 43477   9.449%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.195821/  1.466235, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0562%\n",
      "layer   2  Sparsity: 68.9821%\n",
      "layer   3  Sparsity: 67.6033%\n",
      "total_backward_count 469920 real_backward_count 44092   9.383%\n",
      "fc layer 1 self.abs_max_out: 3754.0\n",
      "lif layer 1 self.abs_max_v: 6562.5\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.199586/  1.464907, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0381%\n",
      "layer   2  Sparsity: 69.1002%\n",
      "layer   3  Sparsity: 68.8422%\n",
      "total_backward_count 479710 real_backward_count 44716   9.321%\n",
      "fc layer 1 self.abs_max_out: 3799.0\n",
      "lif layer 1 self.abs_max_v: 6638.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.216259/  1.432605, val:  81.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0400%\n",
      "layer   2  Sparsity: 69.0118%\n",
      "layer   3  Sparsity: 69.2963%\n",
      "total_backward_count 489500 real_backward_count 45348   9.264%\n",
      "fc layer 1 self.abs_max_out: 3980.0\n",
      "lif layer 1 self.abs_max_v: 7008.0\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.195337/  1.455356, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0545%\n",
      "layer   2  Sparsity: 69.1370%\n",
      "layer   3  Sparsity: 68.9442%\n",
      "total_backward_count 499290 real_backward_count 45944   9.202%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.208689/  1.447809, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0327%\n",
      "layer   2  Sparsity: 68.8332%\n",
      "layer   3  Sparsity: 68.4490%\n",
      "total_backward_count 509080 real_backward_count 46524   9.139%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.207025/  1.445352, val:  80.83%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0506%\n",
      "layer   2  Sparsity: 68.8559%\n",
      "layer   3  Sparsity: 68.5359%\n",
      "total_backward_count 518870 real_backward_count 47161   9.089%\n",
      "fc layer 3 self.abs_max_out: 751.0\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.199109/  1.435856, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0416%\n",
      "layer   2  Sparsity: 68.9640%\n",
      "layer   3  Sparsity: 68.7282%\n",
      "total_backward_count 528660 real_backward_count 47768   9.036%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.209499/  1.455988, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0509%\n",
      "layer   2  Sparsity: 69.2221%\n",
      "layer   3  Sparsity: 69.2338%\n",
      "total_backward_count 538450 real_backward_count 48330   8.976%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.195387/  1.435103, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0511%\n",
      "layer   2  Sparsity: 69.1826%\n",
      "layer   3  Sparsity: 69.1303%\n",
      "total_backward_count 548240 real_backward_count 48910   8.921%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.181262/  1.441688, val:  81.25%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0390%\n",
      "layer   2  Sparsity: 69.0804%\n",
      "layer   3  Sparsity: 68.9710%\n",
      "total_backward_count 558030 real_backward_count 49486   8.868%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.183107/  1.421065, val:  84.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0500%\n",
      "layer   2  Sparsity: 69.2480%\n",
      "layer   3  Sparsity: 68.6802%\n",
      "total_backward_count 567820 real_backward_count 50056   8.815%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.197155/  1.434246, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0501%\n",
      "layer   2  Sparsity: 69.2128%\n",
      "layer   3  Sparsity: 68.2870%\n",
      "total_backward_count 577610 real_backward_count 50611   8.762%\n",
      "lif layer 2 self.abs_max_v: 4227.5\n",
      "fc layer 1 self.abs_max_out: 4001.0\n",
      "lif layer 1 self.abs_max_v: 7025.0\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.180586/  1.427210, val:  75.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0575%\n",
      "layer   2  Sparsity: 68.9767%\n",
      "layer   3  Sparsity: 67.9443%\n",
      "total_backward_count 587400 real_backward_count 51172   8.712%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.183710/  1.442642, val:  75.42%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0513%\n",
      "layer   2  Sparsity: 69.0379%\n",
      "layer   3  Sparsity: 67.9781%\n",
      "total_backward_count 597190 real_backward_count 51739   8.664%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.181372/  1.392771, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0406%\n",
      "layer   2  Sparsity: 69.0969%\n",
      "layer   3  Sparsity: 68.0334%\n",
      "total_backward_count 606980 real_backward_count 52312   8.618%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.144894/  1.410361, val:  70.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0441%\n",
      "layer   2  Sparsity: 69.0202%\n",
      "layer   3  Sparsity: 67.9057%\n",
      "total_backward_count 616770 real_backward_count 52868   8.572%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.153262/  1.399840, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0629%\n",
      "layer   2  Sparsity: 69.0707%\n",
      "layer   3  Sparsity: 68.2967%\n",
      "total_backward_count 626560 real_backward_count 53361   8.517%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.148664/  1.393272, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0693%\n",
      "layer   2  Sparsity: 68.8480%\n",
      "layer   3  Sparsity: 67.6687%\n",
      "total_backward_count 636350 real_backward_count 53908   8.471%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.127918/  1.395900, val:  70.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0400%\n",
      "layer   2  Sparsity: 68.9630%\n",
      "layer   3  Sparsity: 67.8788%\n",
      "total_backward_count 646140 real_backward_count 54416   8.422%\n",
      "fc layer 3 self.abs_max_out: 759.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.135845/  1.404923, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0402%\n",
      "layer   2  Sparsity: 69.0129%\n",
      "layer   3  Sparsity: 68.2512%\n",
      "total_backward_count 655930 real_backward_count 54931   8.375%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.120731/  1.376228, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0400%\n",
      "layer   2  Sparsity: 68.9087%\n",
      "layer   3  Sparsity: 69.3200%\n",
      "total_backward_count 665720 real_backward_count 55465   8.332%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.130650/  1.384114, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0430%\n",
      "layer   2  Sparsity: 68.7422%\n",
      "layer   3  Sparsity: 69.1178%\n",
      "total_backward_count 675510 real_backward_count 55997   8.290%\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.118702/  1.370256, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0440%\n",
      "layer   2  Sparsity: 68.7713%\n",
      "layer   3  Sparsity: 69.1322%\n",
      "total_backward_count 685300 real_backward_count 56506   8.245%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.110643/  1.385225, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0460%\n",
      "layer   2  Sparsity: 68.8789%\n",
      "layer   3  Sparsity: 69.6728%\n",
      "total_backward_count 695090 real_backward_count 57003   8.201%\n",
      "lif layer 2 self.abs_max_v: 4365.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.128695/  1.403381, val:  73.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0421%\n",
      "layer   2  Sparsity: 68.8187%\n",
      "layer   3  Sparsity: 69.8628%\n",
      "total_backward_count 704880 real_backward_count 57501   8.158%\n",
      "lif layer 2 self.abs_max_v: 4386.5\n",
      "lif layer 2 self.abs_max_v: 4402.5\n",
      "lif layer 2 self.abs_max_v: 4412.0\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.130189/  1.377040, val:  81.67%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0476%\n",
      "layer   2  Sparsity: 68.8954%\n",
      "layer   3  Sparsity: 69.9429%\n",
      "total_backward_count 714670 real_backward_count 58009   8.117%\n",
      "lif layer 2 self.abs_max_v: 4455.0\n",
      "lif layer 2 self.abs_max_v: 4561.5\n",
      "lif layer 2 self.abs_max_v: 4670.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.120406/  1.384155, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0529%\n",
      "layer   2  Sparsity: 69.2025%\n",
      "layer   3  Sparsity: 70.1869%\n",
      "total_backward_count 724460 real_backward_count 58447   8.068%\n",
      "fc layer 2 self.abs_max_out: 2532.0\n",
      "fc layer 2 self.abs_max_out: 2612.0\n",
      "lif layer 2 self.abs_max_v: 4881.5\n",
      "fc layer 2 self.abs_max_out: 2760.0\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.131365/  1.357958, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0544%\n",
      "layer   2  Sparsity: 69.1284%\n",
      "layer   3  Sparsity: 70.1058%\n",
      "total_backward_count 734250 real_backward_count 58961   8.030%\n",
      "lif layer 2 self.abs_max_v: 4991.5\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.129879/  1.333272, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0394%\n",
      "layer   2  Sparsity: 68.9215%\n",
      "layer   3  Sparsity: 69.6645%\n",
      "total_backward_count 744040 real_backward_count 59478   7.994%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.123400/  1.391288, val:  77.08%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0601%\n",
      "layer   2  Sparsity: 68.7414%\n",
      "layer   3  Sparsity: 69.1836%\n",
      "total_backward_count 753830 real_backward_count 59977   7.956%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.138121/  1.431528, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0543%\n",
      "layer   2  Sparsity: 68.5996%\n",
      "layer   3  Sparsity: 69.2579%\n",
      "total_backward_count 763620 real_backward_count 60458   7.917%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.132315/  1.371181, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0563%\n",
      "layer   2  Sparsity: 68.5321%\n",
      "layer   3  Sparsity: 69.6806%\n",
      "total_backward_count 773410 real_backward_count 60922   7.877%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.117610/  1.369701, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0476%\n",
      "layer   2  Sparsity: 68.5461%\n",
      "layer   3  Sparsity: 69.7128%\n",
      "total_backward_count 783200 real_backward_count 61396   7.839%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.140034/  1.342064, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0391%\n",
      "layer   2  Sparsity: 68.6197%\n",
      "layer   3  Sparsity: 69.5453%\n",
      "total_backward_count 792990 real_backward_count 61912   7.807%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.118281/  1.375747, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0532%\n",
      "layer   2  Sparsity: 68.4571%\n",
      "layer   3  Sparsity: 69.2152%\n",
      "total_backward_count 802780 real_backward_count 62359   7.768%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.117375/  1.373401, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0523%\n",
      "layer   2  Sparsity: 68.6819%\n",
      "layer   3  Sparsity: 68.5605%\n",
      "total_backward_count 812570 real_backward_count 62826   7.732%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.120284/  1.360415, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0378%\n",
      "layer   2  Sparsity: 68.8452%\n",
      "layer   3  Sparsity: 68.6817%\n",
      "total_backward_count 822360 real_backward_count 63261   7.693%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.104491/  1.340783, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0557%\n",
      "layer   2  Sparsity: 68.9372%\n",
      "layer   3  Sparsity: 68.4958%\n",
      "total_backward_count 832150 real_backward_count 63677   7.652%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.080742/  1.366405, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0515%\n",
      "layer   2  Sparsity: 68.9990%\n",
      "layer   3  Sparsity: 68.9571%\n",
      "total_backward_count 841940 real_backward_count 64125   7.616%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.113831/  1.410234, val:  71.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0332%\n",
      "layer   2  Sparsity: 69.0001%\n",
      "layer   3  Sparsity: 68.7650%\n",
      "total_backward_count 851730 real_backward_count 64577   7.582%\n",
      "lif layer 1 self.abs_max_v: 7083.5\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.094958/  1.364588, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0318%\n",
      "layer   2  Sparsity: 69.1342%\n",
      "layer   3  Sparsity: 68.9815%\n",
      "total_backward_count 861520 real_backward_count 64996   7.544%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.088917/  1.328395, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0389%\n",
      "layer   2  Sparsity: 68.7936%\n",
      "layer   3  Sparsity: 69.1750%\n",
      "total_backward_count 871310 real_backward_count 65403   7.506%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.116370/  1.350271, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0522%\n",
      "layer   2  Sparsity: 68.5897%\n",
      "layer   3  Sparsity: 69.4994%\n",
      "total_backward_count 881100 real_backward_count 65837   7.472%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.115941/  1.388895, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0499%\n",
      "layer   2  Sparsity: 68.5427%\n",
      "layer   3  Sparsity: 70.1388%\n",
      "total_backward_count 890890 real_backward_count 66254   7.437%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.134561/  1.377750, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0375%\n",
      "layer   2  Sparsity: 68.5397%\n",
      "layer   3  Sparsity: 69.7854%\n",
      "total_backward_count 900680 real_backward_count 66651   7.400%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.122876/  1.347270, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0581%\n",
      "layer   2  Sparsity: 68.7420%\n",
      "layer   3  Sparsity: 69.3813%\n",
      "total_backward_count 910470 real_backward_count 67069   7.366%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.116630/  1.386027, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0600%\n",
      "layer   2  Sparsity: 68.6622%\n",
      "layer   3  Sparsity: 69.0659%\n",
      "total_backward_count 920260 real_backward_count 67475   7.332%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.113725/  1.362243, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0584%\n",
      "layer   2  Sparsity: 68.9960%\n",
      "layer   3  Sparsity: 69.4495%\n",
      "total_backward_count 930050 real_backward_count 67895   7.300%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.101432/  1.351834, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0591%\n",
      "layer   2  Sparsity: 68.7064%\n",
      "layer   3  Sparsity: 69.3625%\n",
      "total_backward_count 939840 real_backward_count 68345   7.272%\n",
      "fc layer 3 self.abs_max_out: 855.0\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.092739/  1.347489, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0346%\n",
      "layer   2  Sparsity: 68.6979%\n",
      "layer   3  Sparsity: 69.8833%\n",
      "total_backward_count 949630 real_backward_count 68777   7.243%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.095930/  1.334956, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0559%\n",
      "layer   2  Sparsity: 68.9452%\n",
      "layer   3  Sparsity: 70.2856%\n",
      "total_backward_count 959420 real_backward_count 69201   7.213%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.086456/  1.371832, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0563%\n",
      "layer   2  Sparsity: 68.7315%\n",
      "layer   3  Sparsity: 70.3537%\n",
      "total_backward_count 969210 real_backward_count 69585   7.180%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.091078/  1.329424, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0618%\n",
      "layer   2  Sparsity: 68.4703%\n",
      "layer   3  Sparsity: 70.0015%\n",
      "total_backward_count 979000 real_backward_count 70001   7.150%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.071374/  1.341130, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0460%\n",
      "layer   2  Sparsity: 68.6183%\n",
      "layer   3  Sparsity: 70.1252%\n",
      "total_backward_count 988790 real_backward_count 70370   7.117%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.089429/  1.335402, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0444%\n",
      "layer   2  Sparsity: 68.7074%\n",
      "layer   3  Sparsity: 70.1834%\n",
      "total_backward_count 998580 real_backward_count 70759   7.086%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.088993/  1.311945, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0670%\n",
      "layer   2  Sparsity: 68.8304%\n",
      "layer   3  Sparsity: 70.3939%\n",
      "total_backward_count 1008370 real_backward_count 71132   7.054%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.062280/  1.334194, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0329%\n",
      "layer   2  Sparsity: 68.8515%\n",
      "layer   3  Sparsity: 70.8342%\n",
      "total_backward_count 1018160 real_backward_count 71456   7.018%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.058349/  1.293017, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0571%\n",
      "layer   2  Sparsity: 69.0719%\n",
      "layer   3  Sparsity: 70.5420%\n",
      "total_backward_count 1027950 real_backward_count 71862   6.991%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.054631/  1.312383, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0441%\n",
      "layer   2  Sparsity: 69.1226%\n",
      "layer   3  Sparsity: 70.3315%\n",
      "total_backward_count 1037740 real_backward_count 72237   6.961%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.060015/  1.308360, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0548%\n",
      "layer   2  Sparsity: 69.1389%\n",
      "layer   3  Sparsity: 70.3033%\n",
      "total_backward_count 1047530 real_backward_count 72627   6.933%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.056558/  1.285388, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0432%\n",
      "layer   2  Sparsity: 68.9674%\n",
      "layer   3  Sparsity: 69.7567%\n",
      "total_backward_count 1057320 real_backward_count 73036   6.908%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.060100/  1.297257, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0457%\n",
      "layer   2  Sparsity: 68.9514%\n",
      "layer   3  Sparsity: 70.2555%\n",
      "total_backward_count 1067110 real_backward_count 73409   6.879%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.052855/  1.288739, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0406%\n",
      "layer   2  Sparsity: 69.0098%\n",
      "layer   3  Sparsity: 70.2743%\n",
      "total_backward_count 1076900 real_backward_count 73781   6.851%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.054967/  1.292799, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0527%\n",
      "layer   2  Sparsity: 69.1044%\n",
      "layer   3  Sparsity: 70.2484%\n",
      "total_backward_count 1086690 real_backward_count 74165   6.825%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.067039/  1.315262, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0546%\n",
      "layer   2  Sparsity: 68.9365%\n",
      "layer   3  Sparsity: 70.2812%\n",
      "total_backward_count 1096480 real_backward_count 74545   6.799%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.062871/  1.306988, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0616%\n",
      "layer   2  Sparsity: 68.7521%\n",
      "layer   3  Sparsity: 70.1104%\n",
      "total_backward_count 1106270 real_backward_count 74921   6.772%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.056863/  1.305490, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0266%\n",
      "layer   2  Sparsity: 68.6795%\n",
      "layer   3  Sparsity: 69.8593%\n",
      "total_backward_count 1116060 real_backward_count 75291   6.746%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.050202/  1.267364, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0630%\n",
      "layer   2  Sparsity: 68.5598%\n",
      "layer   3  Sparsity: 70.0089%\n",
      "total_backward_count 1125850 real_backward_count 75619   6.717%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.037375/  1.275329, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0512%\n",
      "layer   2  Sparsity: 68.4762%\n",
      "layer   3  Sparsity: 69.5228%\n",
      "total_backward_count 1135640 real_backward_count 75995   6.692%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.039561/  1.292470, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0560%\n",
      "layer   2  Sparsity: 68.4702%\n",
      "layer   3  Sparsity: 69.4769%\n",
      "total_backward_count 1145430 real_backward_count 76326   6.664%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.038394/  1.292437, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0550%\n",
      "layer   2  Sparsity: 68.5153%\n",
      "layer   3  Sparsity: 69.8016%\n",
      "total_backward_count 1155220 real_backward_count 76671   6.637%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.027623/  1.329575, val:  72.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0372%\n",
      "layer   2  Sparsity: 68.4746%\n",
      "layer   3  Sparsity: 69.7818%\n",
      "total_backward_count 1165010 real_backward_count 77005   6.610%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.038496/  1.268829, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0424%\n",
      "layer   2  Sparsity: 68.4089%\n",
      "layer   3  Sparsity: 70.0312%\n",
      "total_backward_count 1174800 real_backward_count 77343   6.584%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.049399/  1.310637, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0380%\n",
      "layer   2  Sparsity: 68.6498%\n",
      "layer   3  Sparsity: 70.6286%\n",
      "total_backward_count 1184590 real_backward_count 77711   6.560%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.027712/  1.284304, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0443%\n",
      "layer   2  Sparsity: 68.7332%\n",
      "layer   3  Sparsity: 70.7577%\n",
      "total_backward_count 1194380 real_backward_count 77998   6.530%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.029353/  1.281247, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0393%\n",
      "layer   2  Sparsity: 68.7741%\n",
      "layer   3  Sparsity: 70.5351%\n",
      "total_backward_count 1204170 real_backward_count 78363   6.508%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.020166/  1.288635, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0530%\n",
      "layer   2  Sparsity: 68.7635%\n",
      "layer   3  Sparsity: 70.5798%\n",
      "total_backward_count 1213960 real_backward_count 78683   6.482%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.026876/  1.267306, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0567%\n",
      "layer   2  Sparsity: 68.5891%\n",
      "layer   3  Sparsity: 70.8849%\n",
      "total_backward_count 1223750 real_backward_count 78996   6.455%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.045648/  1.287446, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0383%\n",
      "layer   2  Sparsity: 68.8977%\n",
      "layer   3  Sparsity: 71.0066%\n",
      "total_backward_count 1233540 real_backward_count 79296   6.428%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.033573/  1.283851, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0575%\n",
      "layer   2  Sparsity: 68.9994%\n",
      "layer   3  Sparsity: 71.1425%\n",
      "total_backward_count 1243330 real_backward_count 79596   6.402%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.039460/  1.292311, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0533%\n",
      "layer   2  Sparsity: 68.9182%\n",
      "layer   3  Sparsity: 71.6790%\n",
      "total_backward_count 1253120 real_backward_count 79903   6.376%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.016693/  1.262026, val:  85.00%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0314%\n",
      "layer   2  Sparsity: 68.9902%\n",
      "layer   3  Sparsity: 72.0544%\n",
      "total_backward_count 1262910 real_backward_count 80209   6.351%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.024133/  1.285183, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0584%\n",
      "layer   2  Sparsity: 68.7838%\n",
      "layer   3  Sparsity: 71.7166%\n",
      "total_backward_count 1272700 real_backward_count 80530   6.327%\n",
      "fc layer 1 self.abs_max_out: 4010.0\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.044526/  1.303281, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0324%\n",
      "layer   2  Sparsity: 68.7921%\n",
      "layer   3  Sparsity: 71.7318%\n",
      "total_backward_count 1282490 real_backward_count 80908   6.309%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.047347/  1.268719, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0431%\n",
      "layer   2  Sparsity: 68.6980%\n",
      "layer   3  Sparsity: 71.4139%\n",
      "total_backward_count 1292280 real_backward_count 81254   6.288%\n",
      "fc layer 3 self.abs_max_out: 874.0\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.034283/  1.287075, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0550%\n",
      "layer   2  Sparsity: 68.7948%\n",
      "layer   3  Sparsity: 71.4180%\n",
      "total_backward_count 1302070 real_backward_count 81568   6.264%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.035386/  1.297447, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0554%\n",
      "layer   2  Sparsity: 68.9541%\n",
      "layer   3  Sparsity: 71.5503%\n",
      "total_backward_count 1311860 real_backward_count 81865   6.240%\n",
      "lif layer 1 self.abs_max_v: 7186.0\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.057176/  1.279217, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0327%\n",
      "layer   2  Sparsity: 68.7161%\n",
      "layer   3  Sparsity: 70.9059%\n",
      "total_backward_count 1321650 real_backward_count 82174   6.218%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.049802/  1.284408, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0364%\n",
      "layer   2  Sparsity: 68.7479%\n",
      "layer   3  Sparsity: 71.1664%\n",
      "total_backward_count 1331440 real_backward_count 82473   6.194%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.038331/  1.289652, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0477%\n",
      "layer   2  Sparsity: 68.7688%\n",
      "layer   3  Sparsity: 70.9514%\n",
      "total_backward_count 1341230 real_backward_count 82769   6.171%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.042952/  1.287898, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0606%\n",
      "layer   2  Sparsity: 68.6989%\n",
      "layer   3  Sparsity: 70.7342%\n",
      "total_backward_count 1351020 real_backward_count 83105   6.151%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.040066/  1.291353, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0432%\n",
      "layer   2  Sparsity: 68.6799%\n",
      "layer   3  Sparsity: 71.0767%\n",
      "total_backward_count 1360810 real_backward_count 83422   6.130%\n",
      "fc layer 1 self.abs_max_out: 4044.0\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.028018/  1.296728, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0499%\n",
      "layer   2  Sparsity: 68.7830%\n",
      "layer   3  Sparsity: 70.9475%\n",
      "total_backward_count 1370600 real_backward_count 83736   6.109%\n",
      "fc layer 1 self.abs_max_out: 4050.0\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.027467/  1.271243, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0325%\n",
      "layer   2  Sparsity: 68.7966%\n",
      "layer   3  Sparsity: 71.5320%\n",
      "total_backward_count 1380390 real_backward_count 84034   6.088%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.028498/  1.269772, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0600%\n",
      "layer   2  Sparsity: 68.7757%\n",
      "layer   3  Sparsity: 71.6234%\n",
      "total_backward_count 1390180 real_backward_count 84314   6.065%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.020150/  1.258918, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0387%\n",
      "layer   2  Sparsity: 68.7760%\n",
      "layer   3  Sparsity: 71.2725%\n",
      "total_backward_count 1399970 real_backward_count 84615   6.044%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.007850/  1.235799, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0332%\n",
      "layer   2  Sparsity: 68.7849%\n",
      "layer   3  Sparsity: 71.6009%\n",
      "total_backward_count 1409760 real_backward_count 84887   6.021%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.011837/  1.274390, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0563%\n",
      "layer   2  Sparsity: 68.8473%\n",
      "layer   3  Sparsity: 71.1774%\n",
      "total_backward_count 1419550 real_backward_count 85209   6.003%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.014005/  1.264484, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0530%\n",
      "layer   2  Sparsity: 68.9651%\n",
      "layer   3  Sparsity: 71.7564%\n",
      "total_backward_count 1429340 real_backward_count 85506   5.982%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.007762/  1.250557, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0556%\n",
      "layer   2  Sparsity: 68.9744%\n",
      "layer   3  Sparsity: 71.7183%\n",
      "total_backward_count 1439130 real_backward_count 85790   5.961%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.006976/  1.251823, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0518%\n",
      "layer   2  Sparsity: 68.8283%\n",
      "layer   3  Sparsity: 71.4366%\n",
      "total_backward_count 1448920 real_backward_count 86074   5.941%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.005192/  1.256915, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0524%\n",
      "layer   2  Sparsity: 68.8410%\n",
      "layer   3  Sparsity: 71.2917%\n",
      "total_backward_count 1458710 real_backward_count 86357   5.920%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.004076/  1.280637, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0549%\n",
      "layer   2  Sparsity: 68.9326%\n",
      "layer   3  Sparsity: 71.6111%\n",
      "total_backward_count 1468500 real_backward_count 86636   5.900%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.998313/  1.259101, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0685%\n",
      "layer   2  Sparsity: 68.7602%\n",
      "layer   3  Sparsity: 71.2318%\n",
      "total_backward_count 1478290 real_backward_count 86936   5.881%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.013570/  1.241306, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0533%\n",
      "layer   2  Sparsity: 68.9954%\n",
      "layer   3  Sparsity: 70.8873%\n",
      "total_backward_count 1488080 real_backward_count 87193   5.859%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.001885/  1.267357, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0552%\n",
      "layer   2  Sparsity: 69.0313%\n",
      "layer   3  Sparsity: 70.9267%\n",
      "total_backward_count 1497870 real_backward_count 87454   5.839%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.009426/  1.224326, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0507%\n",
      "layer   2  Sparsity: 68.7873%\n",
      "layer   3  Sparsity: 70.8134%\n",
      "total_backward_count 1507660 real_backward_count 87724   5.819%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.996109/  1.261856, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0411%\n",
      "layer   2  Sparsity: 68.7411%\n",
      "layer   3  Sparsity: 70.8266%\n",
      "total_backward_count 1517450 real_backward_count 87974   5.797%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.999384/  1.240347, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0541%\n",
      "layer   2  Sparsity: 69.0676%\n",
      "layer   3  Sparsity: 70.9441%\n",
      "total_backward_count 1527240 real_backward_count 88246   5.778%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.996075/  1.247128, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0521%\n",
      "layer   2  Sparsity: 68.8725%\n",
      "layer   3  Sparsity: 71.1266%\n",
      "total_backward_count 1537030 real_backward_count 88537   5.760%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.994435/  1.235177, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0383%\n",
      "layer   2  Sparsity: 68.9480%\n",
      "layer   3  Sparsity: 71.0465%\n",
      "total_backward_count 1546820 real_backward_count 88785   5.740%\n",
      "fc layer 1 self.abs_max_out: 4148.0\n",
      "lif layer 1 self.abs_max_v: 7256.0\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.990456/  1.233149, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0569%\n",
      "layer   2  Sparsity: 68.9611%\n",
      "layer   3  Sparsity: 70.8810%\n",
      "total_backward_count 1556610 real_backward_count 89043   5.720%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.994998/  1.261854, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0522%\n",
      "layer   2  Sparsity: 68.9000%\n",
      "layer   3  Sparsity: 70.9757%\n",
      "total_backward_count 1566400 real_backward_count 89270   5.699%\n",
      "fc layer 3 self.abs_max_out: 888.0\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.994010/  1.225514, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0639%\n",
      "layer   2  Sparsity: 68.8362%\n",
      "layer   3  Sparsity: 70.8853%\n",
      "total_backward_count 1576190 real_backward_count 89534   5.680%\n",
      "fc layer 3 self.abs_max_out: 898.0\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.981171/  1.209818, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0468%\n",
      "layer   2  Sparsity: 68.8119%\n",
      "layer   3  Sparsity: 71.1126%\n",
      "total_backward_count 1585980 real_backward_count 89799   5.662%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.971334/  1.224834, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0464%\n",
      "layer   2  Sparsity: 68.8059%\n",
      "layer   3  Sparsity: 71.1213%\n",
      "total_backward_count 1595770 real_backward_count 90060   5.644%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.967592/  1.235489, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0571%\n",
      "layer   2  Sparsity: 69.0123%\n",
      "layer   3  Sparsity: 70.8315%\n",
      "total_backward_count 1605560 real_backward_count 90291   5.624%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.986445/  1.219791, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0455%\n",
      "layer   2  Sparsity: 69.1330%\n",
      "layer   3  Sparsity: 70.7378%\n",
      "total_backward_count 1615350 real_backward_count 90548   5.605%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.982895/  1.226046, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0555%\n",
      "layer   2  Sparsity: 69.0594%\n",
      "layer   3  Sparsity: 70.8664%\n",
      "total_backward_count 1625140 real_backward_count 90835   5.589%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.964812/  1.243347, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0512%\n",
      "layer   2  Sparsity: 68.8926%\n",
      "layer   3  Sparsity: 70.8604%\n",
      "total_backward_count 1634930 real_backward_count 91062   5.570%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.965398/  1.202829, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0388%\n",
      "layer   2  Sparsity: 68.7540%\n",
      "layer   3  Sparsity: 70.6700%\n",
      "total_backward_count 1644720 real_backward_count 91332   5.553%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.963631/  1.238962, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0625%\n",
      "layer   2  Sparsity: 68.9928%\n",
      "layer   3  Sparsity: 71.2674%\n",
      "total_backward_count 1654510 real_backward_count 91578   5.535%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.970667/  1.207808, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0510%\n",
      "layer   2  Sparsity: 68.9978%\n",
      "layer   3  Sparsity: 71.8231%\n",
      "total_backward_count 1664300 real_backward_count 91828   5.518%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.953362/  1.204487, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0468%\n",
      "layer   2  Sparsity: 69.0093%\n",
      "layer   3  Sparsity: 71.6612%\n",
      "total_backward_count 1674090 real_backward_count 92087   5.501%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.953546/  1.230293, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 95.0336%\n",
      "layer   2  Sparsity: 69.0478%\n",
      "layer   3  Sparsity: 71.7237%\n",
      "total_backward_count 1683880 real_backward_count 92337   5.484%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.964968/  1.242575, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0547%\n",
      "layer   2  Sparsity: 68.9807%\n",
      "layer   3  Sparsity: 71.6222%\n",
      "total_backward_count 1693670 real_backward_count 92597   5.467%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.957670/  1.208492, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0444%\n",
      "layer   2  Sparsity: 68.9589%\n",
      "layer   3  Sparsity: 71.4514%\n",
      "total_backward_count 1703460 real_backward_count 92818   5.449%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.955187/  1.210573, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0487%\n",
      "layer   2  Sparsity: 68.9475%\n",
      "layer   3  Sparsity: 71.2595%\n",
      "total_backward_count 1713250 real_backward_count 93055   5.431%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.954678/  1.217780, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0347%\n",
      "layer   2  Sparsity: 68.7916%\n",
      "layer   3  Sparsity: 71.1178%\n",
      "total_backward_count 1723040 real_backward_count 93319   5.416%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.953997/  1.229786, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0463%\n",
      "layer   2  Sparsity: 68.9536%\n",
      "layer   3  Sparsity: 71.4219%\n",
      "total_backward_count 1732830 real_backward_count 93553   5.399%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.964087/  1.210566, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0467%\n",
      "layer   2  Sparsity: 68.8674%\n",
      "layer   3  Sparsity: 71.1612%\n",
      "total_backward_count 1742620 real_backward_count 93760   5.380%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.961965/  1.227277, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0483%\n",
      "layer   2  Sparsity: 68.9160%\n",
      "layer   3  Sparsity: 70.9490%\n",
      "total_backward_count 1752410 real_backward_count 94000   5.364%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.958419/  1.207537, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0570%\n",
      "layer   2  Sparsity: 69.1116%\n",
      "layer   3  Sparsity: 70.9555%\n",
      "total_backward_count 1762200 real_backward_count 94231   5.347%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.952698/  1.224424, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0431%\n",
      "layer   2  Sparsity: 69.0873%\n",
      "layer   3  Sparsity: 71.0443%\n",
      "total_backward_count 1771990 real_backward_count 94463   5.331%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.947881/  1.216892, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0507%\n",
      "layer   2  Sparsity: 69.2922%\n",
      "layer   3  Sparsity: 71.1108%\n",
      "total_backward_count 1781780 real_backward_count 94701   5.315%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.945242/  1.200717, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0297%\n",
      "layer   2  Sparsity: 69.1213%\n",
      "layer   3  Sparsity: 71.1215%\n",
      "total_backward_count 1791570 real_backward_count 94911   5.298%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.926937/  1.198723, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0505%\n",
      "layer   2  Sparsity: 69.1296%\n",
      "layer   3  Sparsity: 71.2995%\n",
      "total_backward_count 1801360 real_backward_count 95134   5.281%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.934359/  1.199884, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0506%\n",
      "layer   2  Sparsity: 69.1425%\n",
      "layer   3  Sparsity: 71.2788%\n",
      "total_backward_count 1811150 real_backward_count 95380   5.266%\n",
      "fc layer 3 self.abs_max_out: 902.0\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.938931/  1.203261, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0351%\n",
      "layer   2  Sparsity: 69.1360%\n",
      "layer   3  Sparsity: 71.6062%\n",
      "total_backward_count 1820940 real_backward_count 95578   5.249%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.933399/  1.186513, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0428%\n",
      "layer   2  Sparsity: 69.1534%\n",
      "layer   3  Sparsity: 72.1616%\n",
      "total_backward_count 1830730 real_backward_count 95768   5.231%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.940552/  1.195227, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0493%\n",
      "layer   2  Sparsity: 69.0457%\n",
      "layer   3  Sparsity: 72.0282%\n",
      "total_backward_count 1840520 real_backward_count 95957   5.214%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.936836/  1.179716, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0445%\n",
      "layer   2  Sparsity: 68.9913%\n",
      "layer   3  Sparsity: 72.2020%\n",
      "total_backward_count 1850310 real_backward_count 96144   5.196%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.940940/  1.187891, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0357%\n",
      "layer   2  Sparsity: 68.9970%\n",
      "layer   3  Sparsity: 72.1997%\n",
      "total_backward_count 1860100 real_backward_count 96344   5.180%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.931507/  1.171374, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0450%\n",
      "layer   2  Sparsity: 68.9771%\n",
      "layer   3  Sparsity: 72.0349%\n",
      "total_backward_count 1869890 real_backward_count 96538   5.163%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.943206/  1.195833, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0533%\n",
      "layer   2  Sparsity: 69.0299%\n",
      "layer   3  Sparsity: 71.7744%\n",
      "total_backward_count 1879680 real_backward_count 96762   5.148%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.938869/  1.181262, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0371%\n",
      "layer   2  Sparsity: 68.9281%\n",
      "layer   3  Sparsity: 71.5710%\n",
      "total_backward_count 1889470 real_backward_count 96979   5.133%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.927628/  1.177667, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0565%\n",
      "layer   2  Sparsity: 68.9891%\n",
      "layer   3  Sparsity: 71.7555%\n",
      "total_backward_count 1899260 real_backward_count 97222   5.119%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.917758/  1.184395, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0534%\n",
      "layer   2  Sparsity: 68.9126%\n",
      "layer   3  Sparsity: 71.5620%\n",
      "total_backward_count 1909050 real_backward_count 97450   5.105%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.919957/  1.181341, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0281%\n",
      "layer   2  Sparsity: 68.9388%\n",
      "layer   3  Sparsity: 71.4844%\n",
      "total_backward_count 1918840 real_backward_count 97661   5.090%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.933414/  1.172158, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0518%\n",
      "layer   2  Sparsity: 68.9733%\n",
      "layer   3  Sparsity: 71.4163%\n",
      "total_backward_count 1928630 real_backward_count 97884   5.075%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.926012/  1.175114, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0743%\n",
      "layer   2  Sparsity: 68.9218%\n",
      "layer   3  Sparsity: 71.6126%\n",
      "total_backward_count 1938420 real_backward_count 98097   5.061%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.930419/  1.207562, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0425%\n",
      "layer   2  Sparsity: 68.6811%\n",
      "layer   3  Sparsity: 71.9204%\n",
      "total_backward_count 1948210 real_backward_count 98327   5.047%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.934263/  1.183259, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0540%\n",
      "layer   2  Sparsity: 68.7932%\n",
      "layer   3  Sparsity: 71.6449%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc277211045f451bad246e03cd7cecec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.93426</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>1.18326</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-337</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/83q04p62' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/83q04p62</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251128_001523-83q04p62/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pynusdwa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_043959-pynusdwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pynusdwa' target=\"_blank\">wobbly-sweep-340</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pynusdwa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pynusdwa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251128_044007_566', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 117.0\n",
      "lif layer 1 self.abs_max_v: 117.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 233.0\n",
      "lif layer 2 self.abs_max_v: 233.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 1 self.abs_max_out: 154.0\n",
      "lif layer 1 self.abs_max_v: 189.0\n",
      "fc layer 2 self.abs_max_out: 467.0\n",
      "lif layer 2 self.abs_max_v: 536.5\n",
      "fc layer 1 self.abs_max_out: 163.0\n",
      "lif layer 1 self.abs_max_v: 250.5\n",
      "lif layer 2 self.abs_max_v: 539.5\n",
      "fc layer 3 self.abs_max_out: 157.0\n",
      "fc layer 1 self.abs_max_out: 191.0\n",
      "fc layer 1 self.abs_max_out: 278.0\n",
      "lif layer 1 self.abs_max_v: 278.0\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "fc layer 1 self.abs_max_out: 332.0\n",
      "lif layer 1 self.abs_max_v: 332.0\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "lif layer 1 self.abs_max_v: 438.5\n",
      "lif layer 2 self.abs_max_v: 576.0\n",
      "lif layer 1 self.abs_max_v: 453.5\n",
      "lif layer 2 self.abs_max_v: 721.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "fc layer 1 self.abs_max_out: 493.0\n",
      "lif layer 1 self.abs_max_v: 509.5\n",
      "lif layer 2 self.abs_max_v: 721.5\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "lif layer 2 self.abs_max_v: 725.0\n",
      "fc layer 1 self.abs_max_out: 555.0\n",
      "lif layer 1 self.abs_max_v: 555.0\n",
      "lif layer 1 self.abs_max_v: 559.5\n",
      "lif layer 1 self.abs_max_v: 597.5\n",
      "fc layer 1 self.abs_max_out: 704.0\n",
      "lif layer 1 self.abs_max_v: 893.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "lif layer 2 self.abs_max_v: 863.5\n",
      "lif layer 2 self.abs_max_v: 911.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "lif layer 2 self.abs_max_v: 912.0\n",
      "lif layer 2 self.abs_max_v: 960.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "lif layer 2 self.abs_max_v: 997.0\n",
      "fc layer 2 self.abs_max_out: 659.0\n",
      "fc layer 2 self.abs_max_out: 743.0\n",
      "fc layer 1 self.abs_max_out: 795.0\n",
      "lif layer 1 self.abs_max_v: 1049.0\n",
      "lif layer 2 self.abs_max_v: 1055.5\n",
      "fc layer 1 self.abs_max_out: 851.0\n",
      "lif layer 2 self.abs_max_v: 1063.5\n",
      "lif layer 2 self.abs_max_v: 1184.0\n",
      "fc layer 1 self.abs_max_out: 861.0\n",
      "lif layer 1 self.abs_max_v: 1075.5\n",
      "lif layer 1 self.abs_max_v: 1163.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "fc layer 1 self.abs_max_out: 988.0\n",
      "fc layer 2 self.abs_max_out: 900.0\n",
      "lif layer 2 self.abs_max_v: 1243.5\n",
      "lif layer 2 self.abs_max_v: 1335.0\n",
      "lif layer 2 self.abs_max_v: 1342.5\n",
      "lif layer 2 self.abs_max_v: 1449.5\n",
      "lif layer 2 self.abs_max_v: 1564.0\n",
      "fc layer 2 self.abs_max_out: 902.0\n",
      "fc layer 2 self.abs_max_out: 1101.0\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "lif layer 1 self.abs_max_v: 1165.5\n",
      "lif layer 1 self.abs_max_v: 1223.5\n",
      "fc layer 1 self.abs_max_out: 1072.0\n",
      "lif layer 2 self.abs_max_v: 1577.0\n",
      "lif layer 1 self.abs_max_v: 1268.5\n",
      "lif layer 1 self.abs_max_v: 1337.5\n",
      "lif layer 1 self.abs_max_v: 1359.5\n",
      "lif layer 1 self.abs_max_v: 1361.5\n",
      "lif layer 1 self.abs_max_v: 1367.0\n",
      "lif layer 1 self.abs_max_v: 1395.5\n",
      "lif layer 1 self.abs_max_v: 1469.0\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "fc layer 2 self.abs_max_out: 1119.0\n",
      "lif layer 2 self.abs_max_v: 1585.5\n",
      "lif layer 2 self.abs_max_v: 1597.0\n",
      "lif layer 2 self.abs_max_v: 1711.5\n",
      "lif layer 1 self.abs_max_v: 1480.5\n",
      "lif layer 1 self.abs_max_v: 1767.5\n",
      "fc layer 1 self.abs_max_out: 1137.0\n",
      "lif layer 2 self.abs_max_v: 1724.0\n",
      "lif layer 1 self.abs_max_v: 1770.5\n",
      "fc layer 1 self.abs_max_out: 1162.0\n",
      "lif layer 2 self.abs_max_v: 1769.5\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "lif layer 2 self.abs_max_v: 1869.5\n",
      "lif layer 2 self.abs_max_v: 1991.0\n",
      "fc layer 2 self.abs_max_out: 1132.0\n",
      "lif layer 2 self.abs_max_v: 2008.0\n",
      "lif layer 2 self.abs_max_v: 2044.0\n",
      "fc layer 1 self.abs_max_out: 1326.0\n",
      "lif layer 1 self.abs_max_v: 1881.5\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 2 self.abs_max_out: 1172.0\n",
      "fc layer 2 self.abs_max_out: 1193.0\n",
      "fc layer 2 self.abs_max_out: 1213.0\n",
      "lif layer 2 self.abs_max_v: 2175.5\n",
      "fc layer 2 self.abs_max_out: 1229.0\n",
      "lif layer 2 self.abs_max_v: 2317.0\n",
      "fc layer 2 self.abs_max_out: 1256.0\n",
      "lif layer 2 self.abs_max_v: 2414.5\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "fc layer 1 self.abs_max_out: 1373.0\n",
      "lif layer 1 self.abs_max_v: 2099.5\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "lif layer 2 self.abs_max_v: 2420.0\n",
      "fc layer 2 self.abs_max_out: 1398.0\n",
      "lif layer 1 self.abs_max_v: 2108.0\n",
      "lif layer 1 self.abs_max_v: 2243.0\n",
      "fc layer 1 self.abs_max_out: 1690.0\n",
      "fc layer 2 self.abs_max_out: 1432.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "fc layer 2 self.abs_max_out: 1475.0\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "lif layer 1 self.abs_max_v: 2247.5\n",
      "fc layer 2 self.abs_max_out: 1522.0\n",
      "fc layer 2 self.abs_max_out: 1529.0\n",
      "fc layer 2 self.abs_max_out: 1570.0\n",
      "fc layer 1 self.abs_max_out: 1692.0\n",
      "lif layer 1 self.abs_max_v: 2358.0\n",
      "lif layer 1 self.abs_max_v: 2691.0\n",
      "fc layer 1 self.abs_max_out: 1695.0\n",
      "lif layer 1 self.abs_max_v: 3040.5\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "fc layer 1 self.abs_max_out: 1837.0\n",
      "lif layer 2 self.abs_max_v: 2504.5\n",
      "lif layer 2 self.abs_max_v: 2534.5\n",
      "lif layer 2 self.abs_max_v: 2548.5\n",
      "lif layer 2 self.abs_max_v: 2586.5\n",
      "lif layer 2 self.abs_max_v: 2614.5\n",
      "lif layer 2 self.abs_max_v: 2646.5\n",
      "lif layer 2 self.abs_max_v: 2720.5\n",
      "lif layer 2 self.abs_max_v: 2819.5\n",
      "lif layer 2 self.abs_max_v: 2904.0\n",
      "lif layer 2 self.abs_max_v: 2998.0\n",
      "lif layer 2 self.abs_max_v: 3050.0\n",
      "fc layer 2 self.abs_max_out: 1578.0\n",
      "lif layer 2 self.abs_max_v: 3103.0\n",
      "fc layer 2 self.abs_max_out: 1582.0\n",
      "lif layer 2 self.abs_max_v: 3133.5\n",
      "fc layer 1 self.abs_max_out: 1938.0\n",
      "fc layer 2 self.abs_max_out: 1654.0\n",
      "fc layer 2 self.abs_max_out: 1684.0\n",
      "fc layer 2 self.abs_max_out: 1707.0\n",
      "lif layer 2 self.abs_max_v: 3145.5\n",
      "lif layer 2 self.abs_max_v: 3216.5\n",
      "lif layer 2 self.abs_max_v: 3269.5\n",
      "lif layer 2 self.abs_max_v: 3289.0\n",
      "lif layer 2 self.abs_max_v: 3292.5\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "fc layer 3 self.abs_max_out: 651.0\n",
      "fc layer 2 self.abs_max_out: 1776.0\n",
      "fc layer 1 self.abs_max_out: 1953.0\n",
      "lif layer 1 self.abs_max_v: 3451.0\n",
      "lif layer 1 self.abs_max_v: 3641.5\n",
      "lif layer 1 self.abs_max_v: 3660.0\n",
      "lif layer 1 self.abs_max_v: 3702.0\n",
      "fc layer 1 self.abs_max_out: 2134.0\n",
      "fc layer 2 self.abs_max_out: 1873.0\n",
      "lif layer 2 self.abs_max_v: 3321.5\n",
      "lif layer 2 self.abs_max_v: 3323.5\n",
      "lif layer 2 self.abs_max_v: 3365.5\n",
      "lif layer 2 self.abs_max_v: 3403.5\n",
      "lif layer 2 self.abs_max_v: 3504.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.425472/  1.926062, val:  28.75%, val_best:  28.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.0434%\n",
      "layer   2  Sparsity: 69.9523%\n",
      "layer   3  Sparsity: 66.8279%\n",
      "total_backward_count 9790 real_backward_count 1530  15.628%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3555.5\n",
      "lif layer 2 self.abs_max_v: 3579.0\n",
      "lif layer 2 self.abs_max_v: 3589.0\n",
      "fc layer 3 self.abs_max_out: 662.0\n",
      "fc layer 3 self.abs_max_out: 667.0\n",
      "fc layer 2 self.abs_max_out: 1911.0\n",
      "fc layer 2 self.abs_max_out: 1914.0\n",
      "lif layer 2 self.abs_max_v: 3658.5\n",
      "lif layer 2 self.abs_max_v: 3705.5\n",
      "fc layer 2 self.abs_max_out: 1926.0\n",
      "lif layer 2 self.abs_max_v: 3736.0\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "fc layer 3 self.abs_max_out: 746.0\n",
      "fc layer 3 self.abs_max_out: 763.0\n",
      "lif layer 1 self.abs_max_v: 3878.0\n",
      "fc layer 3 self.abs_max_out: 766.0\n",
      "fc layer 2 self.abs_max_out: 1992.0\n",
      "fc layer 2 self.abs_max_out: 1995.0\n",
      "lif layer 2 self.abs_max_v: 3779.0\n",
      "fc layer 2 self.abs_max_out: 2106.0\n",
      "lif layer 2 self.abs_max_v: 3995.5\n",
      "lif layer 2 self.abs_max_v: 4058.0\n",
      "fc layer 1 self.abs_max_out: 2136.0\n",
      "fc layer 1 self.abs_max_out: 2358.0\n",
      "lif layer 1 self.abs_max_v: 4026.0\n",
      "lif layer 1 self.abs_max_v: 4136.5\n",
      "fc layer 1 self.abs_max_out: 2528.0\n",
      "lif layer 1 self.abs_max_v: 4490.5\n",
      "fc layer 2 self.abs_max_out: 2133.0\n",
      "fc layer 2 self.abs_max_out: 2150.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.263381/  1.775065, val:  36.67%, val_best:  36.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.0403%\n",
      "layer   2  Sparsity: 69.9699%\n",
      "layer   3  Sparsity: 65.8540%\n",
      "total_backward_count 19580 real_backward_count 2834  14.474%\n",
      "fc layer 3 self.abs_max_out: 769.0\n",
      "fc layer 3 self.abs_max_out: 770.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "fc layer 3 self.abs_max_out: 798.0\n",
      "fc layer 3 self.abs_max_out: 800.0\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "fc layer 3 self.abs_max_out: 871.0\n",
      "fc layer 2 self.abs_max_out: 2191.0\n",
      "fc layer 2 self.abs_max_out: 2203.0\n",
      "fc layer 2 self.abs_max_out: 2286.0\n",
      "fc layer 1 self.abs_max_out: 2576.0\n",
      "fc layer 1 self.abs_max_out: 2922.0\n",
      "lif layer 1 self.abs_max_v: 5121.5\n",
      "lif layer 1 self.abs_max_v: 5160.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.209576/  1.746788, val:  38.75%, val_best:  38.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0479%\n",
      "layer   2  Sparsity: 70.4815%\n",
      "layer   3  Sparsity: 65.1039%\n",
      "total_backward_count 29370 real_backward_count 4131  14.065%\n",
      "fc layer 2 self.abs_max_out: 2321.0\n",
      "fc layer 2 self.abs_max_out: 2326.0\n",
      "lif layer 1 self.abs_max_v: 5205.5\n",
      "lif layer 1 self.abs_max_v: 5350.5\n",
      "fc layer 1 self.abs_max_out: 3319.0\n",
      "lif layer 1 self.abs_max_v: 5994.5\n",
      "fc layer 2 self.abs_max_out: 2327.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.163971/  1.725805, val:  40.00%, val_best:  40.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0436%\n",
      "layer   2  Sparsity: 70.6144%\n",
      "layer   3  Sparsity: 65.2263%\n",
      "total_backward_count 39160 real_backward_count 5388  13.759%\n",
      "fc layer 3 self.abs_max_out: 881.0\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "fc layer 3 self.abs_max_out: 975.0\n",
      "fc layer 3 self.abs_max_out: 1059.0\n",
      "fc layer 3 self.abs_max_out: 1130.0\n",
      "fc layer 1 self.abs_max_out: 3669.0\n",
      "lif layer 1 self.abs_max_v: 6602.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.140383/  1.689574, val:  40.00%, val_best:  40.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0511%\n",
      "layer   2  Sparsity: 71.0420%\n",
      "layer   3  Sparsity: 65.4442%\n",
      "total_backward_count 48950 real_backward_count 6646  13.577%\n",
      "fc layer 2 self.abs_max_out: 2328.0\n",
      "lif layer 2 self.abs_max_v: 4137.5\n",
      "fc layer 2 self.abs_max_out: 2357.0\n",
      "lif layer 1 self.abs_max_v: 6604.5\n",
      "fc layer 2 self.abs_max_out: 2367.0\n",
      "fc layer 2 self.abs_max_out: 2390.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.082033/  1.658795, val:  46.25%, val_best:  46.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0379%\n",
      "layer   2  Sparsity: 71.6058%\n",
      "layer   3  Sparsity: 63.5842%\n",
      "total_backward_count 58740 real_backward_count 7892  13.435%\n",
      "lif layer 2 self.abs_max_v: 4160.0\n",
      "fc layer 2 self.abs_max_out: 2432.0\n",
      "lif layer 2 self.abs_max_v: 4256.0\n",
      "lif layer 2 self.abs_max_v: 4281.0\n",
      "lif layer 2 self.abs_max_v: 4336.0\n",
      "lif layer 2 self.abs_max_v: 4478.0\n",
      "lif layer 2 self.abs_max_v: 4522.0\n",
      "lif layer 2 self.abs_max_v: 4560.5\n",
      "lif layer 1 self.abs_max_v: 6685.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.049386/  1.595738, val:  48.75%, val_best:  48.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0645%\n",
      "layer   2  Sparsity: 71.1386%\n",
      "layer   3  Sparsity: 63.6378%\n",
      "total_backward_count 68530 real_backward_count 9077  13.245%\n",
      "fc layer 2 self.abs_max_out: 2441.0\n",
      "fc layer 2 self.abs_max_out: 2487.0\n",
      "fc layer 1 self.abs_max_out: 3879.0\n",
      "lif layer 1 self.abs_max_v: 6860.5\n",
      "fc layer 1 self.abs_max_out: 4297.0\n",
      "lif layer 1 self.abs_max_v: 7711.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.021202/  1.614352, val:  49.58%, val_best:  49.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0394%\n",
      "layer   2  Sparsity: 70.9052%\n",
      "layer   3  Sparsity: 63.5244%\n",
      "total_backward_count 78320 real_backward_count 10178  12.995%\n",
      "lif layer 2 self.abs_max_v: 4619.0\n",
      "fc layer 2 self.abs_max_out: 2532.0\n",
      "fc layer 2 self.abs_max_out: 2709.0\n",
      "lif layer 2 self.abs_max_v: 4625.5\n",
      "lif layer 2 self.abs_max_v: 4778.0\n",
      "lif layer 2 self.abs_max_v: 4850.0\n",
      "lif layer 2 self.abs_max_v: 4994.0\n",
      "fc layer 2 self.abs_max_out: 2785.0\n",
      "lif layer 2 self.abs_max_v: 5147.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  0.999397/  1.494253, val:  59.17%, val_best:  59.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0518%\n",
      "layer   2  Sparsity: 70.5549%\n",
      "layer   3  Sparsity: 63.6121%\n",
      "total_backward_count 88110 real_backward_count 11334  12.863%\n",
      "fc layer 3 self.abs_max_out: 1164.0\n",
      "fc layer 2 self.abs_max_out: 2820.0\n",
      "lif layer 2 self.abs_max_v: 5209.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  0.972019/  1.604713, val:  42.92%, val_best:  59.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0335%\n",
      "layer   2  Sparsity: 70.3738%\n",
      "layer   3  Sparsity: 64.7960%\n",
      "total_backward_count 97900 real_backward_count 12459  12.726%\n",
      "fc layer 2 self.abs_max_out: 2823.0\n",
      "fc layer 2 self.abs_max_out: 2938.0\n",
      "lif layer 2 self.abs_max_v: 5323.0\n",
      "lif layer 2 self.abs_max_v: 5432.5\n",
      "fc layer 1 self.abs_max_out: 4680.0\n",
      "lif layer 1 self.abs_max_v: 8322.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.982065/  1.595824, val:  46.25%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0345%\n",
      "layer   2  Sparsity: 70.5944%\n",
      "layer   3  Sparsity: 65.2824%\n",
      "total_backward_count 107690 real_backward_count 13573  12.604%\n",
      "fc layer 2 self.abs_max_out: 2955.0\n",
      "fc layer 2 self.abs_max_out: 2957.0\n",
      "fc layer 2 self.abs_max_out: 2970.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.007129/  1.458185, val:  60.42%, val_best:  60.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0427%\n",
      "layer   2  Sparsity: 69.5971%\n",
      "layer   3  Sparsity: 65.1123%\n",
      "total_backward_count 117480 real_backward_count 14644  12.465%\n",
      "fc layer 2 self.abs_max_out: 3254.0\n",
      "lif layer 1 self.abs_max_v: 8831.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.941257/  1.512576, val:  45.00%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0603%\n",
      "layer   2  Sparsity: 69.7374%\n",
      "layer   3  Sparsity: 65.2224%\n",
      "total_backward_count 127270 real_backward_count 15690  12.328%\n",
      "fc layer 3 self.abs_max_out: 1166.0\n",
      "lif layer 2 self.abs_max_v: 5554.5\n",
      "lif layer 2 self.abs_max_v: 5661.5\n",
      "lif layer 2 self.abs_max_v: 5728.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.931964/  1.477817, val:  47.92%, val_best:  60.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.0583%\n",
      "layer   2  Sparsity: 70.1441%\n",
      "layer   3  Sparsity: 66.3199%\n",
      "total_backward_count 137060 real_backward_count 16763  12.230%\n",
      "fc layer 1 self.abs_max_out: 4751.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.942490/  1.460655, val:  52.08%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0609%\n",
      "layer   2  Sparsity: 69.9990%\n",
      "layer   3  Sparsity: 67.4470%\n",
      "total_backward_count 146850 real_backward_count 17788  12.113%\n",
      "fc layer 1 self.abs_max_out: 5105.0\n",
      "lif layer 1 self.abs_max_v: 9287.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.934503/  1.404851, val:  57.50%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0316%\n",
      "layer   2  Sparsity: 70.0500%\n",
      "layer   3  Sparsity: 67.8763%\n",
      "total_backward_count 156640 real_backward_count 18847  12.032%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.913275/  1.366133, val:  61.25%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0432%\n",
      "layer   2  Sparsity: 70.1634%\n",
      "layer   3  Sparsity: 67.6539%\n",
      "total_backward_count 166430 real_backward_count 19848  11.926%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.907871/  1.462969, val:  49.17%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0468%\n",
      "layer   2  Sparsity: 69.4952%\n",
      "layer   3  Sparsity: 67.7971%\n",
      "total_backward_count 176220 real_backward_count 20899  11.860%\n",
      "fc layer 3 self.abs_max_out: 1179.0\n",
      "fc layer 3 self.abs_max_out: 1217.0\n",
      "fc layer 3 self.abs_max_out: 1219.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.935431/  1.423228, val:  55.42%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0372%\n",
      "layer   2  Sparsity: 69.7679%\n",
      "layer   3  Sparsity: 68.5900%\n",
      "total_backward_count 186010 real_backward_count 21891  11.769%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.900033/  1.484591, val:  40.00%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0420%\n",
      "layer   2  Sparsity: 69.3834%\n",
      "layer   3  Sparsity: 68.2320%\n",
      "total_backward_count 195800 real_backward_count 22835  11.662%\n",
      "fc layer 1 self.abs_max_out: 5127.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.900157/  1.461287, val:  52.92%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0358%\n",
      "layer   2  Sparsity: 69.2207%\n",
      "layer   3  Sparsity: 67.5619%\n",
      "total_backward_count 205590 real_backward_count 23794  11.574%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.915594/  1.376034, val:  61.25%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0233%\n",
      "layer   2  Sparsity: 69.1699%\n",
      "layer   3  Sparsity: 68.6137%\n",
      "total_backward_count 215380 real_backward_count 24771  11.501%\n",
      "lif layer 2 self.abs_max_v: 5788.5\n",
      "fc layer 1 self.abs_max_out: 5404.0\n",
      "lif layer 1 self.abs_max_v: 9559.5\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.894435/  1.368994, val:  66.67%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0309%\n",
      "layer   2  Sparsity: 69.0896%\n",
      "layer   3  Sparsity: 69.2521%\n",
      "total_backward_count 225170 real_backward_count 25751  11.436%\n",
      "fc layer 2 self.abs_max_out: 3362.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.891092/  1.304579, val:  66.67%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0526%\n",
      "layer   2  Sparsity: 69.2592%\n",
      "layer   3  Sparsity: 69.1726%\n",
      "total_backward_count 234960 real_backward_count 26720  11.372%\n",
      "lif layer 2 self.abs_max_v: 5829.5\n",
      "fc layer 1 self.abs_max_out: 5515.0\n",
      "lif layer 1 self.abs_max_v: 9764.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.858449/  1.243985, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0262%\n",
      "layer   2  Sparsity: 69.2683%\n",
      "layer   3  Sparsity: 68.6346%\n",
      "total_backward_count 244750 real_backward_count 27603  11.278%\n",
      "lif layer 1 self.abs_max_v: 9769.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.845026/  1.211339, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0348%\n",
      "layer   2  Sparsity: 69.4941%\n",
      "layer   3  Sparsity: 68.8126%\n",
      "total_backward_count 254540 real_backward_count 28514  11.202%\n",
      "fc layer 1 self.abs_max_out: 5628.0\n",
      "lif layer 1 self.abs_max_v: 10015.5\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.815495/  1.272772, val:  71.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0305%\n",
      "layer   2  Sparsity: 69.5027%\n",
      "layer   3  Sparsity: 68.5760%\n",
      "total_backward_count 264330 real_backward_count 29371  11.111%\n",
      "lif layer 2 self.abs_max_v: 5880.0\n",
      "fc layer 2 self.abs_max_out: 3383.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.850221/  1.249433, val:  80.42%, val_best:  80.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0537%\n",
      "layer   2  Sparsity: 69.4266%\n",
      "layer   3  Sparsity: 69.2096%\n",
      "total_backward_count 274120 real_backward_count 30285  11.048%\n",
      "fc layer 3 self.abs_max_out: 1227.0\n",
      "lif layer 2 self.abs_max_v: 6151.0\n",
      "fc layer 2 self.abs_max_out: 3386.0\n",
      "lif layer 2 self.abs_max_v: 6213.0\n",
      "fc layer 2 self.abs_max_out: 3447.0\n",
      "lif layer 2 self.abs_max_v: 6238.5\n",
      "lif layer 2 self.abs_max_v: 6356.0\n",
      "lif layer 2 self.abs_max_v: 6382.0\n",
      "fc layer 2 self.abs_max_out: 3475.0\n",
      "lif layer 2 self.abs_max_v: 6666.0\n",
      "fc layer 2 self.abs_max_out: 3647.0\n",
      "lif layer 2 self.abs_max_v: 6668.0\n",
      "lif layer 2 self.abs_max_v: 6844.0\n",
      "fc layer 2 self.abs_max_out: 3726.0\n",
      "lif layer 2 self.abs_max_v: 6858.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.814083/  1.292464, val:  65.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0554%\n",
      "layer   2  Sparsity: 69.5744%\n",
      "layer   3  Sparsity: 69.7482%\n",
      "total_backward_count 283910 real_backward_count 31114  10.959%\n",
      "fc layer 3 self.abs_max_out: 1276.0\n",
      "fc layer 1 self.abs_max_out: 5630.0\n",
      "lif layer 1 self.abs_max_v: 10109.5\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.835778/  1.328929, val:  52.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0548%\n",
      "layer   2  Sparsity: 69.3693%\n",
      "layer   3  Sparsity: 69.4219%\n",
      "total_backward_count 293700 real_backward_count 31979  10.888%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.827077/  1.279534, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0537%\n",
      "layer   2  Sparsity: 69.0050%\n",
      "layer   3  Sparsity: 69.4779%\n",
      "total_backward_count 303490 real_backward_count 32828  10.817%\n",
      "lif layer 1 self.abs_max_v: 10185.0\n",
      "fc layer 1 self.abs_max_out: 5955.0\n",
      "lif layer 1 self.abs_max_v: 11047.5\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.808020/  1.254783, val:  70.83%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0579%\n",
      "layer   2  Sparsity: 68.8370%\n",
      "layer   3  Sparsity: 70.4148%\n",
      "total_backward_count 313280 real_backward_count 33679  10.750%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.822388/  1.233862, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0321%\n",
      "layer   2  Sparsity: 69.1135%\n",
      "layer   3  Sparsity: 70.6164%\n",
      "total_backward_count 323070 real_backward_count 34491  10.676%\n",
      "fc layer 3 self.abs_max_out: 1294.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.808254/  1.221133, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0493%\n",
      "layer   2  Sparsity: 69.7019%\n",
      "layer   3  Sparsity: 70.6174%\n",
      "total_backward_count 332860 real_backward_count 35300  10.605%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.809406/  1.219994, val:  75.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0570%\n",
      "layer   2  Sparsity: 69.3075%\n",
      "layer   3  Sparsity: 71.5301%\n",
      "total_backward_count 342650 real_backward_count 36033  10.516%\n",
      "fc layer 3 self.abs_max_out: 1307.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.822330/  1.201868, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0577%\n",
      "layer   2  Sparsity: 69.0282%\n",
      "layer   3  Sparsity: 71.0990%\n",
      "total_backward_count 352440 real_backward_count 36801  10.442%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.819652/  1.215210, val:  79.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0443%\n",
      "layer   2  Sparsity: 69.4538%\n",
      "layer   3  Sparsity: 71.4126%\n",
      "total_backward_count 362230 real_backward_count 37540  10.364%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.772890/  1.202832, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0448%\n",
      "layer   2  Sparsity: 69.2584%\n",
      "layer   3  Sparsity: 71.8397%\n",
      "total_backward_count 372020 real_backward_count 38255  10.283%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.778176/  1.184845, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0545%\n",
      "layer   2  Sparsity: 69.2674%\n",
      "layer   3  Sparsity: 71.3042%\n",
      "total_backward_count 381810 real_backward_count 39015  10.218%\n",
      "fc layer 3 self.abs_max_out: 1369.0\n",
      "fc layer 1 self.abs_max_out: 6116.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.759345/  1.167472, val:  79.17%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0614%\n",
      "layer   2  Sparsity: 68.8610%\n",
      "layer   3  Sparsity: 70.9264%\n",
      "total_backward_count 391600 real_backward_count 39690  10.135%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.780763/  1.273832, val:  57.92%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0273%\n",
      "layer   2  Sparsity: 68.3181%\n",
      "layer   3  Sparsity: 70.6976%\n",
      "total_backward_count 401390 real_backward_count 40409  10.067%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.759642/  1.155952, val:  76.25%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0512%\n",
      "layer   2  Sparsity: 67.9691%\n",
      "layer   3  Sparsity: 69.4711%\n",
      "total_backward_count 411180 real_backward_count 41162  10.011%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.743775/  1.090391, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0479%\n",
      "layer   2  Sparsity: 68.7095%\n",
      "layer   3  Sparsity: 69.9081%\n",
      "total_backward_count 420970 real_backward_count 41907   9.955%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.734410/  1.099830, val:  79.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0524%\n",
      "layer   2  Sparsity: 68.6121%\n",
      "layer   3  Sparsity: 69.8046%\n",
      "total_backward_count 430760 real_backward_count 42617   9.893%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.723506/  1.074883, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0653%\n",
      "layer   2  Sparsity: 68.7230%\n",
      "layer   3  Sparsity: 69.8417%\n",
      "total_backward_count 440550 real_backward_count 43331   9.836%\n",
      "lif layer 1 self.abs_max_v: 11158.5\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.709283/  1.050685, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0488%\n",
      "layer   2  Sparsity: 68.8430%\n",
      "layer   3  Sparsity: 69.5875%\n",
      "total_backward_count 450340 real_backward_count 43994   9.769%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.717250/  1.075554, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0291%\n",
      "layer   2  Sparsity: 68.7838%\n",
      "layer   3  Sparsity: 69.5807%\n",
      "total_backward_count 460130 real_backward_count 44632   9.700%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.715792/  1.139389, val:  69.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0466%\n",
      "layer   2  Sparsity: 68.4874%\n",
      "layer   3  Sparsity: 69.7439%\n",
      "total_backward_count 469920 real_backward_count 45281   9.636%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.708133/  1.118028, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0299%\n",
      "layer   2  Sparsity: 68.6137%\n",
      "layer   3  Sparsity: 69.6680%\n",
      "total_backward_count 479710 real_backward_count 45921   9.573%\n",
      "lif layer 1 self.abs_max_v: 11185.5\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.697110/  1.087422, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0434%\n",
      "layer   2  Sparsity: 68.7503%\n",
      "layer   3  Sparsity: 69.7062%\n",
      "total_backward_count 489500 real_backward_count 46607   9.521%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.678088/  1.070782, val:  80.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0395%\n",
      "layer   2  Sparsity: 68.7136%\n",
      "layer   3  Sparsity: 70.0577%\n",
      "total_backward_count 499290 real_backward_count 47248   9.463%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.696037/  1.086482, val:  82.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0543%\n",
      "layer   2  Sparsity: 68.7119%\n",
      "layer   3  Sparsity: 70.4623%\n",
      "total_backward_count 509080 real_backward_count 47885   9.406%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.688585/  1.113815, val:  75.00%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0352%\n",
      "layer   2  Sparsity: 68.7116%\n",
      "layer   3  Sparsity: 70.4744%\n",
      "total_backward_count 518870 real_backward_count 48518   9.351%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.679599/  1.045313, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0461%\n",
      "layer   2  Sparsity: 68.8182%\n",
      "layer   3  Sparsity: 70.1834%\n",
      "total_backward_count 528660 real_backward_count 49129   9.293%\n",
      "fc layer 1 self.abs_max_out: 6257.0\n",
      "lif layer 1 self.abs_max_v: 11218.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.676126/  1.020794, val:  82.50%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0394%\n",
      "layer   2  Sparsity: 68.2628%\n",
      "layer   3  Sparsity: 70.3362%\n",
      "total_backward_count 538450 real_backward_count 49763   9.242%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.678499/  1.081967, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0362%\n",
      "layer   2  Sparsity: 68.4017%\n",
      "layer   3  Sparsity: 70.4061%\n",
      "total_backward_count 548240 real_backward_count 50391   9.191%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.664089/  1.071926, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.0426%\n",
      "layer   2  Sparsity: 68.6397%\n",
      "layer   3  Sparsity: 69.9913%\n",
      "total_backward_count 558030 real_backward_count 50991   9.138%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.659301/  1.081805, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.0569%\n",
      "layer   2  Sparsity: 68.2895%\n",
      "layer   3  Sparsity: 70.1030%\n",
      "total_backward_count 567820 real_backward_count 51577   9.083%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.654375/  1.086465, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0382%\n",
      "layer   2  Sparsity: 68.5514%\n",
      "layer   3  Sparsity: 70.0477%\n",
      "total_backward_count 577610 real_backward_count 52150   9.029%\n",
      "fc layer 3 self.abs_max_out: 1391.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.671051/  1.080303, val:  73.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0523%\n",
      "layer   2  Sparsity: 69.0002%\n",
      "layer   3  Sparsity: 69.7099%\n",
      "total_backward_count 587400 real_backward_count 52737   8.978%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.667128/  1.033535, val:  78.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0330%\n",
      "layer   2  Sparsity: 68.9238%\n",
      "layer   3  Sparsity: 70.3109%\n",
      "total_backward_count 597190 real_backward_count 53341   8.932%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.655020/  1.024536, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0367%\n",
      "layer   2  Sparsity: 68.6390%\n",
      "layer   3  Sparsity: 70.9970%\n",
      "total_backward_count 606980 real_backward_count 53949   8.888%\n",
      "lif layer 2 self.abs_max_v: 6929.5\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.634302/  1.045831, val:  84.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0407%\n",
      "layer   2  Sparsity: 68.7117%\n",
      "layer   3  Sparsity: 70.8946%\n",
      "total_backward_count 616770 real_backward_count 54578   8.849%\n",
      "lif layer 2 self.abs_max_v: 7132.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.658555/  1.041406, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0312%\n",
      "layer   2  Sparsity: 68.7628%\n",
      "layer   3  Sparsity: 70.6979%\n",
      "total_backward_count 626560 real_backward_count 55194   8.809%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.644805/  1.025563, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0598%\n",
      "layer   2  Sparsity: 68.3374%\n",
      "layer   3  Sparsity: 70.0760%\n",
      "total_backward_count 636350 real_backward_count 55802   8.769%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.633842/  1.021479, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0429%\n",
      "layer   2  Sparsity: 68.1025%\n",
      "layer   3  Sparsity: 70.0986%\n",
      "total_backward_count 646140 real_backward_count 56371   8.724%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.623497/  1.089348, val:  75.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0467%\n",
      "layer   2  Sparsity: 68.4564%\n",
      "layer   3  Sparsity: 69.9952%\n",
      "total_backward_count 655930 real_backward_count 56950   8.682%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.625958/  1.014387, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0338%\n",
      "layer   2  Sparsity: 68.3434%\n",
      "layer   3  Sparsity: 69.8984%\n",
      "total_backward_count 665720 real_backward_count 57532   8.642%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.637775/  1.006307, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0265%\n",
      "layer   2  Sparsity: 68.1468%\n",
      "layer   3  Sparsity: 70.3654%\n",
      "total_backward_count 675510 real_backward_count 58107   8.602%\n",
      "fc layer 1 self.abs_max_out: 6380.0\n",
      "lif layer 1 self.abs_max_v: 11452.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.619147/  1.095586, val:  71.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0536%\n",
      "layer   2  Sparsity: 68.2086%\n",
      "layer   3  Sparsity: 70.5237%\n",
      "total_backward_count 685300 real_backward_count 58624   8.555%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.618576/  0.976380, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0666%\n",
      "layer   2  Sparsity: 68.2132%\n",
      "layer   3  Sparsity: 71.0274%\n",
      "total_backward_count 695090 real_backward_count 59209   8.518%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.616402/  1.034323, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0404%\n",
      "layer   2  Sparsity: 68.3582%\n",
      "layer   3  Sparsity: 70.8493%\n",
      "total_backward_count 704880 real_backward_count 59736   8.475%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.622411/  0.981807, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0488%\n",
      "layer   2  Sparsity: 68.5783%\n",
      "layer   3  Sparsity: 70.8876%\n",
      "total_backward_count 714670 real_backward_count 60263   8.432%\n",
      "fc layer 2 self.abs_max_out: 3731.0\n",
      "fc layer 3 self.abs_max_out: 1401.0\n",
      "fc layer 3 self.abs_max_out: 1416.0\n",
      "fc layer 3 self.abs_max_out: 1427.0\n",
      "fc layer 3 self.abs_max_out: 1458.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.615899/  1.027458, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0432%\n",
      "layer   2  Sparsity: 68.7163%\n",
      "layer   3  Sparsity: 71.3136%\n",
      "total_backward_count 724460 real_backward_count 60769   8.388%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.606746/  0.988665, val:  85.83%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0435%\n",
      "layer   2  Sparsity: 68.5769%\n",
      "layer   3  Sparsity: 71.7776%\n",
      "total_backward_count 734250 real_backward_count 61309   8.350%\n",
      "fc layer 3 self.abs_max_out: 1485.0\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.612115/  0.915906, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0253%\n",
      "layer   2  Sparsity: 68.5950%\n",
      "layer   3  Sparsity: 71.4801%\n",
      "total_backward_count 744040 real_backward_count 61776   8.303%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.607922/  1.011754, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0508%\n",
      "layer   2  Sparsity: 68.4083%\n",
      "layer   3  Sparsity: 71.9413%\n",
      "total_backward_count 753830 real_backward_count 62275   8.261%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.622908/  1.059596, val:  73.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0264%\n",
      "layer   2  Sparsity: 68.5143%\n",
      "layer   3  Sparsity: 72.0131%\n",
      "total_backward_count 763620 real_backward_count 62773   8.220%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.626715/  1.031137, val:  77.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0314%\n",
      "layer   2  Sparsity: 68.3363%\n",
      "layer   3  Sparsity: 72.3252%\n",
      "total_backward_count 773410 real_backward_count 63302   8.185%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.624516/  1.050118, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0443%\n",
      "layer   2  Sparsity: 68.4571%\n",
      "layer   3  Sparsity: 72.6094%\n",
      "total_backward_count 783200 real_backward_count 63858   8.153%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.665179/  1.018643, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0452%\n",
      "layer   2  Sparsity: 68.5136%\n",
      "layer   3  Sparsity: 72.5935%\n",
      "total_backward_count 792990 real_backward_count 64421   8.124%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.637001/  1.043503, val:  76.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0342%\n",
      "layer   2  Sparsity: 68.4032%\n",
      "layer   3  Sparsity: 72.2310%\n",
      "total_backward_count 802780 real_backward_count 64916   8.086%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.632056/  1.020183, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0513%\n",
      "layer   2  Sparsity: 68.6995%\n",
      "layer   3  Sparsity: 71.9273%\n",
      "total_backward_count 812570 real_backward_count 65460   8.056%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.615986/  1.053875, val:  77.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0364%\n",
      "layer   2  Sparsity: 68.5132%\n",
      "layer   3  Sparsity: 72.6144%\n",
      "total_backward_count 822360 real_backward_count 65976   8.023%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.625328/  0.898451, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0551%\n",
      "layer   2  Sparsity: 68.5937%\n",
      "layer   3  Sparsity: 72.7347%\n",
      "total_backward_count 832150 real_backward_count 66413   7.981%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.602966/  0.985262, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0614%\n",
      "layer   2  Sparsity: 68.6325%\n",
      "layer   3  Sparsity: 72.2292%\n",
      "total_backward_count 841940 real_backward_count 66883   7.944%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.612458/  1.064877, val:  77.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0525%\n",
      "layer   2  Sparsity: 68.6845%\n",
      "layer   3  Sparsity: 71.6602%\n",
      "total_backward_count 851730 real_backward_count 67374   7.910%\n",
      "fc layer 1 self.abs_max_out: 6402.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.618694/  0.974544, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0443%\n",
      "layer   2  Sparsity: 68.3650%\n",
      "layer   3  Sparsity: 71.7520%\n",
      "total_backward_count 861520 real_backward_count 67857   7.876%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.613895/  0.959936, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0500%\n",
      "layer   2  Sparsity: 68.2603%\n",
      "layer   3  Sparsity: 71.7653%\n",
      "total_backward_count 871310 real_backward_count 68352   7.845%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.592996/  0.951004, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0268%\n",
      "layer   2  Sparsity: 68.3283%\n",
      "layer   3  Sparsity: 71.5280%\n",
      "total_backward_count 881100 real_backward_count 68833   7.812%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.586977/  1.041523, val:  77.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0252%\n",
      "layer   2  Sparsity: 68.6483%\n",
      "layer   3  Sparsity: 71.7508%\n",
      "total_backward_count 890890 real_backward_count 69297   7.778%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.608261/  0.949851, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0441%\n",
      "layer   2  Sparsity: 68.4491%\n",
      "layer   3  Sparsity: 72.7847%\n",
      "total_backward_count 900680 real_backward_count 69766   7.746%\n",
      "fc layer 1 self.abs_max_out: 6801.0\n",
      "lif layer 1 self.abs_max_v: 12149.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.587380/  0.949762, val:  82.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0298%\n",
      "layer   2  Sparsity: 68.3291%\n",
      "layer   3  Sparsity: 72.8968%\n",
      "total_backward_count 910470 real_backward_count 70251   7.716%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.584681/  1.011982, val:  81.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0631%\n",
      "layer   2  Sparsity: 68.2124%\n",
      "layer   3  Sparsity: 72.3740%\n",
      "total_backward_count 920260 real_backward_count 70677   7.680%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.581478/  0.945745, val:  80.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0463%\n",
      "layer   2  Sparsity: 68.1613%\n",
      "layer   3  Sparsity: 72.2771%\n",
      "total_backward_count 930050 real_backward_count 71148   7.650%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.575056/  0.939510, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0345%\n",
      "layer   2  Sparsity: 68.3732%\n",
      "layer   3  Sparsity: 72.0566%\n",
      "total_backward_count 939840 real_backward_count 71636   7.622%\n",
      "fc layer 2 self.abs_max_out: 3744.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.561433/  1.011851, val:  80.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0506%\n",
      "layer   2  Sparsity: 68.0891%\n",
      "layer   3  Sparsity: 71.8598%\n",
      "total_backward_count 949630 real_backward_count 72091   7.591%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.585354/  0.946014, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0458%\n",
      "layer   2  Sparsity: 67.9224%\n",
      "layer   3  Sparsity: 72.0608%\n",
      "total_backward_count 959420 real_backward_count 72535   7.560%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.546853/  1.002548, val:  78.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0370%\n",
      "layer   2  Sparsity: 68.1400%\n",
      "layer   3  Sparsity: 72.0775%\n",
      "total_backward_count 969210 real_backward_count 72949   7.527%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.560615/  0.968929, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0444%\n",
      "layer   2  Sparsity: 68.5311%\n",
      "layer   3  Sparsity: 72.4236%\n",
      "total_backward_count 979000 real_backward_count 73379   7.495%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.543577/  0.882234, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0618%\n",
      "layer   2  Sparsity: 68.6259%\n",
      "layer   3  Sparsity: 72.5803%\n",
      "total_backward_count 988790 real_backward_count 73781   7.462%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.540334/  0.944450, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0392%\n",
      "layer   2  Sparsity: 68.6667%\n",
      "layer   3  Sparsity: 72.9599%\n",
      "total_backward_count 998580 real_backward_count 74208   7.431%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.558176/  0.893494, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0444%\n",
      "layer   2  Sparsity: 68.3582%\n",
      "layer   3  Sparsity: 72.8137%\n",
      "total_backward_count 1008370 real_backward_count 74609   7.399%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.560835/  0.946894, val:  83.33%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0294%\n",
      "layer   2  Sparsity: 68.3122%\n",
      "layer   3  Sparsity: 72.6605%\n",
      "total_backward_count 1018160 real_backward_count 75042   7.370%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.553174/  0.916877, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0421%\n",
      "layer   2  Sparsity: 68.4961%\n",
      "layer   3  Sparsity: 72.7593%\n",
      "total_backward_count 1027950 real_backward_count 75489   7.344%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.552043/  0.873498, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0377%\n",
      "layer   2  Sparsity: 68.7095%\n",
      "layer   3  Sparsity: 72.4441%\n",
      "total_backward_count 1037740 real_backward_count 75898   7.314%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.553858/  0.944543, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0436%\n",
      "layer   2  Sparsity: 68.5964%\n",
      "layer   3  Sparsity: 71.9928%\n",
      "total_backward_count 1047530 real_backward_count 76320   7.286%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.557729/  0.908188, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0362%\n",
      "layer   2  Sparsity: 68.3589%\n",
      "layer   3  Sparsity: 71.9142%\n",
      "total_backward_count 1057320 real_backward_count 76762   7.260%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.537700/  0.943809, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0224%\n",
      "layer   2  Sparsity: 68.2256%\n",
      "layer   3  Sparsity: 72.4038%\n",
      "total_backward_count 1067110 real_backward_count 77162   7.231%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.562404/  0.980777, val:  81.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0566%\n",
      "layer   2  Sparsity: 68.4074%\n",
      "layer   3  Sparsity: 72.8034%\n",
      "total_backward_count 1076900 real_backward_count 77580   7.204%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.561978/  0.925444, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0458%\n",
      "layer   2  Sparsity: 68.4483%\n",
      "layer   3  Sparsity: 72.8672%\n",
      "total_backward_count 1086690 real_backward_count 77947   7.173%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.554754/  0.951083, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0326%\n",
      "layer   2  Sparsity: 68.5162%\n",
      "layer   3  Sparsity: 73.0370%\n",
      "total_backward_count 1096480 real_backward_count 78317   7.143%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.553210/  1.011553, val:  77.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0332%\n",
      "layer   2  Sparsity: 68.4566%\n",
      "layer   3  Sparsity: 73.4433%\n",
      "total_backward_count 1106270 real_backward_count 78711   7.115%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.561535/  1.039970, val:  76.25%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0411%\n",
      "layer   2  Sparsity: 68.3861%\n",
      "layer   3  Sparsity: 73.3157%\n",
      "total_backward_count 1116060 real_backward_count 79111   7.088%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.567576/  1.060043, val:  77.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0554%\n",
      "layer   2  Sparsity: 68.1845%\n",
      "layer   3  Sparsity: 73.5234%\n",
      "total_backward_count 1125850 real_backward_count 79524   7.063%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.547179/  0.944538, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0563%\n",
      "layer   2  Sparsity: 68.6177%\n",
      "layer   3  Sparsity: 73.6949%\n",
      "total_backward_count 1135640 real_backward_count 79941   7.039%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.547204/  0.962626, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0588%\n",
      "layer   2  Sparsity: 68.5797%\n",
      "layer   3  Sparsity: 73.5482%\n",
      "total_backward_count 1145430 real_backward_count 80345   7.014%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.557902/  0.932871, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0404%\n",
      "layer   2  Sparsity: 68.3863%\n",
      "layer   3  Sparsity: 73.8934%\n",
      "total_backward_count 1155220 real_backward_count 80722   6.988%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.570235/  0.943619, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0450%\n",
      "layer   2  Sparsity: 68.4169%\n",
      "layer   3  Sparsity: 73.7916%\n",
      "total_backward_count 1165010 real_backward_count 81092   6.961%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.571442/  0.988463, val:  80.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0506%\n",
      "layer   2  Sparsity: 68.4916%\n",
      "layer   3  Sparsity: 73.3810%\n",
      "total_backward_count 1174800 real_backward_count 81511   6.938%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.562085/  0.939547, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0462%\n",
      "layer   2  Sparsity: 68.5076%\n",
      "layer   3  Sparsity: 73.2938%\n",
      "total_backward_count 1184590 real_backward_count 81899   6.914%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.542784/  0.880329, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0372%\n",
      "layer   2  Sparsity: 68.5079%\n",
      "layer   3  Sparsity: 73.0502%\n",
      "total_backward_count 1194380 real_backward_count 82281   6.889%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.533042/  0.923440, val:  84.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0548%\n",
      "layer   2  Sparsity: 68.9720%\n",
      "layer   3  Sparsity: 73.3658%\n",
      "total_backward_count 1204170 real_backward_count 82662   6.865%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.531300/  0.914330, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0657%\n",
      "layer   2  Sparsity: 68.8187%\n",
      "layer   3  Sparsity: 73.2596%\n",
      "total_backward_count 1213960 real_backward_count 83068   6.843%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.531161/  0.922062, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0416%\n",
      "layer   2  Sparsity: 68.6147%\n",
      "layer   3  Sparsity: 72.7368%\n",
      "total_backward_count 1223750 real_backward_count 83422   6.817%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.526057/  0.923542, val:  81.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0389%\n",
      "layer   2  Sparsity: 68.5618%\n",
      "layer   3  Sparsity: 72.9883%\n",
      "total_backward_count 1233540 real_backward_count 83799   6.793%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.503798/  0.882678, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0400%\n",
      "layer   2  Sparsity: 68.5194%\n",
      "layer   3  Sparsity: 73.1401%\n",
      "total_backward_count 1243330 real_backward_count 84155   6.769%\n",
      "fc layer 3 self.abs_max_out: 1490.0\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.490867/  0.896312, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0332%\n",
      "layer   2  Sparsity: 68.5821%\n",
      "layer   3  Sparsity: 73.2885%\n",
      "total_backward_count 1253120 real_backward_count 84488   6.742%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.507486/  0.910831, val:  84.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0424%\n",
      "layer   2  Sparsity: 68.9588%\n",
      "layer   3  Sparsity: 73.6665%\n",
      "total_backward_count 1262910 real_backward_count 84825   6.717%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.515777/  0.903662, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0542%\n",
      "layer   2  Sparsity: 68.7740%\n",
      "layer   3  Sparsity: 73.2975%\n",
      "total_backward_count 1272700 real_backward_count 85174   6.692%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.509867/  0.903232, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0478%\n",
      "layer   2  Sparsity: 68.6098%\n",
      "layer   3  Sparsity: 73.3184%\n",
      "total_backward_count 1282490 real_backward_count 85567   6.672%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.530406/  0.900850, val:  85.83%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0554%\n",
      "layer   2  Sparsity: 68.4624%\n",
      "layer   3  Sparsity: 73.4844%\n",
      "total_backward_count 1292280 real_backward_count 85976   6.653%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.525975/  0.888304, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0556%\n",
      "layer   2  Sparsity: 68.5310%\n",
      "layer   3  Sparsity: 73.3456%\n",
      "total_backward_count 1302070 real_backward_count 86300   6.628%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.531601/  0.936096, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0389%\n",
      "layer   2  Sparsity: 68.2268%\n",
      "layer   3  Sparsity: 72.9890%\n",
      "total_backward_count 1311860 real_backward_count 86671   6.607%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.531133/  0.967945, val:  81.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0564%\n",
      "layer   2  Sparsity: 68.2520%\n",
      "layer   3  Sparsity: 72.7214%\n",
      "total_backward_count 1321650 real_backward_count 87054   6.587%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.528920/  0.901780, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0335%\n",
      "layer   2  Sparsity: 68.0889%\n",
      "layer   3  Sparsity: 72.4266%\n",
      "total_backward_count 1331440 real_backward_count 87413   6.565%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.511672/  0.864748, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0515%\n",
      "layer   2  Sparsity: 68.3656%\n",
      "layer   3  Sparsity: 72.1704%\n",
      "total_backward_count 1341230 real_backward_count 87752   6.543%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.500889/  0.897604, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0245%\n",
      "layer   2  Sparsity: 68.2002%\n",
      "layer   3  Sparsity: 71.8868%\n",
      "total_backward_count 1351020 real_backward_count 88106   6.521%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.527438/  0.916570, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0296%\n",
      "layer   2  Sparsity: 68.1954%\n",
      "layer   3  Sparsity: 71.7148%\n",
      "total_backward_count 1360810 real_backward_count 88469   6.501%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.525778/  0.900937, val:  84.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0541%\n",
      "layer   2  Sparsity: 68.2015%\n",
      "layer   3  Sparsity: 71.9367%\n",
      "total_backward_count 1370600 real_backward_count 88804   6.479%\n",
      "fc layer 2 self.abs_max_out: 3854.0\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.520502/  0.948958, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0584%\n",
      "layer   2  Sparsity: 68.4226%\n",
      "layer   3  Sparsity: 71.9262%\n",
      "total_backward_count 1380390 real_backward_count 89120   6.456%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.510187/  0.928785, val:  82.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0322%\n",
      "layer   2  Sparsity: 68.6536%\n",
      "layer   3  Sparsity: 71.9406%\n",
      "total_backward_count 1390180 real_backward_count 89449   6.434%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.483657/  0.853569, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0564%\n",
      "layer   2  Sparsity: 68.7247%\n",
      "layer   3  Sparsity: 72.0657%\n",
      "total_backward_count 1399970 real_backward_count 89810   6.415%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.480973/  0.844054, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0704%\n",
      "layer   2  Sparsity: 68.3193%\n",
      "layer   3  Sparsity: 71.9375%\n",
      "total_backward_count 1409760 real_backward_count 90151   6.395%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.490558/  0.860555, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0583%\n",
      "layer   2  Sparsity: 68.1318%\n",
      "layer   3  Sparsity: 71.8701%\n",
      "total_backward_count 1419550 real_backward_count 90452   6.372%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.503796/  0.877143, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0351%\n",
      "layer   2  Sparsity: 68.3491%\n",
      "layer   3  Sparsity: 71.7290%\n",
      "total_backward_count 1429340 real_backward_count 90759   6.350%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.493771/  0.900284, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0524%\n",
      "layer   2  Sparsity: 68.7345%\n",
      "layer   3  Sparsity: 71.8310%\n",
      "total_backward_count 1439130 real_backward_count 91098   6.330%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.486973/  0.901971, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.0598%\n",
      "layer   2  Sparsity: 68.6368%\n",
      "layer   3  Sparsity: 71.6888%\n",
      "total_backward_count 1448920 real_backward_count 91410   6.309%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.483077/  0.919717, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0640%\n",
      "layer   2  Sparsity: 68.5605%\n",
      "layer   3  Sparsity: 72.3261%\n",
      "total_backward_count 1458710 real_backward_count 91692   6.286%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.483889/  0.894688, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0496%\n",
      "layer   2  Sparsity: 68.4959%\n",
      "layer   3  Sparsity: 72.4008%\n",
      "total_backward_count 1468500 real_backward_count 92002   6.265%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.485869/  0.890945, val:  82.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0454%\n",
      "layer   2  Sparsity: 68.5685%\n",
      "layer   3  Sparsity: 72.1893%\n",
      "total_backward_count 1478290 real_backward_count 92311   6.244%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.485450/  0.882207, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0244%\n",
      "layer   2  Sparsity: 68.6166%\n",
      "layer   3  Sparsity: 72.1909%\n",
      "total_backward_count 1488080 real_backward_count 92594   6.222%\n",
      "fc layer 3 self.abs_max_out: 1533.0\n",
      "fc layer 3 self.abs_max_out: 1540.0\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.478889/  0.857233, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0483%\n",
      "layer   2  Sparsity: 68.4765%\n",
      "layer   3  Sparsity: 72.0419%\n",
      "total_backward_count 1497870 real_backward_count 92905   6.202%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.485484/  0.879753, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0024%\n",
      "layer   2  Sparsity: 68.1817%\n",
      "layer   3  Sparsity: 72.2334%\n",
      "total_backward_count 1507660 real_backward_count 93221   6.183%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.471330/  0.927636, val:  80.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0306%\n",
      "layer   2  Sparsity: 68.2695%\n",
      "layer   3  Sparsity: 72.4728%\n",
      "total_backward_count 1517450 real_backward_count 93518   6.163%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.478423/  0.869547, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0449%\n",
      "layer   2  Sparsity: 68.5231%\n",
      "layer   3  Sparsity: 72.3587%\n",
      "total_backward_count 1527240 real_backward_count 93810   6.142%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.475579/  0.839545, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0253%\n",
      "layer   2  Sparsity: 68.8696%\n",
      "layer   3  Sparsity: 72.6432%\n",
      "total_backward_count 1537030 real_backward_count 94134   6.124%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.472019/  0.910094, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0509%\n",
      "layer   2  Sparsity: 68.7359%\n",
      "layer   3  Sparsity: 72.3666%\n",
      "total_backward_count 1546820 real_backward_count 94455   6.106%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.465222/  0.845649, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0392%\n",
      "layer   2  Sparsity: 68.7982%\n",
      "layer   3  Sparsity: 72.5553%\n",
      "total_backward_count 1556610 real_backward_count 94763   6.088%\n",
      "fc layer 3 self.abs_max_out: 1554.0\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.471776/  0.900446, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0420%\n",
      "layer   2  Sparsity: 68.7000%\n",
      "layer   3  Sparsity: 72.7469%\n",
      "total_backward_count 1566400 real_backward_count 95077   6.070%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.453179/  0.859291, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0471%\n",
      "layer   2  Sparsity: 68.7178%\n",
      "layer   3  Sparsity: 73.0595%\n",
      "total_backward_count 1576190 real_backward_count 95367   6.050%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.466739/  0.824202, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0341%\n",
      "layer   2  Sparsity: 68.5235%\n",
      "layer   3  Sparsity: 73.2277%\n",
      "total_backward_count 1585980 real_backward_count 95659   6.032%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.463861/  0.833872, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0340%\n",
      "layer   2  Sparsity: 68.6618%\n",
      "layer   3  Sparsity: 72.4766%\n",
      "total_backward_count 1595770 real_backward_count 95976   6.014%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.462364/  0.888141, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0256%\n",
      "layer   2  Sparsity: 68.6050%\n",
      "layer   3  Sparsity: 72.6272%\n",
      "total_backward_count 1605560 real_backward_count 96294   5.998%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.461746/  0.898483, val:  82.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.0302%\n",
      "layer   2  Sparsity: 68.5511%\n",
      "layer   3  Sparsity: 73.1881%\n",
      "total_backward_count 1615350 real_backward_count 96568   5.978%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.473771/  0.904290, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0382%\n",
      "layer   2  Sparsity: 68.4479%\n",
      "layer   3  Sparsity: 73.0948%\n",
      "total_backward_count 1625140 real_backward_count 96874   5.961%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.480012/  0.881826, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.0406%\n",
      "layer   2  Sparsity: 68.5408%\n",
      "layer   3  Sparsity: 73.1480%\n",
      "total_backward_count 1634930 real_backward_count 97183   5.944%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.464199/  0.875509, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0599%\n",
      "layer   2  Sparsity: 68.6943%\n",
      "layer   3  Sparsity: 73.1841%\n",
      "total_backward_count 1644720 real_backward_count 97475   5.927%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.443838/  0.844011, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0292%\n",
      "layer   2  Sparsity: 68.5803%\n",
      "layer   3  Sparsity: 73.4973%\n",
      "total_backward_count 1654510 real_backward_count 97751   5.908%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.462147/  0.863266, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0560%\n",
      "layer   2  Sparsity: 68.7011%\n",
      "layer   3  Sparsity: 73.4044%\n",
      "total_backward_count 1664300 real_backward_count 98065   5.892%\n",
      "fc layer 3 self.abs_max_out: 1556.0\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.454691/  0.884178, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0269%\n",
      "layer   2  Sparsity: 68.7450%\n",
      "layer   3  Sparsity: 73.0016%\n",
      "total_backward_count 1674090 real_backward_count 98371   5.876%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.453941/  0.844502, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0271%\n",
      "layer   2  Sparsity: 68.3976%\n",
      "layer   3  Sparsity: 73.3486%\n",
      "total_backward_count 1683880 real_backward_count 98657   5.859%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.460783/  0.880588, val:  82.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0566%\n",
      "layer   2  Sparsity: 68.4016%\n",
      "layer   3  Sparsity: 72.9758%\n",
      "total_backward_count 1693670 real_backward_count 98951   5.842%\n",
      "fc layer 3 self.abs_max_out: 1572.0\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.453365/  0.853762, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0494%\n",
      "layer   2  Sparsity: 68.5599%\n",
      "layer   3  Sparsity: 73.0348%\n",
      "total_backward_count 1703460 real_backward_count 99241   5.826%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.428656/  0.861161, val:  84.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0579%\n",
      "layer   2  Sparsity: 68.5437%\n",
      "layer   3  Sparsity: 73.3534%\n",
      "total_backward_count 1713250 real_backward_count 99508   5.808%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.437624/  0.804027, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0138%\n",
      "layer   2  Sparsity: 68.5126%\n",
      "layer   3  Sparsity: 73.2015%\n",
      "total_backward_count 1723040 real_backward_count 99800   5.792%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.441144/  0.839311, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0516%\n",
      "layer   2  Sparsity: 68.5421%\n",
      "layer   3  Sparsity: 73.1891%\n",
      "total_backward_count 1732830 real_backward_count 100074   5.775%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.447682/  0.873840, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0615%\n",
      "layer   2  Sparsity: 68.7148%\n",
      "layer   3  Sparsity: 73.1404%\n",
      "total_backward_count 1742620 real_backward_count 100343   5.758%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.444194/  0.860748, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0430%\n",
      "layer   2  Sparsity: 68.4964%\n",
      "layer   3  Sparsity: 73.1556%\n",
      "total_backward_count 1752410 real_backward_count 100584   5.740%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.458407/  0.909068, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0393%\n",
      "layer   2  Sparsity: 68.4570%\n",
      "layer   3  Sparsity: 73.5909%\n",
      "total_backward_count 1762200 real_backward_count 100871   5.724%\n",
      "fc layer 3 self.abs_max_out: 1633.0\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.475068/  0.845978, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0558%\n",
      "layer   2  Sparsity: 68.5061%\n",
      "layer   3  Sparsity: 73.9594%\n",
      "total_backward_count 1771990 real_backward_count 101126   5.707%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.461554/  0.816601, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0321%\n",
      "layer   2  Sparsity: 68.4837%\n",
      "layer   3  Sparsity: 73.6500%\n",
      "total_backward_count 1781780 real_backward_count 101415   5.692%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.439489/  0.847626, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0439%\n",
      "layer   2  Sparsity: 68.4454%\n",
      "layer   3  Sparsity: 72.8737%\n",
      "total_backward_count 1791570 real_backward_count 101720   5.678%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.445859/  0.849596, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0324%\n",
      "layer   2  Sparsity: 68.8360%\n",
      "layer   3  Sparsity: 72.6937%\n",
      "total_backward_count 1801360 real_backward_count 102003   5.663%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.435047/  0.824827, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0518%\n",
      "layer   2  Sparsity: 68.9985%\n",
      "layer   3  Sparsity: 72.9836%\n",
      "total_backward_count 1811150 real_backward_count 102249   5.646%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.454432/  0.845825, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.0365%\n",
      "layer   2  Sparsity: 68.8380%\n",
      "layer   3  Sparsity: 73.4292%\n",
      "total_backward_count 1820940 real_backward_count 102556   5.632%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.453813/  0.809325, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0398%\n",
      "layer   2  Sparsity: 68.5714%\n",
      "layer   3  Sparsity: 73.3712%\n",
      "total_backward_count 1830730 real_backward_count 102849   5.618%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.430289/  0.858564, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0266%\n",
      "layer   2  Sparsity: 68.3877%\n",
      "layer   3  Sparsity: 73.7043%\n",
      "total_backward_count 1840520 real_backward_count 103095   5.601%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.433559/  0.788476, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0331%\n",
      "layer   2  Sparsity: 68.6193%\n",
      "layer   3  Sparsity: 73.5854%\n",
      "total_backward_count 1850310 real_backward_count 103357   5.586%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.419334/  0.865645, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0522%\n",
      "layer   2  Sparsity: 68.8035%\n",
      "layer   3  Sparsity: 73.2684%\n",
      "total_backward_count 1860100 real_backward_count 103617   5.571%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.428972/  0.842582, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0406%\n",
      "layer   2  Sparsity: 68.5672%\n",
      "layer   3  Sparsity: 73.3014%\n",
      "total_backward_count 1869890 real_backward_count 103889   5.556%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.438708/  0.837821, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0289%\n",
      "layer   2  Sparsity: 68.7158%\n",
      "layer   3  Sparsity: 73.1868%\n",
      "total_backward_count 1879680 real_backward_count 104172   5.542%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.426615/  0.818119, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0347%\n",
      "layer   2  Sparsity: 68.5652%\n",
      "layer   3  Sparsity: 73.6746%\n",
      "total_backward_count 1889470 real_backward_count 104414   5.526%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.420666/  0.834736, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0435%\n",
      "layer   2  Sparsity: 68.5882%\n",
      "layer   3  Sparsity: 73.4696%\n",
      "total_backward_count 1899260 real_backward_count 104678   5.512%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.419579/  0.836317, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0375%\n",
      "layer   2  Sparsity: 68.3883%\n",
      "layer   3  Sparsity: 73.0026%\n",
      "total_backward_count 1909050 real_backward_count 104936   5.497%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.426433/  0.822966, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0628%\n",
      "layer   2  Sparsity: 68.3509%\n",
      "layer   3  Sparsity: 73.0336%\n",
      "total_backward_count 1918840 real_backward_count 105211   5.483%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.416514/  0.819162, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0495%\n",
      "layer   2  Sparsity: 68.5192%\n",
      "layer   3  Sparsity: 73.3231%\n",
      "total_backward_count 1928630 real_backward_count 105479   5.469%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.411227/  0.818378, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.0227%\n",
      "layer   2  Sparsity: 68.5719%\n",
      "layer   3  Sparsity: 73.4388%\n",
      "total_backward_count 1938420 real_backward_count 105746   5.455%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.416269/  0.829494, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0557%\n",
      "layer   2  Sparsity: 68.6174%\n",
      "layer   3  Sparsity: 73.0344%\n",
      "total_backward_count 1948210 real_backward_count 106019   5.442%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.430689/  0.819398, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.0502%\n",
      "layer   2  Sparsity: 68.6974%\n",
      "layer   3  Sparsity: 73.6911%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5187d2bcd74d44b20cf36cd6f888e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÅ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.43069</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>0.8194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-sweep-340</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pynusdwa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pynusdwa</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251128_043959-pynusdwa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7ek5t0qz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_090305-7ek5t0qz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7ek5t0qz' target=\"_blank\">zesty-sweep-343</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7ek5t0qz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7ek5t0qz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251128_090313_872', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 503.0\n",
      "lif layer 1 self.abs_max_v: 503.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1112.0\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 924.0\n",
      "fc layer 1 self.abs_max_out: 660.0\n",
      "lif layer 1 self.abs_max_v: 732.0\n",
      "fc layer 2 self.abs_max_out: 1609.0\n",
      "lif layer 2 self.abs_max_v: 1944.5\n",
      "fc layer 1 self.abs_max_out: 859.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "lif layer 2 self.abs_max_v: 2046.5\n",
      "fc layer 1 self.abs_max_out: 862.0\n",
      "lif layer 2 self.abs_max_v: 2265.5\n",
      "lif layer 1 self.abs_max_v: 1206.5\n",
      "lif layer 2 self.abs_max_v: 2446.0\n",
      "fc layer 1 self.abs_max_out: 900.0\n",
      "lif layer 1 self.abs_max_v: 1498.5\n",
      "fc layer 1 self.abs_max_out: 1091.0\n",
      "lif layer 2 self.abs_max_v: 2517.5\n",
      "fc layer 1 self.abs_max_out: 1183.0\n",
      "fc layer 2 self.abs_max_out: 2203.0\n",
      "lif layer 2 self.abs_max_v: 2725.0\n",
      "lif layer 1 self.abs_max_v: 1603.0\n",
      "lif layer 1 self.abs_max_v: 1642.5\n",
      "lif layer 1 self.abs_max_v: 1881.5\n",
      "fc layer 1 self.abs_max_out: 1422.0\n",
      "lif layer 2 self.abs_max_v: 2814.5\n",
      "fc layer 1 self.abs_max_out: 1506.0\n",
      "fc layer 3 self.abs_max_out: 1138.0\n",
      "fc layer 1 self.abs_max_out: 1576.0\n",
      "fc layer 2 self.abs_max_out: 2325.0\n",
      "lif layer 2 self.abs_max_v: 2906.0\n",
      "lif layer 2 self.abs_max_v: 3081.5\n",
      "lif layer 1 self.abs_max_v: 2138.0\n",
      "fc layer 1 self.abs_max_out: 1577.0\n",
      "lif layer 1 self.abs_max_v: 2333.0\n",
      "fc layer 1 self.abs_max_out: 1624.0\n",
      "fc layer 1 self.abs_max_out: 1659.0\n",
      "lif layer 2 self.abs_max_v: 3255.5\n",
      "fc layer 1 self.abs_max_out: 1704.0\n",
      "fc layer 2 self.abs_max_out: 2391.0\n",
      "fc layer 1 self.abs_max_out: 1709.0\n",
      "lif layer 2 self.abs_max_v: 3572.0\n",
      "fc layer 2 self.abs_max_out: 2644.0\n",
      "lif layer 1 self.abs_max_v: 2339.0\n",
      "lif layer 1 self.abs_max_v: 2540.5\n",
      "lif layer 2 self.abs_max_v: 4207.5\n",
      "fc layer 1 self.abs_max_out: 1748.0\n",
      "fc layer 1 self.abs_max_out: 1817.0\n",
      "fc layer 1 self.abs_max_out: 1829.0\n",
      "fc layer 1 self.abs_max_out: 1839.0\n",
      "fc layer 1 self.abs_max_out: 1939.0\n",
      "fc layer 1 self.abs_max_out: 1947.0\n",
      "fc layer 1 self.abs_max_out: 2055.0\n",
      "lif layer 1 self.abs_max_v: 2895.0\n",
      "fc layer 1 self.abs_max_out: 2463.0\n",
      "fc layer 1 self.abs_max_out: 2519.0\n",
      "fc layer 1 self.abs_max_out: 2581.0\n",
      "lif layer 1 self.abs_max_v: 3360.5\n",
      "fc layer 1 self.abs_max_out: 2590.0\n",
      "fc layer 1 self.abs_max_out: 2795.0\n",
      "lif layer 2 self.abs_max_v: 4271.5\n",
      "lif layer 2 self.abs_max_v: 4628.0\n",
      "lif layer 2 self.abs_max_v: 4680.0\n",
      "fc layer 1 self.abs_max_out: 2876.0\n",
      "fc layer 2 self.abs_max_out: 2739.0\n",
      "fc layer 2 self.abs_max_out: 2851.0\n",
      "fc layer 1 self.abs_max_out: 2984.0\n",
      "lif layer 2 self.abs_max_v: 4776.0\n",
      "lif layer 2 self.abs_max_v: 5093.5\n",
      "fc layer 1 self.abs_max_out: 3176.0\n",
      "fc layer 3 self.abs_max_out: 1250.0\n",
      "lif layer 1 self.abs_max_v: 3531.5\n",
      "lif layer 1 self.abs_max_v: 3743.0\n",
      "lif layer 1 self.abs_max_v: 3748.5\n",
      "lif layer 1 self.abs_max_v: 3799.5\n",
      "lif layer 1 self.abs_max_v: 3812.0\n",
      "lif layer 1 self.abs_max_v: 4525.5\n",
      "fc layer 2 self.abs_max_out: 2929.0\n",
      "fc layer 1 self.abs_max_out: 3335.0\n",
      "fc layer 2 self.abs_max_out: 3005.0\n",
      "fc layer 1 self.abs_max_out: 3591.0\n",
      "fc layer 2 self.abs_max_out: 3228.0\n",
      "fc layer 3 self.abs_max_out: 1259.0\n",
      "fc layer 2 self.abs_max_out: 3241.0\n",
      "fc layer 1 self.abs_max_out: 3649.0\n",
      "fc layer 3 self.abs_max_out: 1278.0\n",
      "fc layer 1 self.abs_max_out: 4164.0\n",
      "fc layer 1 self.abs_max_out: 4205.0\n",
      "lif layer 1 self.abs_max_v: 4748.5\n",
      "lif layer 2 self.abs_max_v: 5264.5\n",
      "fc layer 2 self.abs_max_out: 3284.0\n",
      "lif layer 2 self.abs_max_v: 5916.5\n",
      "lif layer 1 self.abs_max_v: 5079.5\n",
      "lif layer 2 self.abs_max_v: 6181.5\n",
      "fc layer 2 self.abs_max_out: 3579.0\n",
      "fc layer 1 self.abs_max_out: 4470.0\n",
      "lif layer 1 self.abs_max_v: 5919.0\n",
      "fc layer 1 self.abs_max_out: 4524.0\n",
      "fc layer 1 self.abs_max_out: 4573.0\n",
      "lif layer 1 self.abs_max_v: 5936.5\n",
      "lif layer 2 self.abs_max_v: 6356.5\n",
      "lif layer 2 self.abs_max_v: 6547.0\n",
      "lif layer 1 self.abs_max_v: 6237.0\n",
      "fc layer 2 self.abs_max_out: 3951.0\n",
      "lif layer 2 self.abs_max_v: 6698.0\n",
      "fc layer 1 self.abs_max_out: 5015.0\n",
      "fc layer 3 self.abs_max_out: 1284.0\n",
      "lif layer 1 self.abs_max_v: 6953.0\n",
      "lif layer 1 self.abs_max_v: 7358.5\n",
      "lif layer 1 self.abs_max_v: 7460.5\n",
      "lif layer 1 self.abs_max_v: 7464.5\n",
      "lif layer 2 self.abs_max_v: 6745.0\n",
      "lif layer 2 self.abs_max_v: 6779.5\n",
      "lif layer 2 self.abs_max_v: 6858.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.866812/  2.027998, val:  40.83%, val_best:  40.83%, tr:  93.77%, tr_best:  93.77%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.5175%\n",
      "layer   3  Sparsity: 66.6676%\n",
      "total_backward_count 9790 real_backward_count 2617  26.731%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 5105.0\n",
      "fc layer 1 self.abs_max_out: 5523.0\n",
      "fc layer 2 self.abs_max_out: 4050.0\n",
      "fc layer 1 self.abs_max_out: 6183.0\n",
      "fc layer 1 self.abs_max_out: 6317.0\n",
      "lif layer 1 self.abs_max_v: 7744.5\n",
      "lif layer 1 self.abs_max_v: 8239.5\n",
      "lif layer 1 self.abs_max_v: 8366.5\n",
      "lif layer 1 self.abs_max_v: 8965.5\n",
      "fc layer 2 self.abs_max_out: 4143.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.821126/  1.998951, val:  41.25%, val_best:  41.25%, tr:  99.08%, tr_best:  99.08%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 71.0571%\n",
      "layer   3  Sparsity: 63.8137%\n",
      "total_backward_count 19580 real_backward_count 4368  22.308%\n",
      "lif layer 1 self.abs_max_v: 9086.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.807270/  1.976128, val:  51.67%, val_best:  51.67%, tr:  98.88%, tr_best:  99.08%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 72.0037%\n",
      "layer   3  Sparsity: 64.6728%\n",
      "total_backward_count 29370 real_backward_count 6005  20.446%\n",
      "fc layer 3 self.abs_max_out: 1292.0\n",
      "fc layer 1 self.abs_max_out: 6365.0\n",
      "fc layer 1 self.abs_max_out: 6691.0\n",
      "lif layer 1 self.abs_max_v: 9653.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.794990/  1.978873, val:  42.92%, val_best:  51.67%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 71.7409%\n",
      "layer   3  Sparsity: 64.6359%\n",
      "total_backward_count 39160 real_backward_count 7497  19.145%\n",
      "fc layer 3 self.abs_max_out: 1297.0\n",
      "fc layer 3 self.abs_max_out: 1305.0\n",
      "fc layer 1 self.abs_max_out: 6771.0\n",
      "lif layer 1 self.abs_max_v: 10556.0\n",
      "lif layer 1 self.abs_max_v: 12026.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.788815/  1.940043, val:  56.67%, val_best:  56.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 72.2074%\n",
      "layer   3  Sparsity: 66.2718%\n",
      "total_backward_count 48950 real_backward_count 8831  18.041%\n",
      "fc layer 1 self.abs_max_out: 6924.0\n",
      "fc layer 3 self.abs_max_out: 1310.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.776617/  1.931036, val:  57.50%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 71.9378%\n",
      "layer   3  Sparsity: 66.4791%\n",
      "total_backward_count 58740 real_backward_count 10118  17.225%\n",
      "fc layer 3 self.abs_max_out: 1316.0\n",
      "fc layer 3 self.abs_max_out: 1345.0\n",
      "fc layer 1 self.abs_max_out: 6936.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.776523/  1.932311, val:  52.50%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 71.9962%\n",
      "layer   3  Sparsity: 66.6958%\n",
      "total_backward_count 68530 real_backward_count 11375  16.599%\n",
      "lif layer 2 self.abs_max_v: 7144.5\n",
      "fc layer 2 self.abs_max_out: 4180.0\n",
      "fc layer 3 self.abs_max_out: 1375.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.770557/  1.914131, val:  63.75%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 71.2660%\n",
      "layer   3  Sparsity: 67.0889%\n",
      "total_backward_count 78320 real_backward_count 12588  16.073%\n",
      "fc layer 3 self.abs_max_out: 1386.0\n",
      "fc layer 2 self.abs_max_out: 4331.0\n",
      "fc layer 3 self.abs_max_out: 1406.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.750911/  1.887615, val:  59.58%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.9807%\n",
      "layer   3  Sparsity: 66.7555%\n",
      "total_backward_count 88110 real_backward_count 13771  15.629%\n",
      "fc layer 3 self.abs_max_out: 1436.0\n",
      "fc layer 1 self.abs_max_out: 6954.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.745050/  1.910723, val:  56.67%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.7705%\n",
      "layer   3  Sparsity: 67.4216%\n",
      "total_backward_count 97900 real_backward_count 14908  15.228%\n",
      "lif layer 2 self.abs_max_v: 7351.0\n",
      "fc layer 3 self.abs_max_out: 1480.0\n",
      "fc layer 3 self.abs_max_out: 1488.0\n",
      "fc layer 2 self.abs_max_out: 4347.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.731019/  1.879698, val:  62.50%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.2386%\n",
      "layer   3  Sparsity: 67.1635%\n",
      "total_backward_count 107690 real_backward_count 15926  14.789%\n",
      "fc layer 3 self.abs_max_out: 1517.0\n",
      "fc layer 1 self.abs_max_out: 7034.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.719074/  1.877689, val:  62.92%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.8145%\n",
      "layer   3  Sparsity: 67.6344%\n",
      "total_backward_count 117480 real_backward_count 16981  14.454%\n",
      "fc layer 3 self.abs_max_out: 1523.0\n",
      "fc layer 3 self.abs_max_out: 1557.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.705267/  1.864113, val:  57.92%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.3181%\n",
      "layer   3  Sparsity: 68.2283%\n",
      "total_backward_count 127270 real_backward_count 17932  14.090%\n",
      "fc layer 3 self.abs_max_out: 1568.0\n",
      "fc layer 2 self.abs_max_out: 4592.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.683618/  1.851753, val:  54.17%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.4484%\n",
      "layer   3  Sparsity: 68.6316%\n",
      "total_backward_count 137060 real_backward_count 18869  13.767%\n",
      "fc layer 1 self.abs_max_out: 7069.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.667303/  1.817268, val:  66.25%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.6900%\n",
      "layer   3  Sparsity: 69.2986%\n",
      "total_backward_count 146850 real_backward_count 19784  13.472%\n",
      "fc layer 1 self.abs_max_out: 7081.0\n",
      "lif layer 2 self.abs_max_v: 7361.5\n",
      "lif layer 1 self.abs_max_v: 12106.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.659678/  1.821483, val:  59.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.4557%\n",
      "layer   3  Sparsity: 68.5999%\n",
      "total_backward_count 156640 real_backward_count 20618  13.163%\n",
      "fc layer 1 self.abs_max_out: 7148.0\n",
      "lif layer 2 self.abs_max_v: 7371.5\n",
      "fc layer 3 self.abs_max_out: 1572.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.651155/  1.805184, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.0886%\n",
      "layer   3  Sparsity: 69.2882%\n",
      "total_backward_count 166430 real_backward_count 21419  12.870%\n",
      "fc layer 3 self.abs_max_out: 1606.0\n",
      "fc layer 3 self.abs_max_out: 1646.0\n",
      "fc layer 3 self.abs_max_out: 1659.0\n",
      "fc layer 3 self.abs_max_out: 1730.0\n",
      "fc layer 1 self.abs_max_out: 7259.0\n",
      "fc layer 2 self.abs_max_out: 4613.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.648037/  1.810305, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.0196%\n",
      "layer   3  Sparsity: 69.5504%\n",
      "total_backward_count 176220 real_backward_count 22205  12.601%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.661248/  1.803817, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.1983%\n",
      "layer   3  Sparsity: 69.7929%\n",
      "total_backward_count 186010 real_backward_count 22940  12.333%\n",
      "fc layer 1 self.abs_max_out: 7359.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.644724/  1.786824, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.0657%\n",
      "layer   3  Sparsity: 70.1427%\n",
      "total_backward_count 195800 real_backward_count 23621  12.064%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.643050/  1.811491, val:  71.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.1343%\n",
      "layer   3  Sparsity: 70.6410%\n",
      "total_backward_count 205590 real_backward_count 24293  11.816%\n",
      "fc layer 1 self.abs_max_out: 7377.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.634162/  1.809336, val:  72.08%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.8443%\n",
      "layer   3  Sparsity: 70.7256%\n",
      "total_backward_count 215380 real_backward_count 24907  11.564%\n",
      "fc layer 2 self.abs_max_out: 4776.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.623613/  1.756617, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.1748%\n",
      "layer   3  Sparsity: 70.7756%\n",
      "total_backward_count 225170 real_backward_count 25523  11.335%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.607092/  1.762457, val:  73.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.0290%\n",
      "layer   3  Sparsity: 71.2771%\n",
      "total_backward_count 234960 real_backward_count 26127  11.120%\n",
      "lif layer 2 self.abs_max_v: 7479.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.611485/  1.750583, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.8013%\n",
      "layer   3  Sparsity: 71.3486%\n",
      "total_backward_count 244750 real_backward_count 26708  10.912%\n",
      "lif layer 2 self.abs_max_v: 7658.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.600863/  1.732195, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7274%\n",
      "layer   3  Sparsity: 71.3624%\n",
      "total_backward_count 254540 real_backward_count 27326  10.735%\n",
      "fc layer 1 self.abs_max_out: 7418.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.596726/  1.746660, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6886%\n",
      "layer   3  Sparsity: 71.5233%\n",
      "total_backward_count 264330 real_backward_count 27859  10.539%\n",
      "fc layer 1 self.abs_max_out: 7463.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.590827/  1.742196, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4229%\n",
      "layer   3  Sparsity: 71.4194%\n",
      "total_backward_count 274120 real_backward_count 28410  10.364%\n",
      "fc layer 1 self.abs_max_out: 7522.0\n",
      "lif layer 1 self.abs_max_v: 12119.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.585113/  1.753999, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4063%\n",
      "layer   3  Sparsity: 71.6167%\n",
      "total_backward_count 283910 real_backward_count 28888  10.175%\n",
      "fc layer 1 self.abs_max_out: 7543.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.589129/  1.735187, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2359%\n",
      "layer   3  Sparsity: 71.9471%\n",
      "total_backward_count 293700 real_backward_count 29375  10.002%\n",
      "fc layer 1 self.abs_max_out: 7546.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.578511/  1.746380, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.0959%\n",
      "layer   3  Sparsity: 72.0936%\n",
      "total_backward_count 303490 real_backward_count 29824   9.827%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.578254/  1.723170, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3169%\n",
      "layer   3  Sparsity: 72.5390%\n",
      "total_backward_count 313280 real_backward_count 30268   9.662%\n",
      "fc layer 1 self.abs_max_out: 7569.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.569454/  1.734901, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4537%\n",
      "layer   3  Sparsity: 71.8098%\n",
      "total_backward_count 323070 real_backward_count 30704   9.504%\n",
      "fc layer 1 self.abs_max_out: 7596.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.558694/  1.722559, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4700%\n",
      "layer   3  Sparsity: 71.7110%\n",
      "total_backward_count 332860 real_backward_count 31103   9.344%\n",
      "fc layer 1 self.abs_max_out: 7597.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.557270/  1.721219, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5621%\n",
      "layer   3  Sparsity: 72.2671%\n",
      "total_backward_count 342650 real_backward_count 31482   9.188%\n",
      "fc layer 1 self.abs_max_out: 7620.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.544771/  1.718336, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6428%\n",
      "layer   3  Sparsity: 72.4580%\n",
      "total_backward_count 352440 real_backward_count 31861   9.040%\n",
      "fc layer 1 self.abs_max_out: 7647.0\n",
      "lif layer 1 self.abs_max_v: 12318.5\n",
      "lif layer 1 self.abs_max_v: 12828.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.536546/  1.691237, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5973%\n",
      "layer   3  Sparsity: 72.3138%\n",
      "total_backward_count 362230 real_backward_count 32189   8.886%\n",
      "lif layer 1 self.abs_max_v: 13158.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.524130/  1.728245, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5389%\n",
      "layer   3  Sparsity: 72.8498%\n",
      "total_backward_count 372020 real_backward_count 32502   8.737%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.539988/  1.725112, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.8596%\n",
      "layer   3  Sparsity: 73.1131%\n",
      "total_backward_count 381810 real_backward_count 32832   8.599%\n",
      "fc layer 3 self.abs_max_out: 1731.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.526635/  1.693065, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 70.0736%\n",
      "layer   3  Sparsity: 72.7913%\n",
      "total_backward_count 391600 real_backward_count 33162   8.468%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.524101/  1.677746, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.8515%\n",
      "layer   3  Sparsity: 72.7210%\n",
      "total_backward_count 401390 real_backward_count 33469   8.338%\n",
      "fc layer 3 self.abs_max_out: 1762.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.510312/  1.673483, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4402%\n",
      "layer   3  Sparsity: 73.0312%\n",
      "total_backward_count 411180 real_backward_count 33781   8.216%\n",
      "fc layer 1 self.abs_max_out: 7712.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.512131/  1.691135, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4998%\n",
      "layer   3  Sparsity: 73.4495%\n",
      "total_backward_count 420970 real_backward_count 34075   8.094%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.522580/  1.699321, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4657%\n",
      "layer   3  Sparsity: 73.5275%\n",
      "total_backward_count 430760 real_backward_count 34361   7.977%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.519247/  1.688567, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4126%\n",
      "layer   3  Sparsity: 73.4329%\n",
      "total_backward_count 440550 real_backward_count 34667   7.869%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.507633/  1.677090, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4141%\n",
      "layer   3  Sparsity: 73.3526%\n",
      "total_backward_count 450340 real_backward_count 34941   7.759%\n",
      "fc layer 3 self.abs_max_out: 1802.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.507687/  1.685533, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5272%\n",
      "layer   3  Sparsity: 73.2415%\n",
      "total_backward_count 460130 real_backward_count 35192   7.648%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.499764/  1.661320, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7456%\n",
      "layer   3  Sparsity: 72.9439%\n",
      "total_backward_count 469920 real_backward_count 35437   7.541%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.502666/  1.678888, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6439%\n",
      "layer   3  Sparsity: 73.3036%\n",
      "total_backward_count 479710 real_backward_count 35681   7.438%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.501905/  1.665765, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.8137%\n",
      "layer   3  Sparsity: 73.8052%\n",
      "total_backward_count 489500 real_backward_count 35903   7.335%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.499242/  1.679361, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7831%\n",
      "layer   3  Sparsity: 74.0382%\n",
      "total_backward_count 499290 real_backward_count 36117   7.234%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.496712/  1.674973, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7077%\n",
      "layer   3  Sparsity: 73.9218%\n",
      "total_backward_count 509080 real_backward_count 36337   7.138%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.496805/  1.674733, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3787%\n",
      "layer   3  Sparsity: 73.8824%\n",
      "total_backward_count 518870 real_backward_count 36558   7.046%\n",
      "lif layer 2 self.abs_max_v: 7667.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.493473/  1.666121, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6135%\n",
      "layer   3  Sparsity: 74.0298%\n",
      "total_backward_count 528660 real_backward_count 36743   6.950%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.487595/  1.667132, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6338%\n",
      "layer   3  Sparsity: 74.0602%\n",
      "total_backward_count 538450 real_backward_count 36939   6.860%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.490678/  1.652541, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7120%\n",
      "layer   3  Sparsity: 73.8617%\n",
      "total_backward_count 548240 real_backward_count 37118   6.770%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.477772/  1.657258, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4635%\n",
      "layer   3  Sparsity: 73.5026%\n",
      "total_backward_count 558030 real_backward_count 37308   6.686%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.481522/  1.668973, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7645%\n",
      "layer   3  Sparsity: 73.8638%\n",
      "total_backward_count 567820 real_backward_count 37481   6.601%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.485153/  1.652427, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6493%\n",
      "layer   3  Sparsity: 73.6964%\n",
      "total_backward_count 577610 real_backward_count 37638   6.516%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.481675/  1.652654, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.1914%\n",
      "layer   3  Sparsity: 73.7657%\n",
      "total_backward_count 587400 real_backward_count 37783   6.432%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.475646/  1.662771, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.0678%\n",
      "layer   3  Sparsity: 73.4475%\n",
      "total_backward_count 597190 real_backward_count 37959   6.356%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.476047/  1.656632, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.1653%\n",
      "layer   3  Sparsity: 73.5679%\n",
      "total_backward_count 606980 real_backward_count 38108   6.278%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.463489/  1.647004, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.0123%\n",
      "layer   3  Sparsity: 73.1975%\n",
      "total_backward_count 616770 real_backward_count 38306   6.211%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.461313/  1.652451, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2603%\n",
      "layer   3  Sparsity: 73.4403%\n",
      "total_backward_count 626560 real_backward_count 38475   6.141%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.465392/  1.641153, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.1938%\n",
      "layer   3  Sparsity: 73.5075%\n",
      "total_backward_count 636350 real_backward_count 38623   6.069%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.467407/  1.658209, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3495%\n",
      "layer   3  Sparsity: 73.8286%\n",
      "total_backward_count 646140 real_backward_count 38759   5.999%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.471525/  1.651078, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3543%\n",
      "layer   3  Sparsity: 73.9137%\n",
      "total_backward_count 655930 real_backward_count 38920   5.934%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.462593/  1.642288, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3582%\n",
      "layer   3  Sparsity: 74.1909%\n",
      "total_backward_count 665720 real_backward_count 39068   5.869%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.458276/  1.646098, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3646%\n",
      "layer   3  Sparsity: 73.7543%\n",
      "total_backward_count 675510 real_backward_count 39233   5.808%\n",
      "fc layer 3 self.abs_max_out: 1803.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.455815/  1.645622, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6311%\n",
      "layer   3  Sparsity: 74.0884%\n",
      "total_backward_count 685300 real_backward_count 39359   5.743%\n",
      "fc layer 3 self.abs_max_out: 1805.0\n",
      "fc layer 3 self.abs_max_out: 1816.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.452939/  1.644690, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4440%\n",
      "layer   3  Sparsity: 73.9034%\n",
      "total_backward_count 695090 real_backward_count 39514   5.685%\n",
      "fc layer 1 self.abs_max_out: 7737.0\n",
      "fc layer 3 self.abs_max_out: 1884.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.444827/  1.638255, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5513%\n",
      "layer   3  Sparsity: 73.7421%\n",
      "total_backward_count 704880 real_backward_count 39649   5.625%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.451639/  1.640217, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3408%\n",
      "layer   3  Sparsity: 73.5053%\n",
      "total_backward_count 714670 real_backward_count 39789   5.567%\n",
      "fc layer 3 self.abs_max_out: 1885.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.446593/  1.640901, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2692%\n",
      "layer   3  Sparsity: 74.0327%\n",
      "total_backward_count 724460 real_backward_count 39919   5.510%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.446914/  1.630741, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3017%\n",
      "layer   3  Sparsity: 74.4372%\n",
      "total_backward_count 734250 real_backward_count 40047   5.454%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.426982/  1.608105, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5265%\n",
      "layer   3  Sparsity: 74.1061%\n",
      "total_backward_count 744040 real_backward_count 40151   5.396%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.424496/  1.622395, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4998%\n",
      "layer   3  Sparsity: 74.1018%\n",
      "total_backward_count 753830 real_backward_count 40240   5.338%\n",
      "fc layer 3 self.abs_max_out: 1996.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.432096/  1.620920, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4920%\n",
      "layer   3  Sparsity: 74.2577%\n",
      "total_backward_count 763620 real_backward_count 40333   5.282%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.428173/  1.621900, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3949%\n",
      "layer   3  Sparsity: 73.9753%\n",
      "total_backward_count 773410 real_backward_count 40417   5.226%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.430895/  1.607408, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.1970%\n",
      "layer   3  Sparsity: 73.8952%\n",
      "total_backward_count 783200 real_backward_count 40537   5.176%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.418535/  1.623007, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3698%\n",
      "layer   3  Sparsity: 74.5311%\n",
      "total_backward_count 792990 real_backward_count 40643   5.125%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.421910/  1.617248, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4219%\n",
      "layer   3  Sparsity: 74.3763%\n",
      "total_backward_count 802780 real_backward_count 40741   5.075%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.418458/  1.613993, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5619%\n",
      "layer   3  Sparsity: 74.0320%\n",
      "total_backward_count 812570 real_backward_count 40839   5.026%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.417109/  1.608401, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.8322%\n",
      "layer   3  Sparsity: 74.2630%\n",
      "total_backward_count 822360 real_backward_count 40935   4.978%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.415636/  1.608774, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.8749%\n",
      "layer   3  Sparsity: 74.3601%\n",
      "total_backward_count 832150 real_backward_count 41030   4.931%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.409241/  1.608832, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.9530%\n",
      "layer   3  Sparsity: 74.5957%\n",
      "total_backward_count 841940 real_backward_count 41120   4.884%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.399661/  1.591489, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7623%\n",
      "layer   3  Sparsity: 74.1185%\n",
      "total_backward_count 851730 real_backward_count 41219   4.839%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.402797/  1.599506, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7166%\n",
      "layer   3  Sparsity: 74.2537%\n",
      "total_backward_count 861520 real_backward_count 41316   4.796%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.403381/  1.608259, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7573%\n",
      "layer   3  Sparsity: 74.4185%\n",
      "total_backward_count 871310 real_backward_count 41404   4.752%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.408237/  1.602341, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6145%\n",
      "layer   3  Sparsity: 74.3644%\n",
      "total_backward_count 881100 real_backward_count 41481   4.708%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.405256/  1.597431, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5861%\n",
      "layer   3  Sparsity: 74.5797%\n",
      "total_backward_count 890890 real_backward_count 41559   4.665%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.402635/  1.604849, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4301%\n",
      "layer   3  Sparsity: 74.9427%\n",
      "total_backward_count 900680 real_backward_count 41631   4.622%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.405727/  1.608956, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5061%\n",
      "layer   3  Sparsity: 75.3435%\n",
      "total_backward_count 910470 real_backward_count 41681   4.578%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.407631/  1.609457, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4810%\n",
      "layer   3  Sparsity: 75.0355%\n",
      "total_backward_count 920260 real_backward_count 41749   4.537%\n",
      "fc layer 1 self.abs_max_out: 7739.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.404668/  1.591459, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7253%\n",
      "layer   3  Sparsity: 74.8445%\n",
      "total_backward_count 930050 real_backward_count 41819   4.496%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.390444/  1.580052, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7123%\n",
      "layer   3  Sparsity: 74.8888%\n",
      "total_backward_count 939840 real_backward_count 41901   4.458%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.385750/  1.602964, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7921%\n",
      "layer   3  Sparsity: 75.1073%\n",
      "total_backward_count 949630 real_backward_count 41953   4.418%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.394440/  1.601234, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7123%\n",
      "layer   3  Sparsity: 74.9562%\n",
      "total_backward_count 959420 real_backward_count 42018   4.380%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.393054/  1.602906, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5440%\n",
      "layer   3  Sparsity: 75.2293%\n",
      "total_backward_count 969210 real_backward_count 42076   4.341%\n",
      "fc layer 1 self.abs_max_out: 7754.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.396362/  1.611538, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7489%\n",
      "layer   3  Sparsity: 75.3470%\n",
      "total_backward_count 979000 real_backward_count 42145   4.305%\n",
      "fc layer 1 self.abs_max_out: 7778.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.400593/  1.615718, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5381%\n",
      "layer   3  Sparsity: 75.0942%\n",
      "total_backward_count 988790 real_backward_count 42212   4.269%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.403573/  1.610799, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6804%\n",
      "layer   3  Sparsity: 75.0614%\n",
      "total_backward_count 998580 real_backward_count 42280   4.234%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.401122/  1.606390, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6715%\n",
      "layer   3  Sparsity: 74.9707%\n",
      "total_backward_count 1008370 real_backward_count 42356   4.200%\n",
      "fc layer 1 self.abs_max_out: 7789.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.397420/  1.603841, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4281%\n",
      "layer   3  Sparsity: 74.8968%\n",
      "total_backward_count 1018160 real_backward_count 42446   4.169%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.388821/  1.595951, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3308%\n",
      "layer   3  Sparsity: 75.0280%\n",
      "total_backward_count 1027950 real_backward_count 42503   4.135%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.388176/  1.600222, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3561%\n",
      "layer   3  Sparsity: 75.0548%\n",
      "total_backward_count 1037740 real_backward_count 42573   4.102%\n",
      "fc layer 1 self.abs_max_out: 7794.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.387344/  1.599382, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2556%\n",
      "layer   3  Sparsity: 75.0660%\n",
      "total_backward_count 1047530 real_backward_count 42654   4.072%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.389764/  1.595387, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3698%\n",
      "layer   3  Sparsity: 74.8107%\n",
      "total_backward_count 1057320 real_backward_count 42716   4.040%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.392966/  1.596137, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3790%\n",
      "layer   3  Sparsity: 74.7196%\n",
      "total_backward_count 1067110 real_backward_count 42761   4.007%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.387662/  1.592850, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4847%\n",
      "layer   3  Sparsity: 74.7240%\n",
      "total_backward_count 1076900 real_backward_count 42821   3.976%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.388219/  1.591120, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3887%\n",
      "layer   3  Sparsity: 74.7011%\n",
      "total_backward_count 1086690 real_backward_count 42898   3.948%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.388317/  1.595312, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3734%\n",
      "layer   3  Sparsity: 74.8774%\n",
      "total_backward_count 1096480 real_backward_count 42945   3.917%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.388394/  1.592063, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6105%\n",
      "layer   3  Sparsity: 75.4482%\n",
      "total_backward_count 1106270 real_backward_count 42996   3.887%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.386781/  1.598023, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5986%\n",
      "layer   3  Sparsity: 75.3992%\n",
      "total_backward_count 1116060 real_backward_count 43042   3.857%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.392321/  1.598252, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4435%\n",
      "layer   3  Sparsity: 75.2842%\n",
      "total_backward_count 1125850 real_backward_count 43078   3.826%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.398901/  1.590592, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4381%\n",
      "layer   3  Sparsity: 75.3373%\n",
      "total_backward_count 1135640 real_backward_count 43118   3.797%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.393233/  1.589774, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4162%\n",
      "layer   3  Sparsity: 75.3792%\n",
      "total_backward_count 1145430 real_backward_count 43169   3.769%\n",
      "fc layer 3 self.abs_max_out: 2049.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.392224/  1.596775, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4910%\n",
      "layer   3  Sparsity: 75.2240%\n",
      "total_backward_count 1155220 real_backward_count 43220   3.741%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.386966/  1.575540, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5415%\n",
      "layer   3  Sparsity: 74.9672%\n",
      "total_backward_count 1165010 real_backward_count 43271   3.714%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.378672/  1.589489, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4544%\n",
      "layer   3  Sparsity: 74.7463%\n",
      "total_backward_count 1174800 real_backward_count 43344   3.689%\n",
      "fc layer 1 self.abs_max_out: 7801.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.382698/  1.589843, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3915%\n",
      "layer   3  Sparsity: 74.9995%\n",
      "total_backward_count 1184590 real_backward_count 43391   3.663%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.378070/  1.579461, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4931%\n",
      "layer   3  Sparsity: 74.8939%\n",
      "total_backward_count 1194380 real_backward_count 43434   3.637%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.371860/  1.578957, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6640%\n",
      "layer   3  Sparsity: 75.1043%\n",
      "total_backward_count 1204170 real_backward_count 43473   3.610%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.369505/  1.578145, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7095%\n",
      "layer   3  Sparsity: 75.4206%\n",
      "total_backward_count 1213960 real_backward_count 43527   3.586%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.377596/  1.588147, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6459%\n",
      "layer   3  Sparsity: 75.6796%\n",
      "total_backward_count 1223750 real_backward_count 43583   3.561%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.377382/  1.584401, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7092%\n",
      "layer   3  Sparsity: 75.3112%\n",
      "total_backward_count 1233540 real_backward_count 43640   3.538%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.372005/  1.580348, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5560%\n",
      "layer   3  Sparsity: 75.3403%\n",
      "total_backward_count 1243330 real_backward_count 43696   3.514%\n",
      "fc layer 1 self.abs_max_out: 7822.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.371816/  1.588611, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4583%\n",
      "layer   3  Sparsity: 75.1415%\n",
      "total_backward_count 1253120 real_backward_count 43759   3.492%\n",
      "fc layer 1 self.abs_max_out: 7839.0\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.374538/  1.584739, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3994%\n",
      "layer   3  Sparsity: 75.2638%\n",
      "total_backward_count 1262910 real_backward_count 43815   3.469%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.376202/  1.585263, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3993%\n",
      "layer   3  Sparsity: 75.4062%\n",
      "total_backward_count 1272700 real_backward_count 43849   3.445%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.373172/  1.588817, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4403%\n",
      "layer   3  Sparsity: 75.5659%\n",
      "total_backward_count 1282490 real_backward_count 43891   3.422%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.383275/  1.597057, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5016%\n",
      "layer   3  Sparsity: 75.4459%\n",
      "total_backward_count 1292280 real_backward_count 43943   3.400%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.384723/  1.603267, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4475%\n",
      "layer   3  Sparsity: 75.6897%\n",
      "total_backward_count 1302070 real_backward_count 43980   3.378%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.382657/  1.595777, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3707%\n",
      "layer   3  Sparsity: 75.6463%\n",
      "total_backward_count 1311860 real_backward_count 44009   3.355%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.382683/  1.587406, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3520%\n",
      "layer   3  Sparsity: 75.7940%\n",
      "total_backward_count 1321650 real_backward_count 44057   3.333%\n",
      "fc layer 1 self.abs_max_out: 7846.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.379705/  1.590980, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3291%\n",
      "layer   3  Sparsity: 75.9367%\n",
      "total_backward_count 1331440 real_backward_count 44099   3.312%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.382272/  1.598865, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4087%\n",
      "layer   3  Sparsity: 75.8292%\n",
      "total_backward_count 1341230 real_backward_count 44127   3.290%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.384176/  1.590373, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4502%\n",
      "layer   3  Sparsity: 75.6795%\n",
      "total_backward_count 1351020 real_backward_count 44169   3.269%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.373828/  1.583984, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4193%\n",
      "layer   3  Sparsity: 75.4875%\n",
      "total_backward_count 1360810 real_backward_count 44216   3.249%\n",
      "fc layer 1 self.abs_max_out: 7848.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.372875/  1.586477, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3621%\n",
      "layer   3  Sparsity: 75.3221%\n",
      "total_backward_count 1370600 real_backward_count 44252   3.229%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.375659/  1.595465, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3235%\n",
      "layer   3  Sparsity: 75.5313%\n",
      "total_backward_count 1380390 real_backward_count 44298   3.209%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.381067/  1.594322, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3133%\n",
      "layer   3  Sparsity: 75.8336%\n",
      "total_backward_count 1390180 real_backward_count 44344   3.190%\n",
      "fc layer 1 self.abs_max_out: 7863.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.383188/  1.590999, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4311%\n",
      "layer   3  Sparsity: 76.1081%\n",
      "total_backward_count 1399970 real_backward_count 44385   3.170%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.382812/  1.597343, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6208%\n",
      "layer   3  Sparsity: 76.3986%\n",
      "total_backward_count 1409760 real_backward_count 44424   3.151%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.383417/  1.595475, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7364%\n",
      "layer   3  Sparsity: 76.5164%\n",
      "total_backward_count 1419550 real_backward_count 44454   3.132%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.381928/  1.590529, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.7813%\n",
      "layer   3  Sparsity: 76.5329%\n",
      "total_backward_count 1429340 real_backward_count 44486   3.112%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.372238/  1.579967, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5834%\n",
      "layer   3  Sparsity: 76.2936%\n",
      "total_backward_count 1439130 real_backward_count 44527   3.094%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.368549/  1.580442, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4639%\n",
      "layer   3  Sparsity: 76.1918%\n",
      "total_backward_count 1448920 real_backward_count 44549   3.075%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.370579/  1.586149, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4405%\n",
      "layer   3  Sparsity: 76.2288%\n",
      "total_backward_count 1458710 real_backward_count 44569   3.055%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.370957/  1.580549, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3743%\n",
      "layer   3  Sparsity: 76.3536%\n",
      "total_backward_count 1468500 real_backward_count 44605   3.037%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.373827/  1.588129, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3135%\n",
      "layer   3  Sparsity: 76.2507%\n",
      "total_backward_count 1478290 real_backward_count 44634   3.019%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.372646/  1.580242, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2499%\n",
      "layer   3  Sparsity: 76.1743%\n",
      "total_backward_count 1488080 real_backward_count 44668   3.002%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.373658/  1.590114, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 68.9406%\n",
      "layer   3  Sparsity: 76.2857%\n",
      "total_backward_count 1497870 real_backward_count 44697   2.984%\n",
      "fc layer 1 self.abs_max_out: 7883.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.378186/  1.587854, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.0138%\n",
      "layer   3  Sparsity: 76.3659%\n",
      "total_backward_count 1507660 real_backward_count 44725   2.967%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.378042/  1.582049, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.0911%\n",
      "layer   3  Sparsity: 76.2409%\n",
      "total_backward_count 1517450 real_backward_count 44758   2.950%\n",
      "fc layer 1 self.abs_max_out: 7927.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.375923/  1.580266, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.1883%\n",
      "layer   3  Sparsity: 76.2495%\n",
      "total_backward_count 1527240 real_backward_count 44801   2.933%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.369455/  1.578746, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3134%\n",
      "layer   3  Sparsity: 76.1404%\n",
      "total_backward_count 1537030 real_backward_count 44850   2.918%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.364918/  1.586622, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3901%\n",
      "layer   3  Sparsity: 76.1260%\n",
      "total_backward_count 1546820 real_backward_count 44875   2.901%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.366209/  1.584336, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4421%\n",
      "layer   3  Sparsity: 76.2063%\n",
      "total_backward_count 1556610 real_backward_count 44889   2.884%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.366857/  1.584347, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3779%\n",
      "layer   3  Sparsity: 76.2615%\n",
      "total_backward_count 1566400 real_backward_count 44915   2.867%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.368076/  1.588334, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3798%\n",
      "layer   3  Sparsity: 76.0980%\n",
      "total_backward_count 1576190 real_backward_count 44962   2.853%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.369939/  1.576613, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2931%\n",
      "layer   3  Sparsity: 76.1818%\n",
      "total_backward_count 1585980 real_backward_count 44993   2.837%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.369136/  1.583724, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2480%\n",
      "layer   3  Sparsity: 76.1291%\n",
      "total_backward_count 1595770 real_backward_count 45034   2.822%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.366768/  1.588096, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2453%\n",
      "layer   3  Sparsity: 76.0846%\n",
      "total_backward_count 1605560 real_backward_count 45086   2.808%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.368522/  1.577429, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3565%\n",
      "layer   3  Sparsity: 76.1636%\n",
      "total_backward_count 1615350 real_backward_count 45119   2.793%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.360355/  1.574960, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3686%\n",
      "layer   3  Sparsity: 76.1347%\n",
      "total_backward_count 1625140 real_backward_count 45156   2.779%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.360917/  1.574713, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4315%\n",
      "layer   3  Sparsity: 76.0968%\n",
      "total_backward_count 1634930 real_backward_count 45199   2.765%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.361299/  1.575112, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4575%\n",
      "layer   3  Sparsity: 75.9255%\n",
      "total_backward_count 1644720 real_backward_count 45266   2.752%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.360407/  1.578809, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3334%\n",
      "layer   3  Sparsity: 76.3989%\n",
      "total_backward_count 1654510 real_backward_count 45303   2.738%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.359386/  1.571500, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3715%\n",
      "layer   3  Sparsity: 76.3658%\n",
      "total_backward_count 1664300 real_backward_count 45331   2.724%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.357084/  1.577879, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3550%\n",
      "layer   3  Sparsity: 76.2567%\n",
      "total_backward_count 1674090 real_backward_count 45351   2.709%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.361982/  1.579862, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3780%\n",
      "layer   3  Sparsity: 76.2908%\n",
      "total_backward_count 1683880 real_backward_count 45386   2.695%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.360318/  1.590559, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3668%\n",
      "layer   3  Sparsity: 76.3071%\n",
      "total_backward_count 1693670 real_backward_count 45413   2.681%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.366142/  1.580979, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5530%\n",
      "layer   3  Sparsity: 76.1795%\n",
      "total_backward_count 1703460 real_backward_count 45454   2.668%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.362797/  1.581595, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5780%\n",
      "layer   3  Sparsity: 76.3174%\n",
      "total_backward_count 1713250 real_backward_count 45496   2.656%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.352425/  1.578555, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6263%\n",
      "layer   3  Sparsity: 76.3168%\n",
      "total_backward_count 1723040 real_backward_count 45521   2.642%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.351686/  1.576750, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6042%\n",
      "layer   3  Sparsity: 76.1667%\n",
      "total_backward_count 1732830 real_backward_count 45548   2.629%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.349268/  1.574004, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5169%\n",
      "layer   3  Sparsity: 76.3256%\n",
      "total_backward_count 1742620 real_backward_count 45573   2.615%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.351915/  1.579228, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5024%\n",
      "layer   3  Sparsity: 76.4279%\n",
      "total_backward_count 1752410 real_backward_count 45589   2.602%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.354925/  1.578619, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4782%\n",
      "layer   3  Sparsity: 76.5426%\n",
      "total_backward_count 1762200 real_backward_count 45616   2.589%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.355302/  1.569178, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5351%\n",
      "layer   3  Sparsity: 76.3311%\n",
      "total_backward_count 1771990 real_backward_count 45642   2.576%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.350738/  1.577951, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5331%\n",
      "layer   3  Sparsity: 76.2332%\n",
      "total_backward_count 1781780 real_backward_count 45665   2.563%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.351960/  1.579188, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4557%\n",
      "layer   3  Sparsity: 76.3177%\n",
      "total_backward_count 1791570 real_backward_count 45685   2.550%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.353554/  1.579264, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5270%\n",
      "layer   3  Sparsity: 76.4263%\n",
      "total_backward_count 1801360 real_backward_count 45706   2.537%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.348169/  1.572795, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5405%\n",
      "layer   3  Sparsity: 76.4171%\n",
      "total_backward_count 1811150 real_backward_count 45720   2.524%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.345390/  1.568009, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4823%\n",
      "layer   3  Sparsity: 76.2889%\n",
      "total_backward_count 1820940 real_backward_count 45743   2.512%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.345053/  1.565252, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4557%\n",
      "layer   3  Sparsity: 76.0778%\n",
      "total_backward_count 1830730 real_backward_count 45768   2.500%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.345364/  1.565128, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5016%\n",
      "layer   3  Sparsity: 75.8849%\n",
      "total_backward_count 1840520 real_backward_count 45792   2.488%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.342938/  1.566396, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4874%\n",
      "layer   3  Sparsity: 75.7563%\n",
      "total_backward_count 1850310 real_backward_count 45811   2.476%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.343722/  1.570763, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6763%\n",
      "layer   3  Sparsity: 75.8632%\n",
      "total_backward_count 1860100 real_backward_count 45844   2.465%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.343436/  1.565278, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.6425%\n",
      "layer   3  Sparsity: 76.1917%\n",
      "total_backward_count 1869890 real_backward_count 45860   2.453%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.338181/  1.554168, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.5631%\n",
      "layer   3  Sparsity: 76.2194%\n",
      "total_backward_count 1879680 real_backward_count 45886   2.441%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.335346/  1.559744, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4218%\n",
      "layer   3  Sparsity: 75.9756%\n",
      "total_backward_count 1889470 real_backward_count 45914   2.430%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.340230/  1.572608, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4574%\n",
      "layer   3  Sparsity: 76.1081%\n",
      "total_backward_count 1899260 real_backward_count 45933   2.418%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.336599/  1.565853, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4626%\n",
      "layer   3  Sparsity: 76.3747%\n",
      "total_backward_count 1909050 real_backward_count 45971   2.408%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.332715/  1.555494, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3414%\n",
      "layer   3  Sparsity: 76.3962%\n",
      "total_backward_count 1918840 real_backward_count 45990   2.397%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.335798/  1.560960, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.4441%\n",
      "layer   3  Sparsity: 76.3509%\n",
      "total_backward_count 1928630 real_backward_count 46017   2.386%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.336758/  1.566573, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3774%\n",
      "layer   3  Sparsity: 76.1751%\n",
      "total_backward_count 1938420 real_backward_count 46042   2.375%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.339125/  1.559456, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.3444%\n",
      "layer   3  Sparsity: 75.9658%\n",
      "total_backward_count 1948210 real_backward_count 46063   2.364%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.334355/  1.558442, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 69.2518%\n",
      "layer   3  Sparsity: 75.9582%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce1bbd27bb844319568f781b8d5c749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.33435</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>1.55844</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-343</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7ek5t0qz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7ek5t0qz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251128_090305-7ek5t0qz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w2zj0de7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_132633-w2zj0de7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w2zj0de7' target=\"_blank\">swift-sweep-347</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w2zj0de7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w2zj0de7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251128_132642_089', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 391.0\n",
      "lif layer 1 self.abs_max_v: 391.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 712.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "fc layer 1 self.abs_max_out: 510.0\n",
      "lif layer 1 self.abs_max_v: 525.5\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "lif layer 2 self.abs_max_v: 1078.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "lif layer 1 self.abs_max_v: 687.5\n",
      "fc layer 2 self.abs_max_out: 870.0\n",
      "lif layer 2 self.abs_max_v: 1340.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 1 self.abs_max_out: 570.0\n",
      "lif layer 1 self.abs_max_v: 726.5\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "lif layer 2 self.abs_max_v: 1391.5\n",
      "fc layer 1 self.abs_max_out: 745.0\n",
      "lif layer 1 self.abs_max_v: 848.5\n",
      "fc layer 2 self.abs_max_out: 985.0\n",
      "lif layer 2 self.abs_max_v: 1655.0\n",
      "fc layer 1 self.abs_max_out: 924.0\n",
      "lif layer 1 self.abs_max_v: 924.0\n",
      "lif layer 2 self.abs_max_v: 1701.5\n",
      "fc layer 1 self.abs_max_out: 1065.0\n",
      "lif layer 1 self.abs_max_v: 1103.5\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "fc layer 1 self.abs_max_out: 1177.0\n",
      "lif layer 1 self.abs_max_v: 1177.0\n",
      "fc layer 1 self.abs_max_out: 1188.0\n",
      "lif layer 1 self.abs_max_v: 1188.0\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "lif layer 1 self.abs_max_v: 1473.5\n",
      "fc layer 1 self.abs_max_out: 1346.0\n",
      "lif layer 1 self.abs_max_v: 1500.0\n",
      "lif layer 1 self.abs_max_v: 1538.0\n",
      "lif layer 1 self.abs_max_v: 1584.0\n",
      "lif layer 1 self.abs_max_v: 1643.0\n",
      "fc layer 1 self.abs_max_out: 1471.0\n",
      "fc layer 1 self.abs_max_out: 1479.0\n",
      "lif layer 2 self.abs_max_v: 2012.0\n",
      "fc layer 1 self.abs_max_out: 1725.0\n",
      "lif layer 1 self.abs_max_v: 1725.0\n",
      "fc layer 1 self.abs_max_out: 1986.0\n",
      "lif layer 1 self.abs_max_v: 1986.0\n",
      "fc layer 1 self.abs_max_out: 2245.0\n",
      "lif layer 1 self.abs_max_v: 2245.0\n",
      "fc layer 2 self.abs_max_out: 1273.0\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "lif layer 2 self.abs_max_v: 2045.5\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "lif layer 2 self.abs_max_v: 2110.5\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "fc layer 2 self.abs_max_out: 1374.0\n",
      "fc layer 2 self.abs_max_out: 1384.0\n",
      "fc layer 2 self.abs_max_out: 1541.0\n",
      "lif layer 1 self.abs_max_v: 2272.0\n",
      "lif layer 2 self.abs_max_v: 2182.0\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "lif layer 2 self.abs_max_v: 2430.0\n",
      "lif layer 1 self.abs_max_v: 2356.0\n",
      "lif layer 1 self.abs_max_v: 2423.0\n",
      "fc layer 1 self.abs_max_out: 2727.0\n",
      "lif layer 1 self.abs_max_v: 2872.0\n",
      "fc layer 1 self.abs_max_out: 2874.0\n",
      "lif layer 1 self.abs_max_v: 3785.0\n",
      "lif layer 1 self.abs_max_v: 3816.5\n",
      "fc layer 2 self.abs_max_out: 1565.0\n",
      "lif layer 2 self.abs_max_v: 2457.0\n",
      "lif layer 2 self.abs_max_v: 2542.5\n",
      "lif layer 2 self.abs_max_v: 2582.5\n",
      "fc layer 2 self.abs_max_out: 1613.0\n",
      "fc layer 2 self.abs_max_out: 1755.0\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "fc layer 2 self.abs_max_out: 1766.0\n",
      "fc layer 2 self.abs_max_out: 1916.0\n",
      "lif layer 2 self.abs_max_v: 2697.0\n",
      "lif layer 2 self.abs_max_v: 2774.5\n",
      "lif layer 2 self.abs_max_v: 2910.5\n",
      "lif layer 2 self.abs_max_v: 3154.5\n",
      "lif layer 1 self.abs_max_v: 3828.5\n",
      "lif layer 1 self.abs_max_v: 3871.0\n",
      "lif layer 1 self.abs_max_v: 4034.0\n",
      "lif layer 1 self.abs_max_v: 4146.0\n",
      "lif layer 2 self.abs_max_v: 3171.0\n",
      "lif layer 2 self.abs_max_v: 3451.0\n",
      "lif layer 2 self.abs_max_v: 3472.5\n",
      "lif layer 1 self.abs_max_v: 4324.0\n",
      "fc layer 2 self.abs_max_out: 2096.0\n",
      "lif layer 2 self.abs_max_v: 3735.5\n",
      "lif layer 2 self.abs_max_v: 3747.0\n",
      "lif layer 2 self.abs_max_v: 3757.0\n",
      "lif layer 2 self.abs_max_v: 3816.5\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "fc layer 2 self.abs_max_out: 2234.0\n",
      "lif layer 2 self.abs_max_v: 3909.5\n",
      "lif layer 2 self.abs_max_v: 4038.0\n",
      "lif layer 2 self.abs_max_v: 4122.0\n",
      "lif layer 2 self.abs_max_v: 4181.0\n",
      "fc layer 3 self.abs_max_out: 754.0\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "lif layer 2 self.abs_max_v: 4303.5\n",
      "fc layer 2 self.abs_max_out: 2307.0\n",
      "lif layer 2 self.abs_max_v: 4407.5\n",
      "fc layer 1 self.abs_max_out: 2878.0\n",
      "fc layer 1 self.abs_max_out: 2913.0\n",
      "lif layer 1 self.abs_max_v: 4543.5\n",
      "fc layer 1 self.abs_max_out: 2990.0\n",
      "fc layer 1 self.abs_max_out: 3049.0\n",
      "lif layer 1 self.abs_max_v: 4695.0\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "lif layer 1 self.abs_max_v: 4787.5\n",
      "fc layer 2 self.abs_max_out: 2498.0\n",
      "lif layer 2 self.abs_max_v: 4559.0\n",
      "fc layer 1 self.abs_max_out: 3106.0\n",
      "lif layer 1 self.abs_max_v: 4909.5\n",
      "lif layer 1 self.abs_max_v: 5112.5\n",
      "lif layer 1 self.abs_max_v: 5135.0\n",
      "fc layer 1 self.abs_max_out: 3157.0\n",
      "lif layer 2 self.abs_max_v: 4644.5\n",
      "lif layer 2 self.abs_max_v: 4692.0\n",
      "lif layer 2 self.abs_max_v: 4833.0\n",
      "fc layer 2 self.abs_max_out: 2501.0\n",
      "lif layer 2 self.abs_max_v: 4906.5\n",
      "fc layer 2 self.abs_max_out: 2543.0\n",
      "lif layer 2 self.abs_max_v: 4950.5\n",
      "lif layer 2 self.abs_max_v: 4981.5\n",
      "fc layer 2 self.abs_max_out: 2652.0\n",
      "fc layer 3 self.abs_max_out: 831.0\n",
      "fc layer 2 self.abs_max_out: 2660.0\n",
      "fc layer 2 self.abs_max_out: 2690.0\n",
      "lif layer 2 self.abs_max_v: 5085.5\n",
      "fc layer 1 self.abs_max_out: 3442.0\n",
      "lif layer 1 self.abs_max_v: 5260.5\n",
      "fc layer 1 self.abs_max_out: 3574.0\n",
      "fc layer 1 self.abs_max_out: 3578.0\n",
      "lif layer 1 self.abs_max_v: 5443.0\n",
      "lif layer 1 self.abs_max_v: 6037.0\n",
      "fc layer 1 self.abs_max_out: 3802.0\n",
      "lif layer 1 self.abs_max_v: 6665.5\n",
      "fc layer 2 self.abs_max_out: 2693.0\n",
      "lif layer 1 self.abs_max_v: 6679.5\n",
      "lif layer 1 self.abs_max_v: 6911.0\n",
      "lif layer 1 self.abs_max_v: 7011.5\n",
      "fc layer 2 self.abs_max_out: 2887.0\n",
      "fc layer 2 self.abs_max_out: 2935.0\n",
      "lif layer 2 self.abs_max_v: 5126.5\n",
      "lif layer 2 self.abs_max_v: 5265.0\n",
      "fc layer 2 self.abs_max_out: 2941.0\n",
      "fc layer 2 self.abs_max_out: 2984.0\n",
      "lif layer 2 self.abs_max_v: 5330.5\n",
      "lif layer 2 self.abs_max_v: 5582.5\n",
      "fc layer 2 self.abs_max_out: 3018.0\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "fc layer 1 self.abs_max_out: 3892.0\n",
      "fc layer 1 self.abs_max_out: 3987.0\n",
      "lif layer 1 self.abs_max_v: 7138.5\n",
      "fc layer 2 self.abs_max_out: 3069.0\n",
      "fc layer 2 self.abs_max_out: 3118.0\n",
      "fc layer 1 self.abs_max_out: 4181.0\n",
      "fc layer 1 self.abs_max_out: 4688.0\n",
      "lif layer 1 self.abs_max_v: 8007.5\n",
      "lif layer 2 self.abs_max_v: 5584.5\n",
      "fc layer 2 self.abs_max_out: 3128.0\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 3 self.abs_max_out: 857.0\n",
      "lif layer 1 self.abs_max_v: 8318.0\n",
      "fc layer 3 self.abs_max_out: 859.0\n",
      "fc layer 3 self.abs_max_out: 920.0\n",
      "fc layer 1 self.abs_max_out: 4914.0\n",
      "lif layer 1 self.abs_max_v: 8562.0\n",
      "fc layer 2 self.abs_max_out: 3130.0\n",
      "fc layer 2 self.abs_max_out: 3211.0\n",
      "lif layer 2 self.abs_max_v: 5640.5\n",
      "lif layer 2 self.abs_max_v: 5920.5\n",
      "fc layer 1 self.abs_max_out: 5258.0\n",
      "fc layer 1 self.abs_max_out: 5480.0\n",
      "lif layer 1 self.abs_max_v: 9225.5\n",
      "lif layer 1 self.abs_max_v: 9412.0\n",
      "lif layer 1 self.abs_max_v: 9540.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.645768/  1.985437, val:  28.33%, val_best:  28.33%, tr:  98.98%, tr_best:  98.98%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 63.8062%\n",
      "layer   3  Sparsity: 55.9277%\n",
      "total_backward_count 9790 real_backward_count 1503  15.352%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "fc layer 1 self.abs_max_out: 5666.0\n",
      "lif layer 1 self.abs_max_v: 9876.0\n",
      "fc layer 1 self.abs_max_out: 5680.0\n",
      "lif layer 1 self.abs_max_v: 10618.0\n",
      "fc layer 2 self.abs_max_out: 3213.0\n",
      "fc layer 2 self.abs_max_out: 3454.0\n",
      "fc layer 3 self.abs_max_out: 995.0\n",
      "fc layer 3 self.abs_max_out: 1006.0\n",
      "fc layer 1 self.abs_max_out: 5781.0\n",
      "lif layer 1 self.abs_max_v: 10706.5\n",
      "fc layer 1 self.abs_max_out: 5800.0\n",
      "lif layer 1 self.abs_max_v: 10896.0\n",
      "fc layer 1 self.abs_max_out: 5996.0\n",
      "fc layer 3 self.abs_max_out: 1051.0\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "lif layer 2 self.abs_max_v: 5991.0\n",
      "lif layer 2 self.abs_max_v: 6133.5\n",
      "lif layer 2 self.abs_max_v: 6408.0\n",
      "fc layer 1 self.abs_max_out: 6082.0\n",
      "lif layer 1 self.abs_max_v: 11261.0\n",
      "fc layer 1 self.abs_max_out: 6111.0\n",
      "lif layer 1 self.abs_max_v: 11542.0\n",
      "fc layer 2 self.abs_max_out: 3503.0\n",
      "fc layer 1 self.abs_max_out: 6465.0\n",
      "fc layer 2 self.abs_max_out: 3528.0\n",
      "lif layer 2 self.abs_max_v: 6466.5\n",
      "fc layer 2 self.abs_max_out: 3547.0\n",
      "lif layer 2 self.abs_max_v: 6509.5\n",
      "lif layer 2 self.abs_max_v: 6612.0\n",
      "fc layer 2 self.abs_max_out: 3756.0\n",
      "fc layer 2 self.abs_max_out: 3773.0\n",
      "lif layer 2 self.abs_max_v: 6692.5\n",
      "lif layer 2 self.abs_max_v: 6726.5\n",
      "fc layer 1 self.abs_max_out: 6683.0\n",
      "lif layer 1 self.abs_max_v: 11862.0\n",
      "lif layer 1 self.abs_max_v: 11992.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.557034/  1.894894, val:  40.00%, val_best:  40.00%, tr:  98.98%, tr_best:  98.98%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4660%\n",
      "layer   2  Sparsity: 70.4176%\n",
      "layer   3  Sparsity: 59.5415%\n",
      "total_backward_count 19580 real_backward_count 2847  14.540%\n",
      "lif layer 2 self.abs_max_v: 6781.0\n",
      "lif layer 2 self.abs_max_v: 6905.5\n",
      "lif layer 2 self.abs_max_v: 6974.0\n",
      "lif layer 2 self.abs_max_v: 6988.0\n",
      "fc layer 2 self.abs_max_out: 3859.0\n",
      "lif layer 2 self.abs_max_v: 7019.0\n",
      "fc layer 2 self.abs_max_out: 4036.0\n",
      "fc layer 2 self.abs_max_out: 4055.0\n",
      "fc layer 2 self.abs_max_out: 4413.0\n",
      "fc layer 2 self.abs_max_out: 4460.0\n",
      "fc layer 3 self.abs_max_out: 1140.0\n",
      "lif layer 2 self.abs_max_v: 7146.5\n",
      "lif layer 2 self.abs_max_v: 7192.5\n",
      "lif layer 2 self.abs_max_v: 7264.5\n",
      "lif layer 2 self.abs_max_v: 7459.5\n",
      "fc layer 1 self.abs_max_out: 6826.0\n",
      "lif layer 2 self.abs_max_v: 7593.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.528849/  1.835867, val:  45.00%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4776%\n",
      "layer   2  Sparsity: 70.9316%\n",
      "layer   3  Sparsity: 59.6698%\n",
      "total_backward_count 29370 real_backward_count 4124  14.042%\n",
      "fc layer 1 self.abs_max_out: 6892.0\n",
      "fc layer 1 self.abs_max_out: 7311.0\n",
      "lif layer 1 self.abs_max_v: 12109.0\n",
      "lif layer 1 self.abs_max_v: 12353.5\n",
      "fc layer 1 self.abs_max_out: 7470.0\n",
      "fc layer 1 self.abs_max_out: 8003.0\n",
      "fc layer 1 self.abs_max_out: 8466.0\n",
      "lif layer 1 self.abs_max_v: 14101.0\n",
      "lif layer 1 self.abs_max_v: 14682.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.516542/  1.840647, val:  37.92%, val_best:  45.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4286%\n",
      "layer   2  Sparsity: 72.8815%\n",
      "layer   3  Sparsity: 61.9194%\n",
      "total_backward_count 39160 real_backward_count 5394  13.774%\n",
      "fc layer 2 self.abs_max_out: 4463.0\n",
      "fc layer 2 self.abs_max_out: 4480.0\n",
      "lif layer 2 self.abs_max_v: 7806.5\n",
      "fc layer 2 self.abs_max_out: 4494.0\n",
      "fc layer 2 self.abs_max_out: 4529.0\n",
      "fc layer 2 self.abs_max_out: 4633.0\n",
      "fc layer 2 self.abs_max_out: 4739.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.491911/  1.843972, val:  42.08%, val_best:  45.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 71.9084%\n",
      "layer   3  Sparsity: 62.3974%\n",
      "total_backward_count 48950 real_backward_count 6664  13.614%\n",
      "fc layer 2 self.abs_max_out: 4848.0\n",
      "fc layer 1 self.abs_max_out: 8516.0\n",
      "lif layer 2 self.abs_max_v: 8118.0\n",
      "fc layer 1 self.abs_max_out: 9077.0\n",
      "fc layer 1 self.abs_max_out: 9741.0\n",
      "lif layer 1 self.abs_max_v: 16187.5\n",
      "lif layer 1 self.abs_max_v: 17052.0\n",
      "lif layer 2 self.abs_max_v: 8258.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.467505/  1.819654, val:  42.50%, val_best:  45.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 73.5625%\n",
      "layer   3  Sparsity: 63.5183%\n",
      "total_backward_count 58740 real_backward_count 7923  13.488%\n",
      "fc layer 3 self.abs_max_out: 1270.0\n",
      "fc layer 3 self.abs_max_out: 1274.0\n",
      "lif layer 2 self.abs_max_v: 8489.0\n",
      "lif layer 2 self.abs_max_v: 8700.5\n",
      "fc layer 2 self.abs_max_out: 4964.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.477082/  1.770727, val:  47.08%, val_best:  47.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 72.7127%\n",
      "layer   3  Sparsity: 63.4335%\n",
      "total_backward_count 68530 real_backward_count 9137  13.333%\n",
      "lif layer 2 self.abs_max_v: 9137.0\n",
      "fc layer 1 self.abs_max_out: 9957.0\n",
      "lif layer 1 self.abs_max_v: 17070.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.449834/  1.739851, val:  53.33%, val_best:  53.33%, tr:  99.18%, tr_best:  99.80%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4523%\n",
      "layer   2  Sparsity: 73.3975%\n",
      "layer   3  Sparsity: 64.3792%\n",
      "total_backward_count 78320 real_backward_count 10310  13.164%\n",
      "lif layer 1 self.abs_max_v: 17703.5\n",
      "lif layer 1 self.abs_max_v: 17776.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.474462/  1.741217, val:  54.17%, val_best:  54.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4599%\n",
      "layer   2  Sparsity: 73.0749%\n",
      "layer   3  Sparsity: 64.5385%\n",
      "total_backward_count 88110 real_backward_count 11574  13.136%\n",
      "fc layer 2 self.abs_max_out: 5108.0\n",
      "lif layer 1 self.abs_max_v: 18189.0\n",
      "lif layer 1 self.abs_max_v: 18295.5\n",
      "fc layer 1 self.abs_max_out: 10665.0\n",
      "lif layer 1 self.abs_max_v: 18526.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.451582/  1.770555, val:  49.58%, val_best:  54.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4528%\n",
      "layer   2  Sparsity: 73.4453%\n",
      "layer   3  Sparsity: 66.1757%\n",
      "total_backward_count 97900 real_backward_count 12736  13.009%\n",
      "lif layer 1 self.abs_max_v: 18640.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.478233/  1.818172, val:  44.17%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4366%\n",
      "layer   2  Sparsity: 72.7287%\n",
      "layer   3  Sparsity: 65.9479%\n",
      "total_backward_count 107690 real_backward_count 13833  12.845%\n",
      "fc layer 2 self.abs_max_out: 5112.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.465368/  1.747768, val:  52.08%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.4093%\n",
      "layer   2  Sparsity: 72.0166%\n",
      "layer   3  Sparsity: 65.1371%\n",
      "total_backward_count 117480 real_backward_count 14974  12.746%\n",
      "fc layer 1 self.abs_max_out: 10837.0\n",
      "lif layer 1 self.abs_max_v: 18772.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.440431/  1.747895, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4209%\n",
      "layer   2  Sparsity: 72.9000%\n",
      "layer   3  Sparsity: 66.5030%\n",
      "total_backward_count 127270 real_backward_count 16060  12.619%\n",
      "fc layer 1 self.abs_max_out: 10851.0\n",
      "lif layer 1 self.abs_max_v: 19212.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.487060/  1.798458, val:  43.75%, val_best:  57.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 71.5576%\n",
      "layer   3  Sparsity: 66.7673%\n",
      "total_backward_count 137060 real_backward_count 17199  12.549%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2, 4, 6, 8, 10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [10,15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [True]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \n",
    "        \"random_select_ratio\": {\"values\": [1,2,3,4,5]},\n",
    "        \"leaky_temporal_filter\": {\"values\": [0.0, 0.25, 0.5, 0.75, 1.0]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "        random_select_ratio  =  wandb.config.random_select_ratio,\n",
    "        leaky_temporal_filter  =  wandb.config.leaky_temporal_filter,\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'hcapkd0n'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
