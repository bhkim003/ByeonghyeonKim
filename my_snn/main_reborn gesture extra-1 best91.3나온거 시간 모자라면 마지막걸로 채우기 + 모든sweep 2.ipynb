{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6868/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        smallest_now_T = 99999\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                if epoch == 0 and now_T < smallest_now_T:\n",
    "                    smallest_now_T = now_T\n",
    "                    print(f'smallest_now_T updated: {smallest_now_T}')\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                if now_T < now_time_steps:\n",
    "                    # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                    diff = now_time_steps - now_T\n",
    "\n",
    "                    # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                    last_frame = inputs[:, -1:, :, :, :]\n",
    "\n",
    "                    # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                    pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                    # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                    inputs = torch.cat([inputs, pad_frames], dim=1)\n",
    "                else:\n",
    "                    # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                    start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                    # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                    inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            smallest_now_T_val = 99999\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            if epoch == 0 and now_T < smallest_now_T_val:\n",
    "                                smallest_now_T_val = now_T\n",
    "                                print(f'smallest_now_T_val updated: {smallest_now_T_val}')\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "\n",
    "                            if now_T < now_time_steps:\n",
    "                                # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                                diff = now_time_steps - now_T\n",
    "\n",
    "                                # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                                last_frame = inputs_val[:, -1:, :, :, :]\n",
    "\n",
    "                                # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                                pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                                # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                                inputs_val = torch.cat([inputs_val, pad_frames], dim=1)\n",
    "                            else:\n",
    "                                pass\n",
    "                            \n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.8\n",
    "                elif epoch > 150:\n",
    "                    assert val_acc_best > 0.88\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 619q0gug with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_212817-619q0gug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/619q0gug' target=\"_blank\">exalted-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/619q0gug' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/619q0gug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251117_212826_573', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 201.0\n",
      "lif layer 1 self.abs_max_v: 201.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 227.0\n",
      "lif layer 1 self.abs_max_v: 278.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "lif layer 1 self.abs_max_v: 347.5\n",
      "fc layer 1 self.abs_max_out: 275.0\n",
      "lif layer 1 self.abs_max_v: 397.0\n",
      "fc layer 2 self.abs_max_out: 132.0\n",
      "lif layer 2 self.abs_max_v: 125.0\n",
      "fc layer 1 self.abs_max_out: 474.0\n",
      "lif layer 1 self.abs_max_v: 474.0\n",
      "fc layer 2 self.abs_max_out: 146.0\n",
      "lif layer 2 self.abs_max_v: 177.5\n",
      "smallest_now_T updated: 125\n",
      "fc layer 2 self.abs_max_out: 174.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "fc layer 2 self.abs_max_out: 181.0\n",
      "lif layer 2 self.abs_max_v: 208.5\n",
      "fc layer 1 self.abs_max_out: 1103.0\n",
      "lif layer 1 self.abs_max_v: 1192.5\n",
      "fc layer 2 self.abs_max_out: 326.0\n",
      "lif layer 2 self.abs_max_v: 430.5\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 2 self.abs_max_out: 354.0\n",
      "lif layer 2 self.abs_max_v: 496.0\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 1166.0\n",
      "fc layer 3 self.abs_max_out: 61.0\n",
      "fc layer 1 self.abs_max_out: 1224.0\n",
      "lif layer 1 self.abs_max_v: 1224.0\n",
      "lif layer 2 self.abs_max_v: 559.0\n",
      "lif layer 2 self.abs_max_v: 571.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 2 self.abs_max_out: 393.0\n",
      "fc layer 2 self.abs_max_out: 520.0\n",
      "lif layer 2 self.abs_max_v: 646.5\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 2 self.abs_max_out: 664.0\n",
      "lif layer 2 self.abs_max_v: 664.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "fc layer 1 self.abs_max_out: 1296.0\n",
      "lif layer 1 self.abs_max_v: 1296.0\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "lif layer 2 self.abs_max_v: 695.0\n",
      "fc layer 1 self.abs_max_out: 1418.0\n",
      "lif layer 1 self.abs_max_v: 1418.0\n",
      "fc layer 2 self.abs_max_out: 701.0\n",
      "lif layer 2 self.abs_max_v: 747.5\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 2 self.abs_max_out: 754.0\n",
      "lif layer 2 self.abs_max_v: 754.0\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "lif layer 2 self.abs_max_v: 837.0\n",
      "fc layer 1 self.abs_max_out: 1510.0\n",
      "lif layer 1 self.abs_max_v: 1510.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "smallest_now_T updated: 79\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 2 self.abs_max_out: 904.0\n",
      "lif layer 2 self.abs_max_v: 904.0\n",
      "lif layer 2 self.abs_max_v: 927.5\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 2 self.abs_max_out: 966.0\n",
      "lif layer 2 self.abs_max_v: 966.0\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "fc layer 1 self.abs_max_out: 1766.0\n",
      "lif layer 1 self.abs_max_v: 1766.0\n",
      "lif layer 2 self.abs_max_v: 992.5\n",
      "lif layer 1 self.abs_max_v: 1797.0\n",
      "lif layer 2 self.abs_max_v: 1129.5\n",
      "lif layer 2 self.abs_max_v: 1187.0\n",
      "fc layer 2 self.abs_max_out: 976.0\n",
      "fc layer 2 self.abs_max_out: 1025.0\n",
      "fc layer 2 self.abs_max_out: 1172.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "lif layer 2 self.abs_max_v: 1306.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "lif layer 2 self.abs_max_v: 1312.0\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "lif layer 2 self.abs_max_v: 1321.0\n",
      "smallest_now_T updated: 65\n",
      "lif layer 2 self.abs_max_v: 1364.5\n",
      "lif layer 2 self.abs_max_v: 1411.5\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "lif layer 2 self.abs_max_v: 1488.0\n",
      "lif layer 2 self.abs_max_v: 1499.5\n",
      "lif layer 2 self.abs_max_v: 1561.0\n",
      "fc layer 2 self.abs_max_out: 1337.0\n",
      "lif layer 1 self.abs_max_v: 1905.5\n",
      "fc layer 2 self.abs_max_out: 1405.0\n",
      "lif layer 1 self.abs_max_v: 2012.0\n",
      "fc layer 2 self.abs_max_out: 1422.0\n",
      "lif layer 2 self.abs_max_v: 1606.5\n",
      "lif layer 1 self.abs_max_v: 2166.0\n",
      "lif layer 1 self.abs_max_v: 2211.0\n",
      "lif layer 2 self.abs_max_v: 1610.0\n",
      "lif layer 2 self.abs_max_v: 1711.0\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "fc layer 2 self.abs_max_out: 1680.0\n",
      "fc layer 3 self.abs_max_out: 297.0\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 2 self.abs_max_out: 1708.0\n",
      "fc layer 1 self.abs_max_out: 1795.0\n",
      "lif layer 1 self.abs_max_v: 2407.5\n",
      "lif layer 1 self.abs_max_v: 2536.0\n",
      "fc layer 2 self.abs_max_out: 1770.0\n",
      "lif layer 2 self.abs_max_v: 1770.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 43\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 2 self.abs_max_out: 1819.0\n",
      "lif layer 2 self.abs_max_v: 1819.0\n",
      "fc layer 3 self.abs_max_out: 319.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "fc layer 1 self.abs_max_out: 2178.0\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "lif layer 2 self.abs_max_v: 1916.5\n",
      "lif layer 1 self.abs_max_v: 2615.0\n",
      "lif layer 2 self.abs_max_v: 2095.5\n",
      "lif layer 2 self.abs_max_v: 2207.0\n",
      "lif layer 2 self.abs_max_v: 2266.5\n",
      "lif layer 1 self.abs_max_v: 2630.0\n",
      "lif layer 1 self.abs_max_v: 2862.0\n",
      "fc layer 3 self.abs_max_out: 385.0\n",
      "lif layer 1 self.abs_max_v: 2970.5\n",
      "fc layer 1 self.abs_max_out: 2326.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "lif layer 1 self.abs_max_v: 3064.5\n",
      "lif layer 1 self.abs_max_v: 3793.0\n",
      "lif layer 2 self.abs_max_v: 2343.5\n",
      "lif layer 2 self.abs_max_v: 2350.5\n",
      "lif layer 2 self.abs_max_v: 2423.5\n",
      "lif layer 2 self.abs_max_v: 2632.0\n",
      "fc layer 2 self.abs_max_out: 1860.0\n",
      "lif layer 2 self.abs_max_v: 2789.5\n",
      "lif layer 2 self.abs_max_v: 2867.0\n",
      "fc layer 1 self.abs_max_out: 2805.0\n",
      "fc layer 2 self.abs_max_out: 1921.0\n",
      "fc layer 2 self.abs_max_out: 1963.0\n",
      "lif layer 1 self.abs_max_v: 3975.0\n",
      "lif layer 1 self.abs_max_v: 4028.5\n",
      "fc layer 3 self.abs_max_out: 390.0\n",
      "lif layer 2 self.abs_max_v: 2871.0\n",
      "lif layer 1 self.abs_max_v: 4118.0\n",
      "lif layer 1 self.abs_max_v: 4658.0\n",
      "fc layer 2 self.abs_max_out: 2036.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "fc layer 1 self.abs_max_out: 2807.0\n",
      "lif layer 1 self.abs_max_v: 4723.0\n",
      "lif layer 1 self.abs_max_v: 4796.0\n",
      "fc layer 2 self.abs_max_out: 2060.0\n",
      "fc layer 2 self.abs_max_out: 2076.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.766433/  1.892078, val:  43.75%, val_best:  43.75%, tr:  87.03%, tr_best:  87.03%, epoch time: 41.56 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 91.6880%\n",
      "layer   2  Sparsity: 74.9917%\n",
      "layer   3  Sparsity: 79.2543%\n",
      "total_backward_count 4895 real_backward_count 1469  30.010%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 3 self.abs_max_out: 420.0\n",
      "fc layer 3 self.abs_max_out: 425.0\n",
      "fc layer 3 self.abs_max_out: 435.0\n",
      "fc layer 2 self.abs_max_out: 2122.0\n",
      "fc layer 2 self.abs_max_out: 2155.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "fc layer 2 self.abs_max_out: 2158.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 1 self.abs_max_out: 2964.0\n",
      "fc layer 1 self.abs_max_out: 3034.0\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 3 self.abs_max_out: 475.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "lif layer 2 self.abs_max_v: 2930.5\n",
      "fc layer 1 self.abs_max_out: 3106.0\n",
      "fc layer 2 self.abs_max_out: 2176.0\n",
      "fc layer 1 self.abs_max_out: 3545.0\n",
      "fc layer 2 self.abs_max_out: 2190.0\n",
      "fc layer 2 self.abs_max_out: 2199.0\n",
      "lif layer 1 self.abs_max_v: 4842.0\n",
      "lif layer 1 self.abs_max_v: 4885.0\n",
      "fc layer 2 self.abs_max_out: 2371.0\n",
      "fc layer 1 self.abs_max_out: 3668.0\n",
      "lif layer 1 self.abs_max_v: 5230.0\n",
      "lif layer 1 self.abs_max_v: 5974.0\n",
      "lif layer 1 self.abs_max_v: 6471.0\n",
      "lif layer 2 self.abs_max_v: 3059.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.612788/  1.893658, val:  41.67%, val_best:  43.75%, tr:  91.52%, tr_best:  91.52%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6822%\n",
      "layer   2  Sparsity: 76.0534%\n",
      "layer   3  Sparsity: 77.4972%\n",
      "total_backward_count 9790 real_backward_count 2637  26.936%\n",
      "fc layer 2 self.abs_max_out: 2393.0\n",
      "lif layer 2 self.abs_max_v: 3153.5\n",
      "fc layer 1 self.abs_max_out: 3677.0\n",
      "lif layer 2 self.abs_max_v: 3155.5\n",
      "lif layer 2 self.abs_max_v: 3160.5\n",
      "fc layer 2 self.abs_max_out: 2483.0\n",
      "lif layer 2 self.abs_max_v: 3319.0\n",
      "lif layer 2 self.abs_max_v: 3411.5\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "lif layer 2 self.abs_max_v: 3560.5\n",
      "lif layer 2 self.abs_max_v: 3562.5\n",
      "lif layer 2 self.abs_max_v: 3587.5\n",
      "fc layer 3 self.abs_max_out: 495.0\n",
      "lif layer 2 self.abs_max_v: 3593.5\n",
      "lif layer 2 self.abs_max_v: 3858.0\n",
      "fc layer 1 self.abs_max_out: 3722.0\n",
      "fc layer 1 self.abs_max_out: 3979.0\n",
      "lif layer 2 self.abs_max_v: 4183.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.565814/  1.862131, val:  47.08%, val_best:  47.08%, tr:  92.13%, tr_best:  92.13%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7310%\n",
      "layer   2  Sparsity: 75.9683%\n",
      "layer   3  Sparsity: 77.4596%\n",
      "total_backward_count 14685 real_backward_count 3759  25.598%\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 543.0\n",
      "fc layer 2 self.abs_max_out: 2579.0\n",
      "lif layer 2 self.abs_max_v: 4313.5\n",
      "lif layer 2 self.abs_max_v: 4492.0\n",
      "lif layer 2 self.abs_max_v: 4573.5\n",
      "fc layer 1 self.abs_max_out: 4862.0\n",
      "lif layer 1 self.abs_max_v: 7117.0\n",
      "fc layer 2 self.abs_max_out: 2711.0\n",
      "lif layer 2 self.abs_max_v: 4776.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.533217/  1.766446, val:  50.00%, val_best:  50.00%, tr:  92.95%, tr_best:  92.95%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7149%\n",
      "layer   2  Sparsity: 75.6295%\n",
      "layer   3  Sparsity: 77.2926%\n",
      "total_backward_count 19580 real_backward_count 4803  24.530%\n",
      "fc layer 3 self.abs_max_out: 556.0\n",
      "lif layer 2 self.abs_max_v: 4816.5\n",
      "lif layer 2 self.abs_max_v: 4922.5\n",
      "fc layer 2 self.abs_max_out: 2750.0\n",
      "fc layer 2 self.abs_max_out: 2788.0\n",
      "lif layer 2 self.abs_max_v: 5164.5\n",
      "lif layer 2 self.abs_max_v: 5182.5\n",
      "fc layer 2 self.abs_max_out: 2859.0\n",
      "lif layer 2 self.abs_max_v: 5191.5\n",
      "lif layer 2 self.abs_max_v: 5399.0\n",
      "fc layer 3 self.abs_max_out: 588.0\n",
      "fc layer 2 self.abs_max_out: 2878.0\n",
      "fc layer 1 self.abs_max_out: 5378.0\n",
      "lif layer 1 self.abs_max_v: 7303.5\n",
      "lif layer 1 self.abs_max_v: 8156.5\n",
      "lif layer 1 self.abs_max_v: 8773.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.499179/  1.758447, val:  48.75%, val_best:  50.00%, tr:  93.97%, tr_best:  93.97%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6562%\n",
      "layer   2  Sparsity: 75.5478%\n",
      "layer   3  Sparsity: 77.6168%\n",
      "total_backward_count 24475 real_backward_count 5801  23.702%\n",
      "fc layer 2 self.abs_max_out: 2901.0\n",
      "fc layer 2 self.abs_max_out: 2931.0\n",
      "fc layer 2 self.abs_max_out: 3093.0\n",
      "fc layer 1 self.abs_max_out: 5518.0\n",
      "lif layer 1 self.abs_max_v: 9067.5\n",
      "lif layer 1 self.abs_max_v: 9928.0\n",
      "fc layer 1 self.abs_max_out: 5617.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.473384/  1.725697, val:  53.75%, val_best:  53.75%, tr:  94.28%, tr_best:  94.28%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7218%\n",
      "layer   2  Sparsity: 75.0774%\n",
      "layer   3  Sparsity: 77.4905%\n",
      "total_backward_count 29370 real_backward_count 6783  23.095%\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 1 self.abs_max_out: 5722.0\n",
      "lif layer 1 self.abs_max_v: 10399.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.435992/  1.688802, val:  55.83%, val_best:  55.83%, tr:  95.20%, tr_best:  95.20%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6972%\n",
      "layer   2  Sparsity: 74.5631%\n",
      "layer   3  Sparsity: 78.3241%\n",
      "total_backward_count 34265 real_backward_count 7702  22.478%\n",
      "lif layer 2 self.abs_max_v: 5454.0\n",
      "fc layer 3 self.abs_max_out: 605.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.403236/  1.654807, val:  59.17%, val_best:  59.17%, tr:  94.89%, tr_best:  95.20%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7120%\n",
      "layer   2  Sparsity: 74.2667%\n",
      "layer   3  Sparsity: 78.4078%\n",
      "total_backward_count 39160 real_backward_count 8600  21.961%\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "lif layer 2 self.abs_max_v: 5511.0\n",
      "fc layer 1 self.abs_max_out: 5750.0\n",
      "fc layer 1 self.abs_max_out: 6190.0\n",
      "fc layer 2 self.abs_max_out: 3113.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.423052/  1.638381, val:  60.83%, val_best:  60.83%, tr:  96.32%, tr_best:  96.32%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6878%\n",
      "layer   2  Sparsity: 74.4132%\n",
      "layer   3  Sparsity: 79.3579%\n",
      "total_backward_count 44055 real_backward_count 9511  21.589%\n",
      "fc layer 1 self.abs_max_out: 6245.0\n",
      "lif layer 1 self.abs_max_v: 10472.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.386018/  1.605073, val:  62.92%, val_best:  62.92%, tr:  94.89%, tr_best:  96.32%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7217%\n",
      "layer   2  Sparsity: 73.4263%\n",
      "layer   3  Sparsity: 78.9919%\n",
      "total_backward_count 48950 real_backward_count 10346  21.136%\n",
      "fc layer 1 self.abs_max_out: 6264.0\n",
      "fc layer 1 self.abs_max_out: 6472.0\n",
      "lif layer 1 self.abs_max_v: 10669.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.347400/  1.554559, val:  65.42%, val_best:  65.42%, tr:  96.02%, tr_best:  96.32%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6601%\n",
      "layer   2  Sparsity: 73.8253%\n",
      "layer   3  Sparsity: 79.0930%\n",
      "total_backward_count 53845 real_backward_count 11100  20.615%\n",
      "fc layer 2 self.abs_max_out: 3124.0\n",
      "lif layer 2 self.abs_max_v: 5518.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.360157/  1.621584, val:  71.67%, val_best:  71.67%, tr:  96.22%, tr_best:  96.32%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6747%\n",
      "layer   2  Sparsity: 73.8537%\n",
      "layer   3  Sparsity: 79.6243%\n",
      "total_backward_count 58740 real_backward_count 11876  20.218%\n",
      "fc layer 1 self.abs_max_out: 6610.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.354559/  1.539985, val:  69.58%, val_best:  71.67%, tr:  96.83%, tr_best:  96.83%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6668%\n",
      "layer   2  Sparsity: 73.7161%\n",
      "layer   3  Sparsity: 79.3136%\n",
      "total_backward_count 63635 real_backward_count 12613  19.821%\n",
      "fc layer 2 self.abs_max_out: 3176.0\n",
      "lif layer 1 self.abs_max_v: 10715.5\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.294451/  1.549340, val:  68.33%, val_best:  71.67%, tr:  98.26%, tr_best:  98.26%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6796%\n",
      "layer   2  Sparsity: 73.4290%\n",
      "layer   3  Sparsity: 78.5194%\n",
      "total_backward_count 68530 real_backward_count 13260  19.349%\n",
      "lif layer 2 self.abs_max_v: 5634.5\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "fc layer 3 self.abs_max_out: 685.0\n",
      "fc layer 3 self.abs_max_out: 696.0\n",
      "fc layer 2 self.abs_max_out: 3199.0\n",
      "fc layer 1 self.abs_max_out: 6816.0\n",
      "fc layer 2 self.abs_max_out: 3259.0\n",
      "lif layer 1 self.abs_max_v: 11016.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.300705/  1.498892, val:  77.08%, val_best:  77.08%, tr:  97.85%, tr_best:  98.26%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7314%\n",
      "layer   2  Sparsity: 72.6199%\n",
      "layer   3  Sparsity: 78.6822%\n",
      "total_backward_count 73425 real_backward_count 13921  18.959%\n",
      "fc layer 2 self.abs_max_out: 3312.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.313355/  1.496640, val:  72.08%, val_best:  77.08%, tr:  97.75%, tr_best:  98.26%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6899%\n",
      "layer   2  Sparsity: 72.7195%\n",
      "layer   3  Sparsity: 79.5050%\n",
      "total_backward_count 78320 real_backward_count 14570  18.603%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.271910/  1.463883, val:  79.17%, val_best:  79.17%, tr:  98.26%, tr_best:  98.26%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6674%\n",
      "layer   2  Sparsity: 73.2713%\n",
      "layer   3  Sparsity: 79.8149%\n",
      "total_backward_count 83215 real_backward_count 15138  18.191%\n",
      "fc layer 3 self.abs_max_out: 709.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.253156/  1.466684, val:  75.83%, val_best:  79.17%, tr:  98.77%, tr_best:  98.77%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6796%\n",
      "layer   2  Sparsity: 72.9477%\n",
      "layer   3  Sparsity: 80.1234%\n",
      "total_backward_count 88110 real_backward_count 15761  17.888%\n",
      "fc layer 3 self.abs_max_out: 734.0\n",
      "lif layer 2 self.abs_max_v: 5763.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.240064/  1.462094, val:  76.67%, val_best:  79.17%, tr:  98.98%, tr_best:  98.98%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7023%\n",
      "layer   2  Sparsity: 72.8168%\n",
      "layer   3  Sparsity: 79.9881%\n",
      "total_backward_count 93005 real_backward_count 16356  17.586%\n",
      "fc layer 1 self.abs_max_out: 6879.0\n",
      "lif layer 1 self.abs_max_v: 11187.5\n",
      "lif layer 2 self.abs_max_v: 6073.5\n",
      "fc layer 2 self.abs_max_out: 3402.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.262096/  1.548614, val:  69.17%, val_best:  79.17%, tr:  97.85%, tr_best:  98.98%, epoch time: 40.92 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7102%\n",
      "layer   2  Sparsity: 73.0305%\n",
      "layer   3  Sparsity: 79.8036%\n",
      "total_backward_count 97900 real_backward_count 16948  17.312%\n",
      "fc layer 3 self.abs_max_out: 804.0\n",
      "lif layer 1 self.abs_max_v: 11599.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.274961/  1.500937, val:  72.08%, val_best:  79.17%, tr:  97.96%, tr_best:  98.98%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6775%\n",
      "layer   2  Sparsity: 72.5497%\n",
      "layer   3  Sparsity: 79.9975%\n",
      "total_backward_count 102795 real_backward_count 17466  16.991%\n",
      "fc layer 1 self.abs_max_out: 6945.0\n",
      "lif layer 1 self.abs_max_v: 11871.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.242491/  1.469809, val:  80.00%, val_best:  80.00%, tr:  98.06%, tr_best:  98.98%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7039%\n",
      "layer   2  Sparsity: 72.7575%\n",
      "layer   3  Sparsity: 80.3374%\n",
      "total_backward_count 107690 real_backward_count 18013  16.727%\n",
      "lif layer 2 self.abs_max_v: 6114.5\n",
      "fc layer 2 self.abs_max_out: 3572.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.303249/  1.469378, val:  77.92%, val_best:  80.00%, tr:  98.26%, tr_best:  98.98%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6829%\n",
      "layer   2  Sparsity: 72.5894%\n",
      "layer   3  Sparsity: 80.5830%\n",
      "total_backward_count 112585 real_backward_count 18542  16.469%\n",
      "lif layer 2 self.abs_max_v: 6139.5\n",
      "lif layer 2 self.abs_max_v: 6308.0\n",
      "lif layer 2 self.abs_max_v: 6506.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.272472/  1.478570, val:  82.50%, val_best:  82.50%, tr:  98.77%, tr_best:  98.98%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6641%\n",
      "layer   2  Sparsity: 72.8304%\n",
      "layer   3  Sparsity: 80.8281%\n",
      "total_backward_count 117480 real_backward_count 19041  16.208%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.251486/  1.488593, val:  72.08%, val_best:  82.50%, tr:  98.98%, tr_best:  98.98%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6752%\n",
      "layer   2  Sparsity: 72.7640%\n",
      "layer   3  Sparsity: 80.7053%\n",
      "total_backward_count 122375 real_backward_count 19531  15.960%\n",
      "fc layer 1 self.abs_max_out: 7436.0\n",
      "lif layer 1 self.abs_max_v: 12202.0\n",
      "lif layer 1 self.abs_max_v: 12302.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.229561/  1.410935, val:  84.17%, val_best:  84.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7055%\n",
      "layer   2  Sparsity: 72.8188%\n",
      "layer   3  Sparsity: 81.2392%\n",
      "total_backward_count 127270 real_backward_count 20015  15.726%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.219484/  1.419292, val:  76.67%, val_best:  84.17%, tr:  98.57%, tr_best:  99.18%, epoch time: 40.84 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7079%\n",
      "layer   2  Sparsity: 72.4809%\n",
      "layer   3  Sparsity: 80.4262%\n",
      "total_backward_count 132165 real_backward_count 20495  15.507%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.202722/  1.426925, val:  78.75%, val_best:  84.17%, tr:  99.08%, tr_best:  99.18%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6996%\n",
      "layer   2  Sparsity: 72.4767%\n",
      "layer   3  Sparsity: 80.3340%\n",
      "total_backward_count 137060 real_backward_count 20953  15.287%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.171106/  1.419306, val:  82.08%, val_best:  84.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7068%\n",
      "layer   2  Sparsity: 72.2521%\n",
      "layer   3  Sparsity: 80.7829%\n",
      "total_backward_count 141955 real_backward_count 21383  15.063%\n",
      "fc layer 2 self.abs_max_out: 3619.0\n",
      "lif layer 2 self.abs_max_v: 6728.5\n",
      "fc layer 3 self.abs_max_out: 805.0\n",
      "fc layer 2 self.abs_max_out: 3707.0\n",
      "fc layer 2 self.abs_max_out: 3901.0\n",
      "lif layer 2 self.abs_max_v: 6889.5\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.182242/  1.388637, val:  80.42%, val_best:  84.17%, tr:  98.88%, tr_best:  99.18%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7082%\n",
      "layer   2  Sparsity: 72.4460%\n",
      "layer   3  Sparsity: 80.4321%\n",
      "total_backward_count 146850 real_backward_count 21825  14.862%\n",
      "lif layer 1 self.abs_max_v: 13137.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.183615/  1.379149, val:  85.83%, val_best:  85.83%, tr:  99.18%, tr_best:  99.18%, epoch time: 40.50 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6671%\n",
      "layer   2  Sparsity: 72.2131%\n",
      "layer   3  Sparsity: 80.6249%\n",
      "total_backward_count 151745 real_backward_count 22242  14.657%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.191437/  1.409606, val:  78.75%, val_best:  85.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7276%\n",
      "layer   2  Sparsity: 72.1799%\n",
      "layer   3  Sparsity: 80.4827%\n",
      "total_backward_count 156640 real_backward_count 22652  14.461%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.191799/  1.436976, val:  78.75%, val_best:  85.83%, tr:  98.88%, tr_best:  99.28%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6771%\n",
      "layer   2  Sparsity: 72.0722%\n",
      "layer   3  Sparsity: 80.7510%\n",
      "total_backward_count 161535 real_backward_count 23047  14.267%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.181518/  1.405539, val:  77.08%, val_best:  85.83%, tr:  98.77%, tr_best:  99.28%, epoch time: 40.62 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7001%\n",
      "layer   2  Sparsity: 72.2024%\n",
      "layer   3  Sparsity: 80.8890%\n",
      "total_backward_count 166430 real_backward_count 23449  14.089%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.157997/  1.354212, val:  79.58%, val_best:  85.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7388%\n",
      "layer   2  Sparsity: 72.3447%\n",
      "layer   3  Sparsity: 80.1800%\n",
      "total_backward_count 171325 real_backward_count 23825  13.906%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.129320/  1.359023, val:  84.58%, val_best:  85.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6794%\n",
      "layer   2  Sparsity: 72.5299%\n",
      "layer   3  Sparsity: 80.6555%\n",
      "total_backward_count 176220 real_backward_count 24216  13.742%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.133382/  1.387986, val:  78.75%, val_best:  85.83%, tr:  99.18%, tr_best:  99.59%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7464%\n",
      "layer   2  Sparsity: 72.2591%\n",
      "layer   3  Sparsity: 81.6134%\n",
      "total_backward_count 181115 real_backward_count 24558  13.559%\n",
      "fc layer 1 self.abs_max_out: 7690.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.144601/  1.410996, val:  80.42%, val_best:  85.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7172%\n",
      "layer   2  Sparsity: 72.2409%\n",
      "layer   3  Sparsity: 81.4745%\n",
      "total_backward_count 186010 real_backward_count 24891  13.382%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.149980/  1.386327, val:  82.50%, val_best:  85.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6940%\n",
      "layer   2  Sparsity: 71.8626%\n",
      "layer   3  Sparsity: 81.2276%\n",
      "total_backward_count 190905 real_backward_count 25242  13.222%\n",
      "lif layer 2 self.abs_max_v: 6942.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.117785/  1.339041, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7063%\n",
      "layer   2  Sparsity: 72.1731%\n",
      "layer   3  Sparsity: 81.2931%\n",
      "total_backward_count 195800 real_backward_count 25576  13.062%\n",
      "fc layer 1 self.abs_max_out: 7787.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.132808/  1.357932, val:  82.92%, val_best:  85.83%, tr:  99.39%, tr_best:  99.90%, epoch time: 40.50 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7089%\n",
      "layer   2  Sparsity: 72.2254%\n",
      "layer   3  Sparsity: 81.7472%\n",
      "total_backward_count 200695 real_backward_count 25899  12.905%\n",
      "fc layer 1 self.abs_max_out: 7943.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.148332/  1.374806, val:  81.67%, val_best:  85.83%, tr:  99.39%, tr_best:  99.90%, epoch time: 41.56 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 91.7072%\n",
      "layer   2  Sparsity: 71.8998%\n",
      "layer   3  Sparsity: 81.6015%\n",
      "total_backward_count 205590 real_backward_count 26232  12.759%\n",
      "lif layer 2 self.abs_max_v: 7156.0\n",
      "lif layer 1 self.abs_max_v: 13199.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.115321/  1.336111, val:  80.83%, val_best:  85.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7026%\n",
      "layer   2  Sparsity: 71.9688%\n",
      "layer   3  Sparsity: 81.4102%\n",
      "total_backward_count 210485 real_backward_count 26555  12.616%\n",
      "fc layer 1 self.abs_max_out: 8069.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.113446/  1.357155, val:  87.08%, val_best:  87.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6801%\n",
      "layer   2  Sparsity: 71.6328%\n",
      "layer   3  Sparsity: 81.3432%\n",
      "total_backward_count 215380 real_backward_count 26903  12.491%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.127536/  1.310275, val:  82.08%, val_best:  87.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7398%\n",
      "layer   2  Sparsity: 71.7461%\n",
      "layer   3  Sparsity: 80.7486%\n",
      "total_backward_count 220275 real_backward_count 27238  12.365%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.108699/  1.315579, val:  85.42%, val_best:  87.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6870%\n",
      "layer   2  Sparsity: 71.7534%\n",
      "layer   3  Sparsity: 80.2219%\n",
      "total_backward_count 225170 real_backward_count 27572  12.245%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.072620/  1.285572, val:  85.00%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6902%\n",
      "layer   2  Sparsity: 71.6459%\n",
      "layer   3  Sparsity: 80.9287%\n",
      "total_backward_count 230065 real_backward_count 27893  12.124%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.056491/  1.336395, val:  76.67%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6937%\n",
      "layer   2  Sparsity: 71.5034%\n",
      "layer   3  Sparsity: 80.4942%\n",
      "total_backward_count 234960 real_backward_count 28206  12.005%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.056712/  1.279498, val:  83.33%, val_best:  87.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7021%\n",
      "layer   2  Sparsity: 71.8651%\n",
      "layer   3  Sparsity: 81.0971%\n",
      "total_backward_count 239855 real_backward_count 28504  11.884%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.072805/  1.321176, val:  82.92%, val_best:  87.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6828%\n",
      "layer   2  Sparsity: 71.6981%\n",
      "layer   3  Sparsity: 80.8837%\n",
      "total_backward_count 244750 real_backward_count 28831  11.780%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.070258/  1.312392, val:  85.42%, val_best:  87.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7324%\n",
      "layer   2  Sparsity: 71.6265%\n",
      "layer   3  Sparsity: 81.0981%\n",
      "total_backward_count 249645 real_backward_count 29134  11.670%\n",
      "fc layer 2 self.abs_max_out: 4033.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.080004/  1.326677, val:  77.92%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7105%\n",
      "layer   2  Sparsity: 71.5158%\n",
      "layer   3  Sparsity: 80.6456%\n",
      "total_backward_count 254540 real_backward_count 29423  11.559%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.088455/  1.333224, val:  82.08%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6905%\n",
      "layer   2  Sparsity: 71.3634%\n",
      "layer   3  Sparsity: 80.7831%\n",
      "total_backward_count 259435 real_backward_count 29751  11.468%\n",
      "fc layer 1 self.abs_max_out: 8254.0\n",
      "lif layer 1 self.abs_max_v: 13241.5\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.085494/  1.309616, val:  85.00%, val_best:  87.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6896%\n",
      "layer   2  Sparsity: 71.7182%\n",
      "layer   3  Sparsity: 81.3304%\n",
      "total_backward_count 264330 real_backward_count 30042  11.365%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.069849/  1.302528, val:  85.00%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7095%\n",
      "layer   2  Sparsity: 71.9494%\n",
      "layer   3  Sparsity: 80.8631%\n",
      "total_backward_count 269225 real_backward_count 30302  11.255%\n",
      "fc layer 3 self.abs_max_out: 813.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.047141/  1.272865, val:  83.33%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6694%\n",
      "layer   2  Sparsity: 71.9963%\n",
      "layer   3  Sparsity: 81.2080%\n",
      "total_backward_count 274120 real_backward_count 30561  11.149%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.094820/  1.318843, val:  80.83%, val_best:  87.08%, tr:  98.98%, tr_best:  99.90%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7038%\n",
      "layer   2  Sparsity: 72.0178%\n",
      "layer   3  Sparsity: 81.8684%\n",
      "total_backward_count 279015 real_backward_count 30824  11.047%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.051398/  1.282474, val:  84.17%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6597%\n",
      "layer   2  Sparsity: 71.4531%\n",
      "layer   3  Sparsity: 81.8560%\n",
      "total_backward_count 283910 real_backward_count 31102  10.955%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.072717/  1.275375, val:  85.42%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7127%\n",
      "layer   2  Sparsity: 71.4246%\n",
      "layer   3  Sparsity: 81.9767%\n",
      "total_backward_count 288805 real_backward_count 31340  10.852%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.071745/  1.300010, val:  84.58%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6816%\n",
      "layer   2  Sparsity: 71.6199%\n",
      "layer   3  Sparsity: 82.1136%\n",
      "total_backward_count 293700 real_backward_count 31553  10.743%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.083673/  1.324350, val:  82.08%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6935%\n",
      "layer   2  Sparsity: 71.2861%\n",
      "layer   3  Sparsity: 81.7503%\n",
      "total_backward_count 298595 real_backward_count 31808  10.653%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.081285/  1.348422, val:  83.33%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6991%\n",
      "layer   2  Sparsity: 71.4840%\n",
      "layer   3  Sparsity: 82.2600%\n",
      "total_backward_count 303490 real_backward_count 32058  10.563%\n",
      "fc layer 2 self.abs_max_out: 4130.0\n",
      "lif layer 2 self.abs_max_v: 7209.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.075299/  1.322685, val:  80.83%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.82 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6890%\n",
      "layer   2  Sparsity: 71.9848%\n",
      "layer   3  Sparsity: 82.1848%\n",
      "total_backward_count 308385 real_backward_count 32279  10.467%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.072456/  1.274991, val:  89.17%, val_best:  89.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6414%\n",
      "layer   2  Sparsity: 71.7381%\n",
      "layer   3  Sparsity: 81.9841%\n",
      "total_backward_count 313280 real_backward_count 32510  10.377%\n",
      "lif layer 2 self.abs_max_v: 7323.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.052868/  1.288696, val:  85.83%, val_best:  89.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6844%\n",
      "layer   2  Sparsity: 71.8181%\n",
      "layer   3  Sparsity: 82.1969%\n",
      "total_backward_count 318175 real_backward_count 32724  10.285%\n",
      "lif layer 2 self.abs_max_v: 7513.5\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.060936/  1.305212, val:  82.92%, val_best:  89.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7062%\n",
      "layer   2  Sparsity: 71.9014%\n",
      "layer   3  Sparsity: 82.1632%\n",
      "total_backward_count 323070 real_backward_count 32939  10.196%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.050994/  1.309586, val:  85.42%, val_best:  89.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7528%\n",
      "layer   2  Sparsity: 72.0359%\n",
      "layer   3  Sparsity: 82.3304%\n",
      "total_backward_count 327965 real_backward_count 33168  10.113%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.049205/  1.259256, val:  87.08%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7145%\n",
      "layer   2  Sparsity: 72.0641%\n",
      "layer   3  Sparsity: 81.8235%\n",
      "total_backward_count 332860 real_backward_count 33420  10.040%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.017102/  1.246600, val:  85.00%, val_best:  89.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6955%\n",
      "layer   2  Sparsity: 71.7061%\n",
      "layer   3  Sparsity: 81.3996%\n",
      "total_backward_count 337755 real_backward_count 33651   9.963%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.995067/  1.247299, val:  87.08%, val_best:  89.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6702%\n",
      "layer   2  Sparsity: 71.6547%\n",
      "layer   3  Sparsity: 80.8548%\n",
      "total_backward_count 342650 real_backward_count 33856   9.881%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.990516/  1.264608, val:  84.58%, val_best:  89.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.32 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6991%\n",
      "layer   2  Sparsity: 71.5847%\n",
      "layer   3  Sparsity: 81.1794%\n",
      "total_backward_count 347545 real_backward_count 34088   9.808%\n",
      "fc layer 1 self.abs_max_out: 8351.0\n",
      "lif layer 1 self.abs_max_v: 13436.5\n",
      "lif layer 1 self.abs_max_v: 13820.5\n",
      "fc layer 3 self.abs_max_out: 826.0\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.000533/  1.250177, val:  85.42%, val_best:  89.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7000%\n",
      "layer   2  Sparsity: 71.5509%\n",
      "layer   3  Sparsity: 81.5454%\n",
      "total_backward_count 352440 real_backward_count 34291   9.730%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.009938/  1.246177, val:  85.83%, val_best:  89.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 40.69 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7109%\n",
      "layer   2  Sparsity: 71.0738%\n",
      "layer   3  Sparsity: 81.8172%\n",
      "total_backward_count 357335 real_backward_count 34520   9.660%\n",
      "fc layer 1 self.abs_max_out: 8523.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.026172/  1.285059, val:  82.92%, val_best:  89.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6826%\n",
      "layer   2  Sparsity: 71.4792%\n",
      "layer   3  Sparsity: 81.3577%\n",
      "total_backward_count 362230 real_backward_count 34726   9.587%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.029835/  1.279429, val:  83.75%, val_best:  89.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7234%\n",
      "layer   2  Sparsity: 71.8405%\n",
      "layer   3  Sparsity: 81.2853%\n",
      "total_backward_count 367125 real_backward_count 34968   9.525%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.001273/  1.255123, val:  85.42%, val_best:  89.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6638%\n",
      "layer   2  Sparsity: 71.7413%\n",
      "layer   3  Sparsity: 81.5724%\n",
      "total_backward_count 372020 real_backward_count 35191   9.459%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.025903/  1.267329, val:  81.67%, val_best:  89.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7301%\n",
      "layer   2  Sparsity: 72.0090%\n",
      "layer   3  Sparsity: 81.9062%\n",
      "total_backward_count 376915 real_backward_count 35406   9.394%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.993124/  1.279098, val:  81.25%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7235%\n",
      "layer   2  Sparsity: 71.7482%\n",
      "layer   3  Sparsity: 82.2619%\n",
      "total_backward_count 381810 real_backward_count 35573   9.317%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.012681/  1.273247, val:  84.58%, val_best:  89.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6960%\n",
      "layer   2  Sparsity: 71.6868%\n",
      "layer   3  Sparsity: 82.2419%\n",
      "total_backward_count 386705 real_backward_count 35775   9.251%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.013678/  1.295180, val:  85.00%, val_best:  89.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 40.54 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7137%\n",
      "layer   2  Sparsity: 71.6459%\n",
      "layer   3  Sparsity: 82.5886%\n",
      "total_backward_count 391600 real_backward_count 35957   9.182%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.022874/  1.253523, val:  85.83%, val_best:  89.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 39.85 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7087%\n",
      "layer   2  Sparsity: 71.7666%\n",
      "layer   3  Sparsity: 82.3189%\n",
      "total_backward_count 396495 real_backward_count 36151   9.118%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.988822/  1.242221, val:  84.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7193%\n",
      "layer   2  Sparsity: 71.8307%\n",
      "layer   3  Sparsity: 82.5640%\n",
      "total_backward_count 401390 real_backward_count 36347   9.055%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.983995/  1.256834, val:  83.75%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6993%\n",
      "layer   2  Sparsity: 71.6358%\n",
      "layer   3  Sparsity: 82.4715%\n",
      "total_backward_count 406285 real_backward_count 36514   8.987%\n",
      "fc layer 1 self.abs_max_out: 8630.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.989810/  1.245864, val:  85.42%, val_best:  89.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6948%\n",
      "layer   2  Sparsity: 71.8183%\n",
      "layer   3  Sparsity: 82.3411%\n",
      "total_backward_count 411180 real_backward_count 36695   8.924%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.980797/  1.274533, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.78 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6724%\n",
      "layer   2  Sparsity: 71.4079%\n",
      "layer   3  Sparsity: 82.5458%\n",
      "total_backward_count 416075 real_backward_count 36852   8.857%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.004316/  1.257938, val:  87.08%, val_best:  89.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7241%\n",
      "layer   2  Sparsity: 70.9905%\n",
      "layer   3  Sparsity: 82.8460%\n",
      "total_backward_count 420970 real_backward_count 37019   8.794%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.016447/  1.264139, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7226%\n",
      "layer   2  Sparsity: 71.2946%\n",
      "layer   3  Sparsity: 82.7144%\n",
      "total_backward_count 425865 real_backward_count 37188   8.732%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.000021/  1.259481, val:  83.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7395%\n",
      "layer   2  Sparsity: 71.2979%\n",
      "layer   3  Sparsity: 81.9915%\n",
      "total_backward_count 430760 real_backward_count 37388   8.680%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.991420/  1.244148, val:  82.08%, val_best:  89.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6898%\n",
      "layer   2  Sparsity: 71.4183%\n",
      "layer   3  Sparsity: 81.8984%\n",
      "total_backward_count 435655 real_backward_count 37592   8.629%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.993393/  1.231175, val:  82.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6876%\n",
      "layer   2  Sparsity: 71.3279%\n",
      "layer   3  Sparsity: 81.9052%\n",
      "total_backward_count 440550 real_backward_count 37792   8.578%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.973974/  1.230999, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7156%\n",
      "layer   2  Sparsity: 71.1556%\n",
      "layer   3  Sparsity: 81.8855%\n",
      "total_backward_count 445445 real_backward_count 37958   8.521%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.954008/  1.226184, val:  87.08%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7014%\n",
      "layer   2  Sparsity: 70.9814%\n",
      "layer   3  Sparsity: 82.0145%\n",
      "total_backward_count 450340 real_backward_count 38138   8.469%\n",
      "fc layer 1 self.abs_max_out: 8685.0\n",
      "lif layer 1 self.abs_max_v: 13948.5\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.953730/  1.184990, val:  85.83%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6680%\n",
      "layer   2  Sparsity: 71.2917%\n",
      "layer   3  Sparsity: 81.6541%\n",
      "total_backward_count 455235 real_backward_count 38288   8.411%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.944055/  1.229684, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7071%\n",
      "layer   2  Sparsity: 71.3471%\n",
      "layer   3  Sparsity: 81.9793%\n",
      "total_backward_count 460130 real_backward_count 38445   8.355%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.953194/  1.233812, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.23 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6692%\n",
      "layer   2  Sparsity: 71.2894%\n",
      "layer   3  Sparsity: 82.5083%\n",
      "total_backward_count 465025 real_backward_count 38586   8.298%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.973577/  1.238626, val:  84.17%, val_best:  89.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7113%\n",
      "layer   2  Sparsity: 71.1430%\n",
      "layer   3  Sparsity: 82.6691%\n",
      "total_backward_count 469920 real_backward_count 38756   8.247%\n",
      "fc layer 1 self.abs_max_out: 8779.0\n",
      "lif layer 1 self.abs_max_v: 14163.0\n",
      "lif layer 2 self.abs_max_v: 7623.5\n",
      "lif layer 1 self.abs_max_v: 14195.5\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.967186/  1.220482, val:  85.00%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7080%\n",
      "layer   2  Sparsity: 70.9946%\n",
      "layer   3  Sparsity: 82.6743%\n",
      "total_backward_count 474815 real_backward_count 38903   8.193%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.964346/  1.231323, val:  87.92%, val_best:  89.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7222%\n",
      "layer   2  Sparsity: 70.9782%\n",
      "layer   3  Sparsity: 82.2604%\n",
      "total_backward_count 479710 real_backward_count 39044   8.139%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.945144/  1.194207, val:  89.17%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.41 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7013%\n",
      "layer   2  Sparsity: 71.4234%\n",
      "layer   3  Sparsity: 82.4105%\n",
      "total_backward_count 484605 real_backward_count 39216   8.092%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.935980/  1.190285, val:  89.17%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7222%\n",
      "layer   2  Sparsity: 71.3313%\n",
      "layer   3  Sparsity: 82.0991%\n",
      "total_backward_count 489500 real_backward_count 39378   8.045%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.919124/  1.164966, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6846%\n",
      "layer   2  Sparsity: 71.2569%\n",
      "layer   3  Sparsity: 81.9301%\n",
      "total_backward_count 494395 real_backward_count 39508   7.991%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.912144/  1.208256, val:  83.75%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6993%\n",
      "layer   2  Sparsity: 71.0132%\n",
      "layer   3  Sparsity: 82.5258%\n",
      "total_backward_count 499290 real_backward_count 39638   7.939%\n",
      "fc layer 3 self.abs_max_out: 869.0\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.925682/  1.179362, val:  88.33%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6832%\n",
      "layer   2  Sparsity: 71.1158%\n",
      "layer   3  Sparsity: 82.7315%\n",
      "total_backward_count 504185 real_backward_count 39799   7.894%\n",
      "fc layer 3 self.abs_max_out: 923.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.917341/  1.180235, val:  86.67%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6951%\n",
      "layer   2  Sparsity: 71.0324%\n",
      "layer   3  Sparsity: 82.1180%\n",
      "total_backward_count 509080 real_backward_count 39944   7.846%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.904572/  1.170170, val:  86.25%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6942%\n",
      "layer   2  Sparsity: 70.9812%\n",
      "layer   3  Sparsity: 81.9623%\n",
      "total_backward_count 513975 real_backward_count 40094   7.801%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.890821/  1.209354, val:  84.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6855%\n",
      "layer   2  Sparsity: 71.2570%\n",
      "layer   3  Sparsity: 81.8744%\n",
      "total_backward_count 518870 real_backward_count 40230   7.753%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.912225/  1.206596, val:  81.25%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6911%\n",
      "layer   2  Sparsity: 71.0213%\n",
      "layer   3  Sparsity: 82.1219%\n",
      "total_backward_count 523765 real_backward_count 40376   7.709%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.924402/  1.180202, val:  87.08%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7220%\n",
      "layer   2  Sparsity: 71.0013%\n",
      "layer   3  Sparsity: 82.1985%\n",
      "total_backward_count 528660 real_backward_count 40532   7.667%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.928245/  1.193459, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.24 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6794%\n",
      "layer   2  Sparsity: 71.1340%\n",
      "layer   3  Sparsity: 82.6274%\n",
      "total_backward_count 533555 real_backward_count 40658   7.620%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.912770/  1.167354, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7033%\n",
      "layer   2  Sparsity: 71.3742%\n",
      "layer   3  Sparsity: 82.5434%\n",
      "total_backward_count 538450 real_backward_count 40796   7.577%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.891398/  1.166548, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7131%\n",
      "layer   2  Sparsity: 71.2811%\n",
      "layer   3  Sparsity: 82.1636%\n",
      "total_backward_count 543345 real_backward_count 40938   7.534%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.907293/  1.173781, val:  82.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6963%\n",
      "layer   2  Sparsity: 71.3008%\n",
      "layer   3  Sparsity: 82.3084%\n",
      "total_backward_count 548240 real_backward_count 41075   7.492%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.897227/  1.177402, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6979%\n",
      "layer   2  Sparsity: 71.3614%\n",
      "layer   3  Sparsity: 82.2614%\n",
      "total_backward_count 553135 real_backward_count 41191   7.447%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.907950/  1.189884, val:  87.08%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7144%\n",
      "layer   2  Sparsity: 71.0347%\n",
      "layer   3  Sparsity: 82.3386%\n",
      "total_backward_count 558030 real_backward_count 41341   7.408%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.882684/  1.162992, val:  85.42%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.38 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6944%\n",
      "layer   2  Sparsity: 71.1370%\n",
      "layer   3  Sparsity: 81.8208%\n",
      "total_backward_count 562925 real_backward_count 41482   7.369%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.882322/  1.190647, val:  85.42%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6977%\n",
      "layer   2  Sparsity: 71.1126%\n",
      "layer   3  Sparsity: 82.0743%\n",
      "total_backward_count 567820 real_backward_count 41604   7.327%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.879845/  1.151412, val:  86.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7314%\n",
      "layer   2  Sparsity: 71.0910%\n",
      "layer   3  Sparsity: 81.8167%\n",
      "total_backward_count 572715 real_backward_count 41712   7.283%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.882166/  1.157738, val:  85.00%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.90 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7170%\n",
      "layer   2  Sparsity: 70.9495%\n",
      "layer   3  Sparsity: 81.9768%\n",
      "total_backward_count 577610 real_backward_count 41831   7.242%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.874627/  1.190235, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6916%\n",
      "layer   2  Sparsity: 71.1208%\n",
      "layer   3  Sparsity: 82.0879%\n",
      "total_backward_count 582505 real_backward_count 41953   7.202%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.884797/  1.221830, val:  82.08%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7129%\n",
      "layer   2  Sparsity: 71.0218%\n",
      "layer   3  Sparsity: 81.9886%\n",
      "total_backward_count 587400 real_backward_count 42083   7.164%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.896144/  1.175022, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6875%\n",
      "layer   2  Sparsity: 70.9642%\n",
      "layer   3  Sparsity: 82.1867%\n",
      "total_backward_count 592295 real_backward_count 42226   7.129%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.909704/  1.170843, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6814%\n",
      "layer   2  Sparsity: 71.2705%\n",
      "layer   3  Sparsity: 82.3234%\n",
      "total_backward_count 597190 real_backward_count 42360   7.093%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.892149/  1.180794, val:  88.33%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6999%\n",
      "layer   2  Sparsity: 71.4097%\n",
      "layer   3  Sparsity: 82.1865%\n",
      "total_backward_count 602085 real_backward_count 42477   7.055%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.904348/  1.166289, val:  86.67%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7025%\n",
      "layer   2  Sparsity: 71.3085%\n",
      "layer   3  Sparsity: 82.4400%\n",
      "total_backward_count 606980 real_backward_count 42606   7.019%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.880320/  1.150084, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6484%\n",
      "layer   2  Sparsity: 70.9358%\n",
      "layer   3  Sparsity: 82.2117%\n",
      "total_backward_count 611875 real_backward_count 42704   6.979%\n",
      "lif layer 1 self.abs_max_v: 14314.5\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.880131/  1.181099, val:  83.33%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6666%\n",
      "layer   2  Sparsity: 71.0802%\n",
      "layer   3  Sparsity: 82.3947%\n",
      "total_backward_count 616770 real_backward_count 42822   6.943%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.861073/  1.141430, val:  83.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6877%\n",
      "layer   2  Sparsity: 71.2701%\n",
      "layer   3  Sparsity: 82.3354%\n",
      "total_backward_count 621665 real_backward_count 42964   6.911%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.841852/  1.162750, val:  84.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.47 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7022%\n",
      "layer   2  Sparsity: 71.4370%\n",
      "layer   3  Sparsity: 82.2057%\n",
      "total_backward_count 626560 real_backward_count 43087   6.877%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.862023/  1.135820, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6985%\n",
      "layer   2  Sparsity: 71.1991%\n",
      "layer   3  Sparsity: 81.9400%\n",
      "total_backward_count 631455 real_backward_count 43214   6.844%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.858768/  1.116165, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7164%\n",
      "layer   2  Sparsity: 71.0273%\n",
      "layer   3  Sparsity: 81.7851%\n",
      "total_backward_count 636350 real_backward_count 43322   6.808%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.842183/  1.148460, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7094%\n",
      "layer   2  Sparsity: 70.9815%\n",
      "layer   3  Sparsity: 82.3011%\n",
      "total_backward_count 641245 real_backward_count 43436   6.774%\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.832988/  1.123794, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7271%\n",
      "layer   2  Sparsity: 71.0007%\n",
      "layer   3  Sparsity: 82.5760%\n",
      "total_backward_count 646140 real_backward_count 43555   6.741%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.811812/  1.086062, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6922%\n",
      "layer   2  Sparsity: 71.3228%\n",
      "layer   3  Sparsity: 82.6595%\n",
      "total_backward_count 651035 real_backward_count 43679   6.709%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.815746/  1.082411, val:  88.75%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7202%\n",
      "layer   2  Sparsity: 71.3546%\n",
      "layer   3  Sparsity: 82.6692%\n",
      "total_backward_count 655930 real_backward_count 43812   6.679%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.819837/  1.112416, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6922%\n",
      "layer   2  Sparsity: 71.2117%\n",
      "layer   3  Sparsity: 82.2761%\n",
      "total_backward_count 660825 real_backward_count 43915   6.645%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.813733/  1.109186, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6942%\n",
      "layer   2  Sparsity: 71.0920%\n",
      "layer   3  Sparsity: 81.9453%\n",
      "total_backward_count 665720 real_backward_count 44034   6.614%\n",
      "fc layer 2 self.abs_max_out: 4139.0\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.812327/  1.125979, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7036%\n",
      "layer   2  Sparsity: 71.1692%\n",
      "layer   3  Sparsity: 82.2401%\n",
      "total_backward_count 670615 real_backward_count 44137   6.582%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.811715/  1.095990, val:  90.00%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6754%\n",
      "layer   2  Sparsity: 71.1074%\n",
      "layer   3  Sparsity: 82.3651%\n",
      "total_backward_count 675510 real_backward_count 44252   6.551%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.828949/  1.113629, val:  87.92%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7007%\n",
      "layer   2  Sparsity: 71.1005%\n",
      "layer   3  Sparsity: 82.7608%\n",
      "total_backward_count 680405 real_backward_count 44366   6.521%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.819667/  1.106266, val:  85.83%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7208%\n",
      "layer   2  Sparsity: 71.4361%\n",
      "layer   3  Sparsity: 82.6217%\n",
      "total_backward_count 685300 real_backward_count 44459   6.488%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.820347/  1.101183, val:  87.08%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7085%\n",
      "layer   2  Sparsity: 71.1556%\n",
      "layer   3  Sparsity: 82.1336%\n",
      "total_backward_count 690195 real_backward_count 44577   6.459%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.819889/  1.087424, val:  88.75%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6919%\n",
      "layer   2  Sparsity: 71.3982%\n",
      "layer   3  Sparsity: 82.1046%\n",
      "total_backward_count 695090 real_backward_count 44687   6.429%\n",
      "lif layer 1 self.abs_max_v: 14346.5\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.823328/  1.110689, val:  84.17%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 91.7066%\n",
      "layer   2  Sparsity: 71.4482%\n",
      "layer   3  Sparsity: 82.0946%\n",
      "total_backward_count 699985 real_backward_count 44782   6.398%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.814222/  1.097193, val:  86.67%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7116%\n",
      "layer   2  Sparsity: 71.2852%\n",
      "layer   3  Sparsity: 82.3590%\n",
      "total_backward_count 704880 real_backward_count 44869   6.365%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.823119/  1.100973, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7479%\n",
      "layer   2  Sparsity: 71.3725%\n",
      "layer   3  Sparsity: 82.4275%\n",
      "total_backward_count 709775 real_backward_count 44992   6.339%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.809302/  1.095881, val:  86.67%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7212%\n",
      "layer   2  Sparsity: 71.5435%\n",
      "layer   3  Sparsity: 81.8855%\n",
      "total_backward_count 714670 real_backward_count 45095   6.310%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.823222/  1.123219, val:  86.25%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6727%\n",
      "layer   2  Sparsity: 71.4580%\n",
      "layer   3  Sparsity: 82.1598%\n",
      "total_backward_count 719565 real_backward_count 45186   6.280%\n",
      "fc layer 1 self.abs_max_out: 8840.0\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.812440/  1.098213, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6845%\n",
      "layer   2  Sparsity: 71.4220%\n",
      "layer   3  Sparsity: 81.9043%\n",
      "total_backward_count 724460 real_backward_count 45287   6.251%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.810597/  1.135395, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6622%\n",
      "layer   2  Sparsity: 71.3607%\n",
      "layer   3  Sparsity: 82.2339%\n",
      "total_backward_count 729355 real_backward_count 45383   6.222%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.800601/  1.128177, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6906%\n",
      "layer   2  Sparsity: 71.3908%\n",
      "layer   3  Sparsity: 82.3766%\n",
      "total_backward_count 734250 real_backward_count 45457   6.191%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.797265/  1.110344, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6938%\n",
      "layer   2  Sparsity: 71.3665%\n",
      "layer   3  Sparsity: 82.0936%\n",
      "total_backward_count 739145 real_backward_count 45537   6.161%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.791680/  1.106273, val:  86.25%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6939%\n",
      "layer   2  Sparsity: 71.5199%\n",
      "layer   3  Sparsity: 82.0430%\n",
      "total_backward_count 744040 real_backward_count 45617   6.131%\n",
      "fc layer 1 self.abs_max_out: 8843.0\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.794026/  1.075305, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7004%\n",
      "layer   2  Sparsity: 71.5125%\n",
      "layer   3  Sparsity: 82.2187%\n",
      "total_backward_count 748935 real_backward_count 45702   6.102%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.786693/  1.091800, val:  87.50%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7209%\n",
      "layer   2  Sparsity: 71.3406%\n",
      "layer   3  Sparsity: 82.2379%\n",
      "total_backward_count 753830 real_backward_count 45799   6.076%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.798319/  1.097091, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6642%\n",
      "layer   2  Sparsity: 71.1655%\n",
      "layer   3  Sparsity: 82.6167%\n",
      "total_backward_count 758725 real_backward_count 45867   6.045%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.795991/  1.108470, val:  82.50%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7196%\n",
      "layer   2  Sparsity: 71.0715%\n",
      "layer   3  Sparsity: 82.5289%\n",
      "total_backward_count 763620 real_backward_count 45958   6.018%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.815251/  1.114256, val:  85.83%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7001%\n",
      "layer   2  Sparsity: 70.9610%\n",
      "layer   3  Sparsity: 82.6156%\n",
      "total_backward_count 768515 real_backward_count 46058   5.993%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.817228/  1.112016, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6960%\n",
      "layer   2  Sparsity: 71.0808%\n",
      "layer   3  Sparsity: 82.4197%\n",
      "total_backward_count 773410 real_backward_count 46144   5.966%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.821476/  1.139049, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6946%\n",
      "layer   2  Sparsity: 71.2338%\n",
      "layer   3  Sparsity: 82.6921%\n",
      "total_backward_count 778305 real_backward_count 46215   5.938%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.825813/  1.105370, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6919%\n",
      "layer   2  Sparsity: 71.2536%\n",
      "layer   3  Sparsity: 82.8314%\n",
      "total_backward_count 783200 real_backward_count 46290   5.910%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.814577/  1.086508, val:  87.50%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7590%\n",
      "layer   2  Sparsity: 71.3751%\n",
      "layer   3  Sparsity: 82.7702%\n",
      "total_backward_count 788095 real_backward_count 46366   5.883%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.822096/  1.119049, val:  82.92%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6747%\n",
      "layer   2  Sparsity: 71.2933%\n",
      "layer   3  Sparsity: 82.7011%\n",
      "total_backward_count 792990 real_backward_count 46466   5.860%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.822821/  1.132661, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6980%\n",
      "layer   2  Sparsity: 71.3637%\n",
      "layer   3  Sparsity: 82.6866%\n",
      "total_backward_count 797885 real_backward_count 46548   5.834%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.822316/  1.143198, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7346%\n",
      "layer   2  Sparsity: 71.1263%\n",
      "layer   3  Sparsity: 82.7366%\n",
      "total_backward_count 802780 real_backward_count 46627   5.808%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.825004/  1.106615, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6776%\n",
      "layer   2  Sparsity: 70.9418%\n",
      "layer   3  Sparsity: 82.3875%\n",
      "total_backward_count 807675 real_backward_count 46708   5.783%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.825022/  1.106091, val:  86.25%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6860%\n",
      "layer   2  Sparsity: 71.0043%\n",
      "layer   3  Sparsity: 82.4560%\n",
      "total_backward_count 812570 real_backward_count 46802   5.760%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.829001/  1.128175, val:  86.25%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.86 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.7324%\n",
      "layer   2  Sparsity: 71.1443%\n",
      "layer   3  Sparsity: 82.6823%\n",
      "total_backward_count 817465 real_backward_count 46887   5.736%\n",
      "fc layer 1 self.abs_max_out: 8949.0\n",
      "lif layer 1 self.abs_max_v: 14361.0\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.816102/  1.096465, val:  89.58%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7239%\n",
      "layer   2  Sparsity: 71.2386%\n",
      "layer   3  Sparsity: 82.5927%\n",
      "total_backward_count 822360 real_backward_count 46958   5.710%\n",
      "fc layer 3 self.abs_max_out: 945.0\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.819244/  1.121518, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6682%\n",
      "layer   2  Sparsity: 71.0216%\n",
      "layer   3  Sparsity: 82.3515%\n",
      "total_backward_count 827255 real_backward_count 47069   5.690%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.805792/  1.097768, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7134%\n",
      "layer   2  Sparsity: 70.8412%\n",
      "layer   3  Sparsity: 82.3979%\n",
      "total_backward_count 832150 real_backward_count 47143   5.665%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.811448/  1.107141, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.46 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6368%\n",
      "layer   2  Sparsity: 70.8212%\n",
      "layer   3  Sparsity: 82.0399%\n",
      "total_backward_count 837045 real_backward_count 47210   5.640%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.794684/  1.104282, val:  81.67%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6997%\n",
      "layer   2  Sparsity: 70.9377%\n",
      "layer   3  Sparsity: 82.2601%\n",
      "total_backward_count 841940 real_backward_count 47297   5.618%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.809023/  1.078912, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.79 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6942%\n",
      "layer   2  Sparsity: 70.9499%\n",
      "layer   3  Sparsity: 82.3283%\n",
      "total_backward_count 846835 real_backward_count 47374   5.594%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.795049/  1.059995, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7077%\n",
      "layer   2  Sparsity: 71.0751%\n",
      "layer   3  Sparsity: 82.0628%\n",
      "total_backward_count 851730 real_backward_count 47446   5.571%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.781178/  1.061688, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7219%\n",
      "layer   2  Sparsity: 71.1564%\n",
      "layer   3  Sparsity: 81.9771%\n",
      "total_backward_count 856625 real_backward_count 47530   5.549%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.778531/  1.094012, val:  85.42%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7152%\n",
      "layer   2  Sparsity: 71.1281%\n",
      "layer   3  Sparsity: 82.2331%\n",
      "total_backward_count 861520 real_backward_count 47607   5.526%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.796036/  1.090792, val:  84.58%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7360%\n",
      "layer   2  Sparsity: 71.1112%\n",
      "layer   3  Sparsity: 82.2482%\n",
      "total_backward_count 866415 real_backward_count 47704   5.506%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.796807/  1.071217, val:  91.25%, val_best:  91.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 91.6875%\n",
      "layer   2  Sparsity: 71.3509%\n",
      "layer   3  Sparsity: 82.3838%\n",
      "total_backward_count 871310 real_backward_count 47795   5.485%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.791525/  1.077566, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6822%\n",
      "layer   2  Sparsity: 71.2862%\n",
      "layer   3  Sparsity: 82.1203%\n",
      "total_backward_count 876205 real_backward_count 47865   5.463%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.798987/  1.080118, val:  88.33%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7194%\n",
      "layer   2  Sparsity: 71.3032%\n",
      "layer   3  Sparsity: 82.4434%\n",
      "total_backward_count 881100 real_backward_count 47932   5.440%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.792588/  1.081147, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6916%\n",
      "layer   2  Sparsity: 71.1707%\n",
      "layer   3  Sparsity: 82.2698%\n",
      "total_backward_count 885995 real_backward_count 48003   5.418%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.796354/  1.061307, val:  87.08%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6602%\n",
      "layer   2  Sparsity: 71.1016%\n",
      "layer   3  Sparsity: 82.2544%\n",
      "total_backward_count 890890 real_backward_count 48090   5.398%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.787952/  1.086311, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6648%\n",
      "layer   2  Sparsity: 71.1261%\n",
      "layer   3  Sparsity: 82.8658%\n",
      "total_backward_count 895785 real_backward_count 48163   5.377%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.774191/  1.068209, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7269%\n",
      "layer   2  Sparsity: 71.1854%\n",
      "layer   3  Sparsity: 82.5217%\n",
      "total_backward_count 900680 real_backward_count 48237   5.356%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.753421/  1.073005, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7057%\n",
      "layer   2  Sparsity: 71.1670%\n",
      "layer   3  Sparsity: 82.1185%\n",
      "total_backward_count 905575 real_backward_count 48310   5.335%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.760724/  1.072712, val:  86.25%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6562%\n",
      "layer   2  Sparsity: 71.2419%\n",
      "layer   3  Sparsity: 82.2357%\n",
      "total_backward_count 910470 real_backward_count 48375   5.313%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.763574/  1.060073, val:  85.83%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7245%\n",
      "layer   2  Sparsity: 71.0680%\n",
      "layer   3  Sparsity: 82.1416%\n",
      "total_backward_count 915365 real_backward_count 48437   5.292%\n",
      "fc layer 3 self.abs_max_out: 959.0\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.760498/  1.059584, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.7054%\n",
      "layer   2  Sparsity: 71.1770%\n",
      "layer   3  Sparsity: 82.1097%\n",
      "total_backward_count 920260 real_backward_count 48519   5.272%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.748989/  1.045417, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6967%\n",
      "layer   2  Sparsity: 71.1425%\n",
      "layer   3  Sparsity: 81.6820%\n",
      "total_backward_count 925155 real_backward_count 48602   5.253%\n",
      "fc layer 3 self.abs_max_out: 961.0\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.745193/  1.047491, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6802%\n",
      "layer   2  Sparsity: 70.9885%\n",
      "layer   3  Sparsity: 81.9857%\n",
      "total_backward_count 930050 real_backward_count 48660   5.232%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.737566/  1.040670, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6948%\n",
      "layer   2  Sparsity: 70.9124%\n",
      "layer   3  Sparsity: 82.0550%\n",
      "total_backward_count 934945 real_backward_count 48715   5.210%\n",
      "fc layer 3 self.abs_max_out: 993.0\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.738042/  1.070572, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6797%\n",
      "layer   2  Sparsity: 70.9826%\n",
      "layer   3  Sparsity: 82.2680%\n",
      "total_backward_count 939840 real_backward_count 48776   5.190%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.746332/  1.065755, val:  87.08%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6939%\n",
      "layer   2  Sparsity: 71.0494%\n",
      "layer   3  Sparsity: 82.7411%\n",
      "total_backward_count 944735 real_backward_count 48832   5.169%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.731022/  1.043477, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6874%\n",
      "layer   2  Sparsity: 71.0077%\n",
      "layer   3  Sparsity: 82.7363%\n",
      "total_backward_count 949630 real_backward_count 48869   5.146%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.739628/  1.058475, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.50 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7019%\n",
      "layer   2  Sparsity: 71.0285%\n",
      "layer   3  Sparsity: 82.8567%\n",
      "total_backward_count 954525 real_backward_count 48929   5.126%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.738689/  1.053627, val:  87.92%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6770%\n",
      "layer   2  Sparsity: 71.2015%\n",
      "layer   3  Sparsity: 82.7329%\n",
      "total_backward_count 959420 real_backward_count 48998   5.107%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.741118/  1.034547, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.7167%\n",
      "layer   2  Sparsity: 71.0448%\n",
      "layer   3  Sparsity: 82.3541%\n",
      "total_backward_count 964315 real_backward_count 49071   5.089%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.732537/  1.045695, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 37.77 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 91.7323%\n",
      "layer   2  Sparsity: 71.1756%\n",
      "layer   3  Sparsity: 82.5002%\n",
      "total_backward_count 969210 real_backward_count 49152   5.071%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.738750/  1.072316, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 91.6874%\n",
      "layer   2  Sparsity: 71.1261%\n",
      "layer   3  Sparsity: 82.4329%\n",
      "total_backward_count 974105 real_backward_count 49212   5.052%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.735682/  1.053279, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 91.6972%\n",
      "layer   2  Sparsity: 71.2325%\n",
      "layer   3  Sparsity: 82.4926%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382d7bb8d0e542fe88a9891fd9d327db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.73568</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>1.05328</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/619q0gug' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/619q0gug</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_212817-619q0gug/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bn8e6mrn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_234309-bn8e6mrn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bn8e6mrn' target=\"_blank\">revived-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bn8e6mrn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bn8e6mrn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251117_234317_875', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d9ce0347c24289297f14669410ba3e85\n",
      "cache path doesn't exist\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 67\n",
      "fc layer 1 self.abs_max_out: 1447.0\n",
      "lif layer 1 self.abs_max_v: 1447.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 2067.0\n",
      "lif layer 2 self.abs_max_v: 2067.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 1033.0\n",
      "fc layer 1 self.abs_max_out: 1472.0\n",
      "lif layer 1 self.abs_max_v: 1986.5\n",
      "lif layer 2 self.abs_max_v: 2579.5\n",
      "lif layer 2 self.abs_max_v: 2880.0\n",
      "fc layer 1 self.abs_max_out: 1502.0\n",
      "lif layer 1 self.abs_max_v: 2235.0\n",
      "lif layer 2 self.abs_max_v: 3152.0\n",
      "fc layer 1 self.abs_max_out: 1524.0\n",
      "lif layer 1 self.abs_max_v: 2505.5\n",
      "fc layer 1 self.abs_max_out: 2086.0\n",
      "fc layer 2 self.abs_max_out: 2069.0\n",
      "lif layer 2 self.abs_max_v: 3537.0\n",
      "fc layer 2 self.abs_max_out: 2103.0\n",
      "lif layer 2 self.abs_max_v: 3871.5\n",
      "smallest_now_T updated: 60\n",
      "fc layer 1 self.abs_max_out: 2207.0\n",
      "lif layer 1 self.abs_max_v: 2904.5\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "lif layer 1 self.abs_max_v: 3117.5\n",
      "fc layer 2 self.abs_max_out: 2261.0\n",
      "fc layer 2 self.abs_max_out: 2311.0\n",
      "lif layer 1 self.abs_max_v: 3139.5\n",
      "fc layer 1 self.abs_max_out: 2377.0\n",
      "lif layer 1 self.abs_max_v: 3438.0\n",
      "fc layer 1 self.abs_max_out: 2393.0\n",
      "smallest_now_T updated: 5\n",
      "fc layer 1 self.abs_max_out: 2989.0\n",
      "fc layer 2 self.abs_max_out: 2430.0\n",
      "lif layer 1 self.abs_max_v: 3604.5\n",
      "fc layer 1 self.abs_max_out: 3036.0\n",
      "fc layer 1 self.abs_max_out: 3480.0\n",
      "lif layer 1 self.abs_max_v: 4134.0\n",
      "lif layer 1 self.abs_max_v: 4305.5\n",
      "lif layer 1 self.abs_max_v: 4680.5\n",
      "lif layer 1 self.abs_max_v: 4736.0\n",
      "fc layer 1 self.abs_max_out: 3580.0\n",
      "fc layer 2 self.abs_max_out: 2514.0\n",
      "lif layer 1 self.abs_max_v: 4909.0\n",
      "fc layer 2 self.abs_max_out: 2525.0\n",
      "fc layer 1 self.abs_max_out: 3916.0\n",
      "fc layer 2 self.abs_max_out: 2530.0\n",
      "lif layer 1 self.abs_max_v: 4969.0\n",
      "lif layer 1 self.abs_max_v: 5764.5\n",
      "fc layer 2 self.abs_max_out: 2903.0\n",
      "lif layer 2 self.abs_max_v: 4022.5\n",
      "fc layer 2 self.abs_max_out: 2952.0\n",
      "lif layer 2 self.abs_max_v: 4090.0\n",
      "lif layer 2 self.abs_max_v: 4292.5\n",
      "fc layer 2 self.abs_max_out: 3007.0\n",
      "fc layer 2 self.abs_max_out: 3042.0\n",
      "lif layer 1 self.abs_max_v: 5821.0\n",
      "lif layer 1 self.abs_max_v: 6588.5\n",
      "fc layer 2 self.abs_max_out: 3053.0\n",
      "fc layer 1 self.abs_max_out: 4017.0\n",
      "fc layer 1 self.abs_max_out: 4041.0\n",
      "fc layer 2 self.abs_max_out: 3131.0\n",
      "fc layer 2 self.abs_max_out: 3177.0\n",
      "fc layer 2 self.abs_max_out: 3191.0\n",
      "fc layer 2 self.abs_max_out: 3250.0\n",
      "fc layer 2 self.abs_max_out: 3847.0\n",
      "fc layer 1 self.abs_max_out: 4123.0\n",
      "fc layer 1 self.abs_max_out: 4160.0\n",
      "fc layer 1 self.abs_max_out: 4196.0\n",
      "fc layer 1 self.abs_max_out: 4509.0\n",
      "lif layer 2 self.abs_max_v: 4364.5\n",
      "fc layer 1 self.abs_max_out: 4790.0\n",
      "fc layer 1 self.abs_max_out: 5312.0\n",
      "lif layer 1 self.abs_max_v: 6738.0\n",
      "lif layer 1 self.abs_max_v: 6776.0\n",
      "lif layer 2 self.abs_max_v: 4405.0\n",
      "lif layer 2 self.abs_max_v: 4410.5\n",
      "lif layer 2 self.abs_max_v: 4493.5\n",
      "lif layer 2 self.abs_max_v: 4503.0\n",
      "lif layer 2 self.abs_max_v: 4725.0\n",
      "fc layer 1 self.abs_max_out: 5378.0\n",
      "lif layer 1 self.abs_max_v: 6931.0\n",
      "lif layer 1 self.abs_max_v: 7070.5\n",
      "lif layer 2 self.abs_max_v: 4742.5\n",
      "lif layer 1 self.abs_max_v: 7149.5\n",
      "lif layer 1 self.abs_max_v: 7271.0\n",
      "lif layer 1 self.abs_max_v: 7428.0\n",
      "lif layer 1 self.abs_max_v: 8035.0\n",
      "lif layer 1 self.abs_max_v: 8308.0\n",
      "lif layer 1 self.abs_max_v: 8365.0\n",
      "lif layer 1 self.abs_max_v: 8590.5\n",
      "lif layer 1 self.abs_max_v: 8792.5\n",
      "fc layer 1 self.abs_max_out: 5444.0\n",
      "lif layer 1 self.abs_max_v: 9802.0\n",
      "fc layer 1 self.abs_max_out: 5597.0\n",
      "lif layer 1 self.abs_max_v: 10251.0\n",
      "fc layer 1 self.abs_max_out: 5880.0\n",
      "lif layer 1 self.abs_max_v: 10804.5\n",
      "fc layer 3 self.abs_max_out: 1086.0\n",
      "fc layer 1 self.abs_max_out: 6745.0\n",
      "fc layer 3 self.abs_max_out: 1121.0\n",
      "lif layer 1 self.abs_max_v: 11379.0\n",
      "lif layer 1 self.abs_max_v: 11953.5\n",
      "lif layer 1 self.abs_max_v: 12593.0\n",
      "lif layer 1 self.abs_max_v: 12609.5\n",
      "fc layer 1 self.abs_max_out: 7084.0\n",
      "fc layer 3 self.abs_max_out: 1155.0\n",
      "fc layer 3 self.abs_max_out: 1171.0\n",
      "fc layer 3 self.abs_max_out: 1227.0\n",
      "fc layer 1 self.abs_max_out: 7211.0\n",
      "fc layer 3 self.abs_max_out: 1375.0\n",
      "fc layer 1 self.abs_max_out: 7694.0\n",
      "fc layer 1 self.abs_max_out: 7845.0\n",
      "lif layer 1 self.abs_max_v: 12611.0\n",
      "lif layer 1 self.abs_max_v: 13851.5\n",
      "lif layer 1 self.abs_max_v: 13888.5\n",
      "lif layer 1 self.abs_max_v: 14341.0\n",
      "fc layer 1 self.abs_max_out: 7854.0\n",
      "lif layer 1 self.abs_max_v: 14532.0\n",
      "smallest_now_T_val updated: 62\n",
      "smallest_now_T_val updated: 51\n",
      "smallest_now_T_val updated: 50\n",
      "smallest_now_T_val updated: 10\n",
      "fc layer 1 self.abs_max_out: 8234.0\n",
      "fc layer 1 self.abs_max_out: 8691.0\n",
      "lif layer 1 self.abs_max_v: 15430.5\n",
      "smallest_now_T_val updated: 5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.007119/  2.045877, val:  39.17%, val_best:  39.17%, tr:  76.40%, tr_best:  76.40%, epoch time: 82.73 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 74.7507%\n",
      "layer   2  Sparsity: 80.3935%\n",
      "layer   3  Sparsity: 70.0575%\n",
      "total_backward_count 9790 real_backward_count 4003  40.889%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 8765.0\n",
      "lif layer 1 self.abs_max_v: 15550.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.926527/  2.015209, val:  46.25%, val_best:  46.25%, tr:  88.56%, tr_best:  88.56%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7547%\n",
      "layer   2  Sparsity: 85.9507%\n",
      "layer   3  Sparsity: 71.7902%\n",
      "total_backward_count 19580 real_backward_count 6861  35.041%\n",
      "lif layer 1 self.abs_max_v: 15574.0\n",
      "lif layer 1 self.abs_max_v: 15748.0\n",
      "fc layer 1 self.abs_max_out: 8779.0\n",
      "fc layer 1 self.abs_max_out: 9207.0\n",
      "lif layer 1 self.abs_max_v: 15830.0\n",
      "lif layer 1 self.abs_max_v: 16268.0\n",
      "lif layer 1 self.abs_max_v: 16316.5\n",
      "lif layer 1 self.abs_max_v: 16658.5\n",
      "lif layer 1 self.abs_max_v: 16708.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.907732/  1.985796, val:  54.58%, val_best:  54.58%, tr:  92.34%, tr_best:  92.34%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7555%\n",
      "layer   2  Sparsity: 85.3509%\n",
      "layer   3  Sparsity: 70.5491%\n",
      "total_backward_count 29370 real_backward_count 9311  31.702%\n",
      "lif layer 1 self.abs_max_v: 16721.0\n",
      "lif layer 1 self.abs_max_v: 17072.5\n",
      "fc layer 1 self.abs_max_out: 9224.0\n",
      "fc layer 1 self.abs_max_out: 9265.0\n",
      "fc layer 1 self.abs_max_out: 9440.0\n",
      "lif layer 1 self.abs_max_v: 17384.5\n",
      "lif layer 1 self.abs_max_v: 17398.5\n",
      "lif layer 1 self.abs_max_v: 17598.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.911701/  2.019508, val:  49.58%, val_best:  54.58%, tr:  95.20%, tr_best:  95.20%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7602%\n",
      "layer   2  Sparsity: 84.6388%\n",
      "layer   3  Sparsity: 68.4693%\n",
      "total_backward_count 39160 real_backward_count 11432  29.193%\n",
      "fc layer 1 self.abs_max_out: 9559.0\n",
      "lif layer 1 self.abs_max_v: 17830.0\n",
      "fc layer 1 self.abs_max_out: 9617.0\n",
      "lif layer 1 self.abs_max_v: 17984.5\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.929722/  1.988433, val:  54.58%, val_best:  54.58%, tr:  95.30%, tr_best:  95.30%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7567%\n",
      "layer   2  Sparsity: 85.1149%\n",
      "layer   3  Sparsity: 69.4801%\n",
      "total_backward_count 48950 real_backward_count 13373  27.320%\n",
      "fc layer 1 self.abs_max_out: 9778.0\n",
      "lif layer 1 self.abs_max_v: 18120.0\n",
      "lif layer 1 self.abs_max_v: 18505.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.925329/  2.005783, val:  57.50%, val_best:  57.50%, tr:  96.53%, tr_best:  96.53%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7539%\n",
      "layer   2  Sparsity: 83.8588%\n",
      "layer   3  Sparsity: 69.4462%\n",
      "total_backward_count 58740 real_backward_count 15213  25.899%\n",
      "lif layer 2 self.abs_max_v: 4755.5\n",
      "fc layer 1 self.abs_max_out: 9895.0\n",
      "lif layer 1 self.abs_max_v: 18688.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.914465/  1.996354, val:  55.00%, val_best:  57.50%, tr:  96.73%, tr_best:  96.73%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7554%\n",
      "layer   2  Sparsity: 83.3027%\n",
      "layer   3  Sparsity: 69.3813%\n",
      "total_backward_count 68530 real_backward_count 17006  24.815%\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.912254/  1.989089, val:  59.17%, val_best:  59.17%, tr:  96.94%, tr_best:  96.94%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7587%\n",
      "layer   2  Sparsity: 82.8749%\n",
      "layer   3  Sparsity: 68.4797%\n",
      "total_backward_count 78320 real_backward_count 18664  23.830%\n",
      "lif layer 2 self.abs_max_v: 4833.5\n",
      "lif layer 2 self.abs_max_v: 5028.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.916562/  1.984914, val:  56.25%, val_best:  59.17%, tr:  96.94%, tr_best:  96.94%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7472%\n",
      "layer   2  Sparsity: 82.1288%\n",
      "layer   3  Sparsity: 68.0396%\n",
      "total_backward_count 88110 real_backward_count 20283  23.020%\n",
      "lif layer 2 self.abs_max_v: 5048.5\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.900262/  1.994139, val:  57.08%, val_best:  59.17%, tr:  96.73%, tr_best:  96.94%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7619%\n",
      "layer   2  Sparsity: 81.3066%\n",
      "layer   3  Sparsity: 68.9538%\n",
      "total_backward_count 97900 real_backward_count 21804  22.272%\n",
      "lif layer 2 self.abs_max_v: 5117.0\n",
      "fc layer 1 self.abs_max_out: 10163.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.912934/  1.986286, val:  57.92%, val_best:  59.17%, tr:  97.45%, tr_best:  97.45%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7539%\n",
      "layer   2  Sparsity: 81.2178%\n",
      "layer   3  Sparsity: 68.6931%\n",
      "total_backward_count 107690 real_backward_count 23294  21.631%\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.922686/  1.986760, val:  62.08%, val_best:  62.08%, tr:  97.45%, tr_best:  97.45%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7654%\n",
      "layer   2  Sparsity: 80.6945%\n",
      "layer   3  Sparsity: 68.9952%\n",
      "total_backward_count 117480 real_backward_count 24824  21.130%\n",
      "lif layer 2 self.abs_max_v: 5184.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.914310/  2.008843, val:  57.92%, val_best:  62.08%, tr:  97.24%, tr_best:  97.45%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7507%\n",
      "layer   2  Sparsity: 80.2293%\n",
      "layer   3  Sparsity: 67.3858%\n",
      "total_backward_count 127270 real_backward_count 26247  20.623%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.921920/  1.992719, val:  60.42%, val_best:  62.08%, tr:  97.55%, tr_best:  97.55%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7521%\n",
      "layer   2  Sparsity: 80.5105%\n",
      "layer   3  Sparsity: 67.3967%\n",
      "total_backward_count 137060 real_backward_count 27643  20.169%\n",
      "lif layer 2 self.abs_max_v: 5317.0\n",
      "lif layer 2 self.abs_max_v: 5345.0\n",
      "lif layer 2 self.abs_max_v: 5427.5\n",
      "lif layer 2 self.abs_max_v: 6237.5\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.920841/  1.995301, val:  59.17%, val_best:  62.08%, tr:  97.14%, tr_best:  97.55%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7488%\n",
      "layer   2  Sparsity: 80.5772%\n",
      "layer   3  Sparsity: 68.5993%\n",
      "total_backward_count 146850 real_backward_count 28983  19.736%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.907143/  1.988114, val:  55.00%, val_best:  62.08%, tr:  97.55%, tr_best:  97.55%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7627%\n",
      "layer   2  Sparsity: 80.3021%\n",
      "layer   3  Sparsity: 69.6031%\n",
      "total_backward_count 156640 real_backward_count 30343  19.371%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.896158/  1.965760, val:  59.17%, val_best:  62.08%, tr:  97.14%, tr_best:  97.55%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7447%\n",
      "layer   2  Sparsity: 80.0475%\n",
      "layer   3  Sparsity: 68.5702%\n",
      "total_backward_count 166430 real_backward_count 31669  19.028%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.887441/  1.971173, val:  56.25%, val_best:  62.08%, tr:  97.65%, tr_best:  97.65%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7530%\n",
      "layer   2  Sparsity: 80.0304%\n",
      "layer   3  Sparsity: 69.0077%\n",
      "total_backward_count 176220 real_backward_count 32971  18.710%\n",
      "fc layer 1 self.abs_max_out: 10191.0\n",
      "fc layer 1 self.abs_max_out: 10360.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.892053/  1.969144, val:  63.33%, val_best:  63.33%, tr:  97.24%, tr_best:  97.65%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7570%\n",
      "layer   2  Sparsity: 79.9906%\n",
      "layer   3  Sparsity: 69.5406%\n",
      "total_backward_count 186010 real_backward_count 34317  18.449%\n",
      "fc layer 1 self.abs_max_out: 10406.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.883895/  1.951564, val:  59.17%, val_best:  63.33%, tr:  97.34%, tr_best:  97.65%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7547%\n",
      "layer   2  Sparsity: 80.3923%\n",
      "layer   3  Sparsity: 70.4621%\n",
      "total_backward_count 195800 real_backward_count 35596  18.180%\n",
      "fc layer 1 self.abs_max_out: 10605.0\n",
      "lif layer 1 self.abs_max_v: 18765.5\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.889899/  1.968641, val:  59.17%, val_best:  63.33%, tr:  97.34%, tr_best:  97.65%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7553%\n",
      "layer   2  Sparsity: 80.6375%\n",
      "layer   3  Sparsity: 70.5287%\n",
      "total_backward_count 205590 real_backward_count 36870  17.934%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.897804/  1.985440, val:  58.75%, val_best:  63.33%, tr:  97.45%, tr_best:  97.65%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7658%\n",
      "layer   2  Sparsity: 81.0910%\n",
      "layer   3  Sparsity: 71.3152%\n",
      "total_backward_count 215380 real_backward_count 38151  17.713%\n",
      "fc layer 1 self.abs_max_out: 10666.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.896729/  1.962757, val:  59.58%, val_best:  63.33%, tr:  97.65%, tr_best:  97.65%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7553%\n",
      "layer   2  Sparsity: 80.7984%\n",
      "layer   3  Sparsity: 70.9088%\n",
      "total_backward_count 225170 real_backward_count 39409  17.502%\n",
      "lif layer 1 self.abs_max_v: 18900.5\n",
      "lif layer 1 self.abs_max_v: 19340.0\n",
      "fc layer 1 self.abs_max_out: 10718.0\n",
      "lif layer 1 self.abs_max_v: 19433.5\n",
      "fc layer 1 self.abs_max_out: 10892.0\n",
      "fc layer 1 self.abs_max_out: 11097.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.890886/  1.956597, val:  60.83%, val_best:  63.33%, tr:  97.55%, tr_best:  97.65%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7462%\n",
      "layer   2  Sparsity: 80.4447%\n",
      "layer   3  Sparsity: 70.2961%\n",
      "total_backward_count 234960 real_backward_count 40631  17.293%\n",
      "lif layer 1 self.abs_max_v: 19570.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.876270/  1.950986, val:  62.08%, val_best:  63.33%, tr:  97.65%, tr_best:  97.65%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7573%\n",
      "layer   2  Sparsity: 80.4066%\n",
      "layer   3  Sparsity: 70.2534%\n",
      "total_backward_count 244750 real_backward_count 41821  17.087%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.875038/  1.963839, val:  62.50%, val_best:  63.33%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7595%\n",
      "layer   2  Sparsity: 80.6063%\n",
      "layer   3  Sparsity: 70.9696%\n",
      "total_backward_count 254540 real_backward_count 43008  16.896%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.889650/  1.967522, val:  61.67%, val_best:  63.33%, tr:  97.45%, tr_best:  97.75%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7638%\n",
      "layer   2  Sparsity: 80.2367%\n",
      "layer   3  Sparsity: 71.6735%\n",
      "total_backward_count 264330 real_backward_count 44220  16.729%\n",
      "fc layer 1 self.abs_max_out: 11111.0\n",
      "lif layer 1 self.abs_max_v: 19626.0\n",
      "fc layer 1 self.abs_max_out: 11224.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.884652/  1.957377, val:  65.83%, val_best:  65.83%, tr:  97.55%, tr_best:  97.75%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7540%\n",
      "layer   2  Sparsity: 80.0402%\n",
      "layer   3  Sparsity: 70.5550%\n",
      "total_backward_count 274120 real_backward_count 45420  16.569%\n",
      "lif layer 1 self.abs_max_v: 19726.0\n",
      "fc layer 1 self.abs_max_out: 11533.0\n",
      "lif layer 1 self.abs_max_v: 20360.5\n",
      "lif layer 1 self.abs_max_v: 20428.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.867193/  1.949897, val:  62.08%, val_best:  65.83%, tr:  97.55%, tr_best:  97.75%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7458%\n",
      "layer   2  Sparsity: 79.5315%\n",
      "layer   3  Sparsity: 70.5699%\n",
      "total_backward_count 283910 real_backward_count 46545  16.394%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.863510/  1.953147, val:  59.58%, val_best:  65.83%, tr:  97.55%, tr_best:  97.75%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7535%\n",
      "layer   2  Sparsity: 79.5861%\n",
      "layer   3  Sparsity: 70.5885%\n",
      "total_backward_count 293700 real_backward_count 47740  16.255%\n",
      "fc layer 1 self.abs_max_out: 11700.0\n",
      "lif layer 1 self.abs_max_v: 20727.0\n",
      "lif layer 1 self.abs_max_v: 20794.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.862560/  1.942453, val:  60.83%, val_best:  65.83%, tr:  97.55%, tr_best:  97.75%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7624%\n",
      "layer   2  Sparsity: 80.0310%\n",
      "layer   3  Sparsity: 70.7457%\n",
      "total_backward_count 303490 real_backward_count 48869  16.102%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.855147/  1.937730, val:  65.42%, val_best:  65.83%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7543%\n",
      "layer   2  Sparsity: 79.8650%\n",
      "layer   3  Sparsity: 70.5624%\n",
      "total_backward_count 313280 real_backward_count 49973  15.952%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.848711/  1.924984, val:  62.92%, val_best:  65.83%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7540%\n",
      "layer   2  Sparsity: 79.6226%\n",
      "layer   3  Sparsity: 69.9934%\n",
      "total_backward_count 323070 real_backward_count 51017  15.791%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.851248/  1.939384, val:  64.17%, val_best:  65.83%, tr:  97.14%, tr_best:  97.75%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7616%\n",
      "layer   2  Sparsity: 79.3481%\n",
      "layer   3  Sparsity: 71.4795%\n",
      "total_backward_count 332860 real_backward_count 52182  15.677%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.846011/  1.938699, val:  61.25%, val_best:  65.83%, tr:  97.55%, tr_best:  97.75%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7579%\n",
      "layer   2  Sparsity: 79.0703%\n",
      "layer   3  Sparsity: 71.8142%\n",
      "total_backward_count 342650 real_backward_count 53227  15.534%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.853480/  1.948235, val:  60.00%, val_best:  65.83%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7532%\n",
      "layer   2  Sparsity: 78.7958%\n",
      "layer   3  Sparsity: 72.6234%\n",
      "total_backward_count 352440 real_backward_count 54318  15.412%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.843670/  1.942526, val:  63.75%, val_best:  65.83%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7642%\n",
      "layer   2  Sparsity: 79.7025%\n",
      "layer   3  Sparsity: 72.5042%\n",
      "total_backward_count 362230 real_backward_count 55390  15.291%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.841747/  1.930987, val:  60.42%, val_best:  65.83%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7561%\n",
      "layer   2  Sparsity: 79.5939%\n",
      "layer   3  Sparsity: 72.7045%\n",
      "total_backward_count 372020 real_backward_count 56394  15.159%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.839688/  1.924324, val:  64.58%, val_best:  65.83%, tr:  97.24%, tr_best:  97.75%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7571%\n",
      "layer   2  Sparsity: 79.5237%\n",
      "layer   3  Sparsity: 72.6462%\n",
      "total_backward_count 381810 real_backward_count 57470  15.052%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.839433/  1.941226, val:  64.17%, val_best:  65.83%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7615%\n",
      "layer   2  Sparsity: 79.5821%\n",
      "layer   3  Sparsity: 72.9059%\n",
      "total_backward_count 391600 real_backward_count 58510  14.941%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.844573/  1.934161, val:  62.50%, val_best:  65.83%, tr:  97.24%, tr_best:  97.75%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7608%\n",
      "layer   2  Sparsity: 79.7685%\n",
      "layer   3  Sparsity: 73.6146%\n",
      "total_backward_count 401390 real_backward_count 59549  14.836%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.837990/  1.934174, val:  67.92%, val_best:  67.92%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7503%\n",
      "layer   2  Sparsity: 80.1499%\n",
      "layer   3  Sparsity: 73.7071%\n",
      "total_backward_count 411180 real_backward_count 60552  14.726%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.835914/  1.935482, val:  62.92%, val_best:  67.92%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7585%\n",
      "layer   2  Sparsity: 79.9630%\n",
      "layer   3  Sparsity: 73.6852%\n",
      "total_backward_count 420970 real_backward_count 61579  14.628%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.837126/  1.932240, val:  59.58%, val_best:  67.92%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7530%\n",
      "layer   2  Sparsity: 79.5894%\n",
      "layer   3  Sparsity: 74.2771%\n",
      "total_backward_count 430760 real_backward_count 62604  14.533%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.832183/  1.916660, val:  60.83%, val_best:  67.92%, tr:  97.55%, tr_best:  97.75%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7585%\n",
      "layer   2  Sparsity: 78.9596%\n",
      "layer   3  Sparsity: 74.0132%\n",
      "total_backward_count 440550 real_backward_count 63593  14.435%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.832588/  1.924234, val:  64.17%, val_best:  67.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7569%\n",
      "layer   2  Sparsity: 79.3128%\n",
      "layer   3  Sparsity: 75.1513%\n",
      "total_backward_count 450340 real_backward_count 64545  14.333%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.829513/  1.915044, val:  64.17%, val_best:  67.92%, tr:  97.55%, tr_best:  97.75%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7614%\n",
      "layer   2  Sparsity: 79.5780%\n",
      "layer   3  Sparsity: 75.2686%\n",
      "total_backward_count 460130 real_backward_count 65488  14.232%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.826051/  1.924978, val:  68.33%, val_best:  68.33%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7639%\n",
      "layer   2  Sparsity: 79.6035%\n",
      "layer   3  Sparsity: 75.6030%\n",
      "total_backward_count 469920 real_backward_count 66412  14.133%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.832470/  1.911983, val:  69.58%, val_best:  69.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7517%\n",
      "layer   2  Sparsity: 79.2874%\n",
      "layer   3  Sparsity: 75.8404%\n",
      "total_backward_count 479710 real_backward_count 67301  14.030%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.823758/  1.906687, val:  65.83%, val_best:  69.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7590%\n",
      "layer   2  Sparsity: 79.2263%\n",
      "layer   3  Sparsity: 75.1140%\n",
      "total_backward_count 489500 real_backward_count 68261  13.945%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.822685/  1.916208, val:  69.58%, val_best:  69.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7550%\n",
      "layer   2  Sparsity: 79.2384%\n",
      "layer   3  Sparsity: 74.9359%\n",
      "total_backward_count 499290 real_backward_count 69194  13.858%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.829564/  1.916018, val:  68.75%, val_best:  69.58%, tr:  97.45%, tr_best:  97.75%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7540%\n",
      "layer   2  Sparsity: 79.0502%\n",
      "layer   3  Sparsity: 75.7044%\n",
      "total_backward_count 509080 real_backward_count 70093  13.769%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.823366/  1.914374, val:  62.50%, val_best:  69.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7531%\n",
      "layer   2  Sparsity: 79.1842%\n",
      "layer   3  Sparsity: 75.2937%\n",
      "total_backward_count 518870 real_backward_count 71027  13.689%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.825610/  1.910139, val:  66.67%, val_best:  69.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 74.7571%\n",
      "layer   2  Sparsity: 79.5789%\n",
      "layer   3  Sparsity: 75.9628%\n",
      "total_backward_count 528660 real_backward_count 71961  13.612%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.825692/  1.908144, val:  63.75%, val_best:  69.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7597%\n",
      "layer   2  Sparsity: 79.3793%\n",
      "layer   3  Sparsity: 76.1351%\n",
      "total_backward_count 538450 real_backward_count 72876  13.534%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.823796/  1.908353, val:  72.08%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7503%\n",
      "layer   2  Sparsity: 79.2968%\n",
      "layer   3  Sparsity: 75.7192%\n",
      "total_backward_count 548240 real_backward_count 73787  13.459%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.817725/  1.910245, val:  67.08%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7622%\n",
      "layer   2  Sparsity: 79.5256%\n",
      "layer   3  Sparsity: 75.7381%\n",
      "total_backward_count 558030 real_backward_count 74671  13.381%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.817782/  1.902483, val:  68.33%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7557%\n",
      "layer   2  Sparsity: 79.3203%\n",
      "layer   3  Sparsity: 75.8728%\n",
      "total_backward_count 567820 real_backward_count 75502  13.297%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.814389/  1.900123, val:  69.17%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7620%\n",
      "layer   2  Sparsity: 78.9340%\n",
      "layer   3  Sparsity: 76.2562%\n",
      "total_backward_count 577610 real_backward_count 76384  13.224%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.818562/  1.917339, val:  62.08%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7612%\n",
      "layer   2  Sparsity: 79.2820%\n",
      "layer   3  Sparsity: 76.1712%\n",
      "total_backward_count 587400 real_backward_count 77244  13.150%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.817185/  1.908883, val:  60.83%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7443%\n",
      "layer   2  Sparsity: 78.8285%\n",
      "layer   3  Sparsity: 76.1005%\n",
      "total_backward_count 597190 real_backward_count 78115  13.080%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.808176/  1.898033, val:  70.83%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7549%\n",
      "layer   2  Sparsity: 78.8985%\n",
      "layer   3  Sparsity: 76.0437%\n",
      "total_backward_count 606980 real_backward_count 79018  13.018%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.806058/  1.900007, val:  70.00%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7560%\n",
      "layer   2  Sparsity: 78.8801%\n",
      "layer   3  Sparsity: 76.3167%\n",
      "total_backward_count 616770 real_backward_count 79844  12.946%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.812007/  1.907375, val:  63.75%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7614%\n",
      "layer   2  Sparsity: 78.9875%\n",
      "layer   3  Sparsity: 76.4081%\n",
      "total_backward_count 626560 real_backward_count 80683  12.877%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.812421/  1.903403, val:  70.00%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7456%\n",
      "layer   2  Sparsity: 79.0978%\n",
      "layer   3  Sparsity: 76.5497%\n",
      "total_backward_count 636350 real_backward_count 81478  12.804%\n",
      "fc layer 3 self.abs_max_out: 1377.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.814154/  1.909286, val:  66.67%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7596%\n",
      "layer   2  Sparsity: 79.4748%\n",
      "layer   3  Sparsity: 76.6351%\n",
      "total_backward_count 646140 real_backward_count 82302  12.737%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.816468/  1.918129, val:  67.92%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7566%\n",
      "layer   2  Sparsity: 79.3651%\n",
      "layer   3  Sparsity: 77.2560%\n",
      "total_backward_count 655930 real_backward_count 83126  12.673%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.825203/  1.909507, val:  64.58%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7652%\n",
      "layer   2  Sparsity: 78.9772%\n",
      "layer   3  Sparsity: 76.9541%\n",
      "total_backward_count 665720 real_backward_count 83927  12.607%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.825783/  1.909591, val:  66.25%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7634%\n",
      "layer   2  Sparsity: 79.1622%\n",
      "layer   3  Sparsity: 77.2078%\n",
      "total_backward_count 675510 real_backward_count 84731  12.543%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.823612/  1.924919, val:  66.25%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7583%\n",
      "layer   2  Sparsity: 79.0877%\n",
      "layer   3  Sparsity: 77.5691%\n",
      "total_backward_count 685300 real_backward_count 85511  12.478%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.826172/  1.921362, val:  66.25%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 74.7540%\n",
      "layer   2  Sparsity: 78.7468%\n",
      "layer   3  Sparsity: 77.5300%\n",
      "total_backward_count 695090 real_backward_count 86313  12.418%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.824735/  1.924721, val:  65.83%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 74.7542%\n",
      "layer   2  Sparsity: 78.4005%\n",
      "layer   3  Sparsity: 76.7788%\n",
      "total_backward_count 704880 real_backward_count 87148  12.364%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.828449/  1.920301, val:  69.17%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7557%\n",
      "layer   2  Sparsity: 78.7131%\n",
      "layer   3  Sparsity: 76.6806%\n",
      "total_backward_count 714670 real_backward_count 87943  12.305%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.818532/  1.919320, val:  64.58%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7596%\n",
      "layer   2  Sparsity: 78.7174%\n",
      "layer   3  Sparsity: 77.2831%\n",
      "total_backward_count 724460 real_backward_count 88716  12.246%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.822875/  1.915034, val:  67.50%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7588%\n",
      "layer   2  Sparsity: 78.5743%\n",
      "layer   3  Sparsity: 76.9017%\n",
      "total_backward_count 734250 real_backward_count 89507  12.190%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.816086/  1.919456, val:  68.33%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7511%\n",
      "layer   2  Sparsity: 78.6072%\n",
      "layer   3  Sparsity: 76.5528%\n",
      "total_backward_count 744040 real_backward_count 90258  12.131%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.820051/  1.919574, val:  69.17%, val_best:  72.08%, tr:  97.55%, tr_best:  97.75%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7574%\n",
      "layer   2  Sparsity: 78.7877%\n",
      "layer   3  Sparsity: 76.6255%\n",
      "total_backward_count 753830 real_backward_count 91027  12.075%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.820300/  1.922355, val:  66.25%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7626%\n",
      "layer   2  Sparsity: 78.8607%\n",
      "layer   3  Sparsity: 77.5890%\n",
      "total_backward_count 763620 real_backward_count 91771  12.018%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.821162/  1.921796, val:  65.42%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 74.7607%\n",
      "layer   2  Sparsity: 79.0706%\n",
      "layer   3  Sparsity: 77.4340%\n",
      "total_backward_count 773410 real_backward_count 92543  11.966%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.818417/  1.913743, val:  71.67%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7458%\n",
      "layer   2  Sparsity: 78.9455%\n",
      "layer   3  Sparsity: 77.2105%\n",
      "total_backward_count 783200 real_backward_count 93295  11.912%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.818791/  1.918169, val:  70.42%, val_best:  72.08%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7534%\n",
      "layer   2  Sparsity: 78.7633%\n",
      "layer   3  Sparsity: 77.4204%\n",
      "total_backward_count 792990 real_backward_count 94039  11.859%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.816773/  1.921003, val:  62.50%, val_best:  72.08%, tr:  97.65%, tr_best:  97.75%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7536%\n",
      "layer   2  Sparsity: 78.6767%\n",
      "layer   3  Sparsity: 77.7419%\n",
      "total_backward_count 802780 real_backward_count 94760  11.804%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.813955/  1.908991, val:  73.33%, val_best:  73.33%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7580%\n",
      "layer   2  Sparsity: 78.6966%\n",
      "layer   3  Sparsity: 77.6107%\n",
      "total_backward_count 812570 real_backward_count 95515  11.755%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.817420/  1.914433, val:  70.83%, val_best:  73.33%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7465%\n",
      "layer   2  Sparsity: 78.4414%\n",
      "layer   3  Sparsity: 77.1055%\n",
      "total_backward_count 822360 real_backward_count 96257  11.705%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.815688/  1.915444, val:  70.00%, val_best:  73.33%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7631%\n",
      "layer   2  Sparsity: 78.5506%\n",
      "layer   3  Sparsity: 77.1734%\n",
      "total_backward_count 832150 real_backward_count 96998  11.656%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.815988/  1.914614, val:  74.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7564%\n",
      "layer   2  Sparsity: 78.2775%\n",
      "layer   3  Sparsity: 77.3096%\n",
      "total_backward_count 841940 real_backward_count 97732  11.608%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.817877/  1.914328, val:  72.50%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7564%\n",
      "layer   2  Sparsity: 78.6420%\n",
      "layer   3  Sparsity: 77.6910%\n",
      "total_backward_count 851730 real_backward_count 98466  11.561%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.820255/  1.927075, val:  69.17%, val_best:  74.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7457%\n",
      "layer   2  Sparsity: 78.6699%\n",
      "layer   3  Sparsity: 78.0664%\n",
      "total_backward_count 861520 real_backward_count 99179  11.512%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.818426/  1.919332, val:  67.50%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7623%\n",
      "layer   2  Sparsity: 78.5916%\n",
      "layer   3  Sparsity: 77.7279%\n",
      "total_backward_count 871310 real_backward_count 99913  11.467%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.818957/  1.912500, val:  69.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7579%\n",
      "layer   2  Sparsity: 78.5441%\n",
      "layer   3  Sparsity: 78.3915%\n",
      "total_backward_count 881100 real_backward_count 100642  11.422%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.820569/  1.926108, val:  73.33%, val_best:  74.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7584%\n",
      "layer   2  Sparsity: 78.6833%\n",
      "layer   3  Sparsity: 78.9011%\n",
      "total_backward_count 890890 real_backward_count 101330  11.374%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.820256/  1.917967, val:  70.42%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7513%\n",
      "layer   2  Sparsity: 78.5448%\n",
      "layer   3  Sparsity: 78.5612%\n",
      "total_backward_count 900680 real_backward_count 102017  11.327%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.811742/  1.913994, val:  69.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7561%\n",
      "layer   2  Sparsity: 78.4303%\n",
      "layer   3  Sparsity: 78.0884%\n",
      "total_backward_count 910470 real_backward_count 102704  11.280%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.809585/  1.908701, val:  70.00%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7543%\n",
      "layer   2  Sparsity: 78.7135%\n",
      "layer   3  Sparsity: 78.0176%\n",
      "total_backward_count 920260 real_backward_count 103422  11.238%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.812533/  1.910138, val:  65.42%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7611%\n",
      "layer   2  Sparsity: 78.7739%\n",
      "layer   3  Sparsity: 78.3582%\n",
      "total_backward_count 930050 real_backward_count 104108  11.194%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.817151/  1.923504, val:  69.17%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7559%\n",
      "layer   2  Sparsity: 78.8467%\n",
      "layer   3  Sparsity: 78.5455%\n",
      "total_backward_count 939840 real_backward_count 104819  11.153%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.819256/  1.913471, val:  66.25%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7580%\n",
      "layer   2  Sparsity: 79.1720%\n",
      "layer   3  Sparsity: 78.3877%\n",
      "total_backward_count 949630 real_backward_count 105522  11.112%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.818218/  1.921175, val:  72.08%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7479%\n",
      "layer   2  Sparsity: 79.4937%\n",
      "layer   3  Sparsity: 78.8124%\n",
      "total_backward_count 959420 real_backward_count 106205  11.070%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.818446/  1.910675, val:  72.08%, val_best:  74.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7617%\n",
      "layer   2  Sparsity: 79.5406%\n",
      "layer   3  Sparsity: 78.6164%\n",
      "total_backward_count 969210 real_backward_count 106891  11.029%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.814605/  1.906073, val:  71.25%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7593%\n",
      "layer   2  Sparsity: 79.2758%\n",
      "layer   3  Sparsity: 78.3637%\n",
      "total_backward_count 979000 real_backward_count 107552  10.986%\n",
      "lif layer 2 self.abs_max_v: 6265.0\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.815893/  1.903294, val:  72.92%, val_best:  74.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7563%\n",
      "layer   2  Sparsity: 79.3156%\n",
      "layer   3  Sparsity: 78.2114%\n",
      "total_backward_count 988790 real_backward_count 108244  10.947%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.813675/  1.919752, val:  70.00%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7554%\n",
      "layer   2  Sparsity: 79.4272%\n",
      "layer   3  Sparsity: 78.7638%\n",
      "total_backward_count 998580 real_backward_count 108931  10.909%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.815575/  1.915951, val:  62.08%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7531%\n",
      "layer   2  Sparsity: 79.4277%\n",
      "layer   3  Sparsity: 78.9851%\n",
      "total_backward_count 1008370 real_backward_count 109609  10.870%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.814247/  1.911998, val:  69.17%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7613%\n",
      "layer   2  Sparsity: 79.4399%\n",
      "layer   3  Sparsity: 78.8635%\n",
      "total_backward_count 1018160 real_backward_count 110275  10.831%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.815102/  1.912158, val:  65.83%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7489%\n",
      "layer   2  Sparsity: 79.2567%\n",
      "layer   3  Sparsity: 79.2906%\n",
      "total_backward_count 1027950 real_backward_count 110929  10.791%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.815083/  1.918848, val:  69.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7511%\n",
      "layer   2  Sparsity: 79.1608%\n",
      "layer   3  Sparsity: 79.1155%\n",
      "total_backward_count 1037740 real_backward_count 111598  10.754%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.811753/  1.907332, val:  66.67%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7536%\n",
      "layer   2  Sparsity: 79.1217%\n",
      "layer   3  Sparsity: 78.6739%\n",
      "total_backward_count 1047530 real_backward_count 112264  10.717%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.812020/  1.905332, val:  69.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 74.7581%\n",
      "layer   2  Sparsity: 79.2064%\n",
      "layer   3  Sparsity: 78.4349%\n",
      "total_backward_count 1057320 real_backward_count 112947  10.682%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.806719/  1.900977, val:  68.33%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7608%\n",
      "layer   2  Sparsity: 79.2895%\n",
      "layer   3  Sparsity: 78.3832%\n",
      "total_backward_count 1067110 real_backward_count 113595  10.645%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.807012/  1.904298, val:  63.75%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7597%\n",
      "layer   2  Sparsity: 79.3457%\n",
      "layer   3  Sparsity: 78.5899%\n",
      "total_backward_count 1076900 real_backward_count 114266  10.611%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.811098/  1.914977, val:  70.00%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7453%\n",
      "layer   2  Sparsity: 79.2463%\n",
      "layer   3  Sparsity: 78.4724%\n",
      "total_backward_count 1086690 real_backward_count 114931  10.576%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.804192/  1.906433, val:  69.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7565%\n",
      "layer   2  Sparsity: 79.3746%\n",
      "layer   3  Sparsity: 78.1035%\n",
      "total_backward_count 1096480 real_backward_count 115591  10.542%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.807139/  1.910401, val:  72.92%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7502%\n",
      "layer   2  Sparsity: 79.6445%\n",
      "layer   3  Sparsity: 78.9093%\n",
      "total_backward_count 1106270 real_backward_count 116250  10.508%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.808518/  1.912332, val:  68.75%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7516%\n",
      "layer   2  Sparsity: 79.6886%\n",
      "layer   3  Sparsity: 79.0473%\n",
      "total_backward_count 1116060 real_backward_count 116883  10.473%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.809338/  1.905168, val:  70.00%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7576%\n",
      "layer   2  Sparsity: 79.4643%\n",
      "layer   3  Sparsity: 78.9687%\n",
      "total_backward_count 1125850 real_backward_count 117561  10.442%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.810217/  1.914101, val:  69.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7584%\n",
      "layer   2  Sparsity: 79.6517%\n",
      "layer   3  Sparsity: 79.4821%\n",
      "total_backward_count 1135640 real_backward_count 118169  10.405%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.811408/  1.916879, val:  67.50%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7633%\n",
      "layer   2  Sparsity: 79.4868%\n",
      "layer   3  Sparsity: 79.2631%\n",
      "total_backward_count 1145430 real_backward_count 118819  10.373%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.810407/  1.913285, val:  70.83%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7544%\n",
      "layer   2  Sparsity: 79.3222%\n",
      "layer   3  Sparsity: 79.0352%\n",
      "total_backward_count 1155220 real_backward_count 119437  10.339%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.810756/  1.904562, val:  73.75%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7551%\n",
      "layer   2  Sparsity: 79.2551%\n",
      "layer   3  Sparsity: 79.0220%\n",
      "total_backward_count 1165010 real_backward_count 120058  10.305%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.803926/  1.904981, val:  70.83%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7690%\n",
      "layer   2  Sparsity: 79.4702%\n",
      "layer   3  Sparsity: 78.7126%\n",
      "total_backward_count 1174800 real_backward_count 120694  10.274%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.804277/  1.907518, val:  70.00%, val_best:  74.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7652%\n",
      "layer   2  Sparsity: 79.4369%\n",
      "layer   3  Sparsity: 78.6987%\n",
      "total_backward_count 1184590 real_backward_count 121316  10.241%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.810092/  1.912310, val:  68.33%, val_best:  74.58%, tr:  97.65%, tr_best:  97.75%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7581%\n",
      "layer   2  Sparsity: 79.6142%\n",
      "layer   3  Sparsity: 78.9364%\n",
      "total_backward_count 1194380 real_backward_count 121952  10.210%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.811726/  1.918052, val:  72.50%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7462%\n",
      "layer   2  Sparsity: 79.5267%\n",
      "layer   3  Sparsity: 79.4087%\n",
      "total_backward_count 1204170 real_backward_count 122606  10.182%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.814604/  1.913974, val:  69.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7556%\n",
      "layer   2  Sparsity: 79.5128%\n",
      "layer   3  Sparsity: 79.4126%\n",
      "total_backward_count 1213960 real_backward_count 123249  10.153%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.811191/  1.913274, val:  68.33%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7447%\n",
      "layer   2  Sparsity: 79.2761%\n",
      "layer   3  Sparsity: 79.2452%\n",
      "total_backward_count 1223750 real_backward_count 123866  10.122%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.813439/  1.908213, val:  66.67%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7609%\n",
      "layer   2  Sparsity: 79.2463%\n",
      "layer   3  Sparsity: 79.3269%\n",
      "total_backward_count 1233540 real_backward_count 124499  10.093%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.810343/  1.918635, val:  69.58%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7540%\n",
      "layer   2  Sparsity: 79.2546%\n",
      "layer   3  Sparsity: 79.4496%\n",
      "total_backward_count 1243330 real_backward_count 125114  10.063%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.812097/  1.906921, val:  65.83%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7512%\n",
      "layer   2  Sparsity: 79.2413%\n",
      "layer   3  Sparsity: 79.8527%\n",
      "total_backward_count 1253120 real_backward_count 125742  10.034%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.809495/  1.909181, val:  75.42%, val_best:  75.42%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7583%\n",
      "layer   2  Sparsity: 79.2323%\n",
      "layer   3  Sparsity: 79.7991%\n",
      "total_backward_count 1262910 real_backward_count 126347  10.004%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.808305/  1.912137, val:  73.33%, val_best:  75.42%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7593%\n",
      "layer   2  Sparsity: 79.3207%\n",
      "layer   3  Sparsity: 79.8620%\n",
      "total_backward_count 1272700 real_backward_count 126951   9.975%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.807145/  1.908794, val:  70.00%, val_best:  75.42%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7560%\n",
      "layer   2  Sparsity: 79.2218%\n",
      "layer   3  Sparsity: 79.7058%\n",
      "total_backward_count 1282490 real_backward_count 127538   9.945%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.806908/  1.905596, val:  69.58%, val_best:  75.42%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7610%\n",
      "layer   2  Sparsity: 79.1481%\n",
      "layer   3  Sparsity: 79.4361%\n",
      "total_backward_count 1292280 real_backward_count 128162   9.918%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.810855/  1.913808, val:  76.25%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7566%\n",
      "layer   2  Sparsity: 79.1317%\n",
      "layer   3  Sparsity: 79.6600%\n",
      "total_backward_count 1302070 real_backward_count 128769   9.890%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.812892/  1.914754, val:  70.42%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7570%\n",
      "layer   2  Sparsity: 79.1793%\n",
      "layer   3  Sparsity: 79.8880%\n",
      "total_backward_count 1311860 real_backward_count 129380   9.862%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.810447/  1.911381, val:  73.75%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7558%\n",
      "layer   2  Sparsity: 79.2798%\n",
      "layer   3  Sparsity: 79.6272%\n",
      "total_backward_count 1321650 real_backward_count 129989   9.835%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.807655/  1.908134, val:  69.58%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7576%\n",
      "layer   2  Sparsity: 79.3813%\n",
      "layer   3  Sparsity: 79.4979%\n",
      "total_backward_count 1331440 real_backward_count 130610   9.810%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.807896/  1.907294, val:  71.25%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7539%\n",
      "layer   2  Sparsity: 79.2158%\n",
      "layer   3  Sparsity: 79.3749%\n",
      "total_backward_count 1341230 real_backward_count 131240   9.785%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.803861/  1.908873, val:  72.92%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7529%\n",
      "layer   2  Sparsity: 79.1541%\n",
      "layer   3  Sparsity: 79.3257%\n",
      "total_backward_count 1351020 real_backward_count 131847   9.759%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.808260/  1.917935, val:  72.08%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7585%\n",
      "layer   2  Sparsity: 79.2876%\n",
      "layer   3  Sparsity: 79.3738%\n",
      "total_backward_count 1360810 real_backward_count 132459   9.734%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.809346/  1.914625, val:  70.83%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7615%\n",
      "layer   2  Sparsity: 79.3653%\n",
      "layer   3  Sparsity: 79.4652%\n",
      "total_backward_count 1370600 real_backward_count 133076   9.709%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.806496/  1.910120, val:  73.75%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7517%\n",
      "layer   2  Sparsity: 79.5148%\n",
      "layer   3  Sparsity: 79.5580%\n",
      "total_backward_count 1380390 real_backward_count 133664   9.683%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.807988/  1.912937, val:  73.33%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7524%\n",
      "layer   2  Sparsity: 79.4674%\n",
      "layer   3  Sparsity: 79.7428%\n",
      "total_backward_count 1390180 real_backward_count 134256   9.657%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.805830/  1.907020, val:  75.83%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7607%\n",
      "layer   2  Sparsity: 79.2463%\n",
      "layer   3  Sparsity: 79.5302%\n",
      "total_backward_count 1399970 real_backward_count 134860   9.633%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.806634/  1.908561, val:  72.50%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7557%\n",
      "layer   2  Sparsity: 79.3332%\n",
      "layer   3  Sparsity: 79.4929%\n",
      "total_backward_count 1409760 real_backward_count 135445   9.608%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.804737/  1.909601, val:  71.25%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7619%\n",
      "layer   2  Sparsity: 79.3139%\n",
      "layer   3  Sparsity: 79.4184%\n",
      "total_backward_count 1419550 real_backward_count 136045   9.584%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.804873/  1.909373, val:  73.75%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7522%\n",
      "layer   2  Sparsity: 79.3022%\n",
      "layer   3  Sparsity: 79.4244%\n",
      "total_backward_count 1429340 real_backward_count 136621   9.558%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.804858/  1.907096, val:  73.75%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7553%\n",
      "layer   2  Sparsity: 79.2144%\n",
      "layer   3  Sparsity: 79.5050%\n",
      "total_backward_count 1439130 real_backward_count 137222   9.535%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.805230/  1.909828, val:  69.58%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7613%\n",
      "layer   2  Sparsity: 79.1626%\n",
      "layer   3  Sparsity: 79.5998%\n",
      "total_backward_count 1448920 real_backward_count 137791   9.510%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.805411/  1.912248, val:  75.42%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7568%\n",
      "layer   2  Sparsity: 79.2108%\n",
      "layer   3  Sparsity: 79.6728%\n",
      "total_backward_count 1458710 real_backward_count 138384   9.487%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.806331/  1.916416, val:  69.58%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7503%\n",
      "layer   2  Sparsity: 79.1240%\n",
      "layer   3  Sparsity: 80.0949%\n",
      "total_backward_count 1468500 real_backward_count 138989   9.465%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.806723/  1.911007, val:  72.08%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7542%\n",
      "layer   2  Sparsity: 79.2002%\n",
      "layer   3  Sparsity: 80.2485%\n",
      "total_backward_count 1478290 real_backward_count 139593   9.443%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.807643/  1.918586, val:  70.42%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7523%\n",
      "layer   2  Sparsity: 79.1932%\n",
      "layer   3  Sparsity: 80.0141%\n",
      "total_backward_count 1488080 real_backward_count 140203   9.422%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.811150/  1.920707, val:  66.67%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7494%\n",
      "layer   2  Sparsity: 79.3869%\n",
      "layer   3  Sparsity: 79.9491%\n",
      "total_backward_count 1497870 real_backward_count 140790   9.399%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.805891/  1.911202, val:  74.17%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7512%\n",
      "layer   2  Sparsity: 79.3343%\n",
      "layer   3  Sparsity: 79.8725%\n",
      "total_backward_count 1507660 real_backward_count 141371   9.377%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.800756/  1.906395, val:  76.25%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7666%\n",
      "layer   2  Sparsity: 79.1498%\n",
      "layer   3  Sparsity: 79.7521%\n",
      "total_backward_count 1517450 real_backward_count 141975   9.356%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.800706/  1.911338, val:  69.58%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7642%\n",
      "layer   2  Sparsity: 79.0354%\n",
      "layer   3  Sparsity: 79.8724%\n",
      "total_backward_count 1527240 real_backward_count 142569   9.335%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.799989/  1.906719, val:  75.83%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7534%\n",
      "layer   2  Sparsity: 79.2090%\n",
      "layer   3  Sparsity: 79.9889%\n",
      "total_backward_count 1537030 real_backward_count 143135   9.312%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.799729/  1.908218, val:  70.00%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7583%\n",
      "layer   2  Sparsity: 79.0728%\n",
      "layer   3  Sparsity: 80.2900%\n",
      "total_backward_count 1546820 real_backward_count 143704   9.290%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.799246/  1.910990, val:  75.00%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7535%\n",
      "layer   2  Sparsity: 79.0494%\n",
      "layer   3  Sparsity: 80.5190%\n",
      "total_backward_count 1556610 real_backward_count 144254   9.267%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.801397/  1.908112, val:  76.25%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7523%\n",
      "layer   2  Sparsity: 79.0551%\n",
      "layer   3  Sparsity: 80.4167%\n",
      "total_backward_count 1566400 real_backward_count 144822   9.246%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.804920/  1.914261, val:  74.17%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7575%\n",
      "layer   2  Sparsity: 79.1779%\n",
      "layer   3  Sparsity: 80.4113%\n",
      "total_backward_count 1576190 real_backward_count 145395   9.224%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.806105/  1.911011, val:  73.33%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7527%\n",
      "layer   2  Sparsity: 79.2334%\n",
      "layer   3  Sparsity: 80.4406%\n",
      "total_backward_count 1585980 real_backward_count 145954   9.203%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.802893/  1.910189, val:  72.08%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7596%\n",
      "layer   2  Sparsity: 79.2797%\n",
      "layer   3  Sparsity: 80.4121%\n",
      "total_backward_count 1595770 real_backward_count 146530   9.182%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.803788/  1.906188, val:  70.83%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7464%\n",
      "layer   2  Sparsity: 79.3518%\n",
      "layer   3  Sparsity: 80.2560%\n",
      "total_backward_count 1605560 real_backward_count 147106   9.162%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.804811/  1.912204, val:  75.00%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7612%\n",
      "layer   2  Sparsity: 79.4854%\n",
      "layer   3  Sparsity: 80.7414%\n",
      "total_backward_count 1615350 real_backward_count 147668   9.142%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.805530/  1.915329, val:  69.58%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7503%\n",
      "layer   2  Sparsity: 79.4341%\n",
      "layer   3  Sparsity: 80.7507%\n",
      "total_backward_count 1625140 real_backward_count 148235   9.121%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.804819/  1.913110, val:  72.92%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7450%\n",
      "layer   2  Sparsity: 79.4030%\n",
      "layer   3  Sparsity: 80.6354%\n",
      "total_backward_count 1634930 real_backward_count 148786   9.100%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.803726/  1.917497, val:  70.00%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7530%\n",
      "layer   2  Sparsity: 79.4003%\n",
      "layer   3  Sparsity: 80.6471%\n",
      "total_backward_count 1644720 real_backward_count 149336   9.080%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.802043/  1.906989, val:  75.83%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7590%\n",
      "layer   2  Sparsity: 79.3705%\n",
      "layer   3  Sparsity: 80.6461%\n",
      "total_backward_count 1654510 real_backward_count 149889   9.059%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.799519/  1.907584, val:  76.25%, val_best:  76.25%, tr:  97.65%, tr_best:  97.75%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7466%\n",
      "layer   2  Sparsity: 79.2517%\n",
      "layer   3  Sparsity: 80.3696%\n",
      "total_backward_count 1664300 real_backward_count 150419   9.038%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.800346/  1.905969, val:  73.33%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7523%\n",
      "layer   2  Sparsity: 79.4130%\n",
      "layer   3  Sparsity: 80.2806%\n",
      "total_backward_count 1674090 real_backward_count 150976   9.018%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.798571/  1.906235, val:  76.25%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7552%\n",
      "layer   2  Sparsity: 79.5008%\n",
      "layer   3  Sparsity: 80.0961%\n",
      "total_backward_count 1683880 real_backward_count 151545   9.000%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.794671/  1.903916, val:  71.25%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7652%\n",
      "layer   2  Sparsity: 79.6643%\n",
      "layer   3  Sparsity: 79.8337%\n",
      "total_backward_count 1693670 real_backward_count 152103   8.981%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.793572/  1.906523, val:  73.33%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7631%\n",
      "layer   2  Sparsity: 79.5424%\n",
      "layer   3  Sparsity: 79.9568%\n",
      "total_backward_count 1703460 real_backward_count 152670   8.962%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.795162/  1.900987, val:  75.83%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7534%\n",
      "layer   2  Sparsity: 79.5725%\n",
      "layer   3  Sparsity: 79.6163%\n",
      "total_backward_count 1713250 real_backward_count 153261   8.946%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.793712/  1.899383, val:  70.83%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7543%\n",
      "layer   2  Sparsity: 79.7486%\n",
      "layer   3  Sparsity: 79.8425%\n",
      "total_backward_count 1723040 real_backward_count 153818   8.927%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.793947/  1.897865, val:  73.75%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7576%\n",
      "layer   2  Sparsity: 79.6986%\n",
      "layer   3  Sparsity: 80.0477%\n",
      "total_backward_count 1732830 real_backward_count 154372   8.909%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.791513/  1.901626, val:  75.00%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7514%\n",
      "layer   2  Sparsity: 79.7069%\n",
      "layer   3  Sparsity: 79.8179%\n",
      "total_backward_count 1742620 real_backward_count 154925   8.890%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.793267/  1.909678, val:  70.83%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7610%\n",
      "layer   2  Sparsity: 79.6374%\n",
      "layer   3  Sparsity: 80.0311%\n",
      "total_backward_count 1752410 real_backward_count 155481   8.872%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.794219/  1.905148, val:  71.67%, val_best:  76.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7504%\n",
      "layer   2  Sparsity: 79.6920%\n",
      "layer   3  Sparsity: 80.1322%\n",
      "total_backward_count 1762200 real_backward_count 156040   8.855%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.793851/  1.897534, val:  77.92%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7537%\n",
      "layer   2  Sparsity: 79.7562%\n",
      "layer   3  Sparsity: 80.0378%\n",
      "total_backward_count 1771990 real_backward_count 156603   8.838%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.795409/  1.908633, val:  75.83%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7559%\n",
      "layer   2  Sparsity: 79.6566%\n",
      "layer   3  Sparsity: 80.2320%\n",
      "total_backward_count 1781780 real_backward_count 157160   8.820%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.797342/  1.904535, val:  74.58%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7518%\n",
      "layer   2  Sparsity: 79.7201%\n",
      "layer   3  Sparsity: 80.1735%\n",
      "total_backward_count 1791570 real_backward_count 157706   8.803%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.795496/  1.895907, val:  74.58%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7506%\n",
      "layer   2  Sparsity: 79.6358%\n",
      "layer   3  Sparsity: 80.0022%\n",
      "total_backward_count 1801360 real_backward_count 158267   8.786%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.793797/  1.900125, val:  68.33%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7555%\n",
      "layer   2  Sparsity: 79.4689%\n",
      "layer   3  Sparsity: 79.8241%\n",
      "total_backward_count 1811150 real_backward_count 158805   8.768%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.791817/  1.899386, val:  71.67%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7596%\n",
      "layer   2  Sparsity: 79.4185%\n",
      "layer   3  Sparsity: 79.9078%\n",
      "total_backward_count 1820940 real_backward_count 159360   8.752%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.793383/  1.893080, val:  75.00%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7631%\n",
      "layer   2  Sparsity: 79.5147%\n",
      "layer   3  Sparsity: 79.7436%\n",
      "total_backward_count 1830730 real_backward_count 159938   8.736%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.791955/  1.901047, val:  75.42%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7543%\n",
      "layer   2  Sparsity: 79.5430%\n",
      "layer   3  Sparsity: 79.7799%\n",
      "total_backward_count 1840520 real_backward_count 160511   8.721%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.793707/  1.905957, val:  75.42%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7475%\n",
      "layer   2  Sparsity: 79.5242%\n",
      "layer   3  Sparsity: 80.3252%\n",
      "total_backward_count 1850310 real_backward_count 161073   8.705%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.791473/  1.902764, val:  74.17%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 74.7569%\n",
      "layer   2  Sparsity: 79.5540%\n",
      "layer   3  Sparsity: 80.3922%\n",
      "total_backward_count 1860100 real_backward_count 161631   8.689%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.790515/  1.909711, val:  72.50%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7549%\n",
      "layer   2  Sparsity: 79.5919%\n",
      "layer   3  Sparsity: 80.4595%\n",
      "total_backward_count 1869890 real_backward_count 162175   8.673%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.795016/  1.907543, val:  72.92%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7575%\n",
      "layer   2  Sparsity: 79.5550%\n",
      "layer   3  Sparsity: 80.6333%\n",
      "total_backward_count 1879680 real_backward_count 162709   8.656%\n",
      "lif layer 2 self.abs_max_v: 6277.5\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.794284/  1.908816, val:  73.75%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 74.7543%\n",
      "layer   2  Sparsity: 79.5593%\n",
      "layer   3  Sparsity: 80.6833%\n",
      "total_backward_count 1889470 real_backward_count 163246   8.640%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.797430/  1.909170, val:  66.67%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 74.7659%\n",
      "layer   2  Sparsity: 79.5215%\n",
      "layer   3  Sparsity: 80.6946%\n",
      "total_backward_count 1899260 real_backward_count 163819   8.625%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.794962/  1.902004, val:  74.17%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7593%\n",
      "layer   2  Sparsity: 79.4020%\n",
      "layer   3  Sparsity: 80.5882%\n",
      "total_backward_count 1909050 real_backward_count 164361   8.610%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.794618/  1.909252, val:  72.50%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7614%\n",
      "layer   2  Sparsity: 79.6303%\n",
      "layer   3  Sparsity: 80.3382%\n",
      "total_backward_count 1918840 real_backward_count 164902   8.594%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.796841/  1.906014, val:  75.42%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 74.7602%\n",
      "layer   2  Sparsity: 79.6642%\n",
      "layer   3  Sparsity: 80.4268%\n",
      "total_backward_count 1928630 real_backward_count 165441   8.578%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.794609/  1.907261, val:  71.67%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7534%\n",
      "layer   2  Sparsity: 79.6688%\n",
      "layer   3  Sparsity: 80.7519%\n",
      "total_backward_count 1938420 real_backward_count 165987   8.563%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.795131/  1.908768, val:  74.17%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 74.7574%\n",
      "layer   2  Sparsity: 79.6839%\n",
      "layer   3  Sparsity: 80.7100%\n",
      "total_backward_count 1948210 real_backward_count 166538   8.548%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.796212/  1.908717, val:  67.50%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 74.7610%\n",
      "layer   2  Sparsity: 79.6645%\n",
      "layer   3  Sparsity: 80.6216%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5377a3f0274027a204dd0b13499662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97753</td></tr><tr><td>tr_epoch_loss</td><td>1.79621</td></tr><tr><td>val_acc_best</td><td>0.77917</td></tr><tr><td>val_acc_now</td><td>0.675</td></tr><tr><td>val_loss</td><td>1.90872</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-5</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bn8e6mrn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bn8e6mrn</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_234309-bn8e6mrn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gcjgt957 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_040023-gcjgt957</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcjgt957' target=\"_blank\">bumbling-sweep-8</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcjgt957' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcjgt957</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251118_040033_070', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c247f45ff938aa370993ba27bace6d15\n",
      "cache path doesn't exist\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 424.0\n",
      "lif layer 1 self.abs_max_v: 424.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 375.0\n",
      "lif layer 2 self.abs_max_v: 375.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 1 self.abs_max_out: 434.0\n",
      "lif layer 1 self.abs_max_v: 508.0\n",
      "fc layer 2 self.abs_max_out: 534.0\n",
      "lif layer 2 self.abs_max_v: 701.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 1 self.abs_max_out: 558.0\n",
      "lif layer 1 self.abs_max_v: 636.5\n",
      "fc layer 1 self.abs_max_out: 626.0\n",
      "fc layer 2 self.abs_max_out: 668.0\n",
      "lif layer 2 self.abs_max_v: 763.5\n",
      "fc layer 1 self.abs_max_out: 674.0\n",
      "lif layer 1 self.abs_max_v: 674.0\n",
      "fc layer 2 self.abs_max_out: 759.0\n",
      "lif layer 2 self.abs_max_v: 1053.5\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 1 self.abs_max_out: 728.0\n",
      "lif layer 1 self.abs_max_v: 729.5\n",
      "lif layer 2 self.abs_max_v: 1194.0\n",
      "fc layer 1 self.abs_max_out: 857.0\n",
      "lif layer 1 self.abs_max_v: 857.0\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 2 self.abs_max_out: 874.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "lif layer 1 self.abs_max_v: 1005.0\n",
      "fc layer 1 self.abs_max_out: 1286.0\n",
      "lif layer 1 self.abs_max_v: 1286.0\n",
      "fc layer 1 self.abs_max_out: 1578.0\n",
      "lif layer 1 self.abs_max_v: 1578.0\n",
      "fc layer 2 self.abs_max_out: 970.0\n",
      "lif layer 2 self.abs_max_v: 1320.0\n",
      "fc layer 1 self.abs_max_out: 1738.0\n",
      "lif layer 1 self.abs_max_v: 1738.0\n",
      "lif layer 2 self.abs_max_v: 1463.0\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 1 self.abs_max_out: 1786.0\n",
      "lif layer 1 self.abs_max_v: 1786.0\n",
      "fc layer 2 self.abs_max_out: 1009.0\n",
      "fc layer 1 self.abs_max_out: 1934.0\n",
      "lif layer 1 self.abs_max_v: 1934.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 2 self.abs_max_out: 1042.0\n",
      "lif layer 2 self.abs_max_v: 1568.5\n",
      "lif layer 1 self.abs_max_v: 2419.0\n",
      "lif layer 1 self.abs_max_v: 2563.5\n",
      "fc layer 2 self.abs_max_out: 1050.0\n",
      "lif layer 2 self.abs_max_v: 1632.5\n",
      "fc layer 2 self.abs_max_out: 1171.0\n",
      "lif layer 2 self.abs_max_v: 1676.5\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "lif layer 2 self.abs_max_v: 1727.5\n",
      "fc layer 2 self.abs_max_out: 1493.0\n",
      "fc layer 1 self.abs_max_out: 2147.0\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "lif layer 2 self.abs_max_v: 1757.5\n",
      "lif layer 2 self.abs_max_v: 1789.0\n",
      "lif layer 2 self.abs_max_v: 1827.5\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "fc layer 1 self.abs_max_out: 2590.0\n",
      "lif layer 1 self.abs_max_v: 2902.0\n",
      "lif layer 2 self.abs_max_v: 1851.5\n",
      "lif layer 2 self.abs_max_v: 1975.5\n",
      "smallest_now_T updated: 345\n",
      "lif layer 2 self.abs_max_v: 2049.5\n",
      "lif layer 2 self.abs_max_v: 2138.5\n",
      "lif layer 2 self.abs_max_v: 2217.5\n",
      "lif layer 1 self.abs_max_v: 3048.5\n",
      "lif layer 1 self.abs_max_v: 3466.5\n",
      "lif layer 2 self.abs_max_v: 2276.5\n",
      "lif layer 1 self.abs_max_v: 3626.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "smallest_now_T updated: 317\n",
      "lif layer 1 self.abs_max_v: 3737.0\n",
      "lif layer 1 self.abs_max_v: 4073.5\n",
      "lif layer 1 self.abs_max_v: 4159.0\n",
      "fc layer 1 self.abs_max_out: 2801.0\n",
      "lif layer 1 self.abs_max_v: 4178.0\n",
      "fc layer 2 self.abs_max_out: 1604.0\n",
      "fc layer 1 self.abs_max_out: 2883.0\n",
      "lif layer 1 self.abs_max_v: 4206.0\n",
      "lif layer 2 self.abs_max_v: 2303.0\n",
      "fc layer 2 self.abs_max_out: 1710.0\n",
      "lif layer 2 self.abs_max_v: 2412.0\n",
      "lif layer 2 self.abs_max_v: 2449.0\n",
      "lif layer 2 self.abs_max_v: 2510.5\n",
      "fc layer 2 self.abs_max_out: 1822.0\n",
      "lif layer 2 self.abs_max_v: 2632.0\n",
      "fc layer 2 self.abs_max_out: 1976.0\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 2 self.abs_max_out: 2052.0\n",
      "lif layer 2 self.abs_max_v: 2642.0\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "lif layer 2 self.abs_max_v: 2669.5\n",
      "lif layer 2 self.abs_max_v: 2888.5\n",
      "lif layer 2 self.abs_max_v: 2980.5\n",
      "fc layer 2 self.abs_max_out: 2058.0\n",
      "lif layer 2 self.abs_max_v: 3020.5\n",
      "lif layer 2 self.abs_max_v: 3037.0\n",
      "lif layer 2 self.abs_max_v: 3072.5\n",
      "lif layer 2 self.abs_max_v: 3073.5\n",
      "fc layer 1 self.abs_max_out: 3137.0\n",
      "lif layer 1 self.abs_max_v: 4232.5\n",
      "fc layer 2 self.abs_max_out: 2083.0\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "lif layer 2 self.abs_max_v: 3127.0\n",
      "smallest_now_T updated: 286\n",
      "lif layer 2 self.abs_max_v: 3185.5\n",
      "lif layer 2 self.abs_max_v: 3261.5\n",
      "fc layer 2 self.abs_max_out: 2197.0\n",
      "lif layer 2 self.abs_max_v: 3326.5\n",
      "fc layer 2 self.abs_max_out: 2291.0\n",
      "lif layer 1 self.abs_max_v: 4253.5\n",
      "lif layer 1 self.abs_max_v: 4448.0\n",
      "lif layer 2 self.abs_max_v: 3433.0\n",
      "lif layer 2 self.abs_max_v: 3469.5\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "lif layer 2 self.abs_max_v: 3524.0\n",
      "lif layer 2 self.abs_max_v: 3657.5\n",
      "lif layer 2 self.abs_max_v: 3745.0\n",
      "fc layer 2 self.abs_max_out: 2299.0\n",
      "lif layer 1 self.abs_max_v: 4758.5\n",
      "lif layer 1 self.abs_max_v: 4769.0\n",
      "lif layer 1 self.abs_max_v: 4947.0\n",
      "lif layer 1 self.abs_max_v: 5442.5\n",
      "lif layer 1 self.abs_max_v: 5571.5\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "fc layer 2 self.abs_max_out: 2351.0\n",
      "fc layer 2 self.abs_max_out: 2425.0\n",
      "fc layer 1 self.abs_max_out: 3257.0\n",
      "fc layer 2 self.abs_max_out: 2474.0\n",
      "smallest_now_T updated: 247\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "smallest_now_T updated: 192\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "lif layer 2 self.abs_max_v: 3745.5\n",
      "fc layer 2 self.abs_max_out: 2535.0\n",
      "fc layer 3 self.abs_max_out: 819.0\n",
      "fc layer 3 self.abs_max_out: 831.0\n",
      "fc layer 3 self.abs_max_out: 878.0\n",
      "fc layer 2 self.abs_max_out: 2538.0\n",
      "fc layer 1 self.abs_max_out: 3516.0\n",
      "lif layer 1 self.abs_max_v: 5805.0\n",
      "lif layer 1 self.abs_max_v: 5992.0\n",
      "fc layer 2 self.abs_max_out: 2594.0\n",
      "lif layer 2 self.abs_max_v: 3769.5\n",
      "fc layer 2 self.abs_max_out: 2596.0\n",
      "lif layer 2 self.abs_max_v: 3903.5\n",
      "fc layer 2 self.abs_max_out: 2605.0\n",
      "fc layer 2 self.abs_max_out: 2829.0\n",
      "fc layer 2 self.abs_max_out: 2854.0\n",
      "fc layer 2 self.abs_max_out: 2974.0\n",
      "lif layer 2 self.abs_max_v: 4072.0\n",
      "fc layer 1 self.abs_max_out: 3552.0\n",
      "lif layer 1 self.abs_max_v: 6064.0\n",
      "lif layer 2 self.abs_max_v: 4090.5\n",
      "lif layer 2 self.abs_max_v: 4174.5\n",
      "lif layer 2 self.abs_max_v: 4262.0\n",
      "lif layer 2 self.abs_max_v: 4301.0\n",
      "fc layer 1 self.abs_max_out: 3561.0\n",
      "fc layer 1 self.abs_max_out: 3813.0\n",
      "fc layer 1 self.abs_max_out: 3880.0\n",
      "lif layer 2 self.abs_max_v: 4372.0\n",
      "lif layer 2 self.abs_max_v: 4401.5\n",
      "lif layer 2 self.abs_max_v: 4413.0\n",
      "lif layer 2 self.abs_max_v: 4520.5\n",
      "fc layer 1 self.abs_max_out: 4239.0\n",
      "lif layer 1 self.abs_max_v: 6265.5\n",
      "lif layer 1 self.abs_max_v: 6712.0\n",
      "lif layer 1 self.abs_max_v: 7097.0\n",
      "lif layer 1 self.abs_max_v: 7534.0\n",
      "fc layer 1 self.abs_max_out: 4244.0\n",
      "lif layer 1 self.abs_max_v: 8011.0\n",
      "lif layer 1 self.abs_max_v: 8098.5\n",
      "fc layer 1 self.abs_max_out: 4319.0\n",
      "lif layer 1 self.abs_max_v: 8291.0\n",
      "lif layer 2 self.abs_max_v: 4652.0\n",
      "fc layer 2 self.abs_max_out: 3051.0\n",
      "lif layer 2 self.abs_max_v: 4776.0\n",
      "lif layer 2 self.abs_max_v: 5019.0\n",
      "lif layer 2 self.abs_max_v: 5176.5\n",
      "fc layer 1 self.abs_max_out: 4373.0\n",
      "fc layer 1 self.abs_max_out: 4533.0\n",
      "lif layer 1 self.abs_max_v: 8351.0\n",
      "lif layer 1 self.abs_max_v: 8499.5\n",
      "fc layer 1 self.abs_max_out: 4562.0\n",
      "lif layer 1 self.abs_max_v: 8767.0\n",
      "lif layer 1 self.abs_max_v: 8789.5\n",
      "lif layer 2 self.abs_max_v: 5180.5\n",
      "fc layer 2 self.abs_max_out: 3076.0\n",
      "lif layer 2 self.abs_max_v: 5196.5\n",
      "lif layer 2 self.abs_max_v: 5256.0\n",
      "lif layer 2 self.abs_max_v: 5348.0\n",
      "fc layer 2 self.abs_max_out: 3081.0\n",
      "fc layer 1 self.abs_max_out: 4754.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "fc layer 2 self.abs_max_out: 3191.0\n",
      "smallest_now_T_val updated: 137\n",
      "lif layer 2 self.abs_max_v: 5374.5\n",
      "lif layer 2 self.abs_max_v: 5494.5\n",
      "fc layer 1 self.abs_max_out: 4763.0\n",
      "lif layer 2 self.abs_max_v: 5554.0\n",
      "fc layer 1 self.abs_max_out: 4893.0\n",
      "lif layer 1 self.abs_max_v: 8821.0\n",
      "lif layer 1 self.abs_max_v: 9147.5\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.717752/  1.949958, val:  31.67%, val_best:  31.67%, tr:  97.55%, tr_best:  97.55%, epoch time: 83.90 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 88.0199%\n",
      "layer   2  Sparsity: 70.0920%\n",
      "layer   3  Sparsity: 67.2181%\n",
      "total_backward_count 9790 real_backward_count 1935  19.765%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3204.0\n",
      "fc layer 2 self.abs_max_out: 3272.0\n",
      "fc layer 1 self.abs_max_out: 5218.0\n",
      "lif layer 1 self.abs_max_v: 9230.5\n",
      "fc layer 2 self.abs_max_out: 3287.0\n",
      "fc layer 2 self.abs_max_out: 3337.0\n",
      "lif layer 2 self.abs_max_v: 5559.5\n",
      "lif layer 2 self.abs_max_v: 5667.5\n",
      "lif layer 2 self.abs_max_v: 5670.0\n",
      "lif layer 2 self.abs_max_v: 5723.5\n",
      "lif layer 2 self.abs_max_v: 5831.5\n",
      "lif layer 2 self.abs_max_v: 5935.0\n",
      "lif layer 2 self.abs_max_v: 6269.5\n",
      "lif layer 2 self.abs_max_v: 6308.0\n",
      "fc layer 2 self.abs_max_out: 3483.0\n",
      "fc layer 2 self.abs_max_out: 3547.0\n",
      "lif layer 2 self.abs_max_v: 6491.5\n",
      "lif layer 2 self.abs_max_v: 6569.5\n",
      "fc layer 2 self.abs_max_out: 3565.0\n",
      "fc layer 3 self.abs_max_out: 897.0\n",
      "fc layer 2 self.abs_max_out: 3796.0\n",
      "lif layer 2 self.abs_max_v: 6666.0\n",
      "fc layer 1 self.abs_max_out: 5462.0\n",
      "lif layer 1 self.abs_max_v: 9306.5\n",
      "lif layer 1 self.abs_max_v: 9494.5\n",
      "lif layer 2 self.abs_max_v: 6805.0\n",
      "lif layer 2 self.abs_max_v: 7093.5\n",
      "fc layer 1 self.abs_max_out: 5996.0\n",
      "lif layer 1 self.abs_max_v: 10219.0\n",
      "lif layer 1 self.abs_max_v: 11032.5\n",
      "fc layer 1 self.abs_max_out: 6165.0\n",
      "lif layer 1 self.abs_max_v: 11461.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.655616/  1.930380, val:  41.67%, val_best:  41.67%, tr:  98.98%, tr_best:  98.98%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0070%\n",
      "layer   2  Sparsity: 72.5466%\n",
      "layer   3  Sparsity: 68.1944%\n",
      "total_backward_count 19580 real_backward_count 3524  17.998%\n",
      "fc layer 2 self.abs_max_out: 3855.0\n",
      "fc layer 2 self.abs_max_out: 3871.0\n",
      "fc layer 2 self.abs_max_out: 3985.0\n",
      "fc layer 1 self.abs_max_out: 6195.0\n",
      "lif layer 1 self.abs_max_v: 11705.5\n",
      "fc layer 1 self.abs_max_out: 6219.0\n",
      "lif layer 1 self.abs_max_v: 12072.0\n",
      "fc layer 1 self.abs_max_out: 6253.0\n",
      "fc layer 1 self.abs_max_out: 7009.0\n",
      "lif layer 1 self.abs_max_v: 12805.5\n",
      "lif layer 1 self.abs_max_v: 12899.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.643487/  1.878864, val:  39.58%, val_best:  41.67%, tr:  98.98%, tr_best:  98.98%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9883%\n",
      "layer   2  Sparsity: 74.1324%\n",
      "layer   3  Sparsity: 68.6124%\n",
      "total_backward_count 29370 real_backward_count 5050  17.194%\n",
      "fc layer 3 self.abs_max_out: 949.0\n",
      "fc layer 3 self.abs_max_out: 951.0\n",
      "fc layer 2 self.abs_max_out: 4029.0\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "fc layer 3 self.abs_max_out: 961.0\n",
      "fc layer 3 self.abs_max_out: 972.0\n",
      "fc layer 1 self.abs_max_out: 7055.0\n",
      "lif layer 1 self.abs_max_v: 13022.0\n",
      "fc layer 1 self.abs_max_out: 7236.0\n",
      "lif layer 1 self.abs_max_v: 13676.0\n",
      "lif layer 1 self.abs_max_v: 13713.0\n",
      "fc layer 2 self.abs_max_out: 4179.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.627308/  1.919265, val:  35.42%, val_best:  41.67%, tr:  99.18%, tr_best:  99.18%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9313%\n",
      "layer   2  Sparsity: 73.5973%\n",
      "layer   3  Sparsity: 69.3603%\n",
      "total_backward_count 39160 real_backward_count 6546  16.716%\n",
      "fc layer 1 self.abs_max_out: 7422.0\n",
      "lif layer 1 self.abs_max_v: 14079.0\n",
      "fc layer 1 self.abs_max_out: 8055.0\n",
      "fc layer 1 self.abs_max_out: 8327.0\n",
      "lif layer 1 self.abs_max_v: 15210.5\n",
      "fc layer 1 self.abs_max_out: 8792.0\n",
      "lif layer 1 self.abs_max_v: 16208.5\n",
      "lif layer 1 self.abs_max_v: 16529.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.614605/  1.931654, val:  45.00%, val_best:  45.00%, tr:  98.67%, tr_best:  99.18%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9548%\n",
      "layer   2  Sparsity: 73.1566%\n",
      "layer   3  Sparsity: 68.8436%\n",
      "total_backward_count 48950 real_backward_count 8028  16.400%\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "fc layer 3 self.abs_max_out: 1006.0\n",
      "fc layer 2 self.abs_max_out: 4222.0\n",
      "fc layer 2 self.abs_max_out: 4245.0\n",
      "fc layer 1 self.abs_max_out: 9118.0\n",
      "lif layer 1 self.abs_max_v: 16730.0\n",
      "fc layer 1 self.abs_max_out: 9592.0\n",
      "lif layer 1 self.abs_max_v: 17957.0\n",
      "lif layer 1 self.abs_max_v: 18053.5\n",
      "fc layer 2 self.abs_max_out: 4260.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.603872/  1.929149, val:  38.33%, val_best:  45.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.0316%\n",
      "layer   2  Sparsity: 73.4250%\n",
      "layer   3  Sparsity: 67.1963%\n",
      "total_backward_count 58740 real_backward_count 9432  16.057%\n",
      "fc layer 2 self.abs_max_out: 4285.0\n",
      "fc layer 3 self.abs_max_out: 1009.0\n",
      "fc layer 2 self.abs_max_out: 4290.0\n",
      "fc layer 1 self.abs_max_out: 9849.0\n",
      "lif layer 1 self.abs_max_v: 18292.5\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.568284/  1.875417, val:  51.25%, val_best:  51.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0157%\n",
      "layer   2  Sparsity: 73.6305%\n",
      "layer   3  Sparsity: 67.1601%\n",
      "total_backward_count 68530 real_backward_count 10787  15.741%\n",
      "fc layer 1 self.abs_max_out: 10139.0\n",
      "lif layer 1 self.abs_max_v: 18416.5\n",
      "lif layer 1 self.abs_max_v: 18642.5\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.577834/  1.849140, val:  47.50%, val_best:  51.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9521%\n",
      "layer   2  Sparsity: 73.8548%\n",
      "layer   3  Sparsity: 67.7100%\n",
      "total_backward_count 78320 real_backward_count 12149  15.512%\n",
      "fc layer 2 self.abs_max_out: 4322.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.558109/  1.852269, val:  47.92%, val_best:  51.25%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0002%\n",
      "layer   2  Sparsity: 72.8495%\n",
      "layer   3  Sparsity: 67.3034%\n",
      "total_backward_count 88110 real_backward_count 13558  15.388%\n",
      "lif layer 2 self.abs_max_v: 7264.5\n",
      "lif layer 2 self.abs_max_v: 7404.5\n",
      "fc layer 3 self.abs_max_out: 1010.0\n",
      "fc layer 1 self.abs_max_out: 10277.0\n",
      "lif layer 1 self.abs_max_v: 18898.0\n",
      "lif layer 1 self.abs_max_v: 19043.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.552018/  1.896558, val:  36.67%, val_best:  51.25%, tr:  98.98%, tr_best:  99.59%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.9632%\n",
      "layer   2  Sparsity: 72.9874%\n",
      "layer   3  Sparsity: 67.7345%\n",
      "total_backward_count 97900 real_backward_count 14912  15.232%\n",
      "fc layer 3 self.abs_max_out: 1031.0\n",
      "fc layer 3 self.abs_max_out: 1060.0\n",
      "fc layer 2 self.abs_max_out: 4543.0\n",
      "fc layer 2 self.abs_max_out: 4675.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.509821/  1.841503, val:  41.25%, val_best:  51.25%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9491%\n",
      "layer   2  Sparsity: 72.9432%\n",
      "layer   3  Sparsity: 66.6758%\n",
      "total_backward_count 107690 real_backward_count 16243  15.083%\n",
      "lif layer 2 self.abs_max_v: 7655.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.528994/  1.823735, val:  47.50%, val_best:  51.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0020%\n",
      "layer   2  Sparsity: 72.5269%\n",
      "layer   3  Sparsity: 66.1082%\n",
      "total_backward_count 117480 real_backward_count 17541  14.931%\n",
      "fc layer 3 self.abs_max_out: 1079.0\n",
      "fc layer 3 self.abs_max_out: 1124.0\n",
      "fc layer 1 self.abs_max_out: 11087.0\n",
      "lif layer 1 self.abs_max_v: 20408.0\n",
      "lif layer 1 self.abs_max_v: 20599.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.495798/  1.799242, val:  46.67%, val_best:  51.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0095%\n",
      "layer   2  Sparsity: 72.9518%\n",
      "layer   3  Sparsity: 65.9265%\n",
      "total_backward_count 127270 real_backward_count 18782  14.758%\n",
      "fc layer 3 self.abs_max_out: 1173.0\n",
      "lif layer 2 self.abs_max_v: 7729.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.485925/  1.844451, val:  32.92%, val_best:  51.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9786%\n",
      "layer   2  Sparsity: 72.2908%\n",
      "layer   3  Sparsity: 66.3202%\n",
      "total_backward_count 137060 real_backward_count 20083  14.653%\n",
      "lif layer 2 self.abs_max_v: 7937.5\n",
      "lif layer 2 self.abs_max_v: 7959.0\n",
      "lif layer 2 self.abs_max_v: 8081.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.491788/  1.832231, val:  45.83%, val_best:  51.25%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9928%\n",
      "layer   2  Sparsity: 72.0800%\n",
      "layer   3  Sparsity: 66.5055%\n",
      "total_backward_count 146850 real_backward_count 21339  14.531%\n",
      "lif layer 2 self.abs_max_v: 8108.5\n",
      "lif layer 2 self.abs_max_v: 8158.5\n",
      "lif layer 2 self.abs_max_v: 8495.5\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.501964/  1.843415, val:  41.25%, val_best:  51.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0063%\n",
      "layer   2  Sparsity: 72.7643%\n",
      "layer   3  Sparsity: 65.7859%\n",
      "total_backward_count 156640 real_backward_count 22599  14.427%\n",
      "fc layer 2 self.abs_max_out: 4786.0\n",
      "lif layer 2 self.abs_max_v: 8825.0\n",
      "fc layer 1 self.abs_max_out: 11144.0\n",
      "lif layer 1 self.abs_max_v: 20836.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.508106/  1.862558, val:  45.00%, val_best:  51.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9415%\n",
      "layer   2  Sparsity: 72.9580%\n",
      "layer   3  Sparsity: 65.9865%\n",
      "total_backward_count 166430 real_backward_count 23807  14.305%\n",
      "fc layer 1 self.abs_max_out: 11215.0\n",
      "lif layer 1 self.abs_max_v: 20873.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.519676/  1.811183, val:  50.42%, val_best:  51.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9185%\n",
      "layer   2  Sparsity: 72.2516%\n",
      "layer   3  Sparsity: 66.3160%\n",
      "total_backward_count 176220 real_backward_count 25066  14.224%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.496867/  1.758517, val:  55.42%, val_best:  55.42%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0022%\n",
      "layer   2  Sparsity: 72.4752%\n",
      "layer   3  Sparsity: 65.6445%\n",
      "total_backward_count 186010 real_backward_count 26386  14.185%\n",
      "fc layer 1 self.abs_max_out: 11701.0\n",
      "lif layer 1 self.abs_max_v: 21604.5\n",
      "lif layer 1 self.abs_max_v: 21737.5\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.456623/  1.832487, val:  36.67%, val_best:  55.42%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9206%\n",
      "layer   2  Sparsity: 72.9568%\n",
      "layer   3  Sparsity: 65.2947%\n",
      "total_backward_count 195800 real_backward_count 27593  14.092%\n",
      "fc layer 2 self.abs_max_out: 5049.0\n",
      "fc layer 2 self.abs_max_out: 5061.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.449600/  1.812100, val:  38.33%, val_best:  55.42%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9738%\n",
      "layer   2  Sparsity: 72.0285%\n",
      "layer   3  Sparsity: 65.5242%\n",
      "total_backward_count 205590 real_backward_count 28818  14.017%\n",
      "fc layer 2 self.abs_max_out: 5160.0\n",
      "lif layer 2 self.abs_max_v: 9333.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.457891/  1.806028, val:  33.75%, val_best:  55.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9927%\n",
      "layer   2  Sparsity: 72.0436%\n",
      "layer   3  Sparsity: 66.3303%\n",
      "total_backward_count 215380 real_backward_count 30121  13.985%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.450051/  1.750120, val:  53.75%, val_best:  55.42%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9115%\n",
      "layer   2  Sparsity: 71.8254%\n",
      "layer   3  Sparsity: 65.1958%\n",
      "total_backward_count 225170 real_backward_count 31338  13.917%\n",
      "fc layer 3 self.abs_max_out: 1253.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.438306/  1.751709, val:  52.50%, val_best:  55.42%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.0157%\n",
      "layer   2  Sparsity: 71.5320%\n",
      "layer   3  Sparsity: 66.1429%\n",
      "total_backward_count 234960 real_backward_count 32560  13.858%\n",
      "fc layer 1 self.abs_max_out: 11755.0\n",
      "lif layer 1 self.abs_max_v: 21757.5\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.446338/  1.754667, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0182%\n",
      "layer   2  Sparsity: 71.7425%\n",
      "layer   3  Sparsity: 67.0579%\n",
      "total_backward_count 244750 real_backward_count 33799  13.810%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.473193/  1.753500, val:  60.00%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9429%\n",
      "layer   2  Sparsity: 72.4068%\n",
      "layer   3  Sparsity: 66.7607%\n",
      "total_backward_count 254540 real_backward_count 35166  13.816%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.441948/  1.746557, val:  55.00%, val_best:  60.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9579%\n",
      "layer   2  Sparsity: 71.8568%\n",
      "layer   3  Sparsity: 66.1807%\n",
      "total_backward_count 264330 real_backward_count 36403  13.772%\n",
      "fc layer 3 self.abs_max_out: 1262.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.425848/  1.739902, val:  54.17%, val_best:  60.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0124%\n",
      "layer   2  Sparsity: 71.2788%\n",
      "layer   3  Sparsity: 64.8316%\n",
      "total_backward_count 274120 real_backward_count 37636  13.730%\n",
      "fc layer 1 self.abs_max_out: 12467.0\n",
      "fc layer 1 self.abs_max_out: 12536.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.397098/  1.750954, val:  49.58%, val_best:  60.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0037%\n",
      "layer   2  Sparsity: 71.2324%\n",
      "layer   3  Sparsity: 63.9270%\n",
      "total_backward_count 283910 real_backward_count 38815  13.672%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.399973/  1.721862, val:  52.50%, val_best:  60.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9938%\n",
      "layer   2  Sparsity: 70.5731%\n",
      "layer   3  Sparsity: 63.7444%\n",
      "total_backward_count 293700 real_backward_count 40007  13.622%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.422503/  1.719965, val:  57.50%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9532%\n",
      "layer   2  Sparsity: 70.0147%\n",
      "layer   3  Sparsity: 65.5861%\n",
      "total_backward_count 303490 real_backward_count 41222  13.583%\n",
      "lif layer 1 self.abs_max_v: 21800.5\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.434714/  1.738617, val:  54.17%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9541%\n",
      "layer   2  Sparsity: 70.8441%\n",
      "layer   3  Sparsity: 65.3982%\n",
      "total_backward_count 313280 real_backward_count 42435  13.545%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.432423/  1.775131, val:  45.83%, val_best:  60.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9848%\n",
      "layer   2  Sparsity: 70.5907%\n",
      "layer   3  Sparsity: 63.6899%\n",
      "total_backward_count 323070 real_backward_count 43661  13.514%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.416656/  1.733997, val:  52.92%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9731%\n",
      "layer   2  Sparsity: 69.8724%\n",
      "layer   3  Sparsity: 63.8634%\n",
      "total_backward_count 332860 real_backward_count 44828  13.468%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.396510/  1.704303, val:  53.33%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9399%\n",
      "layer   2  Sparsity: 69.9797%\n",
      "layer   3  Sparsity: 65.4521%\n",
      "total_backward_count 342650 real_backward_count 46006  13.427%\n",
      "fc layer 1 self.abs_max_out: 12576.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.390993/  1.742932, val:  55.83%, val_best:  60.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9398%\n",
      "layer   2  Sparsity: 69.8589%\n",
      "layer   3  Sparsity: 64.2752%\n",
      "total_backward_count 352440 real_backward_count 47173  13.385%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.396563/  1.696187, val:  57.50%, val_best:  60.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9363%\n",
      "layer   2  Sparsity: 70.1421%\n",
      "layer   3  Sparsity: 65.3390%\n",
      "total_backward_count 362230 real_backward_count 48367  13.353%\n",
      "fc layer 2 self.abs_max_out: 5251.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.386715/  1.740827, val:  39.17%, val_best:  60.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0141%\n",
      "layer   2  Sparsity: 69.9972%\n",
      "layer   3  Sparsity: 64.4623%\n",
      "total_backward_count 372020 real_backward_count 49498  13.305%\n",
      "fc layer 3 self.abs_max_out: 1270.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.414327/  1.742668, val:  57.50%, val_best:  60.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9995%\n",
      "layer   2  Sparsity: 70.3698%\n",
      "layer   3  Sparsity: 66.1654%\n",
      "total_backward_count 381810 real_backward_count 50649  13.265%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.434382/  1.764735, val:  44.58%, val_best:  60.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0465%\n",
      "layer   2  Sparsity: 69.8016%\n",
      "layer   3  Sparsity: 65.6385%\n",
      "total_backward_count 391600 real_backward_count 51810  13.230%\n",
      "fc layer 2 self.abs_max_out: 5405.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.412050/  1.707903, val:  60.00%, val_best:  60.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9271%\n",
      "layer   2  Sparsity: 69.4729%\n",
      "layer   3  Sparsity: 65.5018%\n",
      "total_backward_count 401390 real_backward_count 52961  13.194%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.385515/  1.703874, val:  55.83%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9799%\n",
      "layer   2  Sparsity: 69.8504%\n",
      "layer   3  Sparsity: 64.9185%\n",
      "total_backward_count 411180 real_backward_count 54106  13.159%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.407430/  1.746544, val:  51.25%, val_best:  60.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9218%\n",
      "layer   2  Sparsity: 69.9879%\n",
      "layer   3  Sparsity: 65.7562%\n",
      "total_backward_count 420970 real_backward_count 55226  13.119%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.399754/  1.710096, val:  56.25%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9674%\n",
      "layer   2  Sparsity: 69.7546%\n",
      "layer   3  Sparsity: 64.4230%\n",
      "total_backward_count 430760 real_backward_count 56420  13.098%\n",
      "lif layer 1 self.abs_max_v: 21807.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.421868/  1.703521, val:  62.50%, val_best:  62.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0408%\n",
      "layer   2  Sparsity: 70.0120%\n",
      "layer   3  Sparsity: 65.9734%\n",
      "total_backward_count 440550 real_backward_count 57587  13.072%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.398444/  1.724137, val:  48.33%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9748%\n",
      "layer   2  Sparsity: 69.6937%\n",
      "layer   3  Sparsity: 66.2698%\n",
      "total_backward_count 450340 real_backward_count 58746  13.045%\n",
      "fc layer 1 self.abs_max_out: 13288.0\n",
      "lif layer 1 self.abs_max_v: 23592.5\n",
      "lif layer 1 self.abs_max_v: 24933.5\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.413016/  1.747041, val:  48.33%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9213%\n",
      "layer   2  Sparsity: 69.5847%\n",
      "layer   3  Sparsity: 66.5587%\n",
      "total_backward_count 460130 real_backward_count 59918  13.022%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.410768/  1.716859, val:  59.58%, val_best:  62.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0274%\n",
      "layer   2  Sparsity: 69.2667%\n",
      "layer   3  Sparsity: 67.1059%\n",
      "total_backward_count 469920 real_backward_count 61070  12.996%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.403570/  1.719473, val:  48.75%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0562%\n",
      "layer   2  Sparsity: 69.3547%\n",
      "layer   3  Sparsity: 66.0482%\n",
      "total_backward_count 479710 real_backward_count 62182  12.962%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.411859/  1.684035, val:  58.33%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9657%\n",
      "layer   2  Sparsity: 69.5136%\n",
      "layer   3  Sparsity: 67.2639%\n",
      "total_backward_count 489500 real_backward_count 63408  12.954%\n",
      "fc layer 3 self.abs_max_out: 1279.0\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.410866/  1.717307, val:  51.25%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.0247%\n",
      "layer   2  Sparsity: 68.9944%\n",
      "layer   3  Sparsity: 67.7972%\n",
      "total_backward_count 499290 real_backward_count 64559  12.930%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.382647/  1.645238, val:  63.33%, val_best:  63.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9704%\n",
      "layer   2  Sparsity: 69.0408%\n",
      "layer   3  Sparsity: 68.0790%\n",
      "total_backward_count 509080 real_backward_count 65734  12.912%\n",
      "fc layer 3 self.abs_max_out: 1358.0\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.370331/  1.670278, val:  55.83%, val_best:  63.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.0014%\n",
      "layer   2  Sparsity: 69.3187%\n",
      "layer   3  Sparsity: 66.2721%\n",
      "total_backward_count 518870 real_backward_count 66880  12.890%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.385468/  1.671390, val:  48.75%, val_best:  63.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.0161%\n",
      "layer   2  Sparsity: 69.1301%\n",
      "layer   3  Sparsity: 66.8988%\n",
      "total_backward_count 528660 real_backward_count 68012  12.865%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.366756/  1.689413, val:  55.00%, val_best:  63.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9062%\n",
      "layer   2  Sparsity: 67.9906%\n",
      "layer   3  Sparsity: 65.7255%\n",
      "total_backward_count 538450 real_backward_count 69174  12.847%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.378487/  1.740125, val:  50.00%, val_best:  63.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9608%\n",
      "layer   2  Sparsity: 69.0678%\n",
      "layer   3  Sparsity: 66.3417%\n",
      "total_backward_count 548240 real_backward_count 70310  12.825%\n",
      "fc layer 1 self.abs_max_out: 14078.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.394928/  1.736759, val:  48.33%, val_best:  63.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9477%\n",
      "layer   2  Sparsity: 69.0824%\n",
      "layer   3  Sparsity: 67.0308%\n",
      "total_backward_count 558030 real_backward_count 71493  12.812%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.377601/  1.677098, val:  55.00%, val_best:  63.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0449%\n",
      "layer   2  Sparsity: 69.3020%\n",
      "layer   3  Sparsity: 66.0440%\n",
      "total_backward_count 567820 real_backward_count 72601  12.786%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.357133/  1.700399, val:  56.25%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9333%\n",
      "layer   2  Sparsity: 69.1253%\n",
      "layer   3  Sparsity: 65.1591%\n",
      "total_backward_count 577610 real_backward_count 73685  12.757%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.360630/  1.713232, val:  55.00%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0134%\n",
      "layer   2  Sparsity: 69.0863%\n",
      "layer   3  Sparsity: 65.5686%\n",
      "total_backward_count 587400 real_backward_count 74797  12.734%\n",
      "fc layer 3 self.abs_max_out: 1390.0\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.353160/  1.718052, val:  41.67%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0018%\n",
      "layer   2  Sparsity: 69.0555%\n",
      "layer   3  Sparsity: 65.9454%\n",
      "total_backward_count 597190 real_backward_count 75947  12.717%\n",
      "fc layer 3 self.abs_max_out: 1397.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.347375/  1.679787, val:  57.92%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9356%\n",
      "layer   2  Sparsity: 68.9223%\n",
      "layer   3  Sparsity: 66.7730%\n",
      "total_backward_count 606980 real_backward_count 77084  12.700%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.340308/  1.716804, val:  47.50%, val_best:  63.33%, tr:  99.08%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9760%\n",
      "layer   2  Sparsity: 69.3912%\n",
      "layer   3  Sparsity: 67.3064%\n",
      "total_backward_count 616770 real_backward_count 78168  12.674%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.363168/  1.693684, val:  55.00%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0348%\n",
      "layer   2  Sparsity: 68.6210%\n",
      "layer   3  Sparsity: 66.4842%\n",
      "total_backward_count 626560 real_backward_count 79280  12.653%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.384742/  1.676380, val:  52.92%, val_best:  63.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9473%\n",
      "layer   2  Sparsity: 68.9602%\n",
      "layer   3  Sparsity: 66.2397%\n",
      "total_backward_count 636350 real_backward_count 80430  12.639%\n",
      "fc layer 3 self.abs_max_out: 1410.0\n",
      "fc layer 3 self.abs_max_out: 1460.0\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.354604/  1.682805, val:  55.83%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9974%\n",
      "layer   2  Sparsity: 69.2414%\n",
      "layer   3  Sparsity: 66.1566%\n",
      "total_backward_count 646140 real_backward_count 81517  12.616%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.334642/  1.663433, val:  47.50%, val_best:  63.33%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8926%\n",
      "layer   2  Sparsity: 68.7087%\n",
      "layer   3  Sparsity: 64.6102%\n",
      "total_backward_count 655930 real_backward_count 82664  12.603%\n",
      "lif layer 1 self.abs_max_v: 25462.5\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.338763/  1.663697, val:  56.25%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9406%\n",
      "layer   2  Sparsity: 69.0158%\n",
      "layer   3  Sparsity: 65.3664%\n",
      "total_backward_count 665720 real_backward_count 83745  12.580%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.348918/  1.652937, val:  60.83%, val_best:  63.33%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0019%\n",
      "layer   2  Sparsity: 68.5654%\n",
      "layer   3  Sparsity: 65.0837%\n",
      "total_backward_count 675510 real_backward_count 84895  12.568%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.341144/  1.703575, val:  55.83%, val_best:  63.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9172%\n",
      "layer   2  Sparsity: 68.4244%\n",
      "layer   3  Sparsity: 65.9837%\n",
      "total_backward_count 685300 real_backward_count 85999  12.549%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.350751/  1.659063, val:  58.75%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0129%\n",
      "layer   2  Sparsity: 68.5707%\n",
      "layer   3  Sparsity: 66.6543%\n",
      "total_backward_count 695090 real_backward_count 87091  12.529%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.339765/  1.727188, val:  55.00%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9565%\n",
      "layer   2  Sparsity: 68.5783%\n",
      "layer   3  Sparsity: 65.6471%\n",
      "total_backward_count 704880 real_backward_count 88165  12.508%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.340805/  1.658081, val:  65.42%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0002%\n",
      "layer   2  Sparsity: 68.3162%\n",
      "layer   3  Sparsity: 65.6559%\n",
      "total_backward_count 714670 real_backward_count 89270  12.491%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.358440/  1.682340, val:  54.58%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0338%\n",
      "layer   2  Sparsity: 68.2606%\n",
      "layer   3  Sparsity: 67.3185%\n",
      "total_backward_count 724460 real_backward_count 90385  12.476%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.362724/  1.739559, val:  47.50%, val_best:  65.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9783%\n",
      "layer   2  Sparsity: 68.2159%\n",
      "layer   3  Sparsity: 67.8770%\n",
      "total_backward_count 734250 real_backward_count 91514  12.464%\n",
      "fc layer 3 self.abs_max_out: 1471.0\n",
      "fc layer 3 self.abs_max_out: 1518.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.355671/  1.619818, val:  64.17%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9601%\n",
      "layer   2  Sparsity: 68.4674%\n",
      "layer   3  Sparsity: 65.3172%\n",
      "total_backward_count 744040 real_backward_count 92602  12.446%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.342122/  1.720473, val:  55.00%, val_best:  65.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9728%\n",
      "layer   2  Sparsity: 69.2071%\n",
      "layer   3  Sparsity: 66.1092%\n",
      "total_backward_count 753830 real_backward_count 93699  12.430%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.366434/  1.648504, val:  52.08%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9745%\n",
      "layer   2  Sparsity: 69.1511%\n",
      "layer   3  Sparsity: 66.8773%\n",
      "total_backward_count 763620 real_backward_count 94788  12.413%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.337014/  1.703507, val:  51.25%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9518%\n",
      "layer   2  Sparsity: 68.6947%\n",
      "layer   3  Sparsity: 66.5154%\n",
      "total_backward_count 773410 real_backward_count 95871  12.396%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.356164/  1.666354, val:  60.42%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9311%\n",
      "layer   2  Sparsity: 68.9248%\n",
      "layer   3  Sparsity: 66.3738%\n",
      "total_backward_count 783200 real_backward_count 96935  12.377%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.332955/  1.623950, val:  58.33%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9855%\n",
      "layer   2  Sparsity: 68.9365%\n",
      "layer   3  Sparsity: 64.8200%\n",
      "total_backward_count 792990 real_backward_count 98017  12.360%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.320127/  1.728780, val:  42.08%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9486%\n",
      "layer   2  Sparsity: 68.5246%\n",
      "layer   3  Sparsity: 65.6285%\n",
      "total_backward_count 802780 real_backward_count 99133  12.349%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.337519/  1.640235, val:  57.50%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9090%\n",
      "layer   2  Sparsity: 68.0482%\n",
      "layer   3  Sparsity: 65.9904%\n",
      "total_backward_count 812570 real_backward_count 100190  12.330%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.323693/  1.731342, val:  45.83%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9579%\n",
      "layer   2  Sparsity: 67.9043%\n",
      "layer   3  Sparsity: 66.2041%\n",
      "total_backward_count 822360 real_backward_count 101257  12.313%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.323660/  1.641759, val:  58.75%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9816%\n",
      "layer   2  Sparsity: 68.2026%\n",
      "layer   3  Sparsity: 66.4108%\n",
      "total_backward_count 832150 real_backward_count 102336  12.298%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.317704/  1.713731, val:  52.50%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9774%\n",
      "layer   2  Sparsity: 68.1775%\n",
      "layer   3  Sparsity: 66.6323%\n",
      "total_backward_count 841940 real_backward_count 103432  12.285%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.332203/  1.664469, val:  52.50%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0190%\n",
      "layer   2  Sparsity: 68.3139%\n",
      "layer   3  Sparsity: 66.0291%\n",
      "total_backward_count 851730 real_backward_count 104535  12.273%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.334174/  1.643601, val:  62.08%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9752%\n",
      "layer   2  Sparsity: 68.5879%\n",
      "layer   3  Sparsity: 66.4715%\n",
      "total_backward_count 861520 real_backward_count 105654  12.264%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.306330/  1.671591, val:  55.42%, val_best:  65.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9420%\n",
      "layer   2  Sparsity: 67.7101%\n",
      "layer   3  Sparsity: 64.5117%\n",
      "total_backward_count 871310 real_backward_count 106742  12.251%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.329735/  1.630377, val:  59.58%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9826%\n",
      "layer   2  Sparsity: 68.3743%\n",
      "layer   3  Sparsity: 65.7809%\n",
      "total_backward_count 881100 real_backward_count 107813  12.236%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.323841/  1.721493, val:  53.33%, val_best:  65.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9155%\n",
      "layer   2  Sparsity: 68.6153%\n",
      "layer   3  Sparsity: 66.6490%\n",
      "total_backward_count 890890 real_backward_count 108906  12.224%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.332245/  1.640629, val:  55.83%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9636%\n",
      "layer   2  Sparsity: 68.3713%\n",
      "layer   3  Sparsity: 65.7276%\n",
      "total_backward_count 900680 real_backward_count 109951  12.208%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.358949/  1.652294, val:  59.17%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9937%\n",
      "layer   2  Sparsity: 68.6603%\n",
      "layer   3  Sparsity: 66.3459%\n",
      "total_backward_count 910470 real_backward_count 111064  12.199%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.330805/  1.671068, val:  51.67%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9952%\n",
      "layer   2  Sparsity: 67.6414%\n",
      "layer   3  Sparsity: 65.9324%\n",
      "total_backward_count 920260 real_backward_count 112190  12.191%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.297506/  1.618054, val:  57.50%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9150%\n",
      "layer   2  Sparsity: 67.7246%\n",
      "layer   3  Sparsity: 65.5048%\n",
      "total_backward_count 930050 real_backward_count 113273  12.179%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.280465/  1.607014, val:  59.58%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9441%\n",
      "layer   2  Sparsity: 68.3686%\n",
      "layer   3  Sparsity: 65.5162%\n",
      "total_backward_count 939840 real_backward_count 114312  12.163%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.286111/  1.622195, val:  55.42%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8956%\n",
      "layer   2  Sparsity: 68.1839%\n",
      "layer   3  Sparsity: 65.7250%\n",
      "total_backward_count 949630 real_backward_count 115403  12.152%\n",
      "fc layer 3 self.abs_max_out: 1533.0\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.273716/  1.605521, val:  58.75%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9633%\n",
      "layer   2  Sparsity: 68.0719%\n",
      "layer   3  Sparsity: 65.1796%\n",
      "total_backward_count 959420 real_backward_count 116465  12.139%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.283642/  1.680141, val:  51.67%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9630%\n",
      "layer   2  Sparsity: 68.1452%\n",
      "layer   3  Sparsity: 64.1496%\n",
      "total_backward_count 969210 real_backward_count 117564  12.130%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.288331/  1.604318, val:  59.58%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0007%\n",
      "layer   2  Sparsity: 68.1295%\n",
      "layer   3  Sparsity: 64.4671%\n",
      "total_backward_count 979000 real_backward_count 118634  12.118%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.262728/  1.641387, val:  57.08%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0116%\n",
      "layer   2  Sparsity: 67.7691%\n",
      "layer   3  Sparsity: 64.0441%\n",
      "total_backward_count 988790 real_backward_count 119639  12.100%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.273945/  1.571415, val:  63.75%, val_best:  65.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9378%\n",
      "layer   2  Sparsity: 67.5073%\n",
      "layer   3  Sparsity: 64.8945%\n",
      "total_backward_count 998580 real_backward_count 120728  12.090%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.288659/  1.640334, val:  54.17%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9949%\n",
      "layer   2  Sparsity: 68.4383%\n",
      "layer   3  Sparsity: 65.5213%\n",
      "total_backward_count 1008370 real_backward_count 121751  12.074%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.316181/  1.665063, val:  52.50%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9767%\n",
      "layer   2  Sparsity: 68.1364%\n",
      "layer   3  Sparsity: 65.3609%\n",
      "total_backward_count 1018160 real_backward_count 122817  12.063%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.305248/  1.602552, val:  67.08%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9195%\n",
      "layer   2  Sparsity: 68.1399%\n",
      "layer   3  Sparsity: 65.5174%\n",
      "total_backward_count 1027950 real_backward_count 123896  12.053%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.305133/  1.629857, val:  56.67%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9588%\n",
      "layer   2  Sparsity: 67.7783%\n",
      "layer   3  Sparsity: 64.9366%\n",
      "total_backward_count 1037740 real_backward_count 124960  12.042%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.279265/  1.699897, val:  46.25%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0057%\n",
      "layer   2  Sparsity: 67.3348%\n",
      "layer   3  Sparsity: 64.5362%\n",
      "total_backward_count 1047530 real_backward_count 126035  12.032%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.333421/  1.643915, val:  56.67%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9187%\n",
      "layer   2  Sparsity: 66.7810%\n",
      "layer   3  Sparsity: 64.9167%\n",
      "total_backward_count 1057320 real_backward_count 127108  12.022%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.267120/  1.627134, val:  67.08%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9184%\n",
      "layer   2  Sparsity: 67.4909%\n",
      "layer   3  Sparsity: 64.5244%\n",
      "total_backward_count 1067110 real_backward_count 128161  12.010%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.284627/  1.624297, val:  54.17%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0249%\n",
      "layer   2  Sparsity: 67.7527%\n",
      "layer   3  Sparsity: 64.2604%\n",
      "total_backward_count 1076900 real_backward_count 129203  11.998%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.302853/  1.618704, val:  65.42%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9753%\n",
      "layer   2  Sparsity: 67.4680%\n",
      "layer   3  Sparsity: 66.0205%\n",
      "total_backward_count 1086690 real_backward_count 130252  11.986%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.309283/  1.607057, val:  63.75%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9634%\n",
      "layer   2  Sparsity: 67.4715%\n",
      "layer   3  Sparsity: 66.3063%\n",
      "total_backward_count 1096480 real_backward_count 131300  11.975%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.293046/  1.630923, val:  61.67%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.0303%\n",
      "layer   2  Sparsity: 66.9439%\n",
      "layer   3  Sparsity: 65.9731%\n",
      "total_backward_count 1106270 real_backward_count 132431  11.971%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.311303/  1.642447, val:  59.58%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9681%\n",
      "layer   2  Sparsity: 66.8438%\n",
      "layer   3  Sparsity: 65.5044%\n",
      "total_backward_count 1116060 real_backward_count 133512  11.963%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.290423/  1.644228, val:  57.50%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9411%\n",
      "layer   2  Sparsity: 66.9978%\n",
      "layer   3  Sparsity: 63.3880%\n",
      "total_backward_count 1125850 real_backward_count 134558  11.952%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.296487/  1.640012, val:  59.58%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9871%\n",
      "layer   2  Sparsity: 67.3772%\n",
      "layer   3  Sparsity: 64.1988%\n",
      "total_backward_count 1135640 real_backward_count 135594  11.940%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.300995/  1.638833, val:  58.75%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9626%\n",
      "layer   2  Sparsity: 67.9804%\n",
      "layer   3  Sparsity: 66.3961%\n",
      "total_backward_count 1145430 real_backward_count 136623  11.928%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.295849/  1.625163, val:  59.17%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9690%\n",
      "layer   2  Sparsity: 67.0652%\n",
      "layer   3  Sparsity: 66.6504%\n",
      "total_backward_count 1155220 real_backward_count 137689  11.919%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.310843/  1.662674, val:  54.17%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9886%\n",
      "layer   2  Sparsity: 67.2094%\n",
      "layer   3  Sparsity: 66.8321%\n",
      "total_backward_count 1165010 real_backward_count 138727  11.908%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.315384/  1.673259, val:  53.75%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9293%\n",
      "layer   2  Sparsity: 66.8410%\n",
      "layer   3  Sparsity: 65.8044%\n",
      "total_backward_count 1174800 real_backward_count 139823  11.902%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.285049/  1.597294, val:  63.75%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9636%\n",
      "layer   2  Sparsity: 67.2012%\n",
      "layer   3  Sparsity: 67.2934%\n",
      "total_backward_count 1184590 real_backward_count 140898  11.894%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.298468/  1.650390, val:  57.08%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0492%\n",
      "layer   2  Sparsity: 67.3755%\n",
      "layer   3  Sparsity: 67.4101%\n",
      "total_backward_count 1194380 real_backward_count 141999  11.889%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.325078/  1.708649, val:  48.75%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9144%\n",
      "layer   2  Sparsity: 66.9719%\n",
      "layer   3  Sparsity: 66.6684%\n",
      "total_backward_count 1204170 real_backward_count 143120  11.885%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.307041/  1.595146, val:  62.08%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9698%\n",
      "layer   2  Sparsity: 66.8533%\n",
      "layer   3  Sparsity: 66.0056%\n",
      "total_backward_count 1213960 real_backward_count 144219  11.880%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.307475/  1.621855, val:  62.08%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9057%\n",
      "layer   2  Sparsity: 67.5338%\n",
      "layer   3  Sparsity: 66.3044%\n",
      "total_backward_count 1223750 real_backward_count 145355  11.878%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.308680/  1.635175, val:  55.00%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9629%\n",
      "layer   2  Sparsity: 67.2734%\n",
      "layer   3  Sparsity: 65.4854%\n",
      "total_backward_count 1233540 real_backward_count 146431  11.871%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.323447/  1.724245, val:  45.00%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.0232%\n",
      "layer   2  Sparsity: 67.3135%\n",
      "layer   3  Sparsity: 66.3719%\n",
      "total_backward_count 1243330 real_backward_count 147503  11.864%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.290740/  1.629949, val:  57.08%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9723%\n",
      "layer   2  Sparsity: 66.9267%\n",
      "layer   3  Sparsity: 66.8244%\n",
      "total_backward_count 1253120 real_backward_count 148548  11.854%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.271680/  1.640092, val:  52.50%, val_best:  67.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0423%\n",
      "layer   2  Sparsity: 66.8662%\n",
      "layer   3  Sparsity: 65.8044%\n",
      "total_backward_count 1262910 real_backward_count 149590  11.845%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.274769/  1.608304, val:  57.08%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.0410%\n",
      "layer   2  Sparsity: 67.0657%\n",
      "layer   3  Sparsity: 66.8616%\n",
      "total_backward_count 1272700 real_backward_count 150652  11.837%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.266784/  1.681846, val:  48.33%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9510%\n",
      "layer   2  Sparsity: 66.9828%\n",
      "layer   3  Sparsity: 66.5442%\n",
      "total_backward_count 1282490 real_backward_count 151690  11.828%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.256772/  1.570883, val:  58.33%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9709%\n",
      "layer   2  Sparsity: 66.7573%\n",
      "layer   3  Sparsity: 66.3942%\n",
      "total_backward_count 1292280 real_backward_count 152834  11.827%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.277014/  1.601163, val:  57.08%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0135%\n",
      "layer   2  Sparsity: 66.3377%\n",
      "layer   3  Sparsity: 66.1578%\n",
      "total_backward_count 1302070 real_backward_count 153881  11.818%\n",
      "fc layer 3 self.abs_max_out: 1555.0\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.261389/  1.594363, val:  59.58%, val_best:  67.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9713%\n",
      "layer   2  Sparsity: 65.9052%\n",
      "layer   3  Sparsity: 64.7555%\n",
      "total_backward_count 1311860 real_backward_count 155017  11.817%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.276602/  1.627171, val:  53.75%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9708%\n",
      "layer   2  Sparsity: 66.5185%\n",
      "layer   3  Sparsity: 65.8925%\n",
      "total_backward_count 1321650 real_backward_count 156030  11.806%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.279737/  1.607871, val:  54.17%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0518%\n",
      "layer   2  Sparsity: 66.5307%\n",
      "layer   3  Sparsity: 66.2089%\n",
      "total_backward_count 1331440 real_backward_count 157057  11.796%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.292446/  1.617885, val:  56.67%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9831%\n",
      "layer   2  Sparsity: 66.3631%\n",
      "layer   3  Sparsity: 65.2849%\n",
      "total_backward_count 1341230 real_backward_count 158131  11.790%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.283210/  1.571641, val:  65.83%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9234%\n",
      "layer   2  Sparsity: 66.1829%\n",
      "layer   3  Sparsity: 65.4399%\n",
      "total_backward_count 1351020 real_backward_count 159198  11.784%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.284406/  1.625470, val:  60.83%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9271%\n",
      "layer   2  Sparsity: 66.4804%\n",
      "layer   3  Sparsity: 66.4522%\n",
      "total_backward_count 1360810 real_backward_count 160237  11.775%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.303697/  1.654864, val:  54.58%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9436%\n",
      "layer   2  Sparsity: 66.2938%\n",
      "layer   3  Sparsity: 66.5466%\n",
      "total_backward_count 1370600 real_backward_count 161343  11.772%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.302176/  1.670047, val:  51.25%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0315%\n",
      "layer   2  Sparsity: 66.8907%\n",
      "layer   3  Sparsity: 67.7494%\n",
      "total_backward_count 1380390 real_backward_count 162422  11.766%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.285756/  1.608256, val:  59.58%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 66.5533%\n",
      "layer   3  Sparsity: 65.8620%\n",
      "total_backward_count 1390180 real_backward_count 163485  11.760%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.262819/  1.625509, val:  50.42%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.0237%\n",
      "layer   2  Sparsity: 66.9497%\n",
      "layer   3  Sparsity: 65.1958%\n",
      "total_backward_count 1399970 real_backward_count 164521  11.752%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.270076/  1.629836, val:  50.83%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9556%\n",
      "layer   2  Sparsity: 66.9263%\n",
      "layer   3  Sparsity: 65.0260%\n",
      "total_backward_count 1409760 real_backward_count 165554  11.743%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.294482/  1.663250, val:  49.17%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9472%\n",
      "layer   2  Sparsity: 66.8581%\n",
      "layer   3  Sparsity: 65.9966%\n",
      "total_backward_count 1419550 real_backward_count 166574  11.734%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.291564/  1.592197, val:  56.67%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0130%\n",
      "layer   2  Sparsity: 67.1674%\n",
      "layer   3  Sparsity: 66.3439%\n",
      "total_backward_count 1429340 real_backward_count 167579  11.724%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.293974/  1.665281, val:  52.92%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9501%\n",
      "layer   2  Sparsity: 66.7094%\n",
      "layer   3  Sparsity: 66.3647%\n",
      "total_backward_count 1439130 real_backward_count 168758  11.726%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.257890/  1.604291, val:  52.50%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9863%\n",
      "layer   2  Sparsity: 66.3461%\n",
      "layer   3  Sparsity: 64.5028%\n",
      "total_backward_count 1448920 real_backward_count 169775  11.717%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.263045/  1.607530, val:  57.92%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0474%\n",
      "layer   2  Sparsity: 66.6670%\n",
      "layer   3  Sparsity: 64.8336%\n",
      "total_backward_count 1458710 real_backward_count 170791  11.708%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.253995/  1.607033, val:  54.58%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0628%\n",
      "layer   2  Sparsity: 66.6357%\n",
      "layer   3  Sparsity: 64.6157%\n",
      "total_backward_count 1468500 real_backward_count 171883  11.705%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.256724/  1.651113, val:  56.67%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9154%\n",
      "layer   2  Sparsity: 66.4229%\n",
      "layer   3  Sparsity: 65.4333%\n",
      "total_backward_count 1478290 real_backward_count 172918  11.697%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.266198/  1.587051, val:  57.50%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9747%\n",
      "layer   2  Sparsity: 66.0842%\n",
      "layer   3  Sparsity: 65.3110%\n",
      "total_backward_count 1488080 real_backward_count 173937  11.689%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.244043/  1.627319, val:  55.42%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9793%\n",
      "layer   2  Sparsity: 65.8800%\n",
      "layer   3  Sparsity: 65.3374%\n",
      "total_backward_count 1497870 real_backward_count 174890  11.676%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.238177/  1.531387, val:  69.17%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9426%\n",
      "layer   2  Sparsity: 66.6178%\n",
      "layer   3  Sparsity: 66.1980%\n",
      "total_backward_count 1507660 real_backward_count 175947  11.670%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.263693/  1.589559, val:  57.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9708%\n",
      "layer   2  Sparsity: 66.7529%\n",
      "layer   3  Sparsity: 64.9943%\n",
      "total_backward_count 1517450 real_backward_count 176961  11.662%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.245313/  1.651730, val:  54.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9687%\n",
      "layer   2  Sparsity: 66.7700%\n",
      "layer   3  Sparsity: 65.5005%\n",
      "total_backward_count 1527240 real_backward_count 177957  11.652%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.268752/  1.625570, val:  58.33%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0144%\n",
      "layer   2  Sparsity: 66.3042%\n",
      "layer   3  Sparsity: 65.4589%\n",
      "total_backward_count 1537030 real_backward_count 179027  11.648%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.252872/  1.592956, val:  60.00%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9435%\n",
      "layer   2  Sparsity: 66.4311%\n",
      "layer   3  Sparsity: 65.8538%\n",
      "total_backward_count 1546820 real_backward_count 179995  11.636%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.248469/  1.579199, val:  58.33%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9936%\n",
      "layer   2  Sparsity: 66.8760%\n",
      "layer   3  Sparsity: 66.3649%\n",
      "total_backward_count 1556610 real_backward_count 181071  11.632%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.265618/  1.653047, val:  52.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0147%\n",
      "layer   2  Sparsity: 66.9974%\n",
      "layer   3  Sparsity: 66.0671%\n",
      "total_backward_count 1566400 real_backward_count 182116  11.626%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.254397/  1.559293, val:  64.58%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9704%\n",
      "layer   2  Sparsity: 66.3129%\n",
      "layer   3  Sparsity: 65.3091%\n",
      "total_backward_count 1576190 real_backward_count 183120  11.618%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.250113/  1.568749, val:  64.17%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9761%\n",
      "layer   2  Sparsity: 66.7688%\n",
      "layer   3  Sparsity: 67.0392%\n",
      "total_backward_count 1585980 real_backward_count 184206  11.615%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.249953/  1.630110, val:  54.17%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 66.7630%\n",
      "layer   3  Sparsity: 66.0881%\n",
      "total_backward_count 1595770 real_backward_count 185270  11.610%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.261342/  1.680496, val:  52.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9797%\n",
      "layer   2  Sparsity: 66.7203%\n",
      "layer   3  Sparsity: 65.4077%\n",
      "total_backward_count 1605560 real_backward_count 186320  11.605%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.277447/  1.650702, val:  49.58%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0159%\n",
      "layer   2  Sparsity: 66.7411%\n",
      "layer   3  Sparsity: 66.6619%\n",
      "total_backward_count 1615350 real_backward_count 187376  11.600%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.270136/  1.613433, val:  57.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0227%\n",
      "layer   2  Sparsity: 67.0034%\n",
      "layer   3  Sparsity: 66.8578%\n",
      "total_backward_count 1625140 real_backward_count 188424  11.594%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.272578/  1.651666, val:  56.67%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9808%\n",
      "layer   2  Sparsity: 67.2368%\n",
      "layer   3  Sparsity: 66.9673%\n",
      "total_backward_count 1634930 real_backward_count 189474  11.589%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.259750/  1.629687, val:  53.75%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9950%\n",
      "layer   2  Sparsity: 67.7870%\n",
      "layer   3  Sparsity: 65.3071%\n",
      "total_backward_count 1644720 real_backward_count 190507  11.583%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.229440/  1.607728, val:  57.50%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9587%\n",
      "layer   2  Sparsity: 66.6842%\n",
      "layer   3  Sparsity: 64.8485%\n",
      "total_backward_count 1654510 real_backward_count 191540  11.577%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.246105/  1.624251, val:  50.00%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9262%\n",
      "layer   2  Sparsity: 67.1630%\n",
      "layer   3  Sparsity: 65.3037%\n",
      "total_backward_count 1664300 real_backward_count 192642  11.575%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.239508/  1.580174, val:  65.00%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9715%\n",
      "layer   2  Sparsity: 67.4259%\n",
      "layer   3  Sparsity: 64.6034%\n",
      "total_backward_count 1674090 real_backward_count 193662  11.568%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.242198/  1.574881, val:  58.33%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9467%\n",
      "layer   2  Sparsity: 67.2463%\n",
      "layer   3  Sparsity: 64.2632%\n",
      "total_backward_count 1683880 real_backward_count 194723  11.564%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.235369/  1.611632, val:  59.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9446%\n",
      "layer   2  Sparsity: 67.2506%\n",
      "layer   3  Sparsity: 65.1676%\n",
      "total_backward_count 1693670 real_backward_count 195787  11.560%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.250094/  1.601882, val:  57.92%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.0749%\n",
      "layer   2  Sparsity: 66.8666%\n",
      "layer   3  Sparsity: 66.6938%\n",
      "total_backward_count 1703460 real_backward_count 196834  11.555%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.267687/  1.620464, val:  55.83%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9488%\n",
      "layer   2  Sparsity: 66.4953%\n",
      "layer   3  Sparsity: 65.9728%\n",
      "total_backward_count 1713250 real_backward_count 197882  11.550%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.263189/  1.611349, val:  56.25%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9321%\n",
      "layer   2  Sparsity: 66.6057%\n",
      "layer   3  Sparsity: 65.2735%\n",
      "total_backward_count 1723040 real_backward_count 198944  11.546%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.237689/  1.615743, val:  61.25%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9973%\n",
      "layer   2  Sparsity: 66.2740%\n",
      "layer   3  Sparsity: 66.1268%\n",
      "total_backward_count 1732830 real_backward_count 200021  11.543%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.242827/  1.554351, val:  59.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9726%\n",
      "layer   2  Sparsity: 66.8011%\n",
      "layer   3  Sparsity: 65.3891%\n",
      "total_backward_count 1742620 real_backward_count 201044  11.537%\n",
      "fc layer 2 self.abs_max_out: 5414.0\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.250001/  1.609863, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9135%\n",
      "layer   2  Sparsity: 66.2868%\n",
      "layer   3  Sparsity: 66.0864%\n",
      "total_backward_count 1752410 real_backward_count 202047  11.530%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.257611/  1.572172, val:  59.58%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.0015%\n",
      "layer   2  Sparsity: 65.6701%\n",
      "layer   3  Sparsity: 66.2661%\n",
      "total_backward_count 1762200 real_backward_count 203052  11.523%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.270857/  1.600926, val:  58.75%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9245%\n",
      "layer   2  Sparsity: 66.4130%\n",
      "layer   3  Sparsity: 65.6610%\n",
      "total_backward_count 1771990 real_backward_count 204069  11.516%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.219748/  1.606807, val:  52.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9722%\n",
      "layer   2  Sparsity: 66.4540%\n",
      "layer   3  Sparsity: 64.5422%\n",
      "total_backward_count 1781780 real_backward_count 205015  11.506%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.231764/  1.591254, val:  63.33%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9958%\n",
      "layer   2  Sparsity: 66.5220%\n",
      "layer   3  Sparsity: 64.0068%\n",
      "total_backward_count 1791570 real_backward_count 206013  11.499%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.219268/  1.600805, val:  54.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9835%\n",
      "layer   2  Sparsity: 66.6718%\n",
      "layer   3  Sparsity: 64.1260%\n",
      "total_backward_count 1801360 real_backward_count 206983  11.490%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.228058/  1.578568, val:  60.83%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9432%\n",
      "layer   2  Sparsity: 66.4311%\n",
      "layer   3  Sparsity: 64.6692%\n",
      "total_backward_count 1811150 real_backward_count 207958  11.482%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.224358/  1.570096, val:  65.42%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9664%\n",
      "layer   2  Sparsity: 66.2114%\n",
      "layer   3  Sparsity: 64.7501%\n",
      "total_backward_count 1820940 real_backward_count 208986  11.477%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.236269/  1.568988, val:  61.25%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9612%\n",
      "layer   2  Sparsity: 66.6217%\n",
      "layer   3  Sparsity: 65.4137%\n",
      "total_backward_count 1830730 real_backward_count 209994  11.471%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.216856/  1.660582, val:  54.17%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.0301%\n",
      "layer   2  Sparsity: 66.6078%\n",
      "layer   3  Sparsity: 65.5559%\n",
      "total_backward_count 1840520 real_backward_count 211037  11.466%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.221801/  1.583535, val:  62.50%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9568%\n",
      "layer   2  Sparsity: 66.6150%\n",
      "layer   3  Sparsity: 65.1751%\n",
      "total_backward_count 1850310 real_backward_count 212046  11.460%\n",
      "fc layer 2 self.abs_max_out: 5445.0\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.246444/  1.579888, val:  61.25%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9700%\n",
      "layer   2  Sparsity: 66.1724%\n",
      "layer   3  Sparsity: 65.2696%\n",
      "total_backward_count 1860100 real_backward_count 213072  11.455%\n",
      "fc layer 2 self.abs_max_out: 5551.0\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.224014/  1.548513, val:  62.08%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9380%\n",
      "layer   2  Sparsity: 66.5114%\n",
      "layer   3  Sparsity: 64.7270%\n",
      "total_backward_count 1869890 real_backward_count 214034  11.446%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.234677/  1.661186, val:  45.00%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9900%\n",
      "layer   2  Sparsity: 66.8993%\n",
      "layer   3  Sparsity: 64.6708%\n",
      "total_backward_count 1879680 real_backward_count 215016  11.439%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.223147/  1.596476, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0165%\n",
      "layer   2  Sparsity: 66.8543%\n",
      "layer   3  Sparsity: 65.4096%\n",
      "total_backward_count 1889470 real_backward_count 216014  11.433%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.217419/  1.659843, val:  45.83%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0031%\n",
      "layer   2  Sparsity: 66.3225%\n",
      "layer   3  Sparsity: 66.2394%\n",
      "total_backward_count 1899260 real_backward_count 216989  11.425%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.226251/  1.593921, val:  53.33%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9472%\n",
      "layer   2  Sparsity: 66.8504%\n",
      "layer   3  Sparsity: 66.8214%\n",
      "total_backward_count 1909050 real_backward_count 217991  11.419%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.212471/  1.551654, val:  53.75%, val_best:  69.17%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9844%\n",
      "layer   2  Sparsity: 66.2815%\n",
      "layer   3  Sparsity: 65.7658%\n",
      "total_backward_count 1918840 real_backward_count 219056  11.416%\n",
      "fc layer 3 self.abs_max_out: 1613.0\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.239743/  1.622966, val:  56.67%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9447%\n",
      "layer   2  Sparsity: 66.1375%\n",
      "layer   3  Sparsity: 65.8590%\n",
      "total_backward_count 1928630 real_backward_count 220119  11.413%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.239719/  1.573859, val:  70.00%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.0014%\n",
      "layer   2  Sparsity: 66.7849%\n",
      "layer   3  Sparsity: 65.3849%\n",
      "total_backward_count 1938420 real_backward_count 221161  11.409%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.201597/  1.583055, val:  64.17%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9244%\n",
      "layer   2  Sparsity: 66.8684%\n",
      "layer   3  Sparsity: 65.4320%\n",
      "total_backward_count 1948210 real_backward_count 222118  11.401%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.201734/  1.578131, val:  57.92%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9593%\n",
      "layer   2  Sparsity: 66.2112%\n",
      "layer   3  Sparsity: 65.1580%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f21d8128dc3442f893227a2739bcda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>1.20173</td></tr><tr><td>val_acc_best</td><td>0.7</td></tr><tr><td>val_acc_now</td><td>0.57917</td></tr><tr><td>val_loss</td><td>1.57813</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-sweep-8</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcjgt957' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcjgt957</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_040023-gcjgt957/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m2cgd826 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_081802-m2cgd826</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m2cgd826' target=\"_blank\">crimson-sweep-12</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m2cgd826' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m2cgd826</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251118_081811_887', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 282\n",
      "fc layer 1 self.abs_max_out: 195.0\n",
      "lif layer 1 self.abs_max_v: 195.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 189.0\n",
      "lif layer 2 self.abs_max_v: 189.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "fc layer 1 self.abs_max_out: 257.0\n",
      "lif layer 1 self.abs_max_v: 264.0\n",
      "fc layer 2 self.abs_max_out: 264.0\n",
      "lif layer 2 self.abs_max_v: 271.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "lif layer 1 self.abs_max_v: 339.0\n",
      "fc layer 2 self.abs_max_out: 323.0\n",
      "lif layer 2 self.abs_max_v: 439.0\n",
      "fc layer 3 self.abs_max_out: 132.0\n",
      "lif layer 2 self.abs_max_v: 497.5\n",
      "fc layer 1 self.abs_max_out: 293.0\n",
      "lif layer 1 self.abs_max_v: 340.0\n",
      "fc layer 2 self.abs_max_out: 341.0\n",
      "fc layer 1 self.abs_max_out: 479.0\n",
      "lif layer 1 self.abs_max_v: 479.0\n",
      "lif layer 2 self.abs_max_v: 504.5\n",
      "lif layer 2 self.abs_max_v: 565.0\n",
      "smallest_now_T updated: 254\n",
      "fc layer 1 self.abs_max_out: 544.0\n",
      "lif layer 1 self.abs_max_v: 544.0\n",
      "fc layer 2 self.abs_max_out: 420.0\n",
      "fc layer 3 self.abs_max_out: 157.0\n",
      "lif layer 1 self.abs_max_v: 618.5\n",
      "fc layer 2 self.abs_max_out: 422.0\n",
      "fc layer 2 self.abs_max_out: 471.0\n",
      "fc layer 3 self.abs_max_out: 160.0\n",
      "lif layer 2 self.abs_max_v: 569.0\n",
      "fc layer 1 self.abs_max_out: 580.0\n",
      "fc layer 1 self.abs_max_out: 693.0\n",
      "lif layer 1 self.abs_max_v: 759.5\n",
      "lif layer 2 self.abs_max_v: 579.0\n",
      "fc layer 2 self.abs_max_out: 597.0\n",
      "lif layer 2 self.abs_max_v: 624.5\n",
      "smallest_now_T updated: 193\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 1 self.abs_max_out: 838.0\n",
      "lif layer 1 self.abs_max_v: 838.0\n",
      "lif layer 2 self.abs_max_v: 693.5\n",
      "fc layer 3 self.abs_max_out: 224.0\n",
      "lif layer 2 self.abs_max_v: 700.0\n",
      "lif layer 2 self.abs_max_v: 702.0\n",
      "fc layer 2 self.abs_max_out: 608.0\n",
      "lif layer 2 self.abs_max_v: 715.0\n",
      "fc layer 1 self.abs_max_out: 989.0\n",
      "lif layer 1 self.abs_max_v: 989.0\n",
      "lif layer 2 self.abs_max_v: 752.5\n",
      "lif layer 2 self.abs_max_v: 802.5\n",
      "lif layer 2 self.abs_max_v: 861.5\n",
      "lif layer 2 self.abs_max_v: 883.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 2 self.abs_max_out: 613.0\n",
      "fc layer 1 self.abs_max_out: 1018.0\n",
      "lif layer 1 self.abs_max_v: 1018.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "lif layer 2 self.abs_max_v: 935.0\n",
      "lif layer 2 self.abs_max_v: 1029.5\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "fc layer 1 self.abs_max_out: 1098.0\n",
      "lif layer 1 self.abs_max_v: 1098.0\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "fc layer 1 self.abs_max_out: 1214.0\n",
      "lif layer 1 self.abs_max_v: 1214.0\n",
      "fc layer 1 self.abs_max_out: 1423.0\n",
      "lif layer 1 self.abs_max_v: 1423.0\n",
      "lif layer 2 self.abs_max_v: 1068.0\n",
      "lif layer 2 self.abs_max_v: 1140.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "smallest_now_T updated: 163\n",
      "fc layer 2 self.abs_max_out: 726.0\n",
      "lif layer 2 self.abs_max_v: 1173.0\n",
      "lif layer 2 self.abs_max_v: 1210.5\n",
      "fc layer 2 self.abs_max_out: 764.0\n",
      "smallest_now_T updated: 150\n",
      "fc layer 2 self.abs_max_out: 769.0\n",
      "fc layer 2 self.abs_max_out: 800.0\n",
      "fc layer 2 self.abs_max_out: 833.0\n",
      "lif layer 1 self.abs_max_v: 1452.5\n",
      "lif layer 1 self.abs_max_v: 1468.5\n",
      "fc layer 2 self.abs_max_out: 880.0\n",
      "lif layer 1 self.abs_max_v: 1521.0\n",
      "fc layer 3 self.abs_max_out: 309.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 2 self.abs_max_out: 882.0\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "smallest_now_T updated: 135\n",
      "lif layer 1 self.abs_max_v: 1556.0\n",
      "lif layer 1 self.abs_max_v: 1809.0\n",
      "lif layer 2 self.abs_max_v: 1229.0\n",
      "lif layer 2 self.abs_max_v: 1256.5\n",
      "lif layer 1 self.abs_max_v: 1938.0\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "lif layer 2 self.abs_max_v: 1270.5\n",
      "lif layer 2 self.abs_max_v: 1306.5\n",
      "lif layer 2 self.abs_max_v: 1388.5\n",
      "fc layer 1 self.abs_max_out: 1508.0\n",
      "fc layer 1 self.abs_max_out: 1588.0\n",
      "fc layer 1 self.abs_max_out: 1720.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "lif layer 1 self.abs_max_v: 1985.0\n",
      "lif layer 1 self.abs_max_v: 2044.0\n",
      "smallest_now_T updated: 116\n",
      "smallest_now_T updated: 90\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "lif layer 2 self.abs_max_v: 1427.0\n",
      "lif layer 1 self.abs_max_v: 2054.0\n",
      "lif layer 1 self.abs_max_v: 2093.0\n",
      "lif layer 1 self.abs_max_v: 2231.5\n",
      "lif layer 1 self.abs_max_v: 2549.0\n",
      "lif layer 1 self.abs_max_v: 2652.5\n",
      "fc layer 1 self.abs_max_out: 1779.0\n",
      "lif layer 2 self.abs_max_v: 1482.5\n",
      "lif layer 2 self.abs_max_v: 1568.5\n",
      "lif layer 2 self.abs_max_v: 1570.5\n",
      "lif layer 2 self.abs_max_v: 1603.5\n",
      "lif layer 1 self.abs_max_v: 2735.0\n",
      "lif layer 1 self.abs_max_v: 2925.5\n",
      "lif layer 1 self.abs_max_v: 3096.0\n",
      "lif layer 1 self.abs_max_v: 3166.0\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "fc layer 2 self.abs_max_out: 1072.0\n",
      "fc layer 2 self.abs_max_out: 1089.0\n",
      "fc layer 2 self.abs_max_out: 1116.0\n",
      "fc layer 2 self.abs_max_out: 1129.0\n",
      "fc layer 1 self.abs_max_out: 1850.0\n",
      "lif layer 1 self.abs_max_v: 3424.0\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "fc layer 1 self.abs_max_out: 1952.0\n",
      "lif layer 2 self.abs_max_v: 1639.5\n",
      "lif layer 2 self.abs_max_v: 1732.0\n",
      "lif layer 2 self.abs_max_v: 1748.0\n",
      "lif layer 2 self.abs_max_v: 1749.0\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "lif layer 1 self.abs_max_v: 3485.0\n",
      "lif layer 1 self.abs_max_v: 3506.5\n",
      "lif layer 1 self.abs_max_v: 3665.5\n",
      "lif layer 1 self.abs_max_v: 3682.0\n",
      "fc layer 2 self.abs_max_out: 1179.0\n",
      "fc layer 1 self.abs_max_out: 2005.0\n",
      "lif layer 2 self.abs_max_v: 1791.0\n",
      "fc layer 1 self.abs_max_out: 2068.0\n",
      "lif layer 1 self.abs_max_v: 3751.5\n",
      "lif layer 2 self.abs_max_v: 1859.5\n",
      "smallest_now_T_val updated: 262\n",
      "smallest_now_T_val updated: 217\n",
      "smallest_now_T_val updated: 213\n",
      "smallest_now_T_val updated: 209\n",
      "smallest_now_T_val updated: 174\n",
      "smallest_now_T_val updated: 63\n",
      "fc layer 1 self.abs_max_out: 2109.0\n",
      "lif layer 1 self.abs_max_v: 3806.0\n",
      "lif layer 1 self.abs_max_v: 3955.0\n",
      "lif layer 1 self.abs_max_v: 4032.5\n",
      "fc layer 1 self.abs_max_out: 2174.0\n",
      "lif layer 1 self.abs_max_v: 4190.5\n",
      "fc layer 1 self.abs_max_out: 2175.0\n",
      "lif layer 1 self.abs_max_v: 4270.5\n",
      "lif layer 2 self.abs_max_v: 1881.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.739432/  1.953589, val:  30.83%, val_best:  30.83%, tr:  98.06%, tr_best:  98.06%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4711%\n",
      "layer   2  Sparsity: 75.3126%\n",
      "layer   3  Sparsity: 70.5225%\n",
      "total_backward_count 9790 real_backward_count 1953  19.949%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1930.5\n",
      "lif layer 2 self.abs_max_v: 1954.0\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "fc layer 2 self.abs_max_out: 1193.0\n",
      "fc layer 2 self.abs_max_out: 1245.0\n",
      "fc layer 2 self.abs_max_out: 1260.0\n",
      "fc layer 2 self.abs_max_out: 1271.0\n",
      "fc layer 2 self.abs_max_out: 1319.0\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "lif layer 2 self.abs_max_v: 1996.0\n",
      "lif layer 2 self.abs_max_v: 2036.0\n",
      "fc layer 1 self.abs_max_out: 2280.0\n",
      "fc layer 1 self.abs_max_out: 2289.0\n",
      "fc layer 2 self.abs_max_out: 1334.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "lif layer 2 self.abs_max_v: 2042.0\n",
      "fc layer 1 self.abs_max_out: 2334.0\n",
      "fc layer 2 self.abs_max_out: 1339.0\n",
      "fc layer 1 self.abs_max_out: 2497.0\n",
      "fc layer 1 self.abs_max_out: 2542.0\n",
      "lif layer 1 self.abs_max_v: 4322.5\n",
      "lif layer 1 self.abs_max_v: 4487.5\n",
      "lif layer 1 self.abs_max_v: 4494.0\n",
      "lif layer 1 self.abs_max_v: 4571.0\n",
      "lif layer 2 self.abs_max_v: 2045.5\n",
      "lif layer 2 self.abs_max_v: 2159.0\n",
      "lif layer 2 self.abs_max_v: 2167.0\n",
      "lif layer 2 self.abs_max_v: 2255.0\n",
      "lif layer 2 self.abs_max_v: 2320.5\n",
      "lif layer 2 self.abs_max_v: 2326.5\n",
      "fc layer 1 self.abs_max_out: 2610.0\n",
      "fc layer 2 self.abs_max_out: 1360.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.664421/  1.893115, val:  42.50%, val_best:  42.50%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5091%\n",
      "layer   2  Sparsity: 75.2716%\n",
      "layer   3  Sparsity: 71.6729%\n",
      "total_backward_count 19580 real_backward_count 3479  17.768%\n",
      "fc layer 2 self.abs_max_out: 1385.0\n",
      "fc layer 2 self.abs_max_out: 1386.0\n",
      "fc layer 2 self.abs_max_out: 1414.0\n",
      "fc layer 2 self.abs_max_out: 1449.0\n",
      "fc layer 2 self.abs_max_out: 1456.0\n",
      "fc layer 2 self.abs_max_out: 1476.0\n",
      "fc layer 2 self.abs_max_out: 1488.0\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "fc layer 1 self.abs_max_out: 2628.0\n",
      "fc layer 1 self.abs_max_out: 2700.0\n",
      "lif layer 1 self.abs_max_v: 4631.0\n",
      "fc layer 3 self.abs_max_out: 513.0\n",
      "lif layer 1 self.abs_max_v: 4660.5\n",
      "lif layer 1 self.abs_max_v: 4821.5\n",
      "lif layer 1 self.abs_max_v: 4988.0\n",
      "fc layer 1 self.abs_max_out: 2858.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.650653/  1.879597, val:  44.58%, val_best:  44.58%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5113%\n",
      "layer   2  Sparsity: 75.3280%\n",
      "layer   3  Sparsity: 71.3039%\n",
      "total_backward_count 29370 real_backward_count 4929  16.782%\n",
      "fc layer 2 self.abs_max_out: 1501.0\n",
      "fc layer 1 self.abs_max_out: 3034.0\n",
      "fc layer 1 self.abs_max_out: 3096.0\n",
      "lif layer 1 self.abs_max_v: 5219.5\n",
      "lif layer 1 self.abs_max_v: 5365.0\n",
      "lif layer 1 self.abs_max_v: 5367.0\n",
      "lif layer 1 self.abs_max_v: 5429.5\n",
      "fc layer 2 self.abs_max_out: 1570.0\n",
      "fc layer 1 self.abs_max_out: 3192.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.657325/  1.930585, val:  42.50%, val_best:  44.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4443%\n",
      "layer   2  Sparsity: 75.8566%\n",
      "layer   3  Sparsity: 71.5290%\n",
      "total_backward_count 39160 real_backward_count 6310  16.113%\n",
      "fc layer 3 self.abs_max_out: 529.0\n",
      "fc layer 1 self.abs_max_out: 3246.0\n",
      "lif layer 1 self.abs_max_v: 5460.5\n",
      "fc layer 1 self.abs_max_out: 3675.0\n",
      "lif layer 1 self.abs_max_v: 6405.5\n",
      "lif layer 1 self.abs_max_v: 6530.5\n",
      "lif layer 1 self.abs_max_v: 6627.5\n",
      "lif layer 2 self.abs_max_v: 2331.0\n",
      "lif layer 2 self.abs_max_v: 2371.5\n",
      "lif layer 2 self.abs_max_v: 2434.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.632004/  1.872776, val:  50.83%, val_best:  50.83%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4714%\n",
      "layer   2  Sparsity: 75.3535%\n",
      "layer   3  Sparsity: 71.3659%\n",
      "total_backward_count 48950 real_backward_count 7601  15.528%\n",
      "fc layer 2 self.abs_max_out: 1586.0\n",
      "lif layer 2 self.abs_max_v: 2473.0\n",
      "lif layer 2 self.abs_max_v: 2474.0\n",
      "lif layer 2 self.abs_max_v: 2493.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.615928/  1.845794, val:  50.00%, val_best:  50.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4404%\n",
      "layer   2  Sparsity: 75.3662%\n",
      "layer   3  Sparsity: 71.2763%\n",
      "total_backward_count 58740 real_backward_count 8876  15.111%\n",
      "lif layer 2 self.abs_max_v: 2558.5\n",
      "lif layer 2 self.abs_max_v: 2582.5\n",
      "lif layer 2 self.abs_max_v: 2662.0\n",
      "lif layer 2 self.abs_max_v: 2669.0\n",
      "lif layer 2 self.abs_max_v: 2700.5\n",
      "fc layer 2 self.abs_max_out: 1591.0\n",
      "fc layer 2 self.abs_max_out: 1612.0\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "lif layer 2 self.abs_max_v: 2783.5\n",
      "fc layer 2 self.abs_max_out: 1635.0\n",
      "fc layer 2 self.abs_max_out: 1704.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.626906/  1.852889, val:  51.67%, val_best:  51.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4898%\n",
      "layer   2  Sparsity: 73.8733%\n",
      "layer   3  Sparsity: 70.7128%\n",
      "total_backward_count 68530 real_backward_count 10140  14.796%\n",
      "fc layer 2 self.abs_max_out: 1713.0\n",
      "fc layer 2 self.abs_max_out: 1720.0\n",
      "fc layer 2 self.abs_max_out: 1746.0\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "fc layer 1 self.abs_max_out: 3837.0\n",
      "lif layer 1 self.abs_max_v: 6753.0\n",
      "lif layer 1 self.abs_max_v: 6765.5\n",
      "lif layer 1 self.abs_max_v: 6851.0\n",
      "lif layer 1 self.abs_max_v: 6924.5\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.608250/  1.794234, val:  55.42%, val_best:  55.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4888%\n",
      "layer   2  Sparsity: 74.3792%\n",
      "layer   3  Sparsity: 71.9438%\n",
      "total_backward_count 78320 real_backward_count 11336  14.474%\n",
      "fc layer 2 self.abs_max_out: 1804.0\n",
      "fc layer 2 self.abs_max_out: 1858.0\n",
      "fc layer 2 self.abs_max_out: 1861.0\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "fc layer 1 self.abs_max_out: 3866.0\n",
      "fc layer 1 self.abs_max_out: 3882.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.618677/  1.812110, val:  50.00%, val_best:  55.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4447%\n",
      "layer   2  Sparsity: 74.7713%\n",
      "layer   3  Sparsity: 72.3860%\n",
      "total_backward_count 88110 real_backward_count 12575  14.272%\n",
      "lif layer 2 self.abs_max_v: 2795.5\n",
      "lif layer 2 self.abs_max_v: 2802.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.596728/  1.841940, val:  52.08%, val_best:  55.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.5260%\n",
      "layer   2  Sparsity: 74.2942%\n",
      "layer   3  Sparsity: 72.1807%\n",
      "total_backward_count 97900 real_backward_count 13771  14.066%\n",
      "fc layer 2 self.abs_max_out: 2141.0\n",
      "fc layer 2 self.abs_max_out: 2156.0\n",
      "lif layer 2 self.abs_max_v: 2844.5\n",
      "lif layer 1 self.abs_max_v: 6968.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.546373/  1.777234, val:  58.75%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4756%\n",
      "layer   2  Sparsity: 73.6025%\n",
      "layer   3  Sparsity: 71.4462%\n",
      "total_backward_count 107690 real_backward_count 14943  13.876%\n",
      "lif layer 1 self.abs_max_v: 6999.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.560345/  1.759217, val:  68.75%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4657%\n",
      "layer   2  Sparsity: 73.3473%\n",
      "layer   3  Sparsity: 70.8984%\n",
      "total_backward_count 117480 real_backward_count 16098  13.703%\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "fc layer 2 self.abs_max_out: 2162.0\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "lif layer 2 self.abs_max_v: 2901.0\n",
      "fc layer 1 self.abs_max_out: 4061.0\n",
      "lif layer 1 self.abs_max_v: 7166.0\n",
      "lif layer 1 self.abs_max_v: 7207.0\n",
      "lif layer 1 self.abs_max_v: 7319.5\n",
      "lif layer 1 self.abs_max_v: 7388.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.537033/  1.782624, val:  45.00%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5163%\n",
      "layer   2  Sparsity: 73.5521%\n",
      "layer   3  Sparsity: 71.5689%\n",
      "total_backward_count 127270 real_backward_count 17270  13.570%\n",
      "lif layer 2 self.abs_max_v: 2977.5\n",
      "lif layer 2 self.abs_max_v: 3043.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.531612/  1.798420, val:  47.92%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5185%\n",
      "layer   2  Sparsity: 73.1073%\n",
      "layer   3  Sparsity: 70.5016%\n",
      "total_backward_count 137060 real_backward_count 18376  13.407%\n",
      "lif layer 2 self.abs_max_v: 3143.0\n",
      "lif layer 2 self.abs_max_v: 3189.5\n",
      "lif layer 2 self.abs_max_v: 3441.5\n",
      "lif layer 2 self.abs_max_v: 3489.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.513937/  1.766197, val:  49.58%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5236%\n",
      "layer   2  Sparsity: 72.9423%\n",
      "layer   3  Sparsity: 70.5144%\n",
      "total_backward_count 146850 real_backward_count 19510  13.286%\n",
      "lif layer 2 self.abs_max_v: 3515.5\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.526595/  1.773147, val:  51.67%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4741%\n",
      "layer   2  Sparsity: 72.1954%\n",
      "layer   3  Sparsity: 70.6410%\n",
      "total_backward_count 156640 real_backward_count 20615  13.161%\n",
      "fc layer 1 self.abs_max_out: 4211.0\n",
      "fc layer 1 self.abs_max_out: 4219.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.503275/  1.728529, val:  60.83%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4813%\n",
      "layer   2  Sparsity: 72.5258%\n",
      "layer   3  Sparsity: 70.5429%\n",
      "total_backward_count 166430 real_backward_count 21700  13.039%\n",
      "fc layer 3 self.abs_max_out: 635.0\n",
      "lif layer 1 self.abs_max_v: 7456.0\n",
      "lif layer 1 self.abs_max_v: 7549.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.503965/  1.742689, val:  60.42%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4532%\n",
      "layer   2  Sparsity: 72.7295%\n",
      "layer   3  Sparsity: 71.7739%\n",
      "total_backward_count 176220 real_backward_count 22805  12.941%\n",
      "lif layer 1 self.abs_max_v: 7562.5\n",
      "lif layer 2 self.abs_max_v: 3590.5\n",
      "lif layer 2 self.abs_max_v: 3741.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.495119/  1.730382, val:  52.08%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4872%\n",
      "layer   2  Sparsity: 73.0934%\n",
      "layer   3  Sparsity: 71.9215%\n",
      "total_backward_count 186010 real_backward_count 23903  12.850%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.475266/  1.771276, val:  44.17%, val_best:  68.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4744%\n",
      "layer   2  Sparsity: 72.8071%\n",
      "layer   3  Sparsity: 72.1243%\n",
      "total_backward_count 195800 real_backward_count 24939  12.737%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.496614/  1.773057, val:  50.83%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4884%\n",
      "layer   2  Sparsity: 73.2309%\n",
      "layer   3  Sparsity: 72.6811%\n",
      "total_backward_count 205590 real_backward_count 26003  12.648%\n",
      "lif layer 1 self.abs_max_v: 7619.5\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.514452/  1.767586, val:  51.67%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4668%\n",
      "layer   2  Sparsity: 73.0283%\n",
      "layer   3  Sparsity: 73.2089%\n",
      "total_backward_count 215380 real_backward_count 27083  12.575%\n",
      "lif layer 2 self.abs_max_v: 3755.0\n",
      "lif layer 2 self.abs_max_v: 3922.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.505608/  1.712721, val:  54.17%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5032%\n",
      "layer   2  Sparsity: 72.4116%\n",
      "layer   3  Sparsity: 72.9485%\n",
      "total_backward_count 225170 real_backward_count 28163  12.507%\n",
      "fc layer 2 self.abs_max_out: 2175.0\n",
      "lif layer 2 self.abs_max_v: 3998.0\n",
      "fc layer 2 self.abs_max_out: 2266.0\n",
      "lif layer 2 self.abs_max_v: 4244.0\n",
      "fc layer 1 self.abs_max_out: 4792.0\n",
      "lif layer 1 self.abs_max_v: 7839.5\n",
      "lif layer 1 self.abs_max_v: 8312.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.486262/  1.727063, val:  58.75%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4460%\n",
      "layer   2  Sparsity: 72.3344%\n",
      "layer   3  Sparsity: 72.3966%\n",
      "total_backward_count 234960 real_backward_count 29237  12.443%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.472826/  1.676582, val:  67.08%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4719%\n",
      "layer   2  Sparsity: 72.0664%\n",
      "layer   3  Sparsity: 71.1673%\n",
      "total_backward_count 244750 real_backward_count 30255  12.362%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.470644/  1.679420, val:  62.92%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.5125%\n",
      "layer   2  Sparsity: 72.1941%\n",
      "layer   3  Sparsity: 71.3181%\n",
      "total_backward_count 254540 real_backward_count 31330  12.308%\n",
      "fc layer 1 self.abs_max_out: 4829.0\n",
      "lif layer 1 self.abs_max_v: 8353.5\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.446271/  1.678628, val:  57.08%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4971%\n",
      "layer   2  Sparsity: 72.3594%\n",
      "layer   3  Sparsity: 71.9099%\n",
      "total_backward_count 264330 real_backward_count 32347  12.237%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.446667/  1.660976, val:  65.00%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4975%\n",
      "layer   2  Sparsity: 72.2534%\n",
      "layer   3  Sparsity: 71.1537%\n",
      "total_backward_count 274120 real_backward_count 33384  12.179%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.423344/  1.669419, val:  58.33%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4947%\n",
      "layer   2  Sparsity: 71.8652%\n",
      "layer   3  Sparsity: 70.9360%\n",
      "total_backward_count 283910 real_backward_count 34367  12.105%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.430658/  1.671154, val:  57.50%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5033%\n",
      "layer   2  Sparsity: 71.7938%\n",
      "layer   3  Sparsity: 71.0430%\n",
      "total_backward_count 293700 real_backward_count 35327  12.028%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.431956/  1.664363, val:  60.42%, val_best:  68.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5295%\n",
      "layer   2  Sparsity: 71.1600%\n",
      "layer   3  Sparsity: 71.3603%\n",
      "total_backward_count 303490 real_backward_count 36319  11.967%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.420313/  1.679245, val:  51.25%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4950%\n",
      "layer   2  Sparsity: 71.7384%\n",
      "layer   3  Sparsity: 71.4067%\n",
      "total_backward_count 313280 real_backward_count 37266  11.895%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.411334/  1.661267, val:  67.08%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4919%\n",
      "layer   2  Sparsity: 71.8700%\n",
      "layer   3  Sparsity: 70.7989%\n",
      "total_backward_count 323070 real_backward_count 38214  11.828%\n",
      "fc layer 1 self.abs_max_out: 5038.0\n",
      "lif layer 1 self.abs_max_v: 8711.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.412959/  1.621304, val:  70.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4775%\n",
      "layer   2  Sparsity: 71.6986%\n",
      "layer   3  Sparsity: 71.0525%\n",
      "total_backward_count 332860 real_backward_count 39174  11.769%\n",
      "fc layer 2 self.abs_max_out: 2282.0\n",
      "fc layer 2 self.abs_max_out: 2284.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.387277/  1.620565, val:  57.92%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5180%\n",
      "layer   2  Sparsity: 72.2404%\n",
      "layer   3  Sparsity: 71.0528%\n",
      "total_backward_count 342650 real_backward_count 40088  11.699%\n",
      "fc layer 2 self.abs_max_out: 2310.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.366262/  1.655930, val:  58.33%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5167%\n",
      "layer   2  Sparsity: 72.0500%\n",
      "layer   3  Sparsity: 70.0694%\n",
      "total_backward_count 352440 real_backward_count 41028  11.641%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.369003/  1.606463, val:  63.75%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4624%\n",
      "layer   2  Sparsity: 71.8283%\n",
      "layer   3  Sparsity: 69.9978%\n",
      "total_backward_count 362230 real_backward_count 41931  11.576%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.348840/  1.629698, val:  65.00%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5245%\n",
      "layer   2  Sparsity: 72.0199%\n",
      "layer   3  Sparsity: 70.0151%\n",
      "total_backward_count 372020 real_backward_count 42826  11.512%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.364349/  1.619638, val:  67.92%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5055%\n",
      "layer   2  Sparsity: 72.3106%\n",
      "layer   3  Sparsity: 71.0395%\n",
      "total_backward_count 381810 real_backward_count 43803  11.472%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.356696/  1.651825, val:  60.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4776%\n",
      "layer   2  Sparsity: 71.7169%\n",
      "layer   3  Sparsity: 70.8203%\n",
      "total_backward_count 391600 real_backward_count 44747  11.427%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.349002/  1.582608, val:  66.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.5281%\n",
      "layer   2  Sparsity: 71.8322%\n",
      "layer   3  Sparsity: 70.5492%\n",
      "total_backward_count 401390 real_backward_count 45660  11.375%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.354820/  1.592845, val:  68.75%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4786%\n",
      "layer   2  Sparsity: 71.6009%\n",
      "layer   3  Sparsity: 70.6471%\n",
      "total_backward_count 411180 real_backward_count 46573  11.327%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.354209/  1.617024, val:  62.08%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4703%\n",
      "layer   2  Sparsity: 72.2343%\n",
      "layer   3  Sparsity: 71.0094%\n",
      "total_backward_count 420970 real_backward_count 47455  11.273%\n",
      "fc layer 1 self.abs_max_out: 5249.0\n",
      "lif layer 1 self.abs_max_v: 8859.5\n",
      "lif layer 1 self.abs_max_v: 8894.5\n",
      "fc layer 2 self.abs_max_out: 2360.0\n",
      "lif layer 1 self.abs_max_v: 9063.0\n",
      "lif layer 1 self.abs_max_v: 9175.5\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.364202/  1.605776, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4534%\n",
      "layer   2  Sparsity: 72.3426%\n",
      "layer   3  Sparsity: 71.0611%\n",
      "total_backward_count 430760 real_backward_count 48401  11.236%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.358503/  1.586214, val:  72.92%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5009%\n",
      "layer   2  Sparsity: 72.3510%\n",
      "layer   3  Sparsity: 70.7569%\n",
      "total_backward_count 440550 real_backward_count 49301  11.191%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.348172/  1.561912, val:  69.58%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4871%\n",
      "layer   2  Sparsity: 72.2922%\n",
      "layer   3  Sparsity: 70.8955%\n",
      "total_backward_count 450340 real_backward_count 50169  11.140%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.342990/  1.561940, val:  71.25%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5136%\n",
      "layer   2  Sparsity: 72.5381%\n",
      "layer   3  Sparsity: 70.4912%\n",
      "total_backward_count 460130 real_backward_count 51055  11.096%\n",
      "fc layer 3 self.abs_max_out: 638.0\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.289162/  1.581891, val:  50.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4729%\n",
      "layer   2  Sparsity: 72.3698%\n",
      "layer   3  Sparsity: 70.6557%\n",
      "total_backward_count 469920 real_backward_count 51936  11.052%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.282161/  1.532962, val:  63.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5214%\n",
      "layer   2  Sparsity: 71.5950%\n",
      "layer   3  Sparsity: 69.7443%\n",
      "total_backward_count 479710 real_backward_count 52777  11.002%\n",
      "fc layer 3 self.abs_max_out: 646.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.291620/  1.544033, val:  67.08%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4113%\n",
      "layer   2  Sparsity: 71.9969%\n",
      "layer   3  Sparsity: 70.0166%\n",
      "total_backward_count 489500 real_backward_count 53633  10.957%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.276748/  1.510031, val:  72.08%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4931%\n",
      "layer   2  Sparsity: 71.7762%\n",
      "layer   3  Sparsity: 70.4529%\n",
      "total_backward_count 499290 real_backward_count 54483  10.912%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.274345/  1.534226, val:  77.92%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4922%\n",
      "layer   2  Sparsity: 71.7863%\n",
      "layer   3  Sparsity: 70.2268%\n",
      "total_backward_count 509080 real_backward_count 55290  10.861%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.273992/  1.532704, val:  65.42%, val_best:  77.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4632%\n",
      "layer   2  Sparsity: 71.7209%\n",
      "layer   3  Sparsity: 69.6329%\n",
      "total_backward_count 518870 real_backward_count 56146  10.821%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.272918/  1.508883, val:  70.42%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4404%\n",
      "layer   2  Sparsity: 71.6320%\n",
      "layer   3  Sparsity: 69.3058%\n",
      "total_backward_count 528660 real_backward_count 56963  10.775%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.248783/  1.547101, val:  61.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4724%\n",
      "layer   2  Sparsity: 71.4731%\n",
      "layer   3  Sparsity: 69.5197%\n",
      "total_backward_count 538450 real_backward_count 57802  10.735%\n",
      "fc layer 3 self.abs_max_out: 673.0\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.240391/  1.500031, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5197%\n",
      "layer   2  Sparsity: 71.6930%\n",
      "layer   3  Sparsity: 69.7590%\n",
      "total_backward_count 548240 real_backward_count 58651  10.698%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.225512/  1.526880, val:  62.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4272%\n",
      "layer   2  Sparsity: 72.0689%\n",
      "layer   3  Sparsity: 70.1737%\n",
      "total_backward_count 558030 real_backward_count 59458  10.655%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.223203/  1.480980, val:  61.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4345%\n",
      "layer   2  Sparsity: 71.8592%\n",
      "layer   3  Sparsity: 69.7484%\n",
      "total_backward_count 567820 real_backward_count 60266  10.614%\n",
      "fc layer 3 self.abs_max_out: 685.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.216251/  1.466615, val:  71.67%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4722%\n",
      "layer   2  Sparsity: 71.4867%\n",
      "layer   3  Sparsity: 69.8743%\n",
      "total_backward_count 577610 real_backward_count 61047  10.569%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.223171/  1.486983, val:  72.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4954%\n",
      "layer   2  Sparsity: 71.1650%\n",
      "layer   3  Sparsity: 69.9454%\n",
      "total_backward_count 587400 real_backward_count 61817  10.524%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.211280/  1.484136, val:  57.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4893%\n",
      "layer   2  Sparsity: 71.1363%\n",
      "layer   3  Sparsity: 69.5559%\n",
      "total_backward_count 597190 real_backward_count 62605  10.483%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.204886/  1.458677, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4316%\n",
      "layer   2  Sparsity: 71.4402%\n",
      "layer   3  Sparsity: 70.7461%\n",
      "total_backward_count 606980 real_backward_count 63450  10.453%\n",
      "fc layer 2 self.abs_max_out: 2409.0\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.209366/  1.491171, val:  62.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4849%\n",
      "layer   2  Sparsity: 71.7145%\n",
      "layer   3  Sparsity: 70.3694%\n",
      "total_backward_count 616770 real_backward_count 64252  10.417%\n",
      "fc layer 2 self.abs_max_out: 2423.0\n",
      "fc layer 1 self.abs_max_out: 5400.0\n",
      "lif layer 1 self.abs_max_v: 9389.5\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.221740/  1.485121, val:  67.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4486%\n",
      "layer   2  Sparsity: 71.2121%\n",
      "layer   3  Sparsity: 70.4308%\n",
      "total_backward_count 626560 real_backward_count 64991  10.373%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.214002/  1.504367, val:  72.08%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5012%\n",
      "layer   2  Sparsity: 71.4406%\n",
      "layer   3  Sparsity: 70.9290%\n",
      "total_backward_count 636350 real_backward_count 65751  10.333%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.199674/  1.508724, val:  75.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4626%\n",
      "layer   2  Sparsity: 71.4272%\n",
      "layer   3  Sparsity: 71.2866%\n",
      "total_backward_count 646140 real_backward_count 66514  10.294%\n",
      "fc layer 2 self.abs_max_out: 2457.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.238588/  1.509585, val:  65.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4685%\n",
      "layer   2  Sparsity: 71.5137%\n",
      "layer   3  Sparsity: 71.0122%\n",
      "total_backward_count 655930 real_backward_count 67271  10.256%\n",
      "lif layer 2 self.abs_max_v: 4331.0\n",
      "lif layer 2 self.abs_max_v: 4366.0\n",
      "fc layer 2 self.abs_max_out: 2485.0\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.218613/  1.445440, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4384%\n",
      "layer   2  Sparsity: 71.3648%\n",
      "layer   3  Sparsity: 71.1952%\n",
      "total_backward_count 665720 real_backward_count 68072  10.225%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.223348/  1.473830, val:  77.08%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4893%\n",
      "layer   2  Sparsity: 71.4475%\n",
      "layer   3  Sparsity: 71.5485%\n",
      "total_backward_count 675510 real_backward_count 68840  10.191%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.217284/  1.504893, val:  60.42%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4955%\n",
      "layer   2  Sparsity: 71.9618%\n",
      "layer   3  Sparsity: 71.0078%\n",
      "total_backward_count 685300 real_backward_count 69558  10.150%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.210029/  1.456746, val:  70.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4705%\n",
      "layer   2  Sparsity: 71.9065%\n",
      "layer   3  Sparsity: 70.3692%\n",
      "total_backward_count 695090 real_backward_count 70310  10.115%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.191130/  1.458766, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5014%\n",
      "layer   2  Sparsity: 71.8428%\n",
      "layer   3  Sparsity: 71.0038%\n",
      "total_backward_count 704880 real_backward_count 71032  10.077%\n",
      "fc layer 3 self.abs_max_out: 753.0\n",
      "fc layer 1 self.abs_max_out: 5633.0\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.189423/  1.442276, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4974%\n",
      "layer   2  Sparsity: 71.6975%\n",
      "layer   3  Sparsity: 70.9453%\n",
      "total_backward_count 714670 real_backward_count 71795  10.046%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.178766/  1.490786, val:  64.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5035%\n",
      "layer   2  Sparsity: 71.5811%\n",
      "layer   3  Sparsity: 70.7173%\n",
      "total_backward_count 724460 real_backward_count 72498  10.007%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.162276/  1.472539, val:  70.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4800%\n",
      "layer   2  Sparsity: 71.8674%\n",
      "layer   3  Sparsity: 69.8967%\n",
      "total_backward_count 734250 real_backward_count 73243   9.975%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.171566/  1.415258, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4893%\n",
      "layer   2  Sparsity: 71.3280%\n",
      "layer   3  Sparsity: 69.9973%\n",
      "total_backward_count 744040 real_backward_count 73953   9.939%\n",
      "fc layer 2 self.abs_max_out: 2487.0\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.185645/  1.475148, val:  72.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4574%\n",
      "layer   2  Sparsity: 71.0080%\n",
      "layer   3  Sparsity: 70.3436%\n",
      "total_backward_count 753830 real_backward_count 74683   9.907%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.173699/  1.473180, val:  60.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5044%\n",
      "layer   2  Sparsity: 71.0893%\n",
      "layer   3  Sparsity: 70.0238%\n",
      "total_backward_count 763620 real_backward_count 75416   9.876%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.179281/  1.432124, val:  74.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4552%\n",
      "layer   2  Sparsity: 71.0151%\n",
      "layer   3  Sparsity: 70.2663%\n",
      "total_backward_count 773410 real_backward_count 76130   9.843%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.157161/  1.440990, val:  75.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4723%\n",
      "layer   2  Sparsity: 71.1541%\n",
      "layer   3  Sparsity: 70.3895%\n",
      "total_backward_count 783200 real_backward_count 76782   9.804%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.151734/  1.449185, val:  66.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5134%\n",
      "layer   2  Sparsity: 71.4918%\n",
      "layer   3  Sparsity: 70.2819%\n",
      "total_backward_count 792990 real_backward_count 77449   9.767%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.140842/  1.453384, val:  63.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4770%\n",
      "layer   2  Sparsity: 71.6014%\n",
      "layer   3  Sparsity: 70.0126%\n",
      "total_backward_count 802780 real_backward_count 78094   9.728%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.136719/  1.418778, val:  77.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4842%\n",
      "layer   2  Sparsity: 71.5644%\n",
      "layer   3  Sparsity: 70.4441%\n",
      "total_backward_count 812570 real_backward_count 78800   9.698%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.159615/  1.440778, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4288%\n",
      "layer   2  Sparsity: 71.3555%\n",
      "layer   3  Sparsity: 71.0874%\n",
      "total_backward_count 822360 real_backward_count 79467   9.663%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.162870/  1.416238, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5186%\n",
      "layer   2  Sparsity: 71.2979%\n",
      "layer   3  Sparsity: 70.5400%\n",
      "total_backward_count 832150 real_backward_count 80158   9.633%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.150651/  1.426205, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4559%\n",
      "layer   2  Sparsity: 71.3091%\n",
      "layer   3  Sparsity: 71.2160%\n",
      "total_backward_count 841940 real_backward_count 80846   9.602%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.164701/  1.416293, val:  80.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5411%\n",
      "layer   2  Sparsity: 70.9209%\n",
      "layer   3  Sparsity: 71.1418%\n",
      "total_backward_count 851730 real_backward_count 81543   9.574%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.145168/  1.428637, val:  65.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5153%\n",
      "layer   2  Sparsity: 70.7792%\n",
      "layer   3  Sparsity: 70.8708%\n",
      "total_backward_count 861520 real_backward_count 82208   9.542%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.147154/  1.411280, val:  79.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4663%\n",
      "layer   2  Sparsity: 70.4707%\n",
      "layer   3  Sparsity: 70.2634%\n",
      "total_backward_count 871310 real_backward_count 82890   9.513%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.158976/  1.382897, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4957%\n",
      "layer   2  Sparsity: 70.3502%\n",
      "layer   3  Sparsity: 70.7206%\n",
      "total_backward_count 881100 real_backward_count 83579   9.486%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.147157/  1.443240, val:  75.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4901%\n",
      "layer   2  Sparsity: 70.2970%\n",
      "layer   3  Sparsity: 70.7183%\n",
      "total_backward_count 890890 real_backward_count 84232   9.455%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.155155/  1.404435, val:  77.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4972%\n",
      "layer   2  Sparsity: 70.4552%\n",
      "layer   3  Sparsity: 70.5137%\n",
      "total_backward_count 900680 real_backward_count 84874   9.423%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.171905/  1.406412, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4601%\n",
      "layer   2  Sparsity: 70.5114%\n",
      "layer   3  Sparsity: 70.8279%\n",
      "total_backward_count 910470 real_backward_count 85542   9.395%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.153238/  1.406938, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5016%\n",
      "layer   2  Sparsity: 70.8306%\n",
      "layer   3  Sparsity: 70.6835%\n",
      "total_backward_count 920260 real_backward_count 86167   9.363%\n",
      "fc layer 1 self.abs_max_out: 5753.0\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.134754/  1.380877, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5048%\n",
      "layer   2  Sparsity: 70.8836%\n",
      "layer   3  Sparsity: 71.0053%\n",
      "total_backward_count 930050 real_backward_count 86806   9.333%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.142781/  1.424779, val:  70.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5321%\n",
      "layer   2  Sparsity: 70.8381%\n",
      "layer   3  Sparsity: 70.8770%\n",
      "total_backward_count 939840 real_backward_count 87454   9.305%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.136299/  1.416465, val:  71.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4841%\n",
      "layer   2  Sparsity: 70.9412%\n",
      "layer   3  Sparsity: 70.3637%\n",
      "total_backward_count 949630 real_backward_count 88096   9.277%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.131974/  1.394240, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5122%\n",
      "layer   2  Sparsity: 71.2631%\n",
      "layer   3  Sparsity: 70.9037%\n",
      "total_backward_count 959420 real_backward_count 88704   9.246%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.129824/  1.456014, val:  64.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4804%\n",
      "layer   2  Sparsity: 71.3059%\n",
      "layer   3  Sparsity: 71.5708%\n",
      "total_backward_count 969210 real_backward_count 89277   9.211%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.125184/  1.379095, val:  77.50%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5338%\n",
      "layer   2  Sparsity: 70.7294%\n",
      "layer   3  Sparsity: 70.6728%\n",
      "total_backward_count 979000 real_backward_count 89867   9.179%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.122113/  1.406688, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4953%\n",
      "layer   2  Sparsity: 70.2231%\n",
      "layer   3  Sparsity: 70.7013%\n",
      "total_backward_count 988790 real_backward_count 90460   9.149%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.130665/  1.406880, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4913%\n",
      "layer   2  Sparsity: 70.6049%\n",
      "layer   3  Sparsity: 70.6998%\n",
      "total_backward_count 998580 real_backward_count 91143   9.127%\n",
      "lif layer 2 self.abs_max_v: 4411.5\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.128318/  1.357663, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5004%\n",
      "layer   2  Sparsity: 70.6142%\n",
      "layer   3  Sparsity: 70.2860%\n",
      "total_backward_count 1008370 real_backward_count 91762   9.100%\n",
      "lif layer 2 self.abs_max_v: 4415.5\n",
      "lif layer 2 self.abs_max_v: 4454.0\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.108937/  1.387430, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4717%\n",
      "layer   2  Sparsity: 70.9671%\n",
      "layer   3  Sparsity: 70.4788%\n",
      "total_backward_count 1018160 real_backward_count 92359   9.071%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.112766/  1.373603, val:  76.67%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4743%\n",
      "layer   2  Sparsity: 70.4521%\n",
      "layer   3  Sparsity: 70.5740%\n",
      "total_backward_count 1027950 real_backward_count 92963   9.044%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.101197/  1.392283, val:  76.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4741%\n",
      "layer   2  Sparsity: 70.1722%\n",
      "layer   3  Sparsity: 70.5692%\n",
      "total_backward_count 1037740 real_backward_count 93584   9.018%\n",
      "lif layer 2 self.abs_max_v: 4506.0\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.086695/  1.387698, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4840%\n",
      "layer   2  Sparsity: 70.4041%\n",
      "layer   3  Sparsity: 71.0541%\n",
      "total_backward_count 1047530 real_backward_count 94130   8.986%\n",
      "lif layer 2 self.abs_max_v: 4578.5\n",
      "fc layer 2 self.abs_max_out: 2575.0\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.088518/  1.363292, val:  75.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4920%\n",
      "layer   2  Sparsity: 70.3098%\n",
      "layer   3  Sparsity: 71.3887%\n",
      "total_backward_count 1057320 real_backward_count 94706   8.957%\n",
      "lif layer 2 self.abs_max_v: 4660.5\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.100220/  1.376084, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4842%\n",
      "layer   2  Sparsity: 70.3570%\n",
      "layer   3  Sparsity: 71.5146%\n",
      "total_backward_count 1067110 real_backward_count 95319   8.932%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.107942/  1.367021, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4417%\n",
      "layer   2  Sparsity: 70.1656%\n",
      "layer   3  Sparsity: 71.7078%\n",
      "total_backward_count 1076900 real_backward_count 95868   8.902%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.112346/  1.352158, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4916%\n",
      "layer   2  Sparsity: 70.1749%\n",
      "layer   3  Sparsity: 70.8021%\n",
      "total_backward_count 1086690 real_backward_count 96435   8.874%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.087122/  1.405009, val:  73.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4657%\n",
      "layer   2  Sparsity: 70.1376%\n",
      "layer   3  Sparsity: 71.4422%\n",
      "total_backward_count 1096480 real_backward_count 96991   8.846%\n",
      "lif layer 2 self.abs_max_v: 4800.0\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.089056/  1.377494, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4571%\n",
      "layer   2  Sparsity: 70.2661%\n",
      "layer   3  Sparsity: 71.5433%\n",
      "total_backward_count 1106270 real_backward_count 97550   8.818%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.084620/  1.332037, val:  80.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5206%\n",
      "layer   2  Sparsity: 70.1211%\n",
      "layer   3  Sparsity: 71.5655%\n",
      "total_backward_count 1116060 real_backward_count 98089   8.789%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.067830/  1.358217, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4624%\n",
      "layer   2  Sparsity: 70.4632%\n",
      "layer   3  Sparsity: 71.7518%\n",
      "total_backward_count 1125850 real_backward_count 98634   8.761%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.073468/  1.346393, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5110%\n",
      "layer   2  Sparsity: 70.7158%\n",
      "layer   3  Sparsity: 71.3198%\n",
      "total_backward_count 1135640 real_backward_count 99170   8.733%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.079413/  1.346275, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4946%\n",
      "layer   2  Sparsity: 70.7321%\n",
      "layer   3  Sparsity: 71.9274%\n",
      "total_backward_count 1145430 real_backward_count 99694   8.704%\n",
      "fc layer 1 self.abs_max_out: 5856.0\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.085149/  1.337487, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4515%\n",
      "layer   2  Sparsity: 70.2303%\n",
      "layer   3  Sparsity: 70.8090%\n",
      "total_backward_count 1155220 real_backward_count 100237   8.677%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.093037/  1.375824, val:  70.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4983%\n",
      "layer   2  Sparsity: 70.2024%\n",
      "layer   3  Sparsity: 71.1555%\n",
      "total_backward_count 1165010 real_backward_count 100780   8.651%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.077427/  1.365869, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4999%\n",
      "layer   2  Sparsity: 70.4607%\n",
      "layer   3  Sparsity: 71.3361%\n",
      "total_backward_count 1174800 real_backward_count 101308   8.623%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.090655/  1.376348, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4953%\n",
      "layer   2  Sparsity: 70.4889%\n",
      "layer   3  Sparsity: 70.9392%\n",
      "total_backward_count 1184590 real_backward_count 101807   8.594%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.076462/  1.377514, val:  74.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4792%\n",
      "layer   2  Sparsity: 70.2267%\n",
      "layer   3  Sparsity: 71.4967%\n",
      "total_backward_count 1194380 real_backward_count 102377   8.572%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.087383/  1.371576, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4674%\n",
      "layer   2  Sparsity: 70.1717%\n",
      "layer   3  Sparsity: 71.9824%\n",
      "total_backward_count 1204170 real_backward_count 102885   8.544%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.078302/  1.322630, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4998%\n",
      "layer   2  Sparsity: 70.2336%\n",
      "layer   3  Sparsity: 71.7005%\n",
      "total_backward_count 1213960 real_backward_count 103399   8.517%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.080340/  1.350970, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.5323%\n",
      "layer   2  Sparsity: 70.3394%\n",
      "layer   3  Sparsity: 71.6364%\n",
      "total_backward_count 1223750 real_backward_count 103911   8.491%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.100740/  1.378652, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.4242%\n",
      "layer   2  Sparsity: 70.1975%\n",
      "layer   3  Sparsity: 71.4396%\n",
      "total_backward_count 1233540 real_backward_count 104453   8.468%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.074170/  1.369500, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4978%\n",
      "layer   2  Sparsity: 70.1362%\n",
      "layer   3  Sparsity: 71.0104%\n",
      "total_backward_count 1243330 real_backward_count 104984   8.444%\n",
      "fc layer 1 self.abs_max_out: 5882.0\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.074451/  1.344351, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4501%\n",
      "layer   2  Sparsity: 70.1300%\n",
      "layer   3  Sparsity: 70.9680%\n",
      "total_backward_count 1253120 real_backward_count 105493   8.418%\n",
      "fc layer 3 self.abs_max_out: 758.0\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.062147/  1.323972, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4546%\n",
      "layer   2  Sparsity: 69.8947%\n",
      "layer   3  Sparsity: 71.3472%\n",
      "total_backward_count 1262910 real_backward_count 105995   8.393%\n",
      "lif layer 1 self.abs_max_v: 9561.5\n",
      "lif layer 1 self.abs_max_v: 9775.0\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.062185/  1.317200, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4792%\n",
      "layer   2  Sparsity: 70.0486%\n",
      "layer   3  Sparsity: 71.5725%\n",
      "total_backward_count 1272700 real_backward_count 106492   8.367%\n",
      "fc layer 1 self.abs_max_out: 6836.0\n",
      "lif layer 1 self.abs_max_v: 11013.5\n",
      "lif layer 1 self.abs_max_v: 11123.0\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.064538/  1.343960, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4899%\n",
      "layer   2  Sparsity: 70.4409%\n",
      "layer   3  Sparsity: 71.4623%\n",
      "total_backward_count 1282490 real_backward_count 106993   8.343%\n",
      "fc layer 1 self.abs_max_out: 6926.0\n",
      "lif layer 1 self.abs_max_v: 11177.5\n",
      "lif layer 1 self.abs_max_v: 11270.0\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.053345/  1.317051, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4777%\n",
      "layer   2  Sparsity: 70.6158%\n",
      "layer   3  Sparsity: 71.3487%\n",
      "total_backward_count 1292280 real_backward_count 107511   8.319%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.027545/  1.355740, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5048%\n",
      "layer   2  Sparsity: 70.6885%\n",
      "layer   3  Sparsity: 71.4208%\n",
      "total_backward_count 1302070 real_backward_count 107961   8.291%\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.036144/  1.322601, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5222%\n",
      "layer   2  Sparsity: 70.3549%\n",
      "layer   3  Sparsity: 71.0966%\n",
      "total_backward_count 1311860 real_backward_count 108450   8.267%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.025812/  1.316038, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5017%\n",
      "layer   2  Sparsity: 70.4877%\n",
      "layer   3  Sparsity: 70.3697%\n",
      "total_backward_count 1321650 real_backward_count 108900   8.240%\n",
      "fc layer 1 self.abs_max_out: 7065.0\n",
      "lif layer 1 self.abs_max_v: 11488.0\n",
      "lif layer 1 self.abs_max_v: 11547.0\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.040534/  1.320170, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4983%\n",
      "layer   2  Sparsity: 70.2806%\n",
      "layer   3  Sparsity: 70.4197%\n",
      "total_backward_count 1331440 real_backward_count 109454   8.221%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.032871/  1.308821, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4891%\n",
      "layer   2  Sparsity: 70.4013%\n",
      "layer   3  Sparsity: 71.4235%\n",
      "total_backward_count 1341230 real_backward_count 109919   8.195%\n",
      "lif layer 1 self.abs_max_v: 11715.5\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.052371/  1.353130, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4523%\n",
      "layer   2  Sparsity: 70.5608%\n",
      "layer   3  Sparsity: 71.3883%\n",
      "total_backward_count 1351020 real_backward_count 110381   8.170%\n",
      "fc layer 1 self.abs_max_out: 7080.0\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.056287/  1.342182, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4960%\n",
      "layer   2  Sparsity: 70.4845%\n",
      "layer   3  Sparsity: 71.0762%\n",
      "total_backward_count 1360810 real_backward_count 110843   8.145%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.051080/  1.314996, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4771%\n",
      "layer   2  Sparsity: 70.4478%\n",
      "layer   3  Sparsity: 71.1431%\n",
      "total_backward_count 1370600 real_backward_count 111315   8.122%\n",
      "fc layer 1 self.abs_max_out: 7204.0\n",
      "lif layer 1 self.abs_max_v: 11800.0\n",
      "lif layer 1 self.abs_max_v: 11839.0\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.038081/  1.331864, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5018%\n",
      "layer   2  Sparsity: 70.5582%\n",
      "layer   3  Sparsity: 71.0290%\n",
      "total_backward_count 1380390 real_backward_count 111766   8.097%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.034328/  1.294570, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5141%\n",
      "layer   2  Sparsity: 69.9449%\n",
      "layer   3  Sparsity: 71.1622%\n",
      "total_backward_count 1390180 real_backward_count 112212   8.072%\n",
      "fc layer 3 self.abs_max_out: 795.0\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.033334/  1.313827, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5185%\n",
      "layer   2  Sparsity: 70.0193%\n",
      "layer   3  Sparsity: 71.1353%\n",
      "total_backward_count 1399970 real_backward_count 112694   8.050%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.014953/  1.309638, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4674%\n",
      "layer   2  Sparsity: 70.0461%\n",
      "layer   3  Sparsity: 71.4143%\n",
      "total_backward_count 1409760 real_backward_count 113171   8.028%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.036757/  1.304588, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4416%\n",
      "layer   2  Sparsity: 69.7304%\n",
      "layer   3  Sparsity: 71.1952%\n",
      "total_backward_count 1419550 real_backward_count 113645   8.006%\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.019952/  1.302187, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4759%\n",
      "layer   2  Sparsity: 70.1634%\n",
      "layer   3  Sparsity: 71.5531%\n",
      "total_backward_count 1429340 real_backward_count 114124   7.984%\n",
      "lif layer 1 self.abs_max_v: 12073.0\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.019175/  1.323846, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4916%\n",
      "layer   2  Sparsity: 70.4558%\n",
      "layer   3  Sparsity: 71.6900%\n",
      "total_backward_count 1439130 real_backward_count 114576   7.961%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.015956/  1.309364, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4679%\n",
      "layer   2  Sparsity: 70.3484%\n",
      "layer   3  Sparsity: 70.9555%\n",
      "total_backward_count 1448920 real_backward_count 115014   7.938%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.026211/  1.305462, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5059%\n",
      "layer   2  Sparsity: 70.3637%\n",
      "layer   3  Sparsity: 71.0431%\n",
      "total_backward_count 1458710 real_backward_count 115464   7.915%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.024696/  1.297232, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5203%\n",
      "layer   2  Sparsity: 70.0128%\n",
      "layer   3  Sparsity: 71.0849%\n",
      "total_backward_count 1468500 real_backward_count 115889   7.892%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.036619/  1.343666, val:  75.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5097%\n",
      "layer   2  Sparsity: 69.8009%\n",
      "layer   3  Sparsity: 70.8782%\n",
      "total_backward_count 1478290 real_backward_count 116301   7.867%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.041009/  1.326306, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4547%\n",
      "layer   2  Sparsity: 70.0295%\n",
      "layer   3  Sparsity: 71.2554%\n",
      "total_backward_count 1488080 real_backward_count 116745   7.845%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.040001/  1.342240, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4904%\n",
      "layer   2  Sparsity: 70.4187%\n",
      "layer   3  Sparsity: 71.2089%\n",
      "total_backward_count 1497870 real_backward_count 117178   7.823%\n",
      "lif layer 2 self.abs_max_v: 4803.0\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 3 self.abs_max_out: 857.0\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.042797/  1.314315, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4610%\n",
      "layer   2  Sparsity: 70.6045%\n",
      "layer   3  Sparsity: 70.6745%\n",
      "total_backward_count 1507660 real_backward_count 117650   7.803%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.022213/  1.312137, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5003%\n",
      "layer   2  Sparsity: 70.3364%\n",
      "layer   3  Sparsity: 70.4804%\n",
      "total_backward_count 1517450 real_backward_count 118076   7.781%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.024434/  1.309277, val:  75.83%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5094%\n",
      "layer   2  Sparsity: 70.1231%\n",
      "layer   3  Sparsity: 70.4224%\n",
      "total_backward_count 1527240 real_backward_count 118520   7.760%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.000993/  1.313692, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5092%\n",
      "layer   2  Sparsity: 70.1709%\n",
      "layer   3  Sparsity: 71.0508%\n",
      "total_backward_count 1537030 real_backward_count 118975   7.741%\n",
      "fc layer 3 self.abs_max_out: 862.0\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.010688/  1.335471, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4693%\n",
      "layer   2  Sparsity: 70.3621%\n",
      "layer   3  Sparsity: 70.9825%\n",
      "total_backward_count 1546820 real_backward_count 119395   7.719%\n",
      "fc layer 2 self.abs_max_out: 2621.0\n",
      "lif layer 2 self.abs_max_v: 4869.0\n",
      "lif layer 2 self.abs_max_v: 4943.5\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.003575/  1.289876, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4645%\n",
      "layer   2  Sparsity: 70.1823%\n",
      "layer   3  Sparsity: 71.0456%\n",
      "total_backward_count 1556610 real_backward_count 119831   7.698%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.005132/  1.281274, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5354%\n",
      "layer   2  Sparsity: 70.2881%\n",
      "layer   3  Sparsity: 70.8404%\n",
      "total_backward_count 1566400 real_backward_count 120223   7.675%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.010776/  1.291364, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4963%\n",
      "layer   2  Sparsity: 70.3779%\n",
      "layer   3  Sparsity: 71.1110%\n",
      "total_backward_count 1576190 real_backward_count 120639   7.654%\n",
      "fc layer 2 self.abs_max_out: 2639.0\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.009416/  1.278173, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4812%\n",
      "layer   2  Sparsity: 70.5205%\n",
      "layer   3  Sparsity: 71.2820%\n",
      "total_backward_count 1585980 real_backward_count 121057   7.633%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.003378/  1.321732, val:  74.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4833%\n",
      "layer   2  Sparsity: 70.4551%\n",
      "layer   3  Sparsity: 71.2365%\n",
      "total_backward_count 1595770 real_backward_count 121449   7.611%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.991197/  1.252279, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4647%\n",
      "layer   2  Sparsity: 70.4873%\n",
      "layer   3  Sparsity: 70.8980%\n",
      "total_backward_count 1605560 real_backward_count 121862   7.590%\n",
      "lif layer 2 self.abs_max_v: 4954.5\n",
      "fc layer 2 self.abs_max_out: 2664.0\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.976266/  1.279652, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4975%\n",
      "layer   2  Sparsity: 70.2741%\n",
      "layer   3  Sparsity: 70.9610%\n",
      "total_backward_count 1615350 real_backward_count 122277   7.570%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.991993/  1.318704, val:  76.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4671%\n",
      "layer   2  Sparsity: 70.1762%\n",
      "layer   3  Sparsity: 70.5261%\n",
      "total_backward_count 1625140 real_backward_count 122660   7.548%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.985703/  1.318266, val:  77.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4698%\n",
      "layer   2  Sparsity: 69.7887%\n",
      "layer   3  Sparsity: 71.0200%\n",
      "total_backward_count 1634930 real_backward_count 123066   7.527%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.002675/  1.329076, val:  76.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4894%\n",
      "layer   2  Sparsity: 69.9590%\n",
      "layer   3  Sparsity: 70.5826%\n",
      "total_backward_count 1644720 real_backward_count 123467   7.507%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.993251/  1.278795, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4844%\n",
      "layer   2  Sparsity: 69.8300%\n",
      "layer   3  Sparsity: 70.9023%\n",
      "total_backward_count 1654510 real_backward_count 123855   7.486%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.983340/  1.273841, val:  80.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4661%\n",
      "layer   2  Sparsity: 69.7082%\n",
      "layer   3  Sparsity: 70.8829%\n",
      "total_backward_count 1664300 real_backward_count 124198   7.462%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.966015/  1.280475, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5333%\n",
      "layer   2  Sparsity: 69.8377%\n",
      "layer   3  Sparsity: 70.7902%\n",
      "total_backward_count 1674090 real_backward_count 124549   7.440%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.973252/  1.259507, val:  80.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4402%\n",
      "layer   2  Sparsity: 69.7577%\n",
      "layer   3  Sparsity: 70.2264%\n",
      "total_backward_count 1683880 real_backward_count 124958   7.421%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.964987/  1.278512, val:  78.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5049%\n",
      "layer   2  Sparsity: 69.7840%\n",
      "layer   3  Sparsity: 70.0901%\n",
      "total_backward_count 1693670 real_backward_count 125358   7.402%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.957274/  1.261337, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4847%\n",
      "layer   2  Sparsity: 69.9116%\n",
      "layer   3  Sparsity: 70.9454%\n",
      "total_backward_count 1703460 real_backward_count 125719   7.380%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.975595/  1.290085, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5232%\n",
      "layer   2  Sparsity: 70.1242%\n",
      "layer   3  Sparsity: 70.7267%\n",
      "total_backward_count 1713250 real_backward_count 126146   7.363%\n",
      "lif layer 1 self.abs_max_v: 12079.0\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.984668/  1.274381, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4544%\n",
      "layer   2  Sparsity: 69.8265%\n",
      "layer   3  Sparsity: 70.1166%\n",
      "total_backward_count 1723040 real_backward_count 126554   7.345%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.985919/  1.282749, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4891%\n",
      "layer   2  Sparsity: 69.7665%\n",
      "layer   3  Sparsity: 70.8014%\n",
      "total_backward_count 1732830 real_backward_count 126932   7.325%\n",
      "fc layer 1 self.abs_max_out: 7209.0\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.986133/  1.281495, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4287%\n",
      "layer   2  Sparsity: 69.8627%\n",
      "layer   3  Sparsity: 70.7031%\n",
      "total_backward_count 1742620 real_backward_count 127289   7.304%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.000630/  1.283098, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4344%\n",
      "layer   2  Sparsity: 69.9351%\n",
      "layer   3  Sparsity: 70.5281%\n",
      "total_backward_count 1752410 real_backward_count 127693   7.287%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.983171/  1.274791, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4565%\n",
      "layer   2  Sparsity: 69.8579%\n",
      "layer   3  Sparsity: 70.6860%\n",
      "total_backward_count 1762200 real_backward_count 128118   7.270%\n",
      "fc layer 1 self.abs_max_out: 7233.0\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.966638/  1.269912, val:  76.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5148%\n",
      "layer   2  Sparsity: 69.6693%\n",
      "layer   3  Sparsity: 70.5444%\n",
      "total_backward_count 1771990 real_backward_count 128498   7.252%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.979407/  1.259622, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4641%\n",
      "layer   2  Sparsity: 69.5591%\n",
      "layer   3  Sparsity: 70.4935%\n",
      "total_backward_count 1781780 real_backward_count 128874   7.233%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.959119/  1.229938, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4501%\n",
      "layer   2  Sparsity: 69.5827%\n",
      "layer   3  Sparsity: 71.2075%\n",
      "total_backward_count 1791570 real_backward_count 129230   7.213%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.955503/  1.287837, val:  73.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4418%\n",
      "layer   2  Sparsity: 69.4904%\n",
      "layer   3  Sparsity: 71.4471%\n",
      "total_backward_count 1801360 real_backward_count 129594   7.194%\n",
      "fc layer 2 self.abs_max_out: 2776.0\n",
      "lif layer 2 self.abs_max_v: 4992.5\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.952231/  1.252467, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4811%\n",
      "layer   2  Sparsity: 69.8683%\n",
      "layer   3  Sparsity: 71.5503%\n",
      "total_backward_count 1811150 real_backward_count 129965   7.176%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.956057/  1.246140, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4638%\n",
      "layer   2  Sparsity: 69.8661%\n",
      "layer   3  Sparsity: 71.0495%\n",
      "total_backward_count 1820940 real_backward_count 130346   7.158%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.943430/  1.249893, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5084%\n",
      "layer   2  Sparsity: 69.9221%\n",
      "layer   3  Sparsity: 70.5308%\n",
      "total_backward_count 1830730 real_backward_count 130725   7.141%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.946805/  1.244842, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4894%\n",
      "layer   2  Sparsity: 69.5591%\n",
      "layer   3  Sparsity: 70.5240%\n",
      "total_backward_count 1840520 real_backward_count 131064   7.121%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.929013/  1.237974, val:  80.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5178%\n",
      "layer   2  Sparsity: 69.6708%\n",
      "layer   3  Sparsity: 70.8876%\n",
      "total_backward_count 1850310 real_backward_count 131397   7.101%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.924047/  1.223806, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4987%\n",
      "layer   2  Sparsity: 69.9287%\n",
      "layer   3  Sparsity: 71.1064%\n",
      "total_backward_count 1860100 real_backward_count 131698   7.080%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.928080/  1.223620, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4984%\n",
      "layer   2  Sparsity: 69.6339%\n",
      "layer   3  Sparsity: 70.7606%\n",
      "total_backward_count 1869890 real_backward_count 132047   7.062%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.934759/  1.263521, val:  77.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4826%\n",
      "layer   2  Sparsity: 69.4171%\n",
      "layer   3  Sparsity: 70.9490%\n",
      "total_backward_count 1879680 real_backward_count 132422   7.045%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.933774/  1.215176, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4872%\n",
      "layer   2  Sparsity: 69.7354%\n",
      "layer   3  Sparsity: 71.3610%\n",
      "total_backward_count 1889470 real_backward_count 132742   7.025%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.930885/  1.275857, val:  75.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4368%\n",
      "layer   2  Sparsity: 69.5074%\n",
      "layer   3  Sparsity: 70.9826%\n",
      "total_backward_count 1899260 real_backward_count 133041   7.005%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.918625/  1.239243, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4315%\n",
      "layer   2  Sparsity: 69.9078%\n",
      "layer   3  Sparsity: 70.6200%\n",
      "total_backward_count 1909050 real_backward_count 133355   6.985%\n",
      "lif layer 1 self.abs_max_v: 12100.0\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.920863/  1.199158, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4749%\n",
      "layer   2  Sparsity: 69.7740%\n",
      "layer   3  Sparsity: 70.6337%\n",
      "total_backward_count 1918840 real_backward_count 133722   6.969%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.927091/  1.214748, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4653%\n",
      "layer   2  Sparsity: 69.7473%\n",
      "layer   3  Sparsity: 71.0559%\n",
      "total_backward_count 1928630 real_backward_count 134079   6.952%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.943483/  1.218922, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5042%\n",
      "layer   2  Sparsity: 69.8791%\n",
      "layer   3  Sparsity: 70.9095%\n",
      "total_backward_count 1938420 real_backward_count 134422   6.935%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.922151/  1.236138, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5052%\n",
      "layer   2  Sparsity: 69.9899%\n",
      "layer   3  Sparsity: 71.1280%\n",
      "total_backward_count 1948210 real_backward_count 134754   6.917%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.930027/  1.226796, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4473%\n",
      "layer   2  Sparsity: 70.0146%\n",
      "layer   3  Sparsity: 71.6623%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db0114202c846169968bdc25c629a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.93003</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.825</td></tr><tr><td>val_loss</td><td>1.2268</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-sweep-12</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m2cgd826' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m2cgd826</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_081802-m2cgd826/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fd9r7eta with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_123458-fd9r7eta</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fd9r7eta' target=\"_blank\">sparkling-sweep-16</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fd9r7eta' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fd9r7eta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251118_123507_638', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 117.0\n",
      "lif layer 1 self.abs_max_v: 117.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 400.0\n",
      "lif layer 2 self.abs_max_v: 400.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "fc layer 1 self.abs_max_out: 130.0\n",
      "lif layer 1 self.abs_max_v: 176.0\n",
      "fc layer 2 self.abs_max_out: 405.0\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "lif layer 2 self.abs_max_v: 472.0\n",
      "fc layer 1 self.abs_max_out: 163.0\n",
      "lif layer 1 self.abs_max_v: 177.5\n",
      "lif layer 2 self.abs_max_v: 604.0\n",
      "fc layer 1 self.abs_max_out: 215.0\n",
      "lif layer 1 self.abs_max_v: 215.0\n",
      "fc layer 2 self.abs_max_out: 451.0\n",
      "lif layer 2 self.abs_max_v: 637.5\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 230.0\n",
      "lif layer 1 self.abs_max_v: 314.0\n",
      "fc layer 2 self.abs_max_out: 531.0\n",
      "lif layer 2 self.abs_max_v: 645.0\n",
      "fc layer 1 self.abs_max_out: 260.0\n",
      "lif layer 1 self.abs_max_v: 390.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "lif layer 2 self.abs_max_v: 667.0\n",
      "fc layer 1 self.abs_max_out: 353.0\n",
      "lif layer 1 self.abs_max_v: 548.0\n",
      "lif layer 2 self.abs_max_v: 726.5\n",
      "lif layer 2 self.abs_max_v: 789.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 402.0\n",
      "lif layer 2 self.abs_max_v: 918.5\n",
      "fc layer 2 self.abs_max_out: 611.0\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "fc layer 1 self.abs_max_out: 482.0\n",
      "fc layer 1 self.abs_max_out: 587.0\n",
      "lif layer 1 self.abs_max_v: 587.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "fc layer 1 self.abs_max_out: 629.0\n",
      "lif layer 1 self.abs_max_v: 702.5\n",
      "lif layer 1 self.abs_max_v: 718.5\n",
      "lif layer 2 self.abs_max_v: 1014.0\n",
      "fc layer 1 self.abs_max_out: 655.0\n",
      "fc layer 1 self.abs_max_out: 768.0\n",
      "lif layer 1 self.abs_max_v: 823.0\n",
      "lif layer 1 self.abs_max_v: 991.5\n",
      "lif layer 2 self.abs_max_v: 1032.0\n",
      "lif layer 2 self.abs_max_v: 1091.0\n",
      "lif layer 2 self.abs_max_v: 1138.0\n",
      "fc layer 2 self.abs_max_out: 735.0\n",
      "lif layer 2 self.abs_max_v: 1141.0\n",
      "smallest_now_T updated: 345\n",
      "lif layer 2 self.abs_max_v: 1142.5\n",
      "lif layer 2 self.abs_max_v: 1210.5\n",
      "lif layer 2 self.abs_max_v: 1305.5\n",
      "lif layer 1 self.abs_max_v: 1052.0\n",
      "fc layer 2 self.abs_max_out: 803.0\n",
      "lif layer 1 self.abs_max_v: 1253.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "fc layer 1 self.abs_max_out: 806.0\n",
      "fc layer 3 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 1345.0\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "fc layer 2 self.abs_max_out: 909.0\n",
      "fc layer 2 self.abs_max_out: 937.0\n",
      "lif layer 2 self.abs_max_v: 1391.5\n",
      "smallest_now_T updated: 317\n",
      "lif layer 2 self.abs_max_v: 1414.5\n",
      "fc layer 1 self.abs_max_out: 865.0\n",
      "fc layer 2 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 977.0\n",
      "lif layer 2 self.abs_max_v: 1648.0\n",
      "lif layer 2 self.abs_max_v: 1713.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "lif layer 1 self.abs_max_v: 1389.0\n",
      "smallest_now_T updated: 286\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 1 self.abs_max_out: 916.0\n",
      "fc layer 1 self.abs_max_out: 1214.0\n",
      "fc layer 2 self.abs_max_out: 980.0\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "lif layer 1 self.abs_max_v: 1458.0\n",
      "lif layer 1 self.abs_max_v: 1595.0\n",
      "lif layer 2 self.abs_max_v: 1757.5\n",
      "fc layer 2 self.abs_max_out: 1067.0\n",
      "lif layer 2 self.abs_max_v: 1760.0\n",
      "fc layer 1 self.abs_max_out: 1279.0\n",
      "lif layer 1 self.abs_max_v: 1788.0\n",
      "fc layer 2 self.abs_max_out: 1171.0\n",
      "lif layer 1 self.abs_max_v: 1910.5\n",
      "lif layer 1 self.abs_max_v: 1933.5\n",
      "lif layer 2 self.abs_max_v: 1788.5\n",
      "lif layer 2 self.abs_max_v: 1793.5\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "smallest_now_T updated: 247\n",
      "lif layer 2 self.abs_max_v: 1822.0\n",
      "lif layer 2 self.abs_max_v: 1860.0\n",
      "smallest_now_T updated: 192\n",
      "lif layer 2 self.abs_max_v: 1886.0\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "lif layer 2 self.abs_max_v: 1931.5\n",
      "lif layer 2 self.abs_max_v: 1961.0\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "fc layer 2 self.abs_max_out: 1186.0\n",
      "lif layer 2 self.abs_max_v: 2128.0\n",
      "fc layer 3 self.abs_max_out: 708.0\n",
      "fc layer 3 self.abs_max_out: 718.0\n",
      "lif layer 1 self.abs_max_v: 1944.5\n",
      "fc layer 1 self.abs_max_out: 1375.0\n",
      "fc layer 2 self.abs_max_out: 1218.0\n",
      "fc layer 2 self.abs_max_out: 1222.0\n",
      "lif layer 1 self.abs_max_v: 2135.0\n",
      "lif layer 1 self.abs_max_v: 2300.5\n",
      "lif layer 1 self.abs_max_v: 2356.5\n",
      "lif layer 1 self.abs_max_v: 2466.0\n",
      "lif layer 2 self.abs_max_v: 2133.0\n",
      "fc layer 1 self.abs_max_out: 1385.0\n",
      "lif layer 1 self.abs_max_v: 2525.5\n",
      "fc layer 1 self.abs_max_out: 1421.0\n",
      "fc layer 2 self.abs_max_out: 1256.0\n",
      "lif layer 2 self.abs_max_v: 2177.5\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "fc layer 2 self.abs_max_out: 1301.0\n",
      "fc layer 2 self.abs_max_out: 1304.0\n",
      "lif layer 2 self.abs_max_v: 2321.5\n",
      "fc layer 2 self.abs_max_out: 1335.0\n",
      "lif layer 2 self.abs_max_v: 2358.5\n",
      "fc layer 1 self.abs_max_out: 1630.0\n",
      "lif layer 1 self.abs_max_v: 2604.0\n",
      "lif layer 1 self.abs_max_v: 2744.0\n",
      "lif layer 2 self.abs_max_v: 2392.5\n",
      "fc layer 2 self.abs_max_out: 1359.0\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "lif layer 2 self.abs_max_v: 2521.0\n",
      "fc layer 2 self.abs_max_out: 1436.0\n",
      "lif layer 2 self.abs_max_v: 2526.5\n",
      "fc layer 2 self.abs_max_out: 1440.0\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "fc layer 2 self.abs_max_out: 1568.0\n",
      "fc layer 1 self.abs_max_out: 1719.0\n",
      "fc layer 1 self.abs_max_out: 1751.0\n",
      "lif layer 1 self.abs_max_v: 2978.5\n",
      "lif layer 1 self.abs_max_v: 3211.5\n",
      "lif layer 2 self.abs_max_v: 2646.5\n",
      "lif layer 2 self.abs_max_v: 2701.0\n",
      "fc layer 2 self.abs_max_out: 1621.0\n",
      "lif layer 2 self.abs_max_v: 2911.5\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "lif layer 2 self.abs_max_v: 3085.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "fc layer 1 self.abs_max_out: 1784.0\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "smallest_now_T_val updated: 137\n",
      "fc layer 2 self.abs_max_out: 1666.0\n",
      "fc layer 2 self.abs_max_out: 1739.0\n",
      "fc layer 1 self.abs_max_out: 2056.0\n",
      "fc layer 1 self.abs_max_out: 2351.0\n",
      "lif layer 1 self.abs_max_v: 3379.0\n",
      "lif layer 1 self.abs_max_v: 3818.5\n",
      "lif layer 1 self.abs_max_v: 4199.5\n",
      "fc layer 1 self.abs_max_out: 2522.0\n",
      "lif layer 1 self.abs_max_v: 4622.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.460226/  1.855719, val:  32.50%, val_best:  32.50%, tr:  91.83%, tr_best:  91.83%, epoch time: 41.14 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0025%\n",
      "layer   2  Sparsity: 66.0299%\n",
      "layer   3  Sparsity: 58.7922%\n",
      "total_backward_count 4895 real_backward_count 1113  22.737%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3123.5\n",
      "fc layer 3 self.abs_max_out: 789.0\n",
      "fc layer 2 self.abs_max_out: 1769.0\n",
      "lif layer 2 self.abs_max_v: 3239.0\n",
      "fc layer 2 self.abs_max_out: 1777.0\n",
      "fc layer 2 self.abs_max_out: 1812.0\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "lif layer 2 self.abs_max_v: 3304.5\n",
      "fc layer 2 self.abs_max_out: 1875.0\n",
      "lif layer 2 self.abs_max_v: 3384.0\n",
      "fc layer 2 self.abs_max_out: 1898.0\n",
      "fc layer 2 self.abs_max_out: 1909.0\n",
      "lif layer 2 self.abs_max_v: 3522.0\n",
      "lif layer 2 self.abs_max_v: 3578.5\n",
      "fc layer 2 self.abs_max_out: 1974.0\n",
      "lif layer 2 self.abs_max_v: 3651.0\n",
      "fc layer 1 self.abs_max_out: 2810.0\n",
      "lif layer 1 self.abs_max_v: 4695.0\n",
      "lif layer 1 self.abs_max_v: 5036.5\n",
      "lif layer 1 self.abs_max_v: 5174.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.386112/  1.887004, val:  35.00%, val_best:  35.00%, tr:  91.83%, tr_best:  91.83%, epoch time: 42.36 seconds, 0.71 minutes\n",
      "layer   1  Sparsity: 92.9987%\n",
      "layer   2  Sparsity: 69.5628%\n",
      "layer   3  Sparsity: 61.7997%\n",
      "total_backward_count 9790 real_backward_count 2227  22.748%\n",
      "fc layer 2 self.abs_max_out: 1986.0\n",
      "fc layer 2 self.abs_max_out: 2129.0\n",
      "lif layer 2 self.abs_max_v: 3710.0\n",
      "lif layer 2 self.abs_max_v: 3787.0\n",
      "fc layer 3 self.abs_max_out: 839.0\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "fc layer 3 self.abs_max_out: 911.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.342431/  1.765926, val:  38.75%, val_best:  38.75%, tr:  91.32%, tr_best:  91.83%, epoch time: 41.76 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 93.0272%\n",
      "layer   2  Sparsity: 69.6580%\n",
      "layer   3  Sparsity: 63.5580%\n",
      "total_backward_count 14685 real_backward_count 3303  22.492%\n",
      "fc layer 3 self.abs_max_out: 941.0\n",
      "fc layer 3 self.abs_max_out: 969.0\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "lif layer 2 self.abs_max_v: 3811.5\n",
      "lif layer 2 self.abs_max_v: 3861.5\n",
      "fc layer 2 self.abs_max_out: 2280.0\n",
      "lif layer 2 self.abs_max_v: 4003.0\n",
      "lif layer 2 self.abs_max_v: 4190.5\n",
      "fc layer 1 self.abs_max_out: 2869.0\n",
      "fc layer 1 self.abs_max_out: 2924.0\n",
      "lif layer 1 self.abs_max_v: 5339.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.316407/  1.747467, val:  41.67%, val_best:  41.67%, tr:  90.81%, tr_best:  91.83%, epoch time: 41.76 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 92.9970%\n",
      "layer   2  Sparsity: 69.0656%\n",
      "layer   3  Sparsity: 63.4536%\n",
      "total_backward_count 19580 real_backward_count 4372  22.329%\n",
      "fc layer 2 self.abs_max_out: 2403.0\n",
      "fc layer 1 self.abs_max_out: 3153.0\n",
      "lif layer 1 self.abs_max_v: 5403.5\n",
      "fc layer 3 self.abs_max_out: 992.0\n",
      "lif layer 2 self.abs_max_v: 4195.5\n",
      "fc layer 2 self.abs_max_out: 2554.0\n",
      "fc layer 1 self.abs_max_out: 3275.0\n",
      "lif layer 1 self.abs_max_v: 5700.5\n",
      "lif layer 1 self.abs_max_v: 6118.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.285668/  1.804618, val:  29.58%, val_best:  41.67%, tr:  93.16%, tr_best:  93.16%, epoch time: 41.34 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9968%\n",
      "layer   2  Sparsity: 68.9089%\n",
      "layer   3  Sparsity: 63.1805%\n",
      "total_backward_count 24475 real_backward_count 5380  21.982%\n",
      "lif layer 2 self.abs_max_v: 4275.0\n",
      "lif layer 2 self.abs_max_v: 4572.5\n",
      "fc layer 2 self.abs_max_out: 2602.0\n",
      "lif layer 2 self.abs_max_v: 4707.0\n",
      "fc layer 2 self.abs_max_out: 2696.0\n",
      "lif layer 2 self.abs_max_v: 4888.5\n",
      "lif layer 2 self.abs_max_v: 4917.5\n",
      "fc layer 2 self.abs_max_out: 2791.0\n",
      "lif layer 2 self.abs_max_v: 5250.0\n",
      "fc layer 2 self.abs_max_out: 2859.0\n",
      "fc layer 1 self.abs_max_out: 3316.0\n",
      "fc layer 1 self.abs_max_out: 3460.0\n",
      "lif layer 1 self.abs_max_v: 6196.5\n",
      "fc layer 1 self.abs_max_out: 3525.0\n",
      "lif layer 1 self.abs_max_v: 6623.5\n",
      "fc layer 2 self.abs_max_out: 2861.0\n",
      "fc layer 2 self.abs_max_out: 2925.0\n",
      "fc layer 2 self.abs_max_out: 2974.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.264262/  1.730267, val:  41.25%, val_best:  41.67%, tr:  91.93%, tr_best:  93.16%, epoch time: 41.54 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9594%\n",
      "layer   2  Sparsity: 69.3014%\n",
      "layer   3  Sparsity: 63.8191%\n",
      "total_backward_count 29370 real_backward_count 6391  21.760%\n",
      "fc layer 3 self.abs_max_out: 1054.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.220960/  1.802069, val:  32.50%, val_best:  41.67%, tr:  91.22%, tr_best:  93.16%, epoch time: 40.87 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0250%\n",
      "layer   2  Sparsity: 68.9436%\n",
      "layer   3  Sparsity: 62.9931%\n",
      "total_backward_count 34265 real_backward_count 7462  21.777%\n",
      "lif layer 2 self.abs_max_v: 5256.5\n",
      "fc layer 2 self.abs_max_out: 2976.0\n",
      "fc layer 3 self.abs_max_out: 1147.0\n",
      "fc layer 1 self.abs_max_out: 3684.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.211812/  1.607185, val:  47.92%, val_best:  47.92%, tr:  92.75%, tr_best:  93.16%, epoch time: 41.47 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0132%\n",
      "layer   2  Sparsity: 69.7934%\n",
      "layer   3  Sparsity: 62.2199%\n",
      "total_backward_count 39160 real_backward_count 8431  21.530%\n",
      "fc layer 2 self.abs_max_out: 3023.0\n",
      "fc layer 2 self.abs_max_out: 3054.0\n",
      "fc layer 1 self.abs_max_out: 3744.0\n",
      "fc layer 1 self.abs_max_out: 3767.0\n",
      "fc layer 1 self.abs_max_out: 3922.0\n",
      "lif layer 1 self.abs_max_v: 7067.0\n",
      "lif layer 1 self.abs_max_v: 7322.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.245867/  1.628724, val:  43.75%, val_best:  47.92%, tr:  92.34%, tr_best:  93.16%, epoch time: 40.77 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0200%\n",
      "layer   2  Sparsity: 69.8386%\n",
      "layer   3  Sparsity: 63.0212%\n",
      "total_backward_count 44055 real_backward_count 9477  21.512%\n",
      "fc layer 1 self.abs_max_out: 3982.0\n",
      "fc layer 2 self.abs_max_out: 3130.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.184399/  1.699572, val:  43.33%, val_best:  47.92%, tr:  92.34%, tr_best:  93.16%, epoch time: 41.46 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9857%\n",
      "layer   2  Sparsity: 71.6066%\n",
      "layer   3  Sparsity: 63.2088%\n",
      "total_backward_count 48950 real_backward_count 10478  21.406%\n",
      "fc layer 1 self.abs_max_out: 4318.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.164393/  1.609443, val:  44.58%, val_best:  47.92%, tr:  93.26%, tr_best:  93.26%, epoch time: 41.49 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9764%\n",
      "layer   2  Sparsity: 71.0928%\n",
      "layer   3  Sparsity: 64.4027%\n",
      "total_backward_count 53845 real_backward_count 11446  21.257%\n",
      "fc layer 1 self.abs_max_out: 4344.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.203112/  1.566703, val:  47.08%, val_best:  47.92%, tr:  92.65%, tr_best:  93.26%, epoch time: 41.27 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0328%\n",
      "layer   2  Sparsity: 71.0310%\n",
      "layer   3  Sparsity: 64.1107%\n",
      "total_backward_count 58740 real_backward_count 12447  21.190%\n",
      "lif layer 2 self.abs_max_v: 5487.5\n",
      "lif layer 1 self.abs_max_v: 7540.5\n",
      "lif layer 1 self.abs_max_v: 7907.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.188782/  1.726960, val:  32.08%, val_best:  47.92%, tr:  92.85%, tr_best:  93.26%, epoch time: 41.62 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9642%\n",
      "layer   2  Sparsity: 70.0939%\n",
      "layer   3  Sparsity: 64.6854%\n",
      "total_backward_count 63635 real_backward_count 13425  21.097%\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.185217/  1.728979, val:  33.33%, val_best:  47.92%, tr:  92.54%, tr_best:  93.26%, epoch time: 41.42 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9947%\n",
      "layer   2  Sparsity: 70.4453%\n",
      "layer   3  Sparsity: 65.1755%\n",
      "total_backward_count 68530 real_backward_count 14406  21.021%\n",
      "fc layer 1 self.abs_max_out: 4831.0\n",
      "lif layer 1 self.abs_max_v: 7963.5\n",
      "lif layer 1 self.abs_max_v: 8568.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.158613/  1.660780, val:  40.00%, val_best:  47.92%, tr:  93.05%, tr_best:  93.26%, epoch time: 41.46 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0110%\n",
      "layer   2  Sparsity: 70.6384%\n",
      "layer   3  Sparsity: 64.9764%\n",
      "total_backward_count 73425 real_backward_count 15345  20.899%\n",
      "fc layer 3 self.abs_max_out: 1165.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.184253/  1.615557, val:  44.17%, val_best:  47.92%, tr:  92.44%, tr_best:  93.26%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0006%\n",
      "layer   2  Sparsity: 70.4958%\n",
      "layer   3  Sparsity: 65.0775%\n",
      "total_backward_count 78320 real_backward_count 16322  20.840%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.169366/  1.635228, val:  45.42%, val_best:  47.92%, tr:  93.46%, tr_best:  93.46%, epoch time: 41.50 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9870%\n",
      "layer   2  Sparsity: 70.2157%\n",
      "layer   3  Sparsity: 65.5806%\n",
      "total_backward_count 83215 real_backward_count 17258  20.739%\n",
      "fc layer 2 self.abs_max_out: 3151.0\n",
      "lif layer 2 self.abs_max_v: 5509.0\n",
      "lif layer 2 self.abs_max_v: 5768.5\n",
      "fc layer 2 self.abs_max_out: 3472.0\n",
      "lif layer 2 self.abs_max_v: 5772.0\n",
      "lif layer 2 self.abs_max_v: 5778.0\n",
      "lif layer 2 self.abs_max_v: 5786.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.160069/  1.590688, val:  47.50%, val_best:  47.92%, tr:  93.16%, tr_best:  93.46%, epoch time: 41.30 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0041%\n",
      "layer   2  Sparsity: 70.1916%\n",
      "layer   3  Sparsity: 64.7463%\n",
      "total_backward_count 88110 real_backward_count 18213  20.671%\n",
      "lif layer 2 self.abs_max_v: 5923.0\n",
      "lif layer 2 self.abs_max_v: 5963.5\n",
      "lif layer 2 self.abs_max_v: 6162.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.170062/  1.599311, val:  38.75%, val_best:  47.92%, tr:  93.46%, tr_best:  93.46%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0129%\n",
      "layer   2  Sparsity: 70.0660%\n",
      "layer   3  Sparsity: 64.6711%\n",
      "total_backward_count 93005 real_backward_count 19150  20.590%\n",
      "fc layer 3 self.abs_max_out: 1178.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.129348/  1.689869, val:  38.33%, val_best:  47.92%, tr:  94.18%, tr_best:  94.18%, epoch time: 41.21 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9639%\n",
      "layer   2  Sparsity: 70.3661%\n",
      "layer   3  Sparsity: 64.4804%\n",
      "total_backward_count 97900 real_backward_count 20035  20.465%\n",
      "fc layer 3 self.abs_max_out: 1243.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.153952/  1.541788, val:  49.17%, val_best:  49.17%, tr:  93.26%, tr_best:  94.18%, epoch time: 41.20 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0093%\n",
      "layer   2  Sparsity: 70.2230%\n",
      "layer   3  Sparsity: 64.1217%\n",
      "total_backward_count 102795 real_backward_count 21015  20.444%\n",
      "fc layer 3 self.abs_max_out: 1299.0\n",
      "fc layer 1 self.abs_max_out: 4989.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.140383/  1.541247, val:  50.42%, val_best:  50.42%, tr:  91.42%, tr_best:  94.18%, epoch time: 41.07 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0038%\n",
      "layer   2  Sparsity: 70.3830%\n",
      "layer   3  Sparsity: 64.4852%\n",
      "total_backward_count 107690 real_backward_count 21968  20.399%\n",
      "lif layer 2 self.abs_max_v: 6277.5\n",
      "fc layer 1 self.abs_max_out: 5031.0\n",
      "lif layer 1 self.abs_max_v: 8783.5\n",
      "fc layer 1 self.abs_max_out: 5306.0\n",
      "lif layer 1 self.abs_max_v: 9687.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.155719/  1.584877, val:  50.00%, val_best:  50.42%, tr:  92.13%, tr_best:  94.18%, epoch time: 41.71 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 93.0261%\n",
      "layer   2  Sparsity: 70.4797%\n",
      "layer   3  Sparsity: 65.1870%\n",
      "total_backward_count 112585 real_backward_count 22928  20.365%\n",
      "fc layer 3 self.abs_max_out: 1321.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.126172/  1.642025, val:  37.50%, val_best:  50.42%, tr:  94.28%, tr_best:  94.28%, epoch time: 40.94 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0019%\n",
      "layer   2  Sparsity: 70.2265%\n",
      "layer   3  Sparsity: 65.4514%\n",
      "total_backward_count 117480 real_backward_count 23858  20.308%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.092189/  1.578547, val:  39.58%, val_best:  50.42%, tr:  94.28%, tr_best:  94.28%, epoch time: 41.59 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0014%\n",
      "layer   2  Sparsity: 69.3326%\n",
      "layer   3  Sparsity: 64.8517%\n",
      "total_backward_count 122375 real_backward_count 24775  20.245%\n",
      "lif layer 2 self.abs_max_v: 6364.0\n",
      "fc layer 2 self.abs_max_out: 3547.0\n",
      "fc layer 2 self.abs_max_out: 3595.0\n",
      "fc layer 2 self.abs_max_out: 3900.0\n",
      "lif layer 2 self.abs_max_v: 6514.5\n",
      "lif layer 2 self.abs_max_v: 6633.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.113697/  1.507834, val:  48.75%, val_best:  50.42%, tr:  94.18%, tr_best:  94.28%, epoch time: 41.26 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0164%\n",
      "layer   2  Sparsity: 69.3764%\n",
      "layer   3  Sparsity: 64.9874%\n",
      "total_backward_count 127270 real_backward_count 25712  20.203%\n",
      "lif layer 2 self.abs_max_v: 6814.5\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.095351/  1.510611, val:  48.75%, val_best:  50.42%, tr:  93.67%, tr_best:  94.28%, epoch time: 41.80 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 92.9827%\n",
      "layer   2  Sparsity: 69.5132%\n",
      "layer   3  Sparsity: 66.2825%\n",
      "total_backward_count 132165 real_backward_count 26631  20.150%\n",
      "fc layer 1 self.abs_max_out: 5562.0\n",
      "lif layer 1 self.abs_max_v: 10014.5\n",
      "fc layer 2 self.abs_max_out: 4049.0\n",
      "lif layer 2 self.abs_max_v: 7018.0\n",
      "lif layer 2 self.abs_max_v: 7104.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.119082/  1.489621, val:  52.92%, val_best:  52.92%, tr:  91.93%, tr_best:  94.28%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9981%\n",
      "layer   2  Sparsity: 68.9034%\n",
      "layer   3  Sparsity: 66.1847%\n",
      "total_backward_count 137060 real_backward_count 27553  20.103%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.139340/  1.575601, val:  53.33%, val_best:  53.33%, tr:  94.69%, tr_best:  94.69%, epoch time: 41.24 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0109%\n",
      "layer   2  Sparsity: 69.0891%\n",
      "layer   3  Sparsity: 66.2892%\n",
      "total_backward_count 141955 real_backward_count 28440  20.035%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.142141/  1.627802, val:  42.50%, val_best:  53.33%, tr:  93.77%, tr_best:  94.69%, epoch time: 41.60 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0340%\n",
      "layer   2  Sparsity: 69.3700%\n",
      "layer   3  Sparsity: 66.9412%\n",
      "total_backward_count 146850 real_backward_count 29335  19.976%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.129539/  1.531716, val:  48.75%, val_best:  53.33%, tr:  94.38%, tr_best:  94.69%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0094%\n",
      "layer   2  Sparsity: 69.2874%\n",
      "layer   3  Sparsity: 66.9398%\n",
      "total_backward_count 151745 real_backward_count 30251  19.935%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.174219/  1.541521, val:  49.17%, val_best:  53.33%, tr:  93.67%, tr_best:  94.69%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0304%\n",
      "layer   2  Sparsity: 69.1030%\n",
      "layer   3  Sparsity: 67.6553%\n",
      "total_backward_count 156640 real_backward_count 31232  19.939%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.126995/  1.579929, val:  45.83%, val_best:  53.33%, tr:  93.97%, tr_best:  94.69%, epoch time: 41.03 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9587%\n",
      "layer   2  Sparsity: 69.3326%\n",
      "layer   3  Sparsity: 67.0048%\n",
      "total_backward_count 161535 real_backward_count 32131  19.891%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.112507/  1.576286, val:  45.00%, val_best:  53.33%, tr:  94.38%, tr_best:  94.69%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9879%\n",
      "layer   2  Sparsity: 69.2060%\n",
      "layer   3  Sparsity: 66.1529%\n",
      "total_backward_count 166430 real_backward_count 33055  19.861%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.067393/  1.580893, val:  42.92%, val_best:  53.33%, tr:  95.81%, tr_best:  95.81%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9387%\n",
      "layer   2  Sparsity: 69.6023%\n",
      "layer   3  Sparsity: 66.4786%\n",
      "total_backward_count 171325 real_backward_count 33865  19.767%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.082242/  1.496876, val:  57.92%, val_best:  57.92%, tr:  94.59%, tr_best:  95.81%, epoch time: 41.19 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0043%\n",
      "layer   2  Sparsity: 69.9319%\n",
      "layer   3  Sparsity: 66.1872%\n",
      "total_backward_count 176220 real_backward_count 34757  19.724%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.088885/  1.547817, val:  48.33%, val_best:  57.92%, tr:  94.69%, tr_best:  95.81%, epoch time: 40.97 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0260%\n",
      "layer   2  Sparsity: 69.3696%\n",
      "layer   3  Sparsity: 66.2025%\n",
      "total_backward_count 181115 real_backward_count 35657  19.687%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.075224/  1.528799, val:  53.33%, val_best:  57.92%, tr:  94.79%, tr_best:  95.81%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9825%\n",
      "layer   2  Sparsity: 69.5470%\n",
      "layer   3  Sparsity: 66.7270%\n",
      "total_backward_count 186010 real_backward_count 36486  19.615%\n",
      "fc layer 1 self.abs_max_out: 5706.0\n",
      "lif layer 1 self.abs_max_v: 10397.5\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.084223/  1.505455, val:  46.67%, val_best:  57.92%, tr:  94.59%, tr_best:  95.81%, epoch time: 40.91 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0176%\n",
      "layer   2  Sparsity: 69.3847%\n",
      "layer   3  Sparsity: 66.9280%\n",
      "total_backward_count 190905 real_backward_count 37345  19.562%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.103428/  1.544586, val:  47.92%, val_best:  57.92%, tr:  94.89%, tr_best:  95.81%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0359%\n",
      "layer   2  Sparsity: 69.2360%\n",
      "layer   3  Sparsity: 66.6737%\n",
      "total_backward_count 195800 real_backward_count 38206  19.513%\n",
      "fc layer 1 self.abs_max_out: 5810.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.092687/  1.526226, val:  46.67%, val_best:  57.92%, tr:  95.40%, tr_best:  95.81%, epoch time: 41.57 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9623%\n",
      "layer   2  Sparsity: 68.6547%\n",
      "layer   3  Sparsity: 67.2053%\n",
      "total_backward_count 200695 real_backward_count 39085  19.475%\n",
      "fc layer 1 self.abs_max_out: 5834.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.094186/  1.526913, val:  48.75%, val_best:  57.92%, tr:  93.87%, tr_best:  95.81%, epoch time: 41.30 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0023%\n",
      "layer   2  Sparsity: 68.7425%\n",
      "layer   3  Sparsity: 67.5829%\n",
      "total_backward_count 205590 real_backward_count 39960  19.437%\n",
      "lif layer 1 self.abs_max_v: 10772.5\n",
      "fc layer 1 self.abs_max_out: 5877.0\n",
      "fc layer 1 self.abs_max_out: 5982.0\n",
      "lif layer 2 self.abs_max_v: 7262.5\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.100341/  1.537603, val:  46.25%, val_best:  57.92%, tr:  95.40%, tr_best:  95.81%, epoch time: 41.50 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9644%\n",
      "layer   2  Sparsity: 69.3531%\n",
      "layer   3  Sparsity: 67.5615%\n",
      "total_backward_count 210485 real_backward_count 40812  19.390%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.108363/  1.591990, val:  47.92%, val_best:  57.92%, tr:  94.08%, tr_best:  95.81%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9794%\n",
      "layer   2  Sparsity: 69.3119%\n",
      "layer   3  Sparsity: 67.7925%\n",
      "total_backward_count 215380 real_backward_count 41747  19.383%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.118377/  1.478928, val:  57.92%, val_best:  57.92%, tr:  94.79%, tr_best:  95.81%, epoch time: 41.16 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0136%\n",
      "layer   2  Sparsity: 68.7774%\n",
      "layer   3  Sparsity: 67.8084%\n",
      "total_backward_count 220275 real_backward_count 42642  19.359%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.097893/  1.554412, val:  44.17%, val_best:  57.92%, tr:  94.08%, tr_best:  95.81%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9861%\n",
      "layer   2  Sparsity: 68.7180%\n",
      "layer   3  Sparsity: 68.2641%\n",
      "total_backward_count 225170 real_backward_count 43539  19.336%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.091946/  1.475493, val:  52.08%, val_best:  57.92%, tr:  94.69%, tr_best:  95.81%, epoch time: 41.58 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9775%\n",
      "layer   2  Sparsity: 68.6748%\n",
      "layer   3  Sparsity: 67.6879%\n",
      "total_backward_count 230065 real_backward_count 44430  19.312%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.080612/  1.547909, val:  45.83%, val_best:  57.92%, tr:  94.79%, tr_best:  95.81%, epoch time: 41.44 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0295%\n",
      "layer   2  Sparsity: 68.4473%\n",
      "layer   3  Sparsity: 67.2878%\n",
      "total_backward_count 234960 real_backward_count 45233  19.251%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.078139/  1.513149, val:  54.17%, val_best:  57.92%, tr:  95.30%, tr_best:  95.81%, epoch time: 41.42 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0477%\n",
      "layer   2  Sparsity: 68.6057%\n",
      "layer   3  Sparsity: 67.5705%\n",
      "total_backward_count 239855 real_backward_count 46100  19.220%\n",
      "fc layer 1 self.abs_max_out: 6133.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.131121/  1.531227, val:  49.58%, val_best:  57.92%, tr:  94.99%, tr_best:  95.81%, epoch time: 41.05 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9770%\n",
      "layer   2  Sparsity: 68.6995%\n",
      "layer   3  Sparsity: 67.9293%\n",
      "total_backward_count 244750 real_backward_count 46966  19.189%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.110935/  1.466893, val:  51.67%, val_best:  57.92%, tr:  94.69%, tr_best:  95.81%, epoch time: 41.09 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9743%\n",
      "layer   2  Sparsity: 68.3589%\n",
      "layer   3  Sparsity: 68.0642%\n",
      "total_backward_count 249645 real_backward_count 47866  19.174%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.086284/  1.501309, val:  52.92%, val_best:  57.92%, tr:  94.69%, tr_best:  95.81%, epoch time: 40.90 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9656%\n",
      "layer   2  Sparsity: 68.1943%\n",
      "layer   3  Sparsity: 68.0036%\n",
      "total_backward_count 254540 real_backward_count 48714  19.138%\n",
      "fc layer 2 self.abs_max_out: 4071.0\n",
      "fc layer 2 self.abs_max_out: 4096.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.081408/  1.490457, val:  48.33%, val_best:  57.92%, tr:  94.69%, tr_best:  95.81%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0017%\n",
      "layer   2  Sparsity: 68.6016%\n",
      "layer   3  Sparsity: 68.7112%\n",
      "total_backward_count 259435 real_backward_count 49608  19.122%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.082048/  1.480597, val:  58.75%, val_best:  58.75%, tr:  94.28%, tr_best:  95.81%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0022%\n",
      "layer   2  Sparsity: 68.4396%\n",
      "layer   3  Sparsity: 68.8807%\n",
      "total_backward_count 264330 real_backward_count 50484  19.099%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.110453/  1.516109, val:  55.83%, val_best:  58.75%, tr:  93.87%, tr_best:  95.81%, epoch time: 41.11 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0037%\n",
      "layer   2  Sparsity: 68.2601%\n",
      "layer   3  Sparsity: 69.1548%\n",
      "total_backward_count 269225 real_backward_count 51362  19.078%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.096362/  1.496870, val:  49.58%, val_best:  58.75%, tr:  93.97%, tr_best:  95.81%, epoch time: 40.96 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0199%\n",
      "layer   2  Sparsity: 68.4508%\n",
      "layer   3  Sparsity: 69.1634%\n",
      "total_backward_count 274120 real_backward_count 52253  19.062%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.087955/  1.526062, val:  50.42%, val_best:  58.75%, tr:  94.59%, tr_best:  95.81%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9879%\n",
      "layer   2  Sparsity: 68.2677%\n",
      "layer   3  Sparsity: 69.4792%\n",
      "total_backward_count 279015 real_backward_count 53087  19.027%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.096441/  1.518204, val:  52.50%, val_best:  58.75%, tr:  94.89%, tr_best:  95.81%, epoch time: 41.41 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0553%\n",
      "layer   2  Sparsity: 67.9172%\n",
      "layer   3  Sparsity: 69.5749%\n",
      "total_backward_count 283910 real_backward_count 53931  18.996%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.096229/  1.515649, val:  50.42%, val_best:  58.75%, tr:  94.99%, tr_best:  95.81%, epoch time: 41.64 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9975%\n",
      "layer   2  Sparsity: 68.1744%\n",
      "layer   3  Sparsity: 69.1460%\n",
      "total_backward_count 288805 real_backward_count 54783  18.969%\n",
      "fc layer 1 self.abs_max_out: 6466.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.065576/  1.523859, val:  50.83%, val_best:  58.75%, tr:  95.61%, tr_best:  95.81%, epoch time: 41.08 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0061%\n",
      "layer   2  Sparsity: 68.7850%\n",
      "layer   3  Sparsity: 68.6671%\n",
      "total_backward_count 293700 real_backward_count 55634  18.942%\n",
      "fc layer 1 self.abs_max_out: 6513.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.073919/  1.694302, val:  31.25%, val_best:  58.75%, tr:  94.89%, tr_best:  95.81%, epoch time: 41.02 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9883%\n",
      "layer   2  Sparsity: 68.9670%\n",
      "layer   3  Sparsity: 68.2083%\n",
      "total_backward_count 298595 real_backward_count 56512  18.926%\n",
      "fc layer 2 self.abs_max_out: 4210.0\n",
      "lif layer 1 self.abs_max_v: 11209.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.056838/  1.435356, val:  58.33%, val_best:  58.75%, tr:  95.71%, tr_best:  95.81%, epoch time: 41.21 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9382%\n",
      "layer   2  Sparsity: 69.2670%\n",
      "layer   3  Sparsity: 69.1523%\n",
      "total_backward_count 303490 real_backward_count 57350  18.897%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.090315/  1.547968, val:  49.58%, val_best:  58.75%, tr:  94.59%, tr_best:  95.81%, epoch time: 41.95 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 93.0030%\n",
      "layer   2  Sparsity: 68.8001%\n",
      "layer   3  Sparsity: 69.4354%\n",
      "total_backward_count 308385 real_backward_count 58187  18.868%\n",
      "fc layer 2 self.abs_max_out: 4231.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.082863/  1.573515, val:  42.50%, val_best:  58.75%, tr:  95.10%, tr_best:  95.81%, epoch time: 41.83 seconds, 0.70 minutes\n",
      "layer   1  Sparsity: 93.0354%\n",
      "layer   2  Sparsity: 68.7801%\n",
      "layer   3  Sparsity: 69.1594%\n",
      "total_backward_count 313280 real_backward_count 59024  18.841%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.100320/  1.518915, val:  48.75%, val_best:  58.75%, tr:  95.61%, tr_best:  95.81%, epoch time: 41.15 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0222%\n",
      "layer   2  Sparsity: 68.5532%\n",
      "layer   3  Sparsity: 68.7690%\n",
      "total_backward_count 318175 real_backward_count 59882  18.820%\n",
      "lif layer 2 self.abs_max_v: 7396.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.061133/  1.467130, val:  53.75%, val_best:  58.75%, tr:  96.32%, tr_best:  96.32%, epoch time: 40.81 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0340%\n",
      "layer   2  Sparsity: 68.0101%\n",
      "layer   3  Sparsity: 69.4468%\n",
      "total_backward_count 323070 real_backward_count 60655  18.775%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.042686/  1.441685, val:  52.50%, val_best:  58.75%, tr:  95.51%, tr_best:  96.32%, epoch time: 41.55 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9940%\n",
      "layer   2  Sparsity: 68.3550%\n",
      "layer   3  Sparsity: 69.2185%\n",
      "total_backward_count 327965 real_backward_count 61454  18.738%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.091469/  1.510198, val:  55.83%, val_best:  58.75%, tr:  94.18%, tr_best:  96.32%, epoch time: 41.33 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 92.9682%\n",
      "layer   2  Sparsity: 68.5495%\n",
      "layer   3  Sparsity: 69.7501%\n",
      "total_backward_count 332860 real_backward_count 62314  18.721%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.077866/  1.440688, val:  56.25%, val_best:  58.75%, tr:  95.40%, tr_best:  96.32%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9898%\n",
      "layer   2  Sparsity: 68.7172%\n",
      "layer   3  Sparsity: 70.0638%\n",
      "total_backward_count 337755 real_backward_count 63163  18.701%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.090007/  1.514081, val:  49.17%, val_best:  58.75%, tr:  94.79%, tr_best:  96.32%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9748%\n",
      "layer   2  Sparsity: 68.4612%\n",
      "layer   3  Sparsity: 70.1318%\n",
      "total_backward_count 342650 real_backward_count 64018  18.683%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.050583/  1.421363, val:  57.08%, val_best:  58.75%, tr:  95.20%, tr_best:  96.32%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9960%\n",
      "layer   2  Sparsity: 68.5600%\n",
      "layer   3  Sparsity: 70.0167%\n",
      "total_backward_count 347545 real_backward_count 64903  18.675%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.068947/  1.538780, val:  45.00%, val_best:  58.75%, tr:  94.69%, tr_best:  96.32%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9574%\n",
      "layer   2  Sparsity: 68.2436%\n",
      "layer   3  Sparsity: 70.3832%\n",
      "total_backward_count 352440 real_backward_count 65769  18.661%\n",
      "lif layer 1 self.abs_max_v: 11475.5\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.059105/  1.424527, val:  61.67%, val_best:  61.67%, tr:  96.02%, tr_best:  96.32%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9440%\n",
      "layer   2  Sparsity: 68.3112%\n",
      "layer   3  Sparsity: 70.1310%\n",
      "total_backward_count 357335 real_backward_count 66610  18.641%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.072314/  1.504760, val:  51.67%, val_best:  61.67%, tr:  95.40%, tr_best:  96.32%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0533%\n",
      "layer   2  Sparsity: 68.3002%\n",
      "layer   3  Sparsity: 70.4543%\n",
      "total_backward_count 362230 real_backward_count 67422  18.613%\n",
      "fc layer 1 self.abs_max_out: 6640.0\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.083507/  1.530113, val:  47.92%, val_best:  61.67%, tr:  94.89%, tr_best:  96.32%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9835%\n",
      "layer   2  Sparsity: 68.0609%\n",
      "layer   3  Sparsity: 70.3042%\n",
      "total_backward_count 367125 real_backward_count 68282  18.599%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.056735/  1.432200, val:  57.08%, val_best:  61.67%, tr:  94.28%, tr_best:  96.32%, epoch time: 41.38 seconds, 0.69 minutes\n",
      "layer   1  Sparsity: 93.0106%\n",
      "layer   2  Sparsity: 67.5967%\n",
      "layer   3  Sparsity: 70.1291%\n",
      "total_backward_count 372020 real_backward_count 69060  18.564%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.067583/  1.514061, val:  48.75%, val_best:  61.67%, tr:  96.53%, tr_best:  96.53%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9732%\n",
      "layer   2  Sparsity: 68.1456%\n",
      "layer   3  Sparsity: 70.0411%\n",
      "total_backward_count 376915 real_backward_count 69850  18.532%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.071756/  1.581951, val:  42.08%, val_best:  61.67%, tr:  95.30%, tr_best:  96.53%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0117%\n",
      "layer   2  Sparsity: 68.3672%\n",
      "layer   3  Sparsity: 69.5568%\n",
      "total_backward_count 381810 real_backward_count 70707  18.519%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.040863/  1.430372, val:  50.00%, val_best:  61.67%, tr:  96.32%, tr_best:  96.53%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0218%\n",
      "layer   2  Sparsity: 68.4519%\n",
      "layer   3  Sparsity: 70.3710%\n",
      "total_backward_count 386705 real_backward_count 71517  18.494%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.062006/  1.515640, val:  53.33%, val_best:  61.67%, tr:  94.08%, tr_best:  96.53%, epoch time: 40.66 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0187%\n",
      "layer   2  Sparsity: 67.8189%\n",
      "layer   3  Sparsity: 70.6862%\n",
      "total_backward_count 391600 real_backward_count 72311  18.466%\n",
      "fc layer 1 self.abs_max_out: 6660.0\n",
      "fc layer 1 self.abs_max_out: 6799.0\n",
      "lif layer 1 self.abs_max_v: 11567.0\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.083602/  1.445873, val:  62.08%, val_best:  62.08%, tr:  94.18%, tr_best:  96.53%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0016%\n",
      "layer   2  Sparsity: 67.5086%\n",
      "layer   3  Sparsity: 70.2265%\n",
      "total_backward_count 396495 real_backward_count 73133  18.445%\n",
      "lif layer 1 self.abs_max_v: 11671.5\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.058480/  1.573723, val:  42.50%, val_best:  62.08%, tr:  95.71%, tr_best:  96.53%, epoch time: 40.93 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0347%\n",
      "layer   2  Sparsity: 67.9738%\n",
      "layer   3  Sparsity: 69.6267%\n",
      "total_backward_count 401390 real_backward_count 73985  18.432%\n",
      "fc layer 1 self.abs_max_out: 7194.0\n",
      "lif layer 1 self.abs_max_v: 12728.5\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.078225/  1.537021, val:  47.08%, val_best:  62.08%, tr:  94.69%, tr_best:  96.53%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9960%\n",
      "layer   2  Sparsity: 68.0788%\n",
      "layer   3  Sparsity: 69.6579%\n",
      "total_backward_count 406285 real_backward_count 74863  18.426%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.047179/  1.527051, val:  49.58%, val_best:  62.08%, tr:  94.99%, tr_best:  96.53%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9657%\n",
      "layer   2  Sparsity: 67.7529%\n",
      "layer   3  Sparsity: 70.1889%\n",
      "total_backward_count 411180 real_backward_count 75714  18.414%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.041704/  1.431490, val:  53.33%, val_best:  62.08%, tr:  94.79%, tr_best:  96.53%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0082%\n",
      "layer   2  Sparsity: 67.5489%\n",
      "layer   3  Sparsity: 70.1090%\n",
      "total_backward_count 416075 real_backward_count 76579  18.405%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.036299/  1.505915, val:  51.25%, val_best:  62.08%, tr:  94.99%, tr_best:  96.53%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0407%\n",
      "layer   2  Sparsity: 67.9888%\n",
      "layer   3  Sparsity: 70.0649%\n",
      "total_backward_count 420970 real_backward_count 77388  18.383%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.026109/  1.486395, val:  52.50%, val_best:  62.08%, tr:  96.53%, tr_best:  96.53%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0661%\n",
      "layer   2  Sparsity: 68.2520%\n",
      "layer   3  Sparsity: 70.0679%\n",
      "total_backward_count 425865 real_backward_count 78160  18.353%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.072441/  1.455442, val:  56.67%, val_best:  62.08%, tr:  96.53%, tr_best:  96.53%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9385%\n",
      "layer   2  Sparsity: 68.2424%\n",
      "layer   3  Sparsity: 70.2544%\n",
      "total_backward_count 430760 real_backward_count 79006  18.341%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.037688/  1.474349, val:  53.33%, val_best:  62.08%, tr:  96.83%, tr_best:  96.83%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0426%\n",
      "layer   2  Sparsity: 68.2520%\n",
      "layer   3  Sparsity: 69.8742%\n",
      "total_backward_count 435655 real_backward_count 79812  18.320%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.044659/  1.454477, val:  54.17%, val_best:  62.08%, tr:  95.81%, tr_best:  96.83%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0596%\n",
      "layer   2  Sparsity: 68.0428%\n",
      "layer   3  Sparsity: 70.3674%\n",
      "total_backward_count 440550 real_backward_count 80621  18.300%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.052595/  1.516844, val:  44.58%, val_best:  62.08%, tr:  96.32%, tr_best:  96.83%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0129%\n",
      "layer   2  Sparsity: 68.1549%\n",
      "layer   3  Sparsity: 70.7538%\n",
      "total_backward_count 445445 real_backward_count 81463  18.288%\n",
      "fc layer 1 self.abs_max_out: 7255.0\n",
      "fc layer 1 self.abs_max_out: 7550.0\n",
      "lif layer 1 self.abs_max_v: 13067.5\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.054008/  1.426555, val:  61.67%, val_best:  62.08%, tr:  96.12%, tr_best:  96.83%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9558%\n",
      "layer   2  Sparsity: 68.0167%\n",
      "layer   3  Sparsity: 71.3447%\n",
      "total_backward_count 450340 real_backward_count 82251  18.264%\n",
      "lif layer 2 self.abs_max_v: 7411.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.044620/  1.481413, val:  55.42%, val_best:  62.08%, tr:  95.61%, tr_best:  96.83%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0188%\n",
      "layer   2  Sparsity: 68.0783%\n",
      "layer   3  Sparsity: 71.8203%\n",
      "total_backward_count 455235 real_backward_count 83033  18.240%\n",
      "lif layer 2 self.abs_max_v: 7444.5\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.045722/  1.400892, val:  62.92%, val_best:  62.92%, tr:  96.42%, tr_best:  96.83%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0094%\n",
      "layer   2  Sparsity: 67.4783%\n",
      "layer   3  Sparsity: 71.3318%\n",
      "total_backward_count 460130 real_backward_count 83824  18.217%\n",
      "lif layer 2 self.abs_max_v: 7451.5\n",
      "lif layer 2 self.abs_max_v: 7601.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.089947/  1.451436, val:  65.83%, val_best:  65.83%, tr:  94.99%, tr_best:  96.83%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0785%\n",
      "layer   2  Sparsity: 67.7787%\n",
      "layer   3  Sparsity: 72.2494%\n",
      "total_backward_count 465025 real_backward_count 84665  18.207%\n",
      "fc layer 2 self.abs_max_out: 4262.0\n",
      "fc layer 2 self.abs_max_out: 4292.0\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.084471/  1.460741, val:  57.08%, val_best:  65.83%, tr:  95.81%, tr_best:  96.83%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9905%\n",
      "layer   2  Sparsity: 67.6701%\n",
      "layer   3  Sparsity: 71.8954%\n",
      "total_backward_count 469920 real_backward_count 85480  18.190%\n",
      "fc layer 2 self.abs_max_out: 4334.0\n",
      "lif layer 2 self.abs_max_v: 7706.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.043965/  1.460523, val:  49.17%, val_best:  65.83%, tr:  96.94%, tr_best:  96.94%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0055%\n",
      "layer   2  Sparsity: 67.4858%\n",
      "layer   3  Sparsity: 71.4506%\n",
      "total_backward_count 474815 real_backward_count 86274  18.170%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.045497/  1.409719, val:  61.67%, val_best:  65.83%, tr:  96.94%, tr_best:  96.94%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0220%\n",
      "layer   2  Sparsity: 68.0596%\n",
      "layer   3  Sparsity: 70.7772%\n",
      "total_backward_count 479710 real_backward_count 87046  18.146%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.029329/  1.519631, val:  46.25%, val_best:  65.83%, tr:  95.40%, tr_best:  96.94%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0702%\n",
      "layer   2  Sparsity: 68.4423%\n",
      "layer   3  Sparsity: 70.9941%\n",
      "total_backward_count 484605 real_backward_count 87801  18.118%\n",
      "fc layer 1 self.abs_max_out: 7659.0\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.004375/  1.415831, val:  60.83%, val_best:  65.83%, tr:  96.83%, tr_best:  96.94%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0278%\n",
      "layer   2  Sparsity: 68.0617%\n",
      "layer   3  Sparsity: 70.7780%\n",
      "total_backward_count 489500 real_backward_count 88537  18.087%\n",
      "lif layer 2 self.abs_max_v: 7953.5\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.005306/  1.431361, val:  57.92%, val_best:  65.83%, tr:  97.45%, tr_best:  97.45%, epoch time: 40.59 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9966%\n",
      "layer   2  Sparsity: 67.9362%\n",
      "layer   3  Sparsity: 70.9831%\n",
      "total_backward_count 494395 real_backward_count 89333  18.069%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.034929/  1.425047, val:  59.58%, val_best:  65.83%, tr:  96.32%, tr_best:  97.45%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9511%\n",
      "layer   2  Sparsity: 67.8694%\n",
      "layer   3  Sparsity: 71.3891%\n",
      "total_backward_count 499290 real_backward_count 90134  18.052%\n",
      "lif layer 1 self.abs_max_v: 13351.5\n",
      "fc layer 2 self.abs_max_out: 4382.0\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.049668/  1.488949, val:  55.00%, val_best:  65.83%, tr:  96.12%, tr_best:  97.45%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0052%\n",
      "layer   2  Sparsity: 67.7617%\n",
      "layer   3  Sparsity: 72.4021%\n",
      "total_backward_count 504185 real_backward_count 90889  18.027%\n",
      "lif layer 1 self.abs_max_v: 13370.5\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.064737/  1.468901, val:  54.58%, val_best:  65.83%, tr:  95.51%, tr_best:  97.45%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9657%\n",
      "layer   2  Sparsity: 67.8696%\n",
      "layer   3  Sparsity: 71.5299%\n",
      "total_backward_count 509080 real_backward_count 91729  18.019%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.063667/  1.437558, val:  57.92%, val_best:  65.83%, tr:  94.99%, tr_best:  97.45%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 92.9704%\n",
      "layer   2  Sparsity: 67.6036%\n",
      "layer   3  Sparsity: 71.2442%\n",
      "total_backward_count 513975 real_backward_count 92586  18.014%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.066228/  1.405439, val:  63.75%, val_best:  65.83%, tr:  95.81%, tr_best:  97.45%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0382%\n",
      "layer   2  Sparsity: 67.5908%\n",
      "layer   3  Sparsity: 71.5744%\n",
      "total_backward_count 518870 real_backward_count 93386  17.998%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.056959/  1.495411, val:  50.42%, val_best:  65.83%, tr:  96.63%, tr_best:  97.45%, epoch time: 40.20 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0546%\n",
      "layer   2  Sparsity: 67.7525%\n",
      "layer   3  Sparsity: 71.3571%\n",
      "total_backward_count 523765 real_backward_count 94131  17.972%\n",
      "lif layer 2 self.abs_max_v: 8020.0\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.066619/  1.431434, val:  52.50%, val_best:  65.83%, tr:  96.42%, tr_best:  97.45%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0370%\n",
      "layer   2  Sparsity: 67.5984%\n",
      "layer   3  Sparsity: 72.0971%\n",
      "total_backward_count 528660 real_backward_count 94891  17.949%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.084982/  1.472243, val:  57.50%, val_best:  65.83%, tr:  96.63%, tr_best:  97.45%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0356%\n",
      "layer   2  Sparsity: 67.5478%\n",
      "layer   3  Sparsity: 72.0227%\n",
      "total_backward_count 533555 real_backward_count 95681  17.933%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.054355/  1.411584, val:  62.92%, val_best:  65.83%, tr:  96.42%, tr_best:  97.45%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0298%\n",
      "layer   2  Sparsity: 67.4644%\n",
      "layer   3  Sparsity: 72.8682%\n",
      "total_backward_count 538450 real_backward_count 96424  17.908%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.071544/  1.405741, val:  62.50%, val_best:  65.83%, tr:  96.12%, tr_best:  97.45%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0372%\n",
      "layer   2  Sparsity: 67.3143%\n",
      "layer   3  Sparsity: 72.8056%\n",
      "total_backward_count 543345 real_backward_count 97223  17.893%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.055842/  1.407480, val:  58.75%, val_best:  65.83%, tr:  95.91%, tr_best:  97.45%, epoch time: 39.55 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0076%\n",
      "layer   2  Sparsity: 67.2021%\n",
      "layer   3  Sparsity: 72.6328%\n",
      "total_backward_count 548240 real_backward_count 97961  17.868%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.055477/  1.432231, val:  55.42%, val_best:  65.83%, tr:  96.73%, tr_best:  97.45%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0121%\n",
      "layer   2  Sparsity: 67.4106%\n",
      "layer   3  Sparsity: 71.9924%\n",
      "total_backward_count 553135 real_backward_count 98745  17.852%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.043724/  1.434172, val:  55.00%, val_best:  65.83%, tr:  96.42%, tr_best:  97.45%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9874%\n",
      "layer   2  Sparsity: 67.3727%\n",
      "layer   3  Sparsity: 72.0849%\n",
      "total_backward_count 558030 real_backward_count 99491  17.829%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.041020/  1.461660, val:  50.83%, val_best:  65.83%, tr:  96.22%, tr_best:  97.45%, epoch time: 40.25 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9926%\n",
      "layer   2  Sparsity: 67.6888%\n",
      "layer   3  Sparsity: 71.6746%\n",
      "total_backward_count 562925 real_backward_count 100266  17.812%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.039560/  1.420260, val:  58.33%, val_best:  65.83%, tr:  96.12%, tr_best:  97.45%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0310%\n",
      "layer   2  Sparsity: 68.0475%\n",
      "layer   3  Sparsity: 71.3454%\n",
      "total_backward_count 567820 real_backward_count 100980  17.784%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.043741/  1.414389, val:  57.08%, val_best:  65.83%, tr:  96.02%, tr_best:  97.45%, epoch time: 40.73 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9992%\n",
      "layer   2  Sparsity: 67.8239%\n",
      "layer   3  Sparsity: 72.1019%\n",
      "total_backward_count 572715 real_backward_count 101743  17.765%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.034040/  1.394102, val:  61.25%, val_best:  65.83%, tr:  97.45%, tr_best:  97.45%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9969%\n",
      "layer   2  Sparsity: 67.4846%\n",
      "layer   3  Sparsity: 71.8303%\n",
      "total_backward_count 577610 real_backward_count 102543  17.753%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.043971/  1.396655, val:  62.08%, val_best:  65.83%, tr:  96.53%, tr_best:  97.45%, epoch time: 40.48 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0359%\n",
      "layer   2  Sparsity: 67.3683%\n",
      "layer   3  Sparsity: 72.7925%\n",
      "total_backward_count 582505 real_backward_count 103272  17.729%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.022661/  1.403043, val:  59.58%, val_best:  65.83%, tr:  96.83%, tr_best:  97.45%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 92.9780%\n",
      "layer   2  Sparsity: 67.5657%\n",
      "layer   3  Sparsity: 72.8687%\n",
      "total_backward_count 587400 real_backward_count 104060  17.715%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.025511/  1.419412, val:  61.67%, val_best:  65.83%, tr:  96.53%, tr_best:  97.45%, epoch time: 40.10 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0012%\n",
      "layer   2  Sparsity: 67.4413%\n",
      "layer   3  Sparsity: 72.9766%\n",
      "total_backward_count 592295 real_backward_count 104796  17.693%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.065565/  1.444159, val:  59.58%, val_best:  65.83%, tr:  96.42%, tr_best:  97.45%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0132%\n",
      "layer   2  Sparsity: 67.3942%\n",
      "layer   3  Sparsity: 73.5112%\n",
      "total_backward_count 597190 real_backward_count 105552  17.675%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.053632/  1.527405, val:  54.17%, val_best:  65.83%, tr:  96.32%, tr_best:  97.45%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9464%\n",
      "layer   2  Sparsity: 67.1498%\n",
      "layer   3  Sparsity: 72.4202%\n",
      "total_backward_count 602085 real_backward_count 106326  17.660%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.046316/  1.409671, val:  66.25%, val_best:  66.25%, tr:  97.04%, tr_best:  97.45%, epoch time: 40.56 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9919%\n",
      "layer   2  Sparsity: 67.0189%\n",
      "layer   3  Sparsity: 71.8675%\n",
      "total_backward_count 606980 real_backward_count 107004  17.629%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.023202/  1.347838, val:  64.58%, val_best:  66.25%, tr:  96.32%, tr_best:  97.45%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9569%\n",
      "layer   2  Sparsity: 67.4372%\n",
      "layer   3  Sparsity: 71.9885%\n",
      "total_backward_count 611875 real_backward_count 107749  17.610%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.012250/  1.395154, val:  65.42%, val_best:  66.25%, tr:  97.24%, tr_best:  97.45%, epoch time: 40.98 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9860%\n",
      "layer   2  Sparsity: 67.7234%\n",
      "layer   3  Sparsity: 72.3080%\n",
      "total_backward_count 616770 real_backward_count 108479  17.588%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.046142/  1.472477, val:  46.25%, val_best:  66.25%, tr:  96.12%, tr_best:  97.45%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9845%\n",
      "layer   2  Sparsity: 67.2744%\n",
      "layer   3  Sparsity: 71.7613%\n",
      "total_backward_count 621665 real_backward_count 109279  17.578%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.034837/  1.388412, val:  64.17%, val_best:  66.25%, tr:  96.83%, tr_best:  97.45%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9988%\n",
      "layer   2  Sparsity: 67.0285%\n",
      "layer   3  Sparsity: 71.6989%\n",
      "total_backward_count 626560 real_backward_count 109975  17.552%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.054259/  1.449193, val:  59.17%, val_best:  66.25%, tr:  95.30%, tr_best:  97.45%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9982%\n",
      "layer   2  Sparsity: 67.1462%\n",
      "layer   3  Sparsity: 72.1622%\n",
      "total_backward_count 631455 real_backward_count 110767  17.542%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.062519/  1.430315, val:  60.83%, val_best:  66.25%, tr:  97.04%, tr_best:  97.45%, epoch time: 40.13 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0126%\n",
      "layer   2  Sparsity: 67.2916%\n",
      "layer   3  Sparsity: 72.3429%\n",
      "total_backward_count 636350 real_backward_count 111515  17.524%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.067349/  1.477491, val:  45.83%, val_best:  66.25%, tr:  96.73%, tr_best:  97.45%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0216%\n",
      "layer   2  Sparsity: 67.2537%\n",
      "layer   3  Sparsity: 72.5847%\n",
      "total_backward_count 641245 real_backward_count 112303  17.513%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.022872/  1.326226, val:  65.42%, val_best:  66.25%, tr:  96.63%, tr_best:  97.45%, epoch time: 40.72 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0147%\n",
      "layer   2  Sparsity: 67.8149%\n",
      "layer   3  Sparsity: 72.2481%\n",
      "total_backward_count 646140 real_backward_count 113005  17.489%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.021049/  1.419803, val:  56.67%, val_best:  66.25%, tr:  97.34%, tr_best:  97.45%, epoch time: 40.39 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0071%\n",
      "layer   2  Sparsity: 67.7833%\n",
      "layer   3  Sparsity: 72.2381%\n",
      "total_backward_count 651035 real_backward_count 113680  17.461%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.076254/  1.473916, val:  50.83%, val_best:  66.25%, tr:  97.45%, tr_best:  97.45%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9260%\n",
      "layer   2  Sparsity: 67.2404%\n",
      "layer   3  Sparsity: 72.9344%\n",
      "total_backward_count 655930 real_backward_count 114448  17.448%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.070763/  1.537288, val:  45.00%, val_best:  66.25%, tr:  96.53%, tr_best:  97.45%, epoch time: 40.76 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0113%\n",
      "layer   2  Sparsity: 67.2613%\n",
      "layer   3  Sparsity: 72.8972%\n",
      "total_backward_count 660825 real_backward_count 115217  17.435%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.073743/  1.440757, val:  52.08%, val_best:  66.25%, tr:  96.42%, tr_best:  97.45%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0282%\n",
      "layer   2  Sparsity: 67.5343%\n",
      "layer   3  Sparsity: 72.2867%\n",
      "total_backward_count 665720 real_backward_count 116007  17.426%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.029184/  1.447628, val:  55.00%, val_best:  66.25%, tr:  96.63%, tr_best:  97.45%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0203%\n",
      "layer   2  Sparsity: 67.4620%\n",
      "layer   3  Sparsity: 71.3584%\n",
      "total_backward_count 670615 real_backward_count 116730  17.406%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.020779/  1.437308, val:  54.58%, val_best:  66.25%, tr:  97.34%, tr_best:  97.45%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 92.9714%\n",
      "layer   2  Sparsity: 67.1197%\n",
      "layer   3  Sparsity: 71.6697%\n",
      "total_backward_count 675510 real_backward_count 117402  17.380%\n",
      "fc layer 1 self.abs_max_out: 7661.0\n",
      "lif layer 1 self.abs_max_v: 13764.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.034584/  1.476848, val:  50.42%, val_best:  66.25%, tr:  97.24%, tr_best:  97.45%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0239%\n",
      "layer   2  Sparsity: 67.0347%\n",
      "layer   3  Sparsity: 71.9349%\n",
      "total_backward_count 680405 real_backward_count 118146  17.364%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.039478/  1.491676, val:  52.08%, val_best:  66.25%, tr:  96.73%, tr_best:  97.45%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9900%\n",
      "layer   2  Sparsity: 67.4272%\n",
      "layer   3  Sparsity: 72.9053%\n",
      "total_backward_count 685300 real_backward_count 118849  17.343%\n",
      "fc layer 1 self.abs_max_out: 7923.0\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.044178/  1.416552, val:  58.75%, val_best:  66.25%, tr:  96.83%, tr_best:  97.45%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0207%\n",
      "layer   2  Sparsity: 67.6879%\n",
      "layer   3  Sparsity: 72.7201%\n",
      "total_backward_count 690195 real_backward_count 119598  17.328%\n",
      "lif layer 1 self.abs_max_v: 14002.0\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.015909/  1.407644, val:  65.83%, val_best:  66.25%, tr:  97.24%, tr_best:  97.45%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0007%\n",
      "layer   2  Sparsity: 67.8594%\n",
      "layer   3  Sparsity: 72.5376%\n",
      "total_backward_count 695090 real_backward_count 120353  17.315%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.031823/  1.467453, val:  58.33%, val_best:  66.25%, tr:  97.96%, tr_best:  97.96%, epoch time: 40.16 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9928%\n",
      "layer   2  Sparsity: 67.2742%\n",
      "layer   3  Sparsity: 72.5245%\n",
      "total_backward_count 699985 real_backward_count 121074  17.297%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.041494/  1.349192, val:  68.75%, val_best:  68.75%, tr:  96.83%, tr_best:  97.96%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 92.9714%\n",
      "layer   2  Sparsity: 67.0499%\n",
      "layer   3  Sparsity: 72.4524%\n",
      "total_backward_count 704880 real_backward_count 121792  17.278%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.051377/  1.424647, val:  55.00%, val_best:  68.75%, tr:  95.91%, tr_best:  97.96%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0035%\n",
      "layer   2  Sparsity: 67.2041%\n",
      "layer   3  Sparsity: 71.6941%\n",
      "total_backward_count 709775 real_backward_count 122501  17.259%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.057519/  1.445615, val:  56.67%, val_best:  68.75%, tr:  97.14%, tr_best:  97.96%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0142%\n",
      "layer   2  Sparsity: 67.1795%\n",
      "layer   3  Sparsity: 71.9663%\n",
      "total_backward_count 714670 real_backward_count 123207  17.240%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.073278/  1.488044, val:  54.58%, val_best:  68.75%, tr:  96.83%, tr_best:  97.96%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0450%\n",
      "layer   2  Sparsity: 67.1813%\n",
      "layer   3  Sparsity: 72.6640%\n",
      "total_backward_count 719565 real_backward_count 123965  17.228%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.033858/  1.569230, val:  40.00%, val_best:  68.75%, tr:  97.04%, tr_best:  97.96%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0233%\n",
      "layer   2  Sparsity: 66.8888%\n",
      "layer   3  Sparsity: 72.8975%\n",
      "total_backward_count 724460 real_backward_count 124680  17.210%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.068922/  1.444723, val:  61.67%, val_best:  68.75%, tr:  97.45%, tr_best:  97.96%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9624%\n",
      "layer   2  Sparsity: 66.7531%\n",
      "layer   3  Sparsity: 73.4285%\n",
      "total_backward_count 729355 real_backward_count 125437  17.198%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.045350/  1.412022, val:  57.92%, val_best:  68.75%, tr:  96.53%, tr_best:  97.96%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0320%\n",
      "layer   2  Sparsity: 67.3783%\n",
      "layer   3  Sparsity: 72.5672%\n",
      "total_backward_count 734250 real_backward_count 126205  17.188%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.048947/  1.403071, val:  52.92%, val_best:  68.75%, tr:  96.53%, tr_best:  97.96%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9741%\n",
      "layer   2  Sparsity: 67.6722%\n",
      "layer   3  Sparsity: 72.6310%\n",
      "total_backward_count 739145 real_backward_count 126881  17.166%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.029610/  1.368785, val:  65.83%, val_best:  68.75%, tr:  97.34%, tr_best:  97.96%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0176%\n",
      "layer   2  Sparsity: 67.8940%\n",
      "layer   3  Sparsity: 72.1463%\n",
      "total_backward_count 744040 real_backward_count 127611  17.151%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.039407/  1.405195, val:  66.25%, val_best:  68.75%, tr:  97.65%, tr_best:  97.96%, epoch time: 40.65 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0216%\n",
      "layer   2  Sparsity: 67.4649%\n",
      "layer   3  Sparsity: 72.7626%\n",
      "total_backward_count 748935 real_backward_count 128327  17.135%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.082774/  1.356474, val:  73.33%, val_best:  73.33%, tr:  97.85%, tr_best:  97.96%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0650%\n",
      "layer   2  Sparsity: 67.5905%\n",
      "layer   3  Sparsity: 72.6983%\n",
      "total_backward_count 753830 real_backward_count 129051  17.119%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.080068/  1.463065, val:  60.42%, val_best:  73.33%, tr:  97.45%, tr_best:  97.96%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0278%\n",
      "layer   2  Sparsity: 67.6576%\n",
      "layer   3  Sparsity: 73.1319%\n",
      "total_backward_count 758725 real_backward_count 129780  17.105%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.068392/  1.446280, val:  57.92%, val_best:  73.33%, tr:  97.45%, tr_best:  97.96%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9905%\n",
      "layer   2  Sparsity: 67.1701%\n",
      "layer   3  Sparsity: 73.1165%\n",
      "total_backward_count 763620 real_backward_count 130480  17.087%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.076211/  1.520277, val:  52.92%, val_best:  73.33%, tr:  97.65%, tr_best:  97.96%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9933%\n",
      "layer   2  Sparsity: 67.1968%\n",
      "layer   3  Sparsity: 73.7167%\n",
      "total_backward_count 768515 real_backward_count 131192  17.071%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.072083/  1.415252, val:  60.00%, val_best:  73.33%, tr:  97.24%, tr_best:  97.96%, epoch time: 40.51 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0334%\n",
      "layer   2  Sparsity: 67.3272%\n",
      "layer   3  Sparsity: 73.5566%\n",
      "total_backward_count 773410 real_backward_count 131893  17.053%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.050063/  1.356137, val:  70.42%, val_best:  73.33%, tr:  97.14%, tr_best:  97.96%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9966%\n",
      "layer   2  Sparsity: 67.4083%\n",
      "layer   3  Sparsity: 72.5311%\n",
      "total_backward_count 778305 real_backward_count 132607  17.038%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.040054/  1.390866, val:  67.08%, val_best:  73.33%, tr:  97.55%, tr_best:  97.96%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9481%\n",
      "layer   2  Sparsity: 67.3234%\n",
      "layer   3  Sparsity: 72.9230%\n",
      "total_backward_count 783200 real_backward_count 133282  17.018%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.035355/  1.370860, val:  70.83%, val_best:  73.33%, tr:  97.14%, tr_best:  97.96%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0066%\n",
      "layer   2  Sparsity: 67.0636%\n",
      "layer   3  Sparsity: 72.9638%\n",
      "total_backward_count 788095 real_backward_count 133997  17.003%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.065200/  1.388773, val:  68.75%, val_best:  73.33%, tr:  97.24%, tr_best:  97.96%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0162%\n",
      "layer   2  Sparsity: 67.1959%\n",
      "layer   3  Sparsity: 73.1040%\n",
      "total_backward_count 792990 real_backward_count 134699  16.986%\n",
      "fc layer 1 self.abs_max_out: 8153.0\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.054690/  1.469223, val:  58.33%, val_best:  73.33%, tr:  97.34%, tr_best:  97.96%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0089%\n",
      "layer   2  Sparsity: 67.2474%\n",
      "layer   3  Sparsity: 73.2529%\n",
      "total_backward_count 797885 real_backward_count 135434  16.974%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.025298/  1.477488, val:  55.42%, val_best:  73.33%, tr:  97.55%, tr_best:  97.96%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9954%\n",
      "layer   2  Sparsity: 66.8849%\n",
      "layer   3  Sparsity: 72.6089%\n",
      "total_backward_count 802780 real_backward_count 136171  16.962%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.050921/  1.416146, val:  64.17%, val_best:  73.33%, tr:  96.83%, tr_best:  97.96%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9721%\n",
      "layer   2  Sparsity: 67.0522%\n",
      "layer   3  Sparsity: 72.9737%\n",
      "total_backward_count 807675 real_backward_count 136911  16.951%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.070204/  1.463409, val:  57.92%, val_best:  73.33%, tr:  96.73%, tr_best:  97.96%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9978%\n",
      "layer   2  Sparsity: 67.2923%\n",
      "layer   3  Sparsity: 73.4363%\n",
      "total_backward_count 812570 real_backward_count 137632  16.938%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.090887/  1.483825, val:  64.58%, val_best:  73.33%, tr:  97.14%, tr_best:  97.96%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0630%\n",
      "layer   2  Sparsity: 67.2178%\n",
      "layer   3  Sparsity: 74.0231%\n",
      "total_backward_count 817465 real_backward_count 138281  16.916%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.081268/  1.448749, val:  58.33%, val_best:  73.33%, tr:  96.94%, tr_best:  97.96%, epoch time: 39.61 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 92.9476%\n",
      "layer   2  Sparsity: 67.1593%\n",
      "layer   3  Sparsity: 73.1999%\n",
      "total_backward_count 822360 real_backward_count 139011  16.904%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.053063/  1.390282, val:  60.42%, val_best:  73.33%, tr:  97.04%, tr_best:  97.96%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0231%\n",
      "layer   2  Sparsity: 67.5260%\n",
      "layer   3  Sparsity: 72.5989%\n",
      "total_backward_count 827255 real_backward_count 139665  16.883%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.059703/  1.424909, val:  62.92%, val_best:  73.33%, tr:  97.45%, tr_best:  97.96%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9788%\n",
      "layer   2  Sparsity: 67.7223%\n",
      "layer   3  Sparsity: 73.8399%\n",
      "total_backward_count 832150 real_backward_count 140354  16.866%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.063317/  1.470190, val:  53.33%, val_best:  73.33%, tr:  96.83%, tr_best:  97.96%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0343%\n",
      "layer   2  Sparsity: 67.2937%\n",
      "layer   3  Sparsity: 73.4864%\n",
      "total_backward_count 837045 real_backward_count 141055  16.852%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.072228/  1.404690, val:  67.50%, val_best:  73.33%, tr:  98.37%, tr_best:  98.37%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0115%\n",
      "layer   2  Sparsity: 67.6249%\n",
      "layer   3  Sparsity: 73.9080%\n",
      "total_backward_count 841940 real_backward_count 141738  16.835%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.079729/  1.435919, val:  64.17%, val_best:  73.33%, tr:  97.55%, tr_best:  98.37%, epoch time: 39.57 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0187%\n",
      "layer   2  Sparsity: 67.8637%\n",
      "layer   3  Sparsity: 74.2857%\n",
      "total_backward_count 846835 real_backward_count 142453  16.822%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.056628/  1.402766, val:  68.33%, val_best:  73.33%, tr:  97.14%, tr_best:  98.37%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0227%\n",
      "layer   2  Sparsity: 67.6103%\n",
      "layer   3  Sparsity: 74.6281%\n",
      "total_backward_count 851730 real_backward_count 143126  16.804%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.043542/  1.423408, val:  62.92%, val_best:  73.33%, tr:  97.75%, tr_best:  98.37%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9877%\n",
      "layer   2  Sparsity: 67.7284%\n",
      "layer   3  Sparsity: 74.1925%\n",
      "total_backward_count 856625 real_backward_count 143839  16.791%\n",
      "fc layer 2 self.abs_max_out: 4409.0\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.050385/  1.357113, val:  72.92%, val_best:  73.33%, tr:  97.14%, tr_best:  98.37%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9804%\n",
      "layer   2  Sparsity: 67.7594%\n",
      "layer   3  Sparsity: 73.6290%\n",
      "total_backward_count 861520 real_backward_count 144546  16.778%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.025935/  1.412918, val:  57.92%, val_best:  73.33%, tr:  97.24%, tr_best:  98.37%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0458%\n",
      "layer   2  Sparsity: 67.9162%\n",
      "layer   3  Sparsity: 74.0369%\n",
      "total_backward_count 866415 real_backward_count 145267  16.766%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.044914/  1.442004, val:  66.67%, val_best:  73.33%, tr:  97.96%, tr_best:  98.37%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9741%\n",
      "layer   2  Sparsity: 67.8139%\n",
      "layer   3  Sparsity: 73.8491%\n",
      "total_backward_count 871310 real_backward_count 145936  16.749%\n",
      "fc layer 2 self.abs_max_out: 4445.0\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.055482/  1.496189, val:  55.00%, val_best:  73.33%, tr:  97.24%, tr_best:  98.37%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0334%\n",
      "layer   2  Sparsity: 67.1150%\n",
      "layer   3  Sparsity: 73.0687%\n",
      "total_backward_count 876205 real_backward_count 146636  16.735%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.056585/  1.443573, val:  59.58%, val_best:  73.33%, tr:  97.34%, tr_best:  98.37%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 93.0267%\n",
      "layer   2  Sparsity: 67.2004%\n",
      "layer   3  Sparsity: 71.9274%\n",
      "total_backward_count 881100 real_backward_count 147318  16.720%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.028849/  1.385767, val:  65.00%, val_best:  73.33%, tr:  97.85%, tr_best:  98.37%, epoch time: 40.50 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9538%\n",
      "layer   2  Sparsity: 67.2944%\n",
      "layer   3  Sparsity: 71.3849%\n",
      "total_backward_count 885995 real_backward_count 147991  16.703%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.027919/  1.439081, val:  64.58%, val_best:  73.33%, tr:  97.45%, tr_best:  98.37%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9766%\n",
      "layer   2  Sparsity: 67.6121%\n",
      "layer   3  Sparsity: 71.5431%\n",
      "total_backward_count 890890 real_backward_count 148678  16.689%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.028445/  1.465474, val:  55.83%, val_best:  73.33%, tr:  98.06%, tr_best:  98.37%, epoch time: 40.19 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9770%\n",
      "layer   2  Sparsity: 67.7395%\n",
      "layer   3  Sparsity: 71.7897%\n",
      "total_backward_count 895785 real_backward_count 149323  16.670%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.014986/  1.524470, val:  49.17%, val_best:  73.33%, tr:  97.34%, tr_best:  98.37%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9709%\n",
      "layer   2  Sparsity: 67.3557%\n",
      "layer   3  Sparsity: 72.8499%\n",
      "total_backward_count 900680 real_backward_count 149976  16.651%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.030358/  1.393384, val:  61.25%, val_best:  73.33%, tr:  97.96%, tr_best:  98.37%, epoch time: 40.67 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9569%\n",
      "layer   2  Sparsity: 67.0754%\n",
      "layer   3  Sparsity: 72.5888%\n",
      "total_backward_count 905575 real_backward_count 150699  16.641%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.020955/  1.370691, val:  66.67%, val_best:  73.33%, tr:  98.16%, tr_best:  98.37%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0164%\n",
      "layer   2  Sparsity: 67.2298%\n",
      "layer   3  Sparsity: 72.4975%\n",
      "total_backward_count 910470 real_backward_count 151369  16.625%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.042739/  1.418332, val:  61.67%, val_best:  73.33%, tr:  97.24%, tr_best:  98.37%, epoch time: 40.70 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0091%\n",
      "layer   2  Sparsity: 67.1360%\n",
      "layer   3  Sparsity: 72.8993%\n",
      "total_backward_count 915365 real_backward_count 152035  16.609%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.033680/  1.449992, val:  54.58%, val_best:  73.33%, tr:  96.83%, tr_best:  98.37%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0299%\n",
      "layer   2  Sparsity: 66.8879%\n",
      "layer   3  Sparsity: 72.9794%\n",
      "total_backward_count 920260 real_backward_count 152736  16.597%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.032795/  1.435226, val:  62.50%, val_best:  73.33%, tr:  96.73%, tr_best:  98.37%, epoch time: 40.15 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9840%\n",
      "layer   2  Sparsity: 66.5471%\n",
      "layer   3  Sparsity: 73.2681%\n",
      "total_backward_count 925155 real_backward_count 153456  16.587%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.017735/  1.432903, val:  62.08%, val_best:  73.33%, tr:  97.65%, tr_best:  98.37%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9832%\n",
      "layer   2  Sparsity: 66.8425%\n",
      "layer   3  Sparsity: 73.2139%\n",
      "total_backward_count 930050 real_backward_count 154128  16.572%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.012528/  1.418968, val:  59.17%, val_best:  73.33%, tr:  97.85%, tr_best:  98.37%, epoch time: 40.34 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0274%\n",
      "layer   2  Sparsity: 67.0507%\n",
      "layer   3  Sparsity: 72.3623%\n",
      "total_backward_count 934945 real_backward_count 154805  16.558%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.017486/  1.468757, val:  50.00%, val_best:  73.33%, tr:  97.24%, tr_best:  98.37%, epoch time: 40.44 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 93.0205%\n",
      "layer   2  Sparsity: 67.0784%\n",
      "layer   3  Sparsity: 71.5515%\n",
      "total_backward_count 939840 real_backward_count 155480  16.543%\n",
      "fc layer 1 self.abs_max_out: 8184.0\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.044420/  1.437832, val:  66.25%, val_best:  73.33%, tr:  97.96%, tr_best:  98.37%, epoch time: 40.60 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9971%\n",
      "layer   2  Sparsity: 67.1149%\n",
      "layer   3  Sparsity: 71.9312%\n",
      "total_backward_count 944735 real_backward_count 156153  16.529%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.032351/  1.469386, val:  55.83%, val_best:  73.33%, tr:  98.67%, tr_best:  98.67%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9386%\n",
      "layer   2  Sparsity: 67.1967%\n",
      "layer   3  Sparsity: 71.0726%\n",
      "total_backward_count 949630 real_backward_count 156803  16.512%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.051134/  1.351528, val:  67.08%, val_best:  73.33%, tr:  97.75%, tr_best:  98.67%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9704%\n",
      "layer   2  Sparsity: 67.2033%\n",
      "layer   3  Sparsity: 71.5361%\n",
      "total_backward_count 954525 real_backward_count 157486  16.499%\n",
      "fc layer 1 self.abs_max_out: 8495.0\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.060162/  1.458653, val:  55.42%, val_best:  73.33%, tr:  98.26%, tr_best:  98.67%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0241%\n",
      "layer   2  Sparsity: 67.6385%\n",
      "layer   3  Sparsity: 71.4352%\n",
      "total_backward_count 959420 real_backward_count 158176  16.487%\n",
      "fc layer 2 self.abs_max_out: 4547.0\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.029497/  1.358626, val:  63.33%, val_best:  73.33%, tr:  97.85%, tr_best:  98.67%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9932%\n",
      "layer   2  Sparsity: 67.6509%\n",
      "layer   3  Sparsity: 72.1103%\n",
      "total_backward_count 964315 real_backward_count 158814  16.469%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.988920/  1.358713, val:  65.83%, val_best:  73.33%, tr:  97.45%, tr_best:  98.67%, epoch time: 40.80 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 92.9567%\n",
      "layer   2  Sparsity: 67.6731%\n",
      "layer   3  Sparsity: 72.7388%\n",
      "total_backward_count 969210 real_backward_count 159472  16.454%\n",
      "lif layer 1 self.abs_max_v: 14065.5\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.969151/  1.386792, val:  59.58%, val_best:  73.33%, tr:  97.14%, tr_best:  98.67%, epoch time: 40.45 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 92.9683%\n",
      "layer   2  Sparsity: 67.6083%\n",
      "layer   3  Sparsity: 72.8436%\n",
      "total_backward_count 974105 real_backward_count 160116  16.437%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.991081/  1.405936, val:  55.83%, val_best:  73.33%, tr:  97.65%, tr_best:  98.67%, epoch time: 40.71 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 93.0076%\n",
      "layer   2  Sparsity: 67.6274%\n",
      "layer   3  Sparsity: 73.1838%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a333cf09d364529ae069bf2ad0550cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÇ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÇ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97651</td></tr><tr><td>tr_epoch_loss</td><td>0.99108</td></tr><tr><td>val_acc_best</td><td>0.73333</td></tr><tr><td>val_acc_now</td><td>0.55833</td></tr><tr><td>val_loss</td><td>1.40594</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-sweep-16</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fd9r7eta' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fd9r7eta</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_123458-fd9r7eta/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g5fz36b5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_145107-g5fz36b5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g5fz36b5' target=\"_blank\">dulcet-sweep-18</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g5fz36b5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g5fz36b5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251118_145115_377', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 50000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = dac77cc348b2d880ae59906e26f08f17\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 139\n",
      "fc layer 1 self.abs_max_out: 918.0\n",
      "lif layer 1 self.abs_max_v: 918.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1040.0\n",
      "lif layer 1 self.abs_max_v: 1499.0\n",
      "fc layer 2 self.abs_max_out: 475.0\n",
      "lif layer 2 self.abs_max_v: 475.0\n",
      "fc layer 1 self.abs_max_out: 1108.0\n",
      "lif layer 2 self.abs_max_v: 585.0\n",
      "fc layer 1 self.abs_max_out: 1460.0\n",
      "fc layer 2 self.abs_max_out: 611.0\n",
      "lif layer 2 self.abs_max_v: 704.5\n",
      "fc layer 1 self.abs_max_out: 2552.0\n",
      "lif layer 1 self.abs_max_v: 2552.0\n",
      "fc layer 2 self.abs_max_out: 833.0\n",
      "lif layer 2 self.abs_max_v: 1109.5\n",
      "smallest_now_T updated: 125\n",
      "fc layer 2 self.abs_max_out: 849.0\n",
      "fc layer 2 self.abs_max_out: 886.0\n",
      "lif layer 2 self.abs_max_v: 1178.0\n",
      "fc layer 3 self.abs_max_out: 66.0\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "lif layer 2 self.abs_max_v: 1487.5\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 1 self.abs_max_out: 5214.0\n",
      "lif layer 1 self.abs_max_v: 5214.0\n",
      "fc layer 2 self.abs_max_out: 1415.0\n",
      "lif layer 2 self.abs_max_v: 1843.0\n",
      "fc layer 1 self.abs_max_out: 5785.0\n",
      "lif layer 1 self.abs_max_v: 5785.0\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "lif layer 2 self.abs_max_v: 2121.5\n",
      "smallest_now_T updated: 94\n",
      "fc layer 1 self.abs_max_out: 6060.0\n",
      "lif layer 1 self.abs_max_v: 6060.0\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "fc layer 1 self.abs_max_out: 6804.0\n",
      "lif layer 1 self.abs_max_v: 6804.0\n",
      "fc layer 2 self.abs_max_out: 1761.0\n",
      "lif layer 2 self.abs_max_v: 2377.5\n",
      "lif layer 2 self.abs_max_v: 2447.0\n",
      "lif layer 2 self.abs_max_v: 2511.0\n",
      "fc layer 2 self.abs_max_out: 2542.0\n",
      "lif layer 2 self.abs_max_v: 2542.0\n",
      "lif layer 2 self.abs_max_v: 2726.0\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "lif layer 2 self.abs_max_v: 2960.0\n",
      "fc layer 3 self.abs_max_out: 522.0\n",
      "fc layer 2 self.abs_max_out: 2996.0\n",
      "lif layer 2 self.abs_max_v: 2996.0\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "lif layer 2 self.abs_max_v: 3047.0\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "lif layer 2 self.abs_max_v: 3191.5\n",
      "fc layer 3 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 3304.0\n",
      "lif layer 2 self.abs_max_v: 3359.0\n",
      "lif layer 2 self.abs_max_v: 3563.5\n",
      "lif layer 2 self.abs_max_v: 3650.0\n",
      "fc layer 3 self.abs_max_out: 703.0\n",
      "fc layer 2 self.abs_max_out: 2998.0\n",
      "smallest_now_T updated: 79\n",
      "fc layer 2 self.abs_max_out: 3638.0\n",
      "lif layer 2 self.abs_max_v: 3870.5\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "fc layer 3 self.abs_max_out: 821.0\n",
      "fc layer 1 self.abs_max_out: 8187.0\n",
      "lif layer 1 self.abs_max_v: 8187.0\n",
      "lif layer 2 self.abs_max_v: 4320.0\n",
      "lif layer 2 self.abs_max_v: 4647.0\n",
      "fc layer 2 self.abs_max_out: 3645.0\n",
      "fc layer 2 self.abs_max_out: 3725.0\n",
      "fc layer 2 self.abs_max_out: 3886.0\n",
      "fc layer 2 self.abs_max_out: 3918.0\n",
      "lif layer 2 self.abs_max_v: 4655.0\n",
      "fc layer 2 self.abs_max_out: 3976.0\n",
      "fc layer 2 self.abs_max_out: 4074.0\n",
      "fc layer 3 self.abs_max_out: 1020.0\n",
      "smallest_now_T updated: 73\n",
      "fc layer 2 self.abs_max_out: 4171.0\n",
      "fc layer 2 self.abs_max_out: 4421.0\n",
      "fc layer 3 self.abs_max_out: 1079.0\n",
      "fc layer 2 self.abs_max_out: 4767.0\n",
      "lif layer 2 self.abs_max_v: 4767.0\n",
      "lif layer 2 self.abs_max_v: 5285.0\n",
      "fc layer 3 self.abs_max_out: 1081.0\n",
      "fc layer 2 self.abs_max_out: 4782.0\n",
      "fc layer 2 self.abs_max_out: 5053.0\n",
      "fc layer 2 self.abs_max_out: 5109.0\n",
      "fc layer 3 self.abs_max_out: 1144.0\n",
      "fc layer 2 self.abs_max_out: 5212.0\n",
      "fc layer 2 self.abs_max_out: 5216.0\n",
      "smallest_now_T updated: 65\n",
      "fc layer 2 self.abs_max_out: 5223.0\n",
      "fc layer 2 self.abs_max_out: 5286.0\n",
      "lif layer 2 self.abs_max_v: 5286.0\n",
      "fc layer 1 self.abs_max_out: 8306.0\n",
      "lif layer 1 self.abs_max_v: 8306.0\n",
      "fc layer 2 self.abs_max_out: 5620.0\n",
      "lif layer 2 self.abs_max_v: 5620.0\n",
      "fc layer 1 self.abs_max_out: 8487.0\n",
      "lif layer 1 self.abs_max_v: 8487.0\n",
      "lif layer 1 self.abs_max_v: 8536.0\n",
      "fc layer 2 self.abs_max_out: 5821.0\n",
      "lif layer 2 self.abs_max_v: 5821.0\n",
      "lif layer 1 self.abs_max_v: 8905.5\n",
      "lif layer 2 self.abs_max_v: 5839.5\n",
      "lif layer 1 self.abs_max_v: 9318.5\n",
      "fc layer 3 self.abs_max_out: 1202.0\n",
      "lif layer 1 self.abs_max_v: 9642.0\n",
      "fc layer 3 self.abs_max_out: 1214.0\n",
      "smallest_now_T updated: 56\n",
      "smallest_now_T updated: 43\n",
      "lif layer 2 self.abs_max_v: 5842.5\n",
      "lif layer 1 self.abs_max_v: 10195.5\n",
      "fc layer 1 self.abs_max_out: 8926.0\n",
      "fc layer 3 self.abs_max_out: 1408.0\n",
      "lif layer 1 self.abs_max_v: 10243.5\n",
      "lif layer 1 self.abs_max_v: 10812.5\n",
      "lif layer 1 self.abs_max_v: 10965.5\n",
      "lif layer 1 self.abs_max_v: 11384.0\n",
      "lif layer 1 self.abs_max_v: 12252.0\n",
      "lif layer 2 self.abs_max_v: 6338.5\n",
      "lif layer 1 self.abs_max_v: 13572.0\n",
      "lif layer 2 self.abs_max_v: 6656.5\n",
      "fc layer 1 self.abs_max_out: 10299.0\n",
      "fc layer 3 self.abs_max_out: 1496.0\n",
      "lif layer 1 self.abs_max_v: 13710.0\n",
      "lif layer 1 self.abs_max_v: 14171.5\n",
      "fc layer 1 self.abs_max_out: 10688.0\n",
      "lif layer 2 self.abs_max_v: 6881.0\n",
      "lif layer 2 self.abs_max_v: 6891.5\n",
      "lif layer 2 self.abs_max_v: 7055.0\n",
      "lif layer 1 self.abs_max_v: 14241.5\n",
      "lif layer 1 self.abs_max_v: 14796.5\n",
      "lif layer 2 self.abs_max_v: 7149.0\n",
      "lif layer 2 self.abs_max_v: 7163.0\n",
      "lif layer 2 self.abs_max_v: 7511.5\n",
      "lif layer 2 self.abs_max_v: 7576.0\n",
      "fc layer 3 self.abs_max_out: 1535.0\n",
      "lif layer 1 self.abs_max_v: 15295.0\n",
      "lif layer 2 self.abs_max_v: 7687.5\n",
      "lif layer 1 self.abs_max_v: 15565.5\n",
      "lif layer 1 self.abs_max_v: 15714.5\n",
      "lif layer 2 self.abs_max_v: 8099.0\n",
      "lif layer 2 self.abs_max_v: 9416.5\n",
      "fc layer 1 self.abs_max_out: 10772.0\n",
      "fc layer 1 self.abs_max_out: 11603.0\n",
      "lif layer 1 self.abs_max_v: 15763.5\n",
      "lif layer 1 self.abs_max_v: 16506.0\n",
      "lif layer 1 self.abs_max_v: 17989.0\n",
      "smallest_now_T_val updated: 129\n",
      "smallest_now_T_val updated: 106\n",
      "smallest_now_T_val updated: 104\n",
      "smallest_now_T_val updated: 102\n",
      "smallest_now_T_val updated: 85\n",
      "smallest_now_T_val updated: 29\n",
      "lif layer 1 self.abs_max_v: 18174.0\n",
      "lif layer 1 self.abs_max_v: 18731.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.839580/  1.967292, val:  33.75%, val_best:  33.75%, tr:  86.01%, tr_best:  86.01%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8491%\n",
      "layer   2  Sparsity: 73.9919%\n",
      "layer   3  Sparsity: 82.0655%\n",
      "total_backward_count 4895 real_backward_count 1519  31.032%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 19122.5\n",
      "fc layer 3 self.abs_max_out: 1563.0\n",
      "fc layer 3 self.abs_max_out: 1587.0\n",
      "fc layer 3 self.abs_max_out: 1727.0\n",
      "fc layer 3 self.abs_max_out: 1758.0\n",
      "fc layer 3 self.abs_max_out: 1797.0\n",
      "lif layer 1 self.abs_max_v: 19928.0\n",
      "lif layer 1 self.abs_max_v: 20357.0\n",
      "fc layer 1 self.abs_max_out: 11895.0\n",
      "fc layer 2 self.abs_max_out: 6170.0\n",
      "fc layer 1 self.abs_max_out: 13522.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.787440/  1.988646, val:  42.50%, val_best:  42.50%, tr:  87.95%, tr_best:  87.95%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8379%\n",
      "layer   2  Sparsity: 77.5178%\n",
      "layer   3  Sparsity: 84.6655%\n",
      "total_backward_count 9790 real_backward_count 2867  29.285%\n",
      "lif layer 1 self.abs_max_v: 20758.5\n",
      "lif layer 1 self.abs_max_v: 21809.5\n",
      "lif layer 1 self.abs_max_v: 22020.0\n",
      "lif layer 1 self.abs_max_v: 22902.0\n",
      "lif layer 1 self.abs_max_v: 23497.0\n",
      "lif layer 1 self.abs_max_v: 24188.5\n",
      "fc layer 1 self.abs_max_out: 14464.0\n",
      "lif layer 1 self.abs_max_v: 24925.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.886377/  2.049749, val:  42.08%, val_best:  42.50%, tr:  87.54%, tr_best:  87.95%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9140%\n",
      "layer   2  Sparsity: 79.0999%\n",
      "layer   3  Sparsity: 87.3683%\n",
      "total_backward_count 14685 real_backward_count 4264  29.036%\n",
      "fc layer 1 self.abs_max_out: 14536.0\n",
      "lif layer 1 self.abs_max_v: 25935.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.908715/  2.003452, val:  36.25%, val_best:  42.50%, tr:  88.46%, tr_best:  88.46%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8813%\n",
      "layer   2  Sparsity: 79.2328%\n",
      "layer   3  Sparsity: 87.6414%\n",
      "total_backward_count 19580 real_backward_count 5579  28.493%\n",
      "fc layer 1 self.abs_max_out: 16419.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.932248/  2.032931, val:  48.33%, val_best:  48.33%, tr:  88.15%, tr_best:  88.46%, epoch time: 40.42 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8118%\n",
      "layer   2  Sparsity: 79.4243%\n",
      "layer   3  Sparsity: 88.3700%\n",
      "total_backward_count 24475 real_backward_count 6876  28.094%\n",
      "fc layer 1 self.abs_max_out: 16865.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.949807/  2.032299, val:  48.33%, val_best:  48.33%, tr:  88.87%, tr_best:  88.87%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8913%\n",
      "layer   2  Sparsity: 79.1039%\n",
      "layer   3  Sparsity: 88.4346%\n",
      "total_backward_count 29370 real_backward_count 8174  27.831%\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.964987/  2.102874, val:  36.67%, val_best:  48.33%, tr:  87.64%, tr_best:  88.87%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8583%\n",
      "layer   2  Sparsity: 80.4660%\n",
      "layer   3  Sparsity: 89.3282%\n",
      "total_backward_count 34265 real_backward_count 9420  27.492%\n",
      "lif layer 1 self.abs_max_v: 26557.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.958672/  2.062685, val:  50.00%, val_best:  50.00%, tr:  89.58%, tr_best:  89.58%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 79.2680%\n",
      "layer   3  Sparsity: 88.9508%\n",
      "total_backward_count 39160 real_backward_count 10659  27.219%\n",
      "lif layer 1 self.abs_max_v: 28194.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.974640/  2.059865, val:  48.33%, val_best:  50.00%, tr:  88.66%, tr_best:  89.58%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8470%\n",
      "layer   2  Sparsity: 80.0517%\n",
      "layer   3  Sparsity: 89.5188%\n",
      "total_backward_count 44055 real_backward_count 12013  27.268%\n",
      "lif layer 1 self.abs_max_v: 28360.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.971931/  2.128286, val:  34.58%, val_best:  50.00%, tr:  89.27%, tr_best:  89.58%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8945%\n",
      "layer   2  Sparsity: 79.4409%\n",
      "layer   3  Sparsity: 88.9438%\n",
      "total_backward_count 48950 real_backward_count 13246  27.060%\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.935446/  2.093645, val:  40.42%, val_best:  50.00%, tr:  87.33%, tr_best:  89.58%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8241%\n",
      "layer   2  Sparsity: 79.2260%\n",
      "layer   3  Sparsity: 88.5626%\n",
      "total_backward_count 53845 real_backward_count 14527  26.979%\n",
      "lif layer 2 self.abs_max_v: 10115.0\n",
      "fc layer 1 self.abs_max_out: 17135.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.940289/  2.045299, val:  50.83%, val_best:  50.83%, tr:  89.07%, tr_best:  89.58%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8326%\n",
      "layer   2  Sparsity: 78.6418%\n",
      "layer   3  Sparsity: 88.5432%\n",
      "total_backward_count 58740 real_backward_count 15720  26.762%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.976793/  2.079916, val:  33.33%, val_best:  50.83%, tr:  85.70%, tr_best:  89.58%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8258%\n",
      "layer   2  Sparsity: 79.0048%\n",
      "layer   3  Sparsity: 89.0554%\n",
      "total_backward_count 63635 real_backward_count 16983  26.688%\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.984296/  2.126101, val:  34.17%, val_best:  50.83%, tr:  88.36%, tr_best:  89.58%, epoch time: 40.29 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8427%\n",
      "layer   2  Sparsity: 78.9818%\n",
      "layer   3  Sparsity: 89.2768%\n",
      "total_backward_count 68530 real_backward_count 18216  26.581%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.976317/  2.080310, val:  36.25%, val_best:  50.83%, tr:  88.66%, tr_best:  89.58%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8979%\n",
      "layer   2  Sparsity: 79.9457%\n",
      "layer   3  Sparsity: 88.9643%\n",
      "total_backward_count 73425 real_backward_count 19429  26.461%\n",
      "fc layer 1 self.abs_max_out: 17837.0\n",
      "lif layer 1 self.abs_max_v: 28786.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.976920/  2.084601, val:  43.33%, val_best:  50.83%, tr:  88.25%, tr_best:  89.58%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8514%\n",
      "layer   2  Sparsity: 80.0829%\n",
      "layer   3  Sparsity: 88.8425%\n",
      "total_backward_count 78320 real_backward_count 20677  26.401%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.973901/  2.069401, val:  42.92%, val_best:  50.83%, tr:  88.15%, tr_best:  89.58%, epoch time: 39.73 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8359%\n",
      "layer   2  Sparsity: 79.6361%\n",
      "layer   3  Sparsity: 88.5581%\n",
      "total_backward_count 83215 real_backward_count 21910  26.329%\n",
      "fc layer 1 self.abs_max_out: 18467.0\n",
      "lif layer 1 self.abs_max_v: 30256.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.984578/  2.115190, val:  44.58%, val_best:  50.83%, tr:  87.95%, tr_best:  89.58%, epoch time: 40.53 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8467%\n",
      "layer   2  Sparsity: 81.0415%\n",
      "layer   3  Sparsity: 89.2078%\n",
      "total_backward_count 88110 real_backward_count 23180  26.308%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  2.013297/  2.080701, val:  52.08%, val_best:  52.08%, tr:  84.98%, tr_best:  89.58%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 80.3664%\n",
      "layer   3  Sparsity: 90.2203%\n",
      "total_backward_count 93005 real_backward_count 24532  26.377%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  2.019085/  2.155374, val:  26.67%, val_best:  52.08%, tr:  87.33%, tr_best:  89.58%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8833%\n",
      "layer   2  Sparsity: 79.9282%\n",
      "layer   3  Sparsity: 90.0043%\n",
      "total_backward_count 97900 real_backward_count 25827  26.381%\n",
      "fc layer 2 self.abs_max_out: 6248.0\n",
      "lif layer 2 self.abs_max_v: 10545.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  2.010113/  2.171272, val:  41.25%, val_best:  52.08%, tr:  86.72%, tr_best:  89.58%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8434%\n",
      "layer   2  Sparsity: 79.2169%\n",
      "layer   3  Sparsity: 89.8217%\n",
      "total_backward_count 102795 real_backward_count 27089  26.352%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  2.015695/  2.123186, val:  50.42%, val_best:  52.08%, tr:  85.09%, tr_best:  89.58%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 80.1287%\n",
      "layer   3  Sparsity: 89.6139%\n",
      "total_backward_count 107690 real_backward_count 28403  26.375%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.979154/  2.044566, val:  47.08%, val_best:  52.08%, tr:  86.62%, tr_best:  89.58%, epoch time: 39.65 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8557%\n",
      "layer   2  Sparsity: 79.4650%\n",
      "layer   3  Sparsity: 88.1151%\n",
      "total_backward_count 112585 real_backward_count 29693  26.374%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.953125/  2.085366, val:  54.17%, val_best:  54.17%, tr:  89.48%, tr_best:  89.58%, epoch time: 39.83 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 80.5879%\n",
      "layer   3  Sparsity: 88.1418%\n",
      "total_backward_count 117480 real_backward_count 30925  26.324%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  2.004998/  2.085911, val:  45.42%, val_best:  54.17%, tr:  87.74%, tr_best:  89.58%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8199%\n",
      "layer   2  Sparsity: 80.0168%\n",
      "layer   3  Sparsity: 88.7604%\n",
      "total_backward_count 122375 real_backward_count 32225  26.333%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.975723/  2.083706, val:  53.75%, val_best:  54.17%, tr:  86.62%, tr_best:  89.58%, epoch time: 39.74 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8771%\n",
      "layer   2  Sparsity: 80.1747%\n",
      "layer   3  Sparsity: 88.1645%\n",
      "total_backward_count 127270 real_backward_count 33561  26.370%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.982709/  2.111871, val:  47.92%, val_best:  54.17%, tr:  86.93%, tr_best:  89.58%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8770%\n",
      "layer   2  Sparsity: 81.2918%\n",
      "layer   3  Sparsity: 89.1751%\n",
      "total_backward_count 132165 real_backward_count 34876  26.388%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.963836/  2.032729, val:  55.42%, val_best:  55.42%, tr:  88.56%, tr_best:  89.58%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8544%\n",
      "layer   2  Sparsity: 80.3616%\n",
      "layer   3  Sparsity: 88.1443%\n",
      "total_backward_count 137060 real_backward_count 36108  26.345%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.988418/  2.109927, val:  43.75%, val_best:  55.42%, tr:  87.84%, tr_best:  89.58%, epoch time: 40.37 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8781%\n",
      "layer   2  Sparsity: 80.2410%\n",
      "layer   3  Sparsity: 89.4998%\n",
      "total_backward_count 141955 real_backward_count 37393  26.341%\n",
      "fc layer 1 self.abs_max_out: 18795.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.977693/  2.087047, val:  49.17%, val_best:  55.42%, tr:  89.07%, tr_best:  89.58%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8778%\n",
      "layer   2  Sparsity: 80.7121%\n",
      "layer   3  Sparsity: 89.0895%\n",
      "total_backward_count 146850 real_backward_count 38694  26.349%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.985988/  2.110593, val:  52.92%, val_best:  55.42%, tr:  85.70%, tr_best:  89.58%, epoch time: 40.74 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8190%\n",
      "layer   2  Sparsity: 80.8740%\n",
      "layer   3  Sparsity: 89.4836%\n",
      "total_backward_count 151745 real_backward_count 40067  26.404%\n",
      "lif layer 1 self.abs_max_v: 30686.5\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.997095/  2.097439, val:  49.17%, val_best:  55.42%, tr:  87.64%, tr_best:  89.58%, epoch time: 39.63 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8897%\n",
      "layer   2  Sparsity: 81.0126%\n",
      "layer   3  Sparsity: 89.4054%\n",
      "total_backward_count 156640 real_backward_count 41360  26.404%\n",
      "lif layer 1 self.abs_max_v: 31418.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  2.004833/  2.093559, val:  43.75%, val_best:  55.42%, tr:  86.11%, tr_best:  89.58%, epoch time: 40.40 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8318%\n",
      "layer   2  Sparsity: 81.9361%\n",
      "layer   3  Sparsity: 89.1383%\n",
      "total_backward_count 161535 real_backward_count 42620  26.384%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  2.001225/  2.124336, val:  40.42%, val_best:  55.42%, tr:  86.52%, tr_best:  89.58%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8477%\n",
      "layer   2  Sparsity: 81.5245%\n",
      "layer   3  Sparsity: 89.2924%\n",
      "total_backward_count 166430 real_backward_count 43898  26.376%\n",
      "fc layer 2 self.abs_max_out: 6439.0\n",
      "lif layer 2 self.abs_max_v: 10909.5\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.988157/  2.072563, val:  48.75%, val_best:  55.42%, tr:  86.52%, tr_best:  89.58%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9184%\n",
      "layer   2  Sparsity: 80.6267%\n",
      "layer   3  Sparsity: 88.9267%\n",
      "total_backward_count 171325 real_backward_count 45200  26.383%\n",
      "fc layer 2 self.abs_max_out: 6479.0\n",
      "lif layer 2 self.abs_max_v: 11004.5\n",
      "lif layer 2 self.abs_max_v: 11528.5\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  2.003199/  2.132443, val:  47.50%, val_best:  55.42%, tr:  87.23%, tr_best:  89.58%, epoch time: 39.91 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8496%\n",
      "layer   2  Sparsity: 80.0817%\n",
      "layer   3  Sparsity: 89.7719%\n",
      "total_backward_count 176220 real_backward_count 46515  26.396%\n",
      "lif layer 1 self.abs_max_v: 31475.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.988318/  2.094909, val:  40.83%, val_best:  55.42%, tr:  88.76%, tr_best:  89.58%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9181%\n",
      "layer   2  Sparsity: 80.8674%\n",
      "layer   3  Sparsity: 88.4509%\n",
      "total_backward_count 181115 real_backward_count 47731  26.354%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.989229/  2.143262, val:  41.67%, val_best:  55.42%, tr:  88.66%, tr_best:  89.58%, epoch time: 39.88 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8904%\n",
      "layer   2  Sparsity: 81.0789%\n",
      "layer   3  Sparsity: 88.5327%\n",
      "total_backward_count 186010 real_backward_count 48975  26.329%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.991379/  2.122221, val:  37.50%, val_best:  55.42%, tr:  88.25%, tr_best:  89.58%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8601%\n",
      "layer   2  Sparsity: 80.8518%\n",
      "layer   3  Sparsity: 88.0403%\n",
      "total_backward_count 190905 real_backward_count 50307  26.352%\n",
      "fc layer 1 self.abs_max_out: 19034.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.964492/  2.099252, val:  34.17%, val_best:  55.42%, tr:  91.62%, tr_best:  91.62%, epoch time: 39.51 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8740%\n",
      "layer   2  Sparsity: 80.0629%\n",
      "layer   3  Sparsity: 87.4758%\n",
      "total_backward_count 195800 real_backward_count 51438  26.271%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.917916/  2.069928, val:  45.00%, val_best:  55.42%, tr:  88.87%, tr_best:  91.62%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8879%\n",
      "layer   2  Sparsity: 80.6984%\n",
      "layer   3  Sparsity: 87.1508%\n",
      "total_backward_count 200695 real_backward_count 52655  26.236%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.975892/  2.114558, val:  44.58%, val_best:  55.42%, tr:  86.62%, tr_best:  91.62%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8772%\n",
      "layer   2  Sparsity: 79.9878%\n",
      "layer   3  Sparsity: 88.3245%\n",
      "total_backward_count 205590 real_backward_count 53900  26.217%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.982356/  2.093862, val:  45.00%, val_best:  55.42%, tr:  87.03%, tr_best:  91.62%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8626%\n",
      "layer   2  Sparsity: 80.7921%\n",
      "layer   3  Sparsity: 88.0937%\n",
      "total_backward_count 210485 real_backward_count 55182  26.217%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.992905/  2.135283, val:  42.50%, val_best:  55.42%, tr:  87.13%, tr_best:  91.62%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8516%\n",
      "layer   2  Sparsity: 81.4864%\n",
      "layer   3  Sparsity: 88.6087%\n",
      "total_backward_count 215380 real_backward_count 56476  26.222%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.988551/  2.096248, val:  40.42%, val_best:  55.42%, tr:  86.72%, tr_best:  91.62%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9153%\n",
      "layer   2  Sparsity: 81.5819%\n",
      "layer   3  Sparsity: 88.9779%\n",
      "total_backward_count 220275 real_backward_count 57727  26.207%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.984107/  2.119855, val:  49.17%, val_best:  55.42%, tr:  87.03%, tr_best:  91.62%, epoch time: 40.50 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8325%\n",
      "layer   2  Sparsity: 82.1963%\n",
      "layer   3  Sparsity: 88.2039%\n",
      "total_backward_count 225170 real_backward_count 58984  26.195%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.992612/  2.129335, val:  47.92%, val_best:  55.42%, tr:  88.05%, tr_best:  91.62%, epoch time: 40.57 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8490%\n",
      "layer   2  Sparsity: 81.4148%\n",
      "layer   3  Sparsity: 88.6499%\n",
      "total_backward_count 230065 real_backward_count 60237  26.183%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.967656/  2.125222, val:  30.00%, val_best:  55.42%, tr:  88.25%, tr_best:  91.62%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8545%\n",
      "layer   2  Sparsity: 81.4065%\n",
      "layer   3  Sparsity: 87.5356%\n",
      "total_backward_count 234960 real_backward_count 61528  26.187%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.945590/  2.027954, val:  45.83%, val_best:  55.42%, tr:  88.87%, tr_best:  91.62%, epoch time: 40.83 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8681%\n",
      "layer   2  Sparsity: 81.0233%\n",
      "layer   3  Sparsity: 86.6548%\n",
      "total_backward_count 239855 real_backward_count 62762  26.167%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.938741/  2.093178, val:  45.83%, val_best:  55.42%, tr:  88.15%, tr_best:  91.62%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8446%\n",
      "layer   2  Sparsity: 81.7035%\n",
      "layer   3  Sparsity: 86.8143%\n",
      "total_backward_count 244750 real_backward_count 64043  26.167%\n",
      "fc layer 2 self.abs_max_out: 6486.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.931085/  2.076273, val:  42.50%, val_best:  55.42%, tr:  87.13%, tr_best:  91.62%, epoch time: 40.61 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9149%\n",
      "layer   2  Sparsity: 80.6158%\n",
      "layer   3  Sparsity: 86.7857%\n",
      "total_backward_count 249645 real_backward_count 65288  26.152%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.937257/  2.102618, val:  30.42%, val_best:  55.42%, tr:  89.07%, tr_best:  91.62%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8718%\n",
      "layer   2  Sparsity: 80.9943%\n",
      "layer   3  Sparsity: 87.3302%\n",
      "total_backward_count 254540 real_backward_count 66542  26.142%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.937577/  2.096611, val:  34.17%, val_best:  55.42%, tr:  87.13%, tr_best:  91.62%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8335%\n",
      "layer   2  Sparsity: 81.6650%\n",
      "layer   3  Sparsity: 87.3836%\n",
      "total_backward_count 259435 real_backward_count 67822  26.142%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.957681/  2.075911, val:  52.92%, val_best:  55.42%, tr:  88.66%, tr_best:  91.62%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8604%\n",
      "layer   2  Sparsity: 81.9716%\n",
      "layer   3  Sparsity: 88.2304%\n",
      "total_backward_count 264330 real_backward_count 69058  26.126%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.978095/  2.137099, val:  43.33%, val_best:  55.42%, tr:  86.41%, tr_best:  91.62%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8794%\n",
      "layer   2  Sparsity: 81.7884%\n",
      "layer   3  Sparsity: 88.6678%\n",
      "total_backward_count 269225 real_backward_count 70375  26.140%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.948517/  2.074641, val:  37.08%, val_best:  55.42%, tr:  85.80%, tr_best:  91.62%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8278%\n",
      "layer   2  Sparsity: 80.9536%\n",
      "layer   3  Sparsity: 87.6175%\n",
      "total_backward_count 274120 real_backward_count 71691  26.153%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.941150/  2.118488, val:  40.42%, val_best:  55.42%, tr:  87.64%, tr_best:  91.62%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 79.8895%\n",
      "layer   3  Sparsity: 88.3644%\n",
      "total_backward_count 279015 real_backward_count 73017  26.170%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.965079/  2.038966, val:  46.25%, val_best:  55.42%, tr:  87.74%, tr_best:  91.62%, epoch time: 39.45 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8060%\n",
      "layer   2  Sparsity: 80.7581%\n",
      "layer   3  Sparsity: 87.4331%\n",
      "total_backward_count 283910 real_backward_count 74288  26.166%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.926192/  2.046669, val:  38.33%, val_best:  55.42%, tr:  89.17%, tr_best:  91.62%, epoch time: 39.47 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8818%\n",
      "layer   2  Sparsity: 81.3058%\n",
      "layer   3  Sparsity: 86.3824%\n",
      "total_backward_count 288805 real_backward_count 75484  26.137%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.916954/  2.070897, val:  47.50%, val_best:  55.42%, tr:  89.27%, tr_best:  91.62%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8368%\n",
      "layer   2  Sparsity: 81.3209%\n",
      "layer   3  Sparsity: 86.7709%\n",
      "total_backward_count 293700 real_backward_count 76682  26.109%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.961385/  2.088680, val:  35.00%, val_best:  55.42%, tr:  88.56%, tr_best:  91.62%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 80.3797%\n",
      "layer   3  Sparsity: 87.3505%\n",
      "total_backward_count 298595 real_backward_count 77954  26.107%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.934508/  2.115965, val:  50.83%, val_best:  55.42%, tr:  88.15%, tr_best:  91.62%, epoch time: 40.01 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8696%\n",
      "layer   2  Sparsity: 81.3783%\n",
      "layer   3  Sparsity: 87.7381%\n",
      "total_backward_count 303490 real_backward_count 79193  26.094%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.976927/  2.112660, val:  36.25%, val_best:  55.42%, tr:  87.44%, tr_best:  91.62%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8453%\n",
      "layer   2  Sparsity: 80.7171%\n",
      "layer   3  Sparsity: 87.9413%\n",
      "total_backward_count 308385 real_backward_count 80448  26.087%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.959893/  2.105429, val:  32.08%, val_best:  55.42%, tr:  88.15%, tr_best:  91.62%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.7822%\n",
      "layer   2  Sparsity: 80.5299%\n",
      "layer   3  Sparsity: 87.5364%\n",
      "total_backward_count 313280 real_backward_count 81663  26.067%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.952598/  2.066266, val:  39.58%, val_best:  55.42%, tr:  89.68%, tr_best:  91.62%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8521%\n",
      "layer   2  Sparsity: 80.4466%\n",
      "layer   3  Sparsity: 87.6001%\n",
      "total_backward_count 318175 real_backward_count 82886  26.050%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.943875/  2.082733, val:  26.25%, val_best:  55.42%, tr:  89.68%, tr_best:  91.62%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 80.5263%\n",
      "layer   3  Sparsity: 86.5381%\n",
      "total_backward_count 323070 real_backward_count 84054  26.017%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.952604/  2.063951, val:  46.25%, val_best:  55.42%, tr:  88.36%, tr_best:  91.62%, epoch time: 39.87 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9290%\n",
      "layer   2  Sparsity: 80.9541%\n",
      "layer   3  Sparsity: 88.4091%\n",
      "total_backward_count 327965 real_backward_count 85344  26.022%\n",
      "fc layer 2 self.abs_max_out: 6608.0\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.991772/  2.100012, val:  47.08%, val_best:  55.42%, tr:  87.33%, tr_best:  91.62%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8769%\n",
      "layer   2  Sparsity: 80.1320%\n",
      "layer   3  Sparsity: 88.7530%\n",
      "total_backward_count 332860 real_backward_count 86631  26.026%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.958315/  2.045061, val:  58.33%, val_best:  58.33%, tr:  89.17%, tr_best:  91.62%, epoch time: 40.22 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 80.9468%\n",
      "layer   3  Sparsity: 87.8475%\n",
      "total_backward_count 337755 real_backward_count 87872  26.016%\n",
      "lif layer 1 self.abs_max_v: 31736.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.936557/  2.135506, val:  27.50%, val_best:  58.33%, tr:  89.17%, tr_best:  91.62%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8243%\n",
      "layer   2  Sparsity: 80.7900%\n",
      "layer   3  Sparsity: 87.0473%\n",
      "total_backward_count 342650 real_backward_count 89113  26.007%\n",
      "lif layer 1 self.abs_max_v: 32195.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.907648/  2.051795, val:  54.58%, val_best:  58.33%, tr:  91.32%, tr_best:  91.62%, epoch time: 40.99 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8597%\n",
      "layer   2  Sparsity: 80.6465%\n",
      "layer   3  Sparsity: 86.2842%\n",
      "total_backward_count 347545 real_backward_count 90281  25.977%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.952769/  2.109049, val:  40.00%, val_best:  58.33%, tr:  86.41%, tr_best:  91.62%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8668%\n",
      "layer   2  Sparsity: 80.6459%\n",
      "layer   3  Sparsity: 87.6311%\n",
      "total_backward_count 352440 real_backward_count 91560  25.979%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.952436/  2.076759, val:  37.08%, val_best:  58.33%, tr:  88.05%, tr_best:  91.62%, epoch time: 40.95 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 80.4590%\n",
      "layer   3  Sparsity: 87.0112%\n",
      "total_backward_count 357335 real_backward_count 92854  25.985%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.945497/  2.047757, val:  47.50%, val_best:  58.33%, tr:  88.36%, tr_best:  91.62%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8231%\n",
      "layer   2  Sparsity: 80.4573%\n",
      "layer   3  Sparsity: 86.9761%\n",
      "total_backward_count 362230 real_backward_count 94098  25.977%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.924385/  2.070589, val:  44.58%, val_best:  58.33%, tr:  88.97%, tr_best:  91.62%, epoch time: 40.49 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8892%\n",
      "layer   2  Sparsity: 80.2473%\n",
      "layer   3  Sparsity: 86.7432%\n",
      "total_backward_count 367125 real_backward_count 95351  25.972%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.923187/  2.075400, val:  47.92%, val_best:  58.33%, tr:  88.66%, tr_best:  91.62%, epoch time: 39.97 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8237%\n",
      "layer   2  Sparsity: 80.1340%\n",
      "layer   3  Sparsity: 86.2945%\n",
      "total_backward_count 372020 real_backward_count 96580  25.961%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.902882/  2.027864, val:  57.08%, val_best:  58.33%, tr:  89.79%, tr_best:  91.62%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8917%\n",
      "layer   2  Sparsity: 80.6454%\n",
      "layer   3  Sparsity: 86.2948%\n",
      "total_backward_count 376915 real_backward_count 97787  25.944%\n",
      "lif layer 2 self.abs_max_v: 11557.0\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.914254/  2.057224, val:  41.25%, val_best:  58.33%, tr:  88.87%, tr_best:  91.62%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9032%\n",
      "layer   2  Sparsity: 80.0799%\n",
      "layer   3  Sparsity: 87.1255%\n",
      "total_backward_count 381810 real_backward_count 98994  25.928%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.937508/  2.018937, val:  54.17%, val_best:  58.33%, tr:  87.84%, tr_best:  91.62%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8504%\n",
      "layer   2  Sparsity: 79.3155%\n",
      "layer   3  Sparsity: 86.9402%\n",
      "total_backward_count 386705 real_backward_count 100267  25.929%\n",
      "lif layer 2 self.abs_max_v: 11823.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.882951/  2.063705, val:  41.25%, val_best:  58.33%, tr:  89.89%, tr_best:  91.62%, epoch time: 40.27 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8860%\n",
      "layer   2  Sparsity: 79.7910%\n",
      "layer   3  Sparsity: 86.6167%\n",
      "total_backward_count 391600 real_backward_count 101459  25.909%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.929953/  2.029606, val:  42.50%, val_best:  58.33%, tr:  87.33%, tr_best:  91.62%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8665%\n",
      "layer   2  Sparsity: 80.5801%\n",
      "layer   3  Sparsity: 86.7157%\n",
      "total_backward_count 396495 real_backward_count 102741  25.912%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.918693/  2.111453, val:  45.83%, val_best:  58.33%, tr:  86.31%, tr_best:  91.62%, epoch time: 39.96 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8784%\n",
      "layer   2  Sparsity: 81.1801%\n",
      "layer   3  Sparsity: 88.0577%\n",
      "total_backward_count 401390 real_backward_count 104006  25.911%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.955990/  2.061214, val:  53.75%, val_best:  58.33%, tr:  86.41%, tr_best:  91.62%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8501%\n",
      "layer   2  Sparsity: 81.1030%\n",
      "layer   3  Sparsity: 87.7065%\n",
      "total_backward_count 406285 real_backward_count 105377  25.937%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.921935/  2.075581, val:  34.17%, val_best:  58.33%, tr:  88.46%, tr_best:  91.62%, epoch time: 40.09 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8629%\n",
      "layer   2  Sparsity: 79.8731%\n",
      "layer   3  Sparsity: 86.4149%\n",
      "total_backward_count 411180 real_backward_count 106678  25.944%\n",
      "fc layer 2 self.abs_max_out: 6836.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.923344/  2.018656, val:  42.50%, val_best:  58.33%, tr:  87.64%, tr_best:  91.62%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 80.5290%\n",
      "layer   3  Sparsity: 86.7112%\n",
      "total_backward_count 416075 real_backward_count 107979  25.952%\n",
      "fc layer 1 self.abs_max_out: 19050.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.875180/  2.054531, val:  37.08%, val_best:  58.33%, tr:  87.95%, tr_best:  91.62%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 79.7444%\n",
      "layer   3  Sparsity: 84.7752%\n",
      "total_backward_count 420970 real_backward_count 109259  25.954%\n",
      "lif layer 2 self.abs_max_v: 11848.5\n",
      "fc layer 1 self.abs_max_out: 19591.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.925587/  2.033440, val:  45.42%, val_best:  58.33%, tr:  87.03%, tr_best:  91.62%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9045%\n",
      "layer   2  Sparsity: 80.3456%\n",
      "layer   3  Sparsity: 86.4819%\n",
      "total_backward_count 425865 real_backward_count 110532  25.955%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.891817/  2.024792, val:  45.83%, val_best:  58.33%, tr:  88.97%, tr_best:  91.62%, epoch time: 40.55 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9119%\n",
      "layer   2  Sparsity: 80.1936%\n",
      "layer   3  Sparsity: 85.4900%\n",
      "total_backward_count 430760 real_backward_count 111726  25.937%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.924579/  2.082673, val:  44.58%, val_best:  58.33%, tr:  88.87%, tr_best:  91.62%, epoch time: 40.07 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8433%\n",
      "layer   2  Sparsity: 80.3297%\n",
      "layer   3  Sparsity: 86.5500%\n",
      "total_backward_count 435655 real_backward_count 112947  25.926%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.940770/  2.064662, val:  43.75%, val_best:  58.33%, tr:  87.33%, tr_best:  91.62%, epoch time: 40.30 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8404%\n",
      "layer   2  Sparsity: 80.3335%\n",
      "layer   3  Sparsity: 86.8862%\n",
      "total_backward_count 440550 real_backward_count 114266  25.937%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.893119/  2.011648, val:  50.00%, val_best:  58.33%, tr:  90.40%, tr_best:  91.62%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8924%\n",
      "layer   2  Sparsity: 80.7495%\n",
      "layer   3  Sparsity: 85.8386%\n",
      "total_backward_count 445445 real_backward_count 115478  25.924%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.896388/  2.042195, val:  52.92%, val_best:  58.33%, tr:  88.05%, tr_best:  91.62%, epoch time: 40.28 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 80.1898%\n",
      "layer   3  Sparsity: 85.5669%\n",
      "total_backward_count 450340 real_backward_count 116662  25.905%\n",
      "lif layer 2 self.abs_max_v: 12095.5\n",
      "lif layer 2 self.abs_max_v: 12251.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.889374/  2.059833, val:  42.08%, val_best:  58.33%, tr:  88.66%, tr_best:  91.62%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8146%\n",
      "layer   2  Sparsity: 79.9534%\n",
      "layer   3  Sparsity: 85.3720%\n",
      "total_backward_count 455235 real_backward_count 117944  25.908%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.925045/  2.116484, val:  42.08%, val_best:  58.33%, tr:  88.87%, tr_best:  91.62%, epoch time: 40.11 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8677%\n",
      "layer   2  Sparsity: 80.6229%\n",
      "layer   3  Sparsity: 87.3759%\n",
      "total_backward_count 460130 real_backward_count 119180  25.901%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.919970/  2.019073, val:  52.50%, val_best:  58.33%, tr:  88.05%, tr_best:  91.62%, epoch time: 39.60 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8234%\n",
      "layer   2  Sparsity: 80.6356%\n",
      "layer   3  Sparsity: 86.7171%\n",
      "total_backward_count 465025 real_backward_count 120467  25.905%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.928656/  2.075687, val:  52.08%, val_best:  58.33%, tr:  88.56%, tr_best:  91.62%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8831%\n",
      "layer   2  Sparsity: 80.4450%\n",
      "layer   3  Sparsity: 86.9569%\n",
      "total_backward_count 469920 real_backward_count 121703  25.899%\n",
      "fc layer 2 self.abs_max_out: 7082.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.933941/  2.033128, val:  42.08%, val_best:  58.33%, tr:  88.56%, tr_best:  91.62%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8758%\n",
      "layer   2  Sparsity: 80.5565%\n",
      "layer   3  Sparsity: 86.6893%\n",
      "total_backward_count 474815 real_backward_count 122943  25.893%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.937136/  2.038987, val:  44.17%, val_best:  58.33%, tr:  85.90%, tr_best:  91.62%, epoch time: 40.52 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9146%\n",
      "layer   2  Sparsity: 80.8411%\n",
      "layer   3  Sparsity: 86.7810%\n",
      "total_backward_count 479710 real_backward_count 124274  25.906%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.917497/  2.088517, val:  39.58%, val_best:  58.33%, tr:  87.13%, tr_best:  91.62%, epoch time: 39.40 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8788%\n",
      "layer   2  Sparsity: 80.1747%\n",
      "layer   3  Sparsity: 86.9036%\n",
      "total_backward_count 484605 real_backward_count 125629  25.924%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.911470/  2.062220, val:  48.75%, val_best:  58.33%, tr:  87.03%, tr_best:  91.62%, epoch time: 39.76 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8845%\n",
      "layer   2  Sparsity: 80.0431%\n",
      "layer   3  Sparsity: 85.9113%\n",
      "total_backward_count 489500 real_backward_count 126955  25.936%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.917950/  2.050937, val:  45.42%, val_best:  58.33%, tr:  87.64%, tr_best:  91.62%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8460%\n",
      "layer   2  Sparsity: 80.6541%\n",
      "layer   3  Sparsity: 86.2947%\n",
      "total_backward_count 494395 real_backward_count 128239  25.939%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.916069/  2.037245, val:  45.42%, val_best:  58.33%, tr:  85.90%, tr_best:  91.62%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8549%\n",
      "layer   2  Sparsity: 80.4893%\n",
      "layer   3  Sparsity: 86.7973%\n",
      "total_backward_count 499290 real_backward_count 129575  25.952%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.906299/  2.020654, val:  43.75%, val_best:  58.33%, tr:  87.95%, tr_best:  91.62%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 80.5782%\n",
      "layer   3  Sparsity: 86.0231%\n",
      "total_backward_count 504185 real_backward_count 130850  25.953%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.905072/  2.019462, val:  45.00%, val_best:  58.33%, tr:  90.40%, tr_best:  91.62%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8394%\n",
      "layer   2  Sparsity: 80.6473%\n",
      "layer   3  Sparsity: 85.7164%\n",
      "total_backward_count 509080 real_backward_count 132048  25.939%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.930106/  2.036157, val:  45.42%, val_best:  58.33%, tr:  85.50%, tr_best:  91.62%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8572%\n",
      "layer   2  Sparsity: 80.7178%\n",
      "layer   3  Sparsity: 86.7442%\n",
      "total_backward_count 513975 real_backward_count 133400  25.955%\n",
      "lif layer 1 self.abs_max_v: 33020.5\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.923603/  2.047455, val:  45.00%, val_best:  58.33%, tr:  87.23%, tr_best:  91.62%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8398%\n",
      "layer   2  Sparsity: 81.2498%\n",
      "layer   3  Sparsity: 86.6157%\n",
      "total_backward_count 518870 real_backward_count 134717  25.964%\n",
      "fc layer 1 self.abs_max_out: 19862.0\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.904353/  2.104257, val:  32.08%, val_best:  58.33%, tr:  88.25%, tr_best:  91.62%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8595%\n",
      "layer   2  Sparsity: 80.2884%\n",
      "layer   3  Sparsity: 86.8664%\n",
      "total_backward_count 523765 real_backward_count 135956  25.957%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.953882/  2.043191, val:  40.83%, val_best:  58.33%, tr:  87.64%, tr_best:  91.62%, epoch time: 39.66 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9072%\n",
      "layer   2  Sparsity: 79.6315%\n",
      "layer   3  Sparsity: 86.9164%\n",
      "total_backward_count 528660 real_backward_count 137275  25.967%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.899399/  1.998838, val:  50.42%, val_best:  58.33%, tr:  89.68%, tr_best:  91.62%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8416%\n",
      "layer   2  Sparsity: 79.6580%\n",
      "layer   3  Sparsity: 86.3694%\n",
      "total_backward_count 533555 real_backward_count 138481  25.954%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.884514/  2.077850, val:  34.17%, val_best:  58.33%, tr:  91.32%, tr_best:  91.62%, epoch time: 40.00 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8736%\n",
      "layer   2  Sparsity: 79.9974%\n",
      "layer   3  Sparsity: 86.3629%\n",
      "total_backward_count 538450 real_backward_count 139736  25.952%\n",
      "fc layer 2 self.abs_max_out: 7509.0\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.905351/  2.020799, val:  48.33%, val_best:  58.33%, tr:  89.17%, tr_best:  91.62%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8588%\n",
      "layer   2  Sparsity: 80.3236%\n",
      "layer   3  Sparsity: 86.7815%\n",
      "total_backward_count 543345 real_backward_count 140958  25.943%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.870345/  2.056958, val:  41.25%, val_best:  58.33%, tr:  91.22%, tr_best:  91.62%, epoch time: 39.14 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8693%\n",
      "layer   2  Sparsity: 80.2578%\n",
      "layer   3  Sparsity: 86.2735%\n",
      "total_backward_count 548240 real_backward_count 142162  25.931%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.933489/  2.053177, val:  53.33%, val_best:  58.33%, tr:  87.64%, tr_best:  91.62%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8683%\n",
      "layer   2  Sparsity: 80.4418%\n",
      "layer   3  Sparsity: 87.4124%\n",
      "total_backward_count 553135 real_backward_count 143427  25.930%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.941554/  2.021598, val:  40.83%, val_best:  58.33%, tr:  88.46%, tr_best:  91.62%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8899%\n",
      "layer   2  Sparsity: 80.3198%\n",
      "layer   3  Sparsity: 86.2709%\n",
      "total_backward_count 558030 real_backward_count 144709  25.932%\n",
      "lif layer 2 self.abs_max_v: 12388.5\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.902824/  2.066380, val:  40.83%, val_best:  58.33%, tr:  87.44%, tr_best:  91.62%, epoch time: 39.18 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8554%\n",
      "layer   2  Sparsity: 80.1174%\n",
      "layer   3  Sparsity: 85.7018%\n",
      "total_backward_count 562925 real_backward_count 145947  25.927%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.886701/  2.048545, val:  49.17%, val_best:  58.33%, tr:  88.76%, tr_best:  91.62%, epoch time: 39.86 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8525%\n",
      "layer   2  Sparsity: 80.5610%\n",
      "layer   3  Sparsity: 86.5290%\n",
      "total_backward_count 567820 real_backward_count 147206  25.925%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.903936/  2.021791, val:  50.00%, val_best:  58.33%, tr:  86.52%, tr_best:  91.62%, epoch time: 39.32 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9155%\n",
      "layer   2  Sparsity: 81.0760%\n",
      "layer   3  Sparsity: 87.6833%\n",
      "total_backward_count 572715 real_backward_count 148492  25.928%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.924500/  2.062259, val:  42.92%, val_best:  58.33%, tr:  86.01%, tr_best:  91.62%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8725%\n",
      "layer   2  Sparsity: 80.8246%\n",
      "layer   3  Sparsity: 86.6925%\n",
      "total_backward_count 577610 real_backward_count 149847  25.943%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.877331/  2.053181, val:  45.42%, val_best:  58.33%, tr:  90.19%, tr_best:  91.62%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8551%\n",
      "layer   2  Sparsity: 80.5957%\n",
      "layer   3  Sparsity: 85.0835%\n",
      "total_backward_count 582505 real_backward_count 151058  25.932%\n",
      "fc layer 3 self.abs_max_out: 1930.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.881448/  2.069274, val:  42.92%, val_best:  58.33%, tr:  89.38%, tr_best:  91.62%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8921%\n",
      "layer   2  Sparsity: 80.3600%\n",
      "layer   3  Sparsity: 86.5608%\n",
      "total_backward_count 587400 real_backward_count 152306  25.929%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.914110/  2.060365, val:  39.17%, val_best:  58.33%, tr:  88.56%, tr_best:  91.62%, epoch time: 40.04 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8458%\n",
      "layer   2  Sparsity: 81.0376%\n",
      "layer   3  Sparsity: 87.4987%\n",
      "total_backward_count 592295 real_backward_count 153542  25.923%\n",
      "lif layer 1 self.abs_max_v: 33126.5\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.897133/  2.043191, val:  44.17%, val_best:  58.33%, tr:  87.74%, tr_best:  91.62%, epoch time: 40.12 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8343%\n",
      "layer   2  Sparsity: 80.6285%\n",
      "layer   3  Sparsity: 86.3164%\n",
      "total_backward_count 597190 real_backward_count 154792  25.920%\n",
      "lif layer 1 self.abs_max_v: 33528.0\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.888394/  2.038033, val:  46.25%, val_best:  58.33%, tr:  89.17%, tr_best:  91.62%, epoch time: 39.68 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8755%\n",
      "layer   2  Sparsity: 80.2758%\n",
      "layer   3  Sparsity: 85.5424%\n",
      "total_backward_count 602085 real_backward_count 156044  25.917%\n",
      "lif layer 1 self.abs_max_v: 33701.0\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.923231/  2.051179, val:  46.67%, val_best:  58.33%, tr:  87.23%, tr_best:  91.62%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8819%\n",
      "layer   2  Sparsity: 80.3098%\n",
      "layer   3  Sparsity: 86.6399%\n",
      "total_backward_count 606980 real_backward_count 157384  25.929%\n",
      "lif layer 1 self.abs_max_v: 33854.5\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.928097/  2.046574, val:  56.25%, val_best:  58.33%, tr:  87.84%, tr_best:  91.62%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.7995%\n",
      "layer   2  Sparsity: 80.0309%\n",
      "layer   3  Sparsity: 87.0307%\n",
      "total_backward_count 611875 real_backward_count 158704  25.937%\n",
      "lif layer 1 self.abs_max_v: 33994.0\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.939505/  2.080880, val:  39.17%, val_best:  58.33%, tr:  86.41%, tr_best:  91.62%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8186%\n",
      "layer   2  Sparsity: 80.8281%\n",
      "layer   3  Sparsity: 88.0020%\n",
      "total_backward_count 616770 real_backward_count 160029  25.946%\n",
      "lif layer 1 self.abs_max_v: 34003.5\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.949802/  2.111337, val:  35.83%, val_best:  58.33%, tr:  85.50%, tr_best:  91.62%, epoch time: 39.82 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8489%\n",
      "layer   2  Sparsity: 80.7085%\n",
      "layer   3  Sparsity: 88.3392%\n",
      "total_backward_count 621665 real_backward_count 161370  25.958%\n",
      "lif layer 1 self.abs_max_v: 34290.5\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.911406/  2.056380, val:  38.75%, val_best:  58.33%, tr:  86.41%, tr_best:  91.62%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 80.8459%\n",
      "layer   3  Sparsity: 86.4080%\n",
      "total_backward_count 626560 real_backward_count 162656  25.960%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.883723/  2.076296, val:  38.33%, val_best:  58.33%, tr:  90.19%, tr_best:  91.62%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8650%\n",
      "layer   2  Sparsity: 80.5761%\n",
      "layer   3  Sparsity: 86.1705%\n",
      "total_backward_count 631455 real_backward_count 163847  25.948%\n",
      "lif layer 2 self.abs_max_v: 12488.5\n",
      "lif layer 1 self.abs_max_v: 34294.5\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.906328/  2.050647, val:  44.17%, val_best:  58.33%, tr:  89.79%, tr_best:  91.62%, epoch time: 40.36 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8589%\n",
      "layer   2  Sparsity: 80.1647%\n",
      "layer   3  Sparsity: 86.5954%\n",
      "total_backward_count 636350 real_backward_count 165086  25.943%\n",
      "lif layer 2 self.abs_max_v: 12753.0\n",
      "lif layer 2 self.abs_max_v: 12753.5\n",
      "lif layer 1 self.abs_max_v: 34378.5\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.910921/  2.075911, val:  40.42%, val_best:  58.33%, tr:  89.38%, tr_best:  91.62%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8669%\n",
      "layer   2  Sparsity: 80.1711%\n",
      "layer   3  Sparsity: 86.7447%\n",
      "total_backward_count 641245 real_backward_count 166412  25.951%\n",
      "lif layer 2 self.abs_max_v: 12949.0\n",
      "fc layer 2 self.abs_max_out: 7646.0\n",
      "lif layer 1 self.abs_max_v: 34529.5\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.922964/  2.062487, val:  45.83%, val_best:  58.33%, tr:  87.33%, tr_best:  91.62%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8929%\n",
      "layer   2  Sparsity: 80.0158%\n",
      "layer   3  Sparsity: 86.2614%\n",
      "total_backward_count 646140 real_backward_count 167741  25.960%\n",
      "lif layer 1 self.abs_max_v: 34548.0\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.891861/  2.047604, val:  34.58%, val_best:  58.33%, tr:  87.13%, tr_best:  91.62%, epoch time: 39.48 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8528%\n",
      "layer   2  Sparsity: 80.5159%\n",
      "layer   3  Sparsity: 84.7726%\n",
      "total_backward_count 651035 real_backward_count 168957  25.952%\n",
      "lif layer 1 self.abs_max_v: 34559.5\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.915851/  2.099379, val:  39.58%, val_best:  58.33%, tr:  87.03%, tr_best:  91.62%, epoch time: 40.35 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9000%\n",
      "layer   2  Sparsity: 80.1244%\n",
      "layer   3  Sparsity: 86.7822%\n",
      "total_backward_count 655930 real_backward_count 170277  25.960%\n",
      "lif layer 1 self.abs_max_v: 34765.5\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.899432/  2.004294, val:  50.42%, val_best:  58.33%, tr:  88.66%, tr_best:  91.62%, epoch time: 40.05 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8680%\n",
      "layer   2  Sparsity: 79.7822%\n",
      "layer   3  Sparsity: 84.9795%\n",
      "total_backward_count 660825 real_backward_count 171592  25.966%\n",
      "lif layer 1 self.abs_max_v: 34773.0\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.854296/  2.018368, val:  46.25%, val_best:  58.33%, tr:  90.50%, tr_best:  91.62%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8526%\n",
      "layer   2  Sparsity: 80.2780%\n",
      "layer   3  Sparsity: 84.7050%\n",
      "total_backward_count 665720 real_backward_count 172849  25.964%\n",
      "fc layer 3 self.abs_max_out: 1960.0\n",
      "fc layer 3 self.abs_max_out: 1969.0\n",
      "lif layer 1 self.abs_max_v: 34782.5\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.861866/  2.029058, val:  42.92%, val_best:  58.33%, tr:  89.27%, tr_best:  91.62%, epoch time: 39.30 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8654%\n",
      "layer   2  Sparsity: 81.2427%\n",
      "layer   3  Sparsity: 84.4896%\n",
      "total_backward_count 670615 real_backward_count 174043  25.953%\n",
      "fc layer 3 self.abs_max_out: 2094.0\n",
      "lif layer 1 self.abs_max_v: 34871.0\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.883043/  2.024657, val:  32.92%, val_best:  58.33%, tr:  88.97%, tr_best:  91.62%, epoch time: 39.95 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8402%\n",
      "layer   2  Sparsity: 80.9244%\n",
      "layer   3  Sparsity: 85.5181%\n",
      "total_backward_count 675510 real_backward_count 175291  25.949%\n",
      "fc layer 3 self.abs_max_out: 2147.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.858144/  2.022934, val:  47.50%, val_best:  58.33%, tr:  89.89%, tr_best:  91.62%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8594%\n",
      "layer   2  Sparsity: 81.0082%\n",
      "layer   3  Sparsity: 84.8259%\n",
      "total_backward_count 680405 real_backward_count 176502  25.941%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.902197/  2.073883, val:  38.75%, val_best:  58.33%, tr:  88.05%, tr_best:  91.62%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8733%\n",
      "layer   2  Sparsity: 80.6696%\n",
      "layer   3  Sparsity: 86.7050%\n",
      "total_backward_count 685300 real_backward_count 177760  25.939%\n",
      "lif layer 1 self.abs_max_v: 34907.5\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.928874/  2.043506, val:  50.00%, val_best:  58.33%, tr:  88.46%, tr_best:  91.62%, epoch time: 39.34 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8719%\n",
      "layer   2  Sparsity: 80.1537%\n",
      "layer   3  Sparsity: 86.8617%\n",
      "total_backward_count 690195 real_backward_count 179057  25.943%\n",
      "lif layer 1 self.abs_max_v: 34989.5\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.934272/  2.097280, val:  45.00%, val_best:  58.33%, tr:  89.79%, tr_best:  91.62%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8507%\n",
      "layer   2  Sparsity: 79.7399%\n",
      "layer   3  Sparsity: 86.1708%\n",
      "total_backward_count 695090 real_backward_count 180303  25.940%\n",
      "lif layer 1 self.abs_max_v: 35171.5\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.940528/  2.063824, val:  49.17%, val_best:  58.33%, tr:  89.68%, tr_best:  91.62%, epoch time: 39.46 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8505%\n",
      "layer   2  Sparsity: 81.3229%\n",
      "layer   3  Sparsity: 86.6124%\n",
      "total_backward_count 699985 real_backward_count 181593  25.942%\n",
      "fc layer 1 self.abs_max_out: 19886.0\n",
      "lif layer 1 self.abs_max_v: 35228.5\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.925418/  2.016737, val:  44.58%, val_best:  58.33%, tr:  88.66%, tr_best:  91.62%, epoch time: 40.02 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8651%\n",
      "layer   2  Sparsity: 80.2307%\n",
      "layer   3  Sparsity: 85.9617%\n",
      "total_backward_count 704880 real_backward_count 182937  25.953%\n",
      "fc layer 1 self.abs_max_out: 19922.0\n",
      "lif layer 1 self.abs_max_v: 35308.5\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.866995/  2.063984, val:  45.83%, val_best:  58.33%, tr:  90.09%, tr_best:  91.62%, epoch time: 40.06 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.9177%\n",
      "layer   2  Sparsity: 80.1306%\n",
      "layer   3  Sparsity: 84.6324%\n",
      "total_backward_count 709775 real_backward_count 184103  25.938%\n",
      "fc layer 1 self.abs_max_out: 19942.0\n",
      "lif layer 1 self.abs_max_v: 35348.5\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.884222/  2.034535, val:  45.00%, val_best:  58.33%, tr:  90.19%, tr_best:  91.62%, epoch time: 39.92 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8903%\n",
      "layer   2  Sparsity: 80.5868%\n",
      "layer   3  Sparsity: 85.7514%\n",
      "total_backward_count 714670 real_backward_count 185289  25.927%\n",
      "fc layer 1 self.abs_max_out: 19978.0\n",
      "lif layer 1 self.abs_max_v: 35427.0\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.894953/  2.031178, val:  50.42%, val_best:  58.33%, tr:  88.46%, tr_best:  91.62%, epoch time: 39.89 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8330%\n",
      "layer   2  Sparsity: 80.2872%\n",
      "layer   3  Sparsity: 86.0486%\n",
      "total_backward_count 719565 real_backward_count 186547  25.925%\n",
      "fc layer 1 self.abs_max_out: 20042.0\n",
      "lif layer 1 self.abs_max_v: 35544.5\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.868676/  2.075361, val:  37.08%, val_best:  58.33%, tr:  89.27%, tr_best:  91.62%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8384%\n",
      "layer   2  Sparsity: 80.4212%\n",
      "layer   3  Sparsity: 85.3003%\n",
      "total_backward_count 724460 real_backward_count 187701  25.909%\n",
      "fc layer 1 self.abs_max_out: 20078.0\n",
      "lif layer 1 self.abs_max_v: 35615.5\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.885148/  2.047252, val:  50.83%, val_best:  58.33%, tr:  89.07%, tr_best:  91.62%, epoch time: 40.18 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8154%\n",
      "layer   2  Sparsity: 79.5893%\n",
      "layer   3  Sparsity: 85.4544%\n",
      "total_backward_count 729355 real_backward_count 188938  25.905%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.912462/  2.068037, val:  50.00%, val_best:  58.33%, tr:  87.84%, tr_best:  91.62%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8546%\n",
      "layer   2  Sparsity: 80.3870%\n",
      "layer   3  Sparsity: 86.4860%\n",
      "total_backward_count 734250 real_backward_count 190231  25.908%\n",
      "fc layer 1 self.abs_max_out: 20090.0\n",
      "lif layer 1 self.abs_max_v: 35655.0\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.885773/  2.024961, val:  41.67%, val_best:  58.33%, tr:  90.19%, tr_best:  91.62%, epoch time: 39.50 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8559%\n",
      "layer   2  Sparsity: 80.1130%\n",
      "layer   3  Sparsity: 85.7486%\n",
      "total_backward_count 739145 real_backward_count 191391  25.894%\n",
      "fc layer 1 self.abs_max_out: 20164.0\n",
      "lif layer 1 self.abs_max_v: 35802.5\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.853472/  1.970726, val:  50.00%, val_best:  58.33%, tr:  90.19%, tr_best:  91.62%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8694%\n",
      "layer   2  Sparsity: 80.3600%\n",
      "layer   3  Sparsity: 85.2908%\n",
      "total_backward_count 744040 real_backward_count 192551  25.879%\n",
      "fc layer 1 self.abs_max_out: 20196.0\n",
      "lif layer 1 self.abs_max_v: 35865.0\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.895456/  2.019277, val:  50.42%, val_best:  58.33%, tr:  89.07%, tr_best:  91.62%, epoch time: 39.90 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8722%\n",
      "layer   2  Sparsity: 79.9668%\n",
      "layer   3  Sparsity: 86.0515%\n",
      "total_backward_count 748935 real_backward_count 193773  25.873%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.930142/  2.060128, val:  50.00%, val_best:  58.33%, tr:  86.62%, tr_best:  91.62%, epoch time: 40.26 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8937%\n",
      "layer   2  Sparsity: 80.4089%\n",
      "layer   3  Sparsity: 86.9655%\n",
      "total_backward_count 753830 real_backward_count 195113  25.883%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.945343/  2.033831, val:  50.42%, val_best:  58.33%, tr:  86.41%, tr_best:  91.62%, epoch time: 39.67 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8184%\n",
      "layer   2  Sparsity: 79.7798%\n",
      "layer   3  Sparsity: 87.1514%\n",
      "total_backward_count 758725 real_backward_count 196433  25.890%\n",
      "fc layer 1 self.abs_max_out: 20240.0\n",
      "lif layer 1 self.abs_max_v: 35947.0\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.947004/  2.116088, val:  36.25%, val_best:  58.33%, tr:  87.23%, tr_best:  91.62%, epoch time: 39.98 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8856%\n",
      "layer   2  Sparsity: 80.5302%\n",
      "layer   3  Sparsity: 87.7565%\n",
      "total_backward_count 763620 real_backward_count 197733  25.894%\n",
      "fc layer 1 self.abs_max_out: 20242.0\n",
      "lif layer 1 self.abs_max_v: 35950.0\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.930132/  2.068105, val:  25.42%, val_best:  58.33%, tr:  87.54%, tr_best:  91.62%, epoch time: 39.99 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8573%\n",
      "layer   2  Sparsity: 80.5647%\n",
      "layer   3  Sparsity: 86.5654%\n",
      "total_backward_count 768515 real_backward_count 199016  25.896%\n",
      "fc layer 1 self.abs_max_out: 20254.0\n",
      "lif layer 1 self.abs_max_v: 35969.5\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.881135/  2.013575, val:  52.92%, val_best:  58.33%, tr:  88.36%, tr_best:  91.62%, epoch time: 40.03 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8612%\n",
      "layer   2  Sparsity: 80.2027%\n",
      "layer   3  Sparsity: 85.6629%\n",
      "total_backward_count 773410 real_backward_count 200252  25.892%\n",
      "fc layer 1 self.abs_max_out: 20262.0\n",
      "lif layer 1 self.abs_max_v: 35989.0\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.919616/  2.090745, val:  39.58%, val_best:  58.33%, tr:  86.52%, tr_best:  91.62%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8592%\n",
      "layer   2  Sparsity: 80.2724%\n",
      "layer   3  Sparsity: 86.8111%\n",
      "total_backward_count 778305 real_backward_count 201504  25.890%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.953776/  2.118918, val:  43.33%, val_best:  58.33%, tr:  87.33%, tr_best:  91.62%, epoch time: 39.94 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8435%\n",
      "layer   2  Sparsity: 79.8789%\n",
      "layer   3  Sparsity: 87.9300%\n",
      "total_backward_count 783200 real_backward_count 202731  25.885%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.929952/  2.064651, val:  47.08%, val_best:  58.33%, tr:  88.46%, tr_best:  91.62%, epoch time: 39.56 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9394%\n",
      "layer   2  Sparsity: 79.5742%\n",
      "layer   3  Sparsity: 86.8573%\n",
      "total_backward_count 788095 real_backward_count 203959  25.880%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.942071/  2.089248, val:  56.67%, val_best:  58.33%, tr:  86.93%, tr_best:  91.62%, epoch time: 39.69 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8288%\n",
      "layer   2  Sparsity: 80.0073%\n",
      "layer   3  Sparsity: 87.3829%\n",
      "total_backward_count 792990 real_backward_count 205268  25.885%\n",
      "fc layer 1 self.abs_max_out: 20266.0\n",
      "lif layer 1 self.abs_max_v: 35995.5\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.920040/  2.045496, val:  41.25%, val_best:  58.33%, tr:  88.76%, tr_best:  91.62%, epoch time: 39.84 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8600%\n",
      "layer   2  Sparsity: 80.1921%\n",
      "layer   3  Sparsity: 86.1281%\n",
      "total_backward_count 797885 real_backward_count 206549  25.887%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.878338/  2.071800, val:  29.58%, val_best:  58.33%, tr:  89.89%, tr_best:  91.62%, epoch time: 40.58 seconds, 0.68 minutes\n",
      "layer   1  Sparsity: 88.9106%\n",
      "layer   2  Sparsity: 80.7891%\n",
      "layer   3  Sparsity: 85.3966%\n",
      "total_backward_count 802780 real_backward_count 207718  25.875%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.924089/  2.136918, val:  39.58%, val_best:  58.33%, tr:  87.95%, tr_best:  91.62%, epoch time: 39.64 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8253%\n",
      "layer   2  Sparsity: 79.9714%\n",
      "layer   3  Sparsity: 86.9717%\n",
      "total_backward_count 807675 real_backward_count 209029  25.880%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.938625/  2.024890, val:  55.42%, val_best:  58.33%, tr:  88.66%, tr_best:  91.62%, epoch time: 40.14 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8552%\n",
      "layer   2  Sparsity: 80.5993%\n",
      "layer   3  Sparsity: 86.7433%\n",
      "total_backward_count 812570 real_backward_count 210257  25.876%\n",
      "fc layer 1 self.abs_max_out: 20304.0\n",
      "lif layer 1 self.abs_max_v: 36067.5\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.882005/  2.060877, val:  47.08%, val_best:  58.33%, tr:  90.91%, tr_best:  91.62%, epoch time: 39.80 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9076%\n",
      "layer   2  Sparsity: 80.1494%\n",
      "layer   3  Sparsity: 85.1644%\n",
      "total_backward_count 817465 real_backward_count 211431  25.864%\n",
      "fc layer 1 self.abs_max_out: 20308.0\n",
      "lif layer 1 self.abs_max_v: 36075.0\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.900617/  2.099676, val:  35.42%, val_best:  58.33%, tr:  89.79%, tr_best:  91.62%, epoch time: 39.77 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8907%\n",
      "layer   2  Sparsity: 80.6150%\n",
      "layer   3  Sparsity: 85.9236%\n",
      "total_backward_count 822360 real_backward_count 212686  25.863%\n",
      "lif layer 2 self.abs_max_v: 13157.5\n",
      "fc layer 1 self.abs_max_out: 20326.0\n",
      "lif layer 1 self.abs_max_v: 36115.5\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.932816/  2.084762, val:  30.00%, val_best:  58.33%, tr:  87.44%, tr_best:  91.62%, epoch time: 40.33 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8238%\n",
      "layer   2  Sparsity: 80.1988%\n",
      "layer   3  Sparsity: 86.4067%\n",
      "total_backward_count 827255 real_backward_count 213969  25.865%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.881252/  2.041241, val:  55.00%, val_best:  58.33%, tr:  88.56%, tr_best:  91.62%, epoch time: 39.79 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8839%\n",
      "layer   2  Sparsity: 80.3573%\n",
      "layer   3  Sparsity: 85.6162%\n",
      "total_backward_count 832150 real_backward_count 215174  25.858%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.907943/  2.101133, val:  36.67%, val_best:  58.33%, tr:  88.05%, tr_best:  91.62%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.7795%\n",
      "layer   2  Sparsity: 80.3345%\n",
      "layer   3  Sparsity: 86.6472%\n",
      "total_backward_count 837045 real_backward_count 216416  25.855%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.926611/  2.011977, val:  44.58%, val_best:  58.33%, tr:  87.95%, tr_best:  91.62%, epoch time: 39.72 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8609%\n",
      "layer   2  Sparsity: 80.4144%\n",
      "layer   3  Sparsity: 86.4517%\n",
      "total_backward_count 841940 real_backward_count 217708  25.858%\n",
      "fc layer 1 self.abs_max_out: 20340.0\n",
      "lif layer 1 self.abs_max_v: 36142.0\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.912478/  2.053512, val:  52.08%, val_best:  58.33%, tr:  88.97%, tr_best:  91.62%, epoch time: 40.21 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8533%\n",
      "layer   2  Sparsity: 80.2903%\n",
      "layer   3  Sparsity: 86.8139%\n",
      "total_backward_count 846835 real_backward_count 218961  25.856%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.926451/  2.047211, val:  44.58%, val_best:  58.33%, tr:  86.52%, tr_best:  91.62%, epoch time: 39.75 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8653%\n",
      "layer   2  Sparsity: 79.2969%\n",
      "layer   3  Sparsity: 87.1326%\n",
      "total_backward_count 851730 real_backward_count 220235  25.857%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.930746/  2.040784, val:  55.00%, val_best:  58.33%, tr:  90.19%, tr_best:  91.62%, epoch time: 39.81 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8844%\n",
      "layer   2  Sparsity: 80.7064%\n",
      "layer   3  Sparsity: 87.4376%\n",
      "total_backward_count 856625 real_backward_count 221419  25.848%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.887380/  1.973110, val:  51.25%, val_best:  58.33%, tr:  88.97%, tr_best:  91.62%, epoch time: 39.38 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8893%\n",
      "layer   2  Sparsity: 79.5518%\n",
      "layer   3  Sparsity: 85.7171%\n",
      "total_backward_count 861520 real_backward_count 222616  25.840%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.883289/  2.038346, val:  51.67%, val_best:  58.33%, tr:  87.74%, tr_best:  91.62%, epoch time: 39.58 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.9205%\n",
      "layer   2  Sparsity: 80.3727%\n",
      "layer   3  Sparsity: 86.0907%\n",
      "total_backward_count 866415 real_backward_count 223864  25.838%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.940539/  2.061169, val:  49.17%, val_best:  58.33%, tr:  89.79%, tr_best:  91.62%, epoch time: 39.42 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8411%\n",
      "layer   2  Sparsity: 80.1943%\n",
      "layer   3  Sparsity: 87.6660%\n",
      "total_backward_count 871310 real_backward_count 225055  25.829%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.934328/  2.016037, val:  47.50%, val_best:  58.33%, tr:  87.64%, tr_best:  91.62%, epoch time: 39.53 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8369%\n",
      "layer   2  Sparsity: 79.4323%\n",
      "layer   3  Sparsity: 87.0705%\n",
      "total_backward_count 876205 real_backward_count 226346  25.833%\n",
      "fc layer 1 self.abs_max_out: 20342.0\n",
      "lif layer 1 self.abs_max_v: 36145.5\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.949125/  2.059833, val:  47.50%, val_best:  58.33%, tr:  87.54%, tr_best:  91.62%, epoch time: 40.08 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8910%\n",
      "layer   2  Sparsity: 79.5175%\n",
      "layer   3  Sparsity: 87.4345%\n",
      "total_backward_count 881100 real_backward_count 227692  25.842%\n",
      "fc layer 1 self.abs_max_out: 20378.0\n",
      "lif layer 1 self.abs_max_v: 36224.0\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.938723/  2.081777, val:  45.42%, val_best:  58.33%, tr:  89.38%, tr_best:  91.62%, epoch time: 39.40 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8480%\n",
      "layer   2  Sparsity: 79.7435%\n",
      "layer   3  Sparsity: 87.1760%\n",
      "total_backward_count 885995 real_backward_count 228933  25.839%\n",
      "fc layer 1 self.abs_max_out: 20428.0\n",
      "lif layer 1 self.abs_max_v: 36329.0\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.879542/  2.025073, val:  45.00%, val_best:  58.33%, tr:  89.27%, tr_best:  91.62%, epoch time: 39.71 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8035%\n",
      "layer   2  Sparsity: 79.7915%\n",
      "layer   3  Sparsity: 85.8019%\n",
      "total_backward_count 890890 real_backward_count 230129  25.831%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.888828/  2.069624, val:  55.00%, val_best:  58.33%, tr:  88.36%, tr_best:  91.62%, epoch time: 39.49 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8373%\n",
      "layer   2  Sparsity: 80.5261%\n",
      "layer   3  Sparsity: 86.7317%\n",
      "total_backward_count 895785 real_backward_count 231327  25.824%\n",
      "fc layer 1 self.abs_max_out: 20436.0\n",
      "lif layer 1 self.abs_max_v: 36344.5\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.904488/  2.037196, val:  40.83%, val_best:  58.33%, tr:  89.68%, tr_best:  91.62%, epoch time: 40.31 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8959%\n",
      "layer   2  Sparsity: 80.0402%\n",
      "layer   3  Sparsity: 85.4071%\n",
      "total_backward_count 900680 real_backward_count 232568  25.821%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.885809/  2.045579, val:  38.75%, val_best:  58.33%, tr:  88.66%, tr_best:  91.62%, epoch time: 39.59 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8702%\n",
      "layer   2  Sparsity: 79.7659%\n",
      "layer   3  Sparsity: 84.9649%\n",
      "total_backward_count 905575 real_backward_count 233803  25.818%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.918588/  2.111068, val:  28.75%, val_best:  58.33%, tr:  88.05%, tr_best:  91.62%, epoch time: 40.43 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8088%\n",
      "layer   2  Sparsity: 79.1829%\n",
      "layer   3  Sparsity: 86.4657%\n",
      "total_backward_count 910470 real_backward_count 235059  25.817%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.921986/  2.027745, val:  45.83%, val_best:  58.33%, tr:  87.64%, tr_best:  91.62%, epoch time: 39.39 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8874%\n",
      "layer   2  Sparsity: 80.1118%\n",
      "layer   3  Sparsity: 86.0391%\n",
      "total_backward_count 915365 real_backward_count 236375  25.823%\n",
      "fc layer 1 self.abs_max_out: 20438.0\n",
      "lif layer 1 self.abs_max_v: 36346.5\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.926416/  2.145124, val:  34.17%, val_best:  58.33%, tr:  86.82%, tr_best:  91.62%, epoch time: 39.78 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8775%\n",
      "layer   2  Sparsity: 80.0312%\n",
      "layer   3  Sparsity: 87.2016%\n",
      "total_backward_count 920260 real_backward_count 237697  25.829%\n",
      "fc layer 1 self.abs_max_out: 20482.0\n",
      "lif layer 1 self.abs_max_v: 36428.5\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.941403/  2.102739, val:  49.58%, val_best:  58.33%, tr:  85.80%, tr_best:  91.62%, epoch time: 39.62 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8535%\n",
      "layer   2  Sparsity: 79.9400%\n",
      "layer   3  Sparsity: 88.3259%\n",
      "total_backward_count 925155 real_backward_count 238992  25.833%\n",
      "fc layer 1 self.abs_max_out: 20526.0\n",
      "lif layer 1 self.abs_max_v: 36510.5\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.902372/  2.086984, val:  30.00%, val_best:  58.33%, tr:  90.50%, tr_best:  91.62%, epoch time: 40.17 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8437%\n",
      "layer   2  Sparsity: 79.1424%\n",
      "layer   3  Sparsity: 87.0434%\n",
      "total_backward_count 930050 real_backward_count 240200  25.827%\n",
      "lif layer 2 self.abs_max_v: 13656.0\n",
      "fc layer 2 self.abs_max_out: 7757.0\n",
      "fc layer 1 self.abs_max_out: 20536.0\n",
      "lif layer 1 self.abs_max_v: 36527.5\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.908987/  2.023345, val:  52.50%, val_best:  58.33%, tr:  87.23%, tr_best:  91.62%, epoch time: 39.52 seconds, 0.66 minutes\n",
      "layer   1  Sparsity: 88.8464%\n",
      "layer   2  Sparsity: 80.3221%\n",
      "layer   3  Sparsity: 86.6788%\n",
      "total_backward_count 934945 real_backward_count 241535  25.834%\n",
      "fc layer 1 self.abs_max_out: 20590.0\n",
      "lif layer 1 self.abs_max_v: 36629.0\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.923140/  2.043285, val:  45.83%, val_best:  58.33%, tr:  87.44%, tr_best:  91.62%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8463%\n",
      "layer   2  Sparsity: 79.9486%\n",
      "layer   3  Sparsity: 86.8197%\n",
      "total_backward_count 939840 real_backward_count 242851  25.840%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.929763/  2.042112, val:  38.33%, val_best:  58.33%, tr:  88.05%, tr_best:  91.62%, epoch time: 39.12 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8605%\n",
      "layer   2  Sparsity: 79.5838%\n",
      "layer   3  Sparsity: 86.0568%\n",
      "total_backward_count 944735 real_backward_count 244135  25.842%\n",
      "fc layer 1 self.abs_max_out: 20592.0\n",
      "lif layer 1 self.abs_max_v: 36632.5\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.874483/  2.071277, val:  35.83%, val_best:  58.33%, tr:  90.19%, tr_best:  91.62%, epoch time: 39.93 seconds, 0.67 minutes\n",
      "layer   1  Sparsity: 88.8641%\n",
      "layer   2  Sparsity: 79.6328%\n",
      "layer   3  Sparsity: 85.4774%\n",
      "total_backward_count 949630 real_backward_count 245332  25.834%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.902084/  2.038445, val:  38.75%, val_best:  58.33%, tr:  89.99%, tr_best:  91.62%, epoch time: 39.04 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8705%\n",
      "layer   2  Sparsity: 79.1961%\n",
      "layer   3  Sparsity: 85.5389%\n",
      "total_backward_count 954525 real_backward_count 246576  25.832%\n",
      "fc layer 1 self.abs_max_out: 20650.0\n",
      "lif layer 1 self.abs_max_v: 36756.5\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.874832/  1.961420, val:  51.67%, val_best:  58.33%, tr:  88.97%, tr_best:  91.62%, epoch time: 38.87 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8267%\n",
      "layer   2  Sparsity: 79.5881%\n",
      "layer   3  Sparsity: 84.7094%\n",
      "total_backward_count 959420 real_backward_count 247839  25.832%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.821042/  1.982877, val:  49.58%, val_best:  58.33%, tr:  90.50%, tr_best:  91.62%, epoch time: 38.06 seconds, 0.63 minutes\n",
      "layer   1  Sparsity: 88.8760%\n",
      "layer   2  Sparsity: 79.7473%\n",
      "layer   3  Sparsity: 83.9071%\n",
      "total_backward_count 964315 real_backward_count 249049  25.827%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.863736/  2.020425, val:  37.08%, val_best:  58.33%, tr:  89.48%, tr_best:  91.62%, epoch time: 38.61 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 88.8955%\n",
      "layer   2  Sparsity: 79.7974%\n",
      "layer   3  Sparsity: 85.0375%\n",
      "total_backward_count 969210 real_backward_count 250237  25.819%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.854749/  2.056881, val:  40.00%, val_best:  58.33%, tr:  88.87%, tr_best:  91.62%, epoch time: 39.00 seconds, 0.65 minutes\n",
      "layer   1  Sparsity: 88.8483%\n",
      "layer   2  Sparsity: 79.9020%\n",
      "layer   3  Sparsity: 85.4179%\n",
      "total_backward_count 974105 real_backward_count 251435  25.812%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.871469/  2.039419, val:  56.67%, val_best:  58.33%, tr:  89.89%, tr_best:  91.62%, epoch time: 38.33 seconds, 0.64 minutes\n",
      "layer   1  Sparsity: 88.8678%\n",
      "layer   2  Sparsity: 79.6352%\n",
      "layer   3  Sparsity: 86.1488%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a29c8328264612a99232a89a2fb1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.89888</td></tr><tr><td>tr_epoch_loss</td><td>1.87147</td></tr><tr><td>val_acc_best</td><td>0.58333</td></tr><tr><td>val_acc_now</td><td>0.56667</td></tr><tr><td>val_loss</td><td>2.03942</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-sweep-18</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g5fz36b5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g5fz36b5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_145107-g5fz36b5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ec6u3zuy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 12000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_170447-ec6u3zuy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ec6u3zuy' target=\"_blank\">comfy-sweep-20</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/pyz704uj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ec6u3zuy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ec6u3zuy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': '20251118_170455_721', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c247f45ff938aa370993ba27bace6d15\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 119.0\n",
      "lif layer 1 self.abs_max_v: 119.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 121.0\n",
      "lif layer 1 self.abs_max_v: 144.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "lif layer 1 self.abs_max_v: 155.0\n",
      "fc layer 2 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 69.0\n",
      "fc layer 1 self.abs_max_out: 124.0\n",
      "fc layer 2 self.abs_max_out: 67.0\n",
      "lif layer 2 self.abs_max_v: 87.0\n",
      "fc layer 2 self.abs_max_out: 91.0\n",
      "lif layer 2 self.abs_max_v: 112.5\n",
      "fc layer 1 self.abs_max_out: 133.0\n",
      "lif layer 1 self.abs_max_v: 161.5\n",
      "fc layer 2 self.abs_max_out: 103.0\n",
      "lif layer 2 self.abs_max_v: 118.0\n",
      "lif layer 1 self.abs_max_v: 173.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 148.0\n",
      "lif layer 1 self.abs_max_v: 188.0\n",
      "lif layer 1 self.abs_max_v: 192.0\n",
      "lif layer 1 self.abs_max_v: 227.0\n",
      "fc layer 2 self.abs_max_out: 143.0\n",
      "lif layer 2 self.abs_max_v: 145.5\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "fc layer 1 self.abs_max_out: 155.0\n",
      "lif layer 1 self.abs_max_v: 267.0\n",
      "fc layer 2 self.abs_max_out: 163.0\n",
      "lif layer 2 self.abs_max_v: 186.0\n",
      "fc layer 3 self.abs_max_out: 56.0\n",
      "fc layer 1 self.abs_max_out: 169.0\n",
      "lif layer 1 self.abs_max_v: 270.5\n",
      "fc layer 2 self.abs_max_out: 212.0\n",
      "lif layer 2 self.abs_max_v: 259.0\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 1 self.abs_max_out: 226.0\n",
      "lif layer 1 self.abs_max_v: 287.5\n",
      "lif layer 1 self.abs_max_v: 328.0\n",
      "fc layer 2 self.abs_max_out: 261.0\n",
      "lif layer 2 self.abs_max_v: 343.5\n",
      "lif layer 1 self.abs_max_v: 366.0\n",
      "fc layer 2 self.abs_max_out: 268.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 227.0\n",
      "fc layer 1 self.abs_max_out: 349.0\n",
      "fc layer 1 self.abs_max_out: 412.0\n",
      "lif layer 1 self.abs_max_v: 412.0\n",
      "lif layer 2 self.abs_max_v: 362.5\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 441.0\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 450.0\n",
      "lif layer 2 self.abs_max_v: 477.5\n",
      "fc layer 2 self.abs_max_out: 349.0\n",
      "fc layer 1 self.abs_max_out: 420.0\n",
      "lif layer 1 self.abs_max_v: 420.0\n",
      "lif layer 2 self.abs_max_v: 506.0\n",
      "fc layer 1 self.abs_max_out: 461.0\n",
      "lif layer 1 self.abs_max_v: 461.0\n",
      "fc layer 2 self.abs_max_out: 355.0\n",
      "lif layer 2 self.abs_max_v: 600.0\n",
      "lif layer 1 self.abs_max_v: 463.0\n",
      "fc layer 2 self.abs_max_out: 398.0\n",
      "fc layer 1 self.abs_max_out: 490.0\n",
      "lif layer 1 self.abs_max_v: 611.5\n",
      "lif layer 2 self.abs_max_v: 612.5\n",
      "fc layer 2 self.abs_max_out: 432.0\n",
      "fc layer 3 self.abs_max_out: 203.0\n",
      "fc layer 2 self.abs_max_out: 433.0\n",
      "fc layer 2 self.abs_max_out: 456.0\n",
      "smallest_now_T updated: 345\n",
      "fc layer 1 self.abs_max_out: 497.0\n",
      "fc layer 2 self.abs_max_out: 458.0\n",
      "fc layer 1 self.abs_max_out: 574.0\n",
      "fc layer 2 self.abs_max_out: 471.0\n",
      "lif layer 2 self.abs_max_v: 646.0\n",
      "fc layer 2 self.abs_max_out: 546.0\n",
      "lif layer 2 self.abs_max_v: 805.5\n",
      "lif layer 1 self.abs_max_v: 639.0\n",
      "lif layer 2 self.abs_max_v: 883.0\n",
      "fc layer 1 self.abs_max_out: 609.0\n",
      "fc layer 1 self.abs_max_out: 666.0\n",
      "lif layer 1 self.abs_max_v: 666.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "lif layer 2 self.abs_max_v: 898.5\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "smallest_now_T updated: 317\n",
      "fc layer 2 self.abs_max_out: 625.0\n",
      "fc layer 1 self.abs_max_out: 690.0\n",
      "lif layer 1 self.abs_max_v: 690.0\n",
      "fc layer 1 self.abs_max_out: 720.0\n",
      "lif layer 1 self.abs_max_v: 720.0\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "fc layer 2 self.abs_max_out: 716.0\n",
      "fc layer 2 self.abs_max_out: 801.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "lif layer 2 self.abs_max_v: 907.5\n",
      "fc layer 1 self.abs_max_out: 753.0\n",
      "lif layer 1 self.abs_max_v: 753.0\n",
      "smallest_now_T updated: 286\n",
      "lif layer 2 self.abs_max_v: 910.5\n",
      "lif layer 2 self.abs_max_v: 1064.5\n",
      "lif layer 2 self.abs_max_v: 1187.0\n",
      "fc layer 1 self.abs_max_out: 779.0\n",
      "lif layer 1 self.abs_max_v: 779.0\n",
      "fc layer 2 self.abs_max_out: 824.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "fc layer 1 self.abs_max_out: 849.0\n",
      "lif layer 1 self.abs_max_v: 849.0\n",
      "fc layer 1 self.abs_max_out: 864.0\n",
      "lif layer 1 self.abs_max_v: 864.0\n",
      "fc layer 2 self.abs_max_out: 901.0\n",
      "fc layer 1 self.abs_max_out: 963.0\n",
      "lif layer 1 self.abs_max_v: 963.0\n",
      "fc layer 3 self.abs_max_out: 246.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 1230.0\n",
      "lif layer 2 self.abs_max_v: 1236.0\n",
      "lif layer 2 self.abs_max_v: 1395.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 2 self.abs_max_out: 1065.0\n",
      "fc layer 1 self.abs_max_out: 1035.0\n",
      "lif layer 1 self.abs_max_v: 1035.0\n",
      "fc layer 1 self.abs_max_out: 1213.0\n",
      "lif layer 1 self.abs_max_v: 1213.0\n",
      "smallest_now_T updated: 247\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "smallest_now_T updated: 192\n",
      "lif layer 2 self.abs_max_v: 1438.0\n",
      "fc layer 2 self.abs_max_out: 1182.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 2 self.abs_max_out: 1242.0\n",
      "fc layer 2 self.abs_max_out: 1297.0\n",
      "lif layer 2 self.abs_max_v: 1443.0\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "lif layer 2 self.abs_max_v: 1464.0\n",
      "fc layer 2 self.abs_max_out: 1299.0\n",
      "fc layer 2 self.abs_max_out: 1367.0\n",
      "fc layer 2 self.abs_max_out: 1374.0\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1430.0\n",
      "fc layer 2 self.abs_max_out: 1450.0\n",
      "lif layer 2 self.abs_max_v: 1533.5\n",
      "fc layer 1 self.abs_max_out: 1215.0\n",
      "lif layer 1 self.abs_max_v: 1215.0\n",
      "fc layer 2 self.abs_max_out: 1482.0\n",
      "fc layer 1 self.abs_max_out: 1289.0\n",
      "lif layer 1 self.abs_max_v: 1289.0\n",
      "fc layer 2 self.abs_max_out: 1487.0\n",
      "fc layer 2 self.abs_max_out: 1567.0\n",
      "lif layer 2 self.abs_max_v: 1567.0\n",
      "lif layer 2 self.abs_max_v: 1620.0\n",
      "lif layer 2 self.abs_max_v: 1669.5\n",
      "lif layer 2 self.abs_max_v: 1679.0\n",
      "fc layer 2 self.abs_max_out: 1652.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 1 self.abs_max_out: 1352.0\n",
      "lif layer 1 self.abs_max_v: 1352.0\n",
      "fc layer 1 self.abs_max_out: 1457.0\n",
      "lif layer 1 self.abs_max_v: 1457.0\n",
      "lif layer 2 self.abs_max_v: 1680.0\n",
      "lif layer 2 self.abs_max_v: 1774.5\n",
      "fc layer 2 self.abs_max_out: 1653.0\n",
      "fc layer 1 self.abs_max_out: 1591.0\n",
      "lif layer 1 self.abs_max_v: 1591.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "lif layer 2 self.abs_max_v: 1779.0\n",
      "smallest_now_T_val updated: 368\n",
      "fc layer 2 self.abs_max_out: 1793.0\n",
      "lif layer 2 self.abs_max_v: 1793.0\n",
      "smallest_now_T_val updated: 137\n",
      "lif layer 2 self.abs_max_v: 1819.5\n",
      "lif layer 2 self.abs_max_v: 1842.0\n",
      "lif layer 2 self.abs_max_v: 1971.0\n",
      "lif layer 2 self.abs_max_v: 2073.5\n",
      "lif layer 1 self.abs_max_v: 1799.5\n",
      "lif layer 1 self.abs_max_v: 1937.0\n",
      "fc layer 1 self.abs_max_out: 1608.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.869235/  2.054141, val:  30.00%, val_best:  30.00%, tr:  90.70%, tr_best:  90.70%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7976%\n",
      "layer   2  Sparsity: 79.0454%\n",
      "layer   3  Sparsity: 74.8107%\n",
      "total_backward_count 9790 real_backward_count 3096  31.624%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1619.0\n",
      "fc layer 2 self.abs_max_out: 1829.0\n",
      "fc layer 1 self.abs_max_out: 1679.0\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "fc layer 1 self.abs_max_out: 1742.0\n",
      "fc layer 1 self.abs_max_out: 1917.0\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "lif layer 1 self.abs_max_v: 1949.0\n",
      "fc layer 2 self.abs_max_out: 1836.0\n",
      "fc layer 1 self.abs_max_out: 1965.0\n",
      "lif layer 1 self.abs_max_v: 1965.0\n",
      "fc layer 2 self.abs_max_out: 1860.0\n",
      "fc layer 2 self.abs_max_out: 1990.0\n",
      "lif layer 2 self.abs_max_v: 2111.5\n",
      "fc layer 2 self.abs_max_out: 2010.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.753331/  1.995866, val:  36.25%, val_best:  36.25%, tr:  97.14%, tr_best:  97.14%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7916%\n",
      "layer   2  Sparsity: 76.2820%\n",
      "layer   3  Sparsity: 69.7342%\n",
      "total_backward_count 19580 real_backward_count 5343  27.288%\n",
      "lif layer 2 self.abs_max_v: 2173.0\n",
      "fc layer 1 self.abs_max_out: 2068.0\n",
      "lif layer 1 self.abs_max_v: 2068.0\n",
      "fc layer 1 self.abs_max_out: 2165.0\n",
      "lif layer 1 self.abs_max_v: 2165.0\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "lif layer 2 self.abs_max_v: 2551.0\n",
      "fc layer 2 self.abs_max_out: 2162.0\n",
      "lif layer 2 self.abs_max_v: 2665.0\n",
      "lif layer 2 self.abs_max_v: 2786.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.720828/  2.032865, val:  30.00%, val_best:  36.25%, tr:  97.65%, tr_best:  97.65%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7896%\n",
      "layer   2  Sparsity: 74.6242%\n",
      "layer   3  Sparsity: 66.7989%\n",
      "total_backward_count 29370 real_backward_count 7415  25.247%\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "fc layer 1 self.abs_max_out: 2308.0\n",
      "lif layer 1 self.abs_max_v: 2308.0\n",
      "fc layer 2 self.abs_max_out: 2189.0\n",
      "fc layer 3 self.abs_max_out: 475.0\n",
      "fc layer 1 self.abs_max_out: 2537.0\n",
      "lif layer 1 self.abs_max_v: 2537.0\n",
      "fc layer 1 self.abs_max_out: 2648.0\n",
      "lif layer 1 self.abs_max_v: 2648.0\n",
      "lif layer 2 self.abs_max_v: 2833.0\n",
      "lif layer 2 self.abs_max_v: 2995.5\n",
      "lif layer 2 self.abs_max_v: 3075.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.681088/  1.984040, val:  45.42%, val_best:  45.42%, tr:  98.77%, tr_best:  98.77%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7851%\n",
      "layer   2  Sparsity: 74.8123%\n",
      "layer   3  Sparsity: 66.3197%\n",
      "total_backward_count 39160 real_backward_count 9421  24.058%\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "fc layer 2 self.abs_max_out: 2190.0\n",
      "fc layer 2 self.abs_max_out: 2221.0\n",
      "fc layer 2 self.abs_max_out: 2372.0\n",
      "lif layer 2 self.abs_max_v: 3107.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.678124/  1.975786, val:  29.17%, val_best:  45.42%, tr:  98.77%, tr_best:  98.77%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7854%\n",
      "layer   2  Sparsity: 75.4217%\n",
      "layer   3  Sparsity: 65.9265%\n",
      "total_backward_count 48950 real_backward_count 11372  23.232%\n",
      "fc layer 1 self.abs_max_out: 2694.0\n",
      "lif layer 1 self.abs_max_v: 2694.0\n",
      "lif layer 2 self.abs_max_v: 3137.0\n",
      "lif layer 1 self.abs_max_v: 2870.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.658371/  2.000356, val:  32.92%, val_best:  45.42%, tr:  98.47%, tr_best:  98.77%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.8195%\n",
      "layer   2  Sparsity: 75.1326%\n",
      "layer   3  Sparsity: 65.4657%\n",
      "total_backward_count 58740 real_backward_count 13252  22.560%\n",
      "fc layer 1 self.abs_max_out: 2924.0\n",
      "lif layer 1 self.abs_max_v: 2924.0\n",
      "fc layer 3 self.abs_max_out: 517.0\n",
      "fc layer 3 self.abs_max_out: 588.0\n",
      "lif layer 2 self.abs_max_v: 3277.0\n",
      "lif layer 2 self.abs_max_v: 3341.5\n",
      "lif layer 1 self.abs_max_v: 3210.5\n",
      "lif layer 2 self.abs_max_v: 3701.5\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.632928/  1.983421, val:  38.33%, val_best:  45.42%, tr:  98.06%, tr_best:  98.77%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7865%\n",
      "layer   2  Sparsity: 74.5351%\n",
      "layer   3  Sparsity: 64.8507%\n",
      "total_backward_count 68530 real_backward_count 15091  22.021%\n",
      "fc layer 2 self.abs_max_out: 2444.0\n",
      "lif layer 2 self.abs_max_v: 3717.5\n",
      "fc layer 2 self.abs_max_out: 2495.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.606754/  1.886315, val:  35.00%, val_best:  45.42%, tr:  98.57%, tr_best:  98.77%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7833%\n",
      "layer   2  Sparsity: 73.6870%\n",
      "layer   3  Sparsity: 64.4626%\n",
      "total_backward_count 78320 real_backward_count 16946  21.637%\n",
      "lif layer 2 self.abs_max_v: 3808.5\n",
      "lif layer 2 self.abs_max_v: 3903.5\n",
      "lif layer 2 self.abs_max_v: 4035.5\n",
      "lif layer 2 self.abs_max_v: 4077.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.611511/  1.904805, val:  39.58%, val_best:  45.42%, tr:  98.06%, tr_best:  98.77%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7848%\n",
      "layer   2  Sparsity: 73.5501%\n",
      "layer   3  Sparsity: 64.3157%\n",
      "total_backward_count 88110 real_backward_count 18801  21.338%\n",
      "fc layer 1 self.abs_max_out: 3127.0\n",
      "fc layer 1 self.abs_max_out: 3250.0\n",
      "lif layer 1 self.abs_max_v: 3250.0\n",
      "lif layer 2 self.abs_max_v: 4078.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.602675/  1.911215, val:  43.33%, val_best:  45.42%, tr:  98.77%, tr_best:  98.77%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 96.7690%\n",
      "layer   2  Sparsity: 73.2800%\n",
      "layer   3  Sparsity: 64.6582%\n",
      "total_backward_count 97900 real_backward_count 20535  20.975%\n",
      "lif layer 2 self.abs_max_v: 4088.5\n",
      "fc layer 1 self.abs_max_out: 3270.0\n",
      "lif layer 1 self.abs_max_v: 3270.0\n",
      "fc layer 1 self.abs_max_out: 3399.0\n",
      "lif layer 1 self.abs_max_v: 3399.0\n",
      "lif layer 1 self.abs_max_v: 3666.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.578464/  1.924555, val:  37.92%, val_best:  45.42%, tr:  98.77%, tr_best:  98.77%, epoch time: 70.62 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 96.7835%\n",
      "layer   2  Sparsity: 73.8365%\n",
      "layer   3  Sparsity: 64.5201%\n",
      "total_backward_count 107690 real_backward_count 22372  20.774%\n",
      "fc layer 1 self.abs_max_out: 3456.0\n",
      "lif layer 1 self.abs_max_v: 3762.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.588899/  1.945012, val:  32.08%, val_best:  45.42%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 96.7925%\n",
      "layer   2  Sparsity: 73.1043%\n",
      "layer   3  Sparsity: 64.2347%\n",
      "total_backward_count 117480 real_backward_count 24171  20.575%\n",
      "fc layer 1 self.abs_max_out: 3534.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.563302/  1.856650, val:  34.17%, val_best:  45.42%, tr:  98.77%, tr_best:  99.49%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7878%\n",
      "layer   2  Sparsity: 72.9729%\n",
      "layer   3  Sparsity: 64.3035%\n",
      "total_backward_count 127270 real_backward_count 25881  20.336%\n",
      "fc layer 2 self.abs_max_out: 2547.0\n",
      "fc layer 1 self.abs_max_out: 3629.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.554910/  1.891440, val:  44.17%, val_best:  45.42%, tr:  98.77%, tr_best:  99.49%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7750%\n",
      "layer   2  Sparsity: 72.6673%\n",
      "layer   3  Sparsity: 63.9371%\n",
      "total_backward_count 137060 real_backward_count 27656  20.178%\n",
      "lif layer 2 self.abs_max_v: 4111.0\n",
      "lif layer 2 self.abs_max_v: 4135.5\n",
      "fc layer 1 self.abs_max_out: 3702.0\n",
      "lif layer 1 self.abs_max_v: 3834.0\n",
      "fc layer 2 self.abs_max_out: 2559.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.554539/  1.850776, val:  52.08%, val_best:  52.08%, tr:  99.28%, tr_best:  99.49%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.8042%\n",
      "layer   2  Sparsity: 71.9720%\n",
      "layer   3  Sparsity: 63.4595%\n",
      "total_backward_count 146850 real_backward_count 29321  19.967%\n",
      "fc layer 2 self.abs_max_out: 2566.0\n",
      "lif layer 2 self.abs_max_v: 4295.0\n",
      "fc layer 2 self.abs_max_out: 2574.0\n",
      "lif layer 1 self.abs_max_v: 3991.5\n",
      "lif layer 1 self.abs_max_v: 4229.0\n",
      "fc layer 2 self.abs_max_out: 2602.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.557056/  1.876693, val:  42.50%, val_best:  52.08%, tr:  98.37%, tr_best:  99.49%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7900%\n",
      "layer   2  Sparsity: 72.2626%\n",
      "layer   3  Sparsity: 64.0493%\n",
      "total_backward_count 156640 real_backward_count 31124  19.870%\n",
      "fc layer 2 self.abs_max_out: 2640.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.545823/  1.833652, val:  52.50%, val_best:  52.50%, tr:  98.98%, tr_best:  99.49%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7715%\n",
      "layer   2  Sparsity: 72.2508%\n",
      "layer   3  Sparsity: 64.8385%\n",
      "total_backward_count 166430 real_backward_count 32812  19.715%\n",
      "lif layer 2 self.abs_max_v: 4310.0\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "fc layer 2 self.abs_max_out: 2667.0\n",
      "fc layer 2 self.abs_max_out: 2674.0\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "fc layer 2 self.abs_max_out: 2778.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.560882/  1.881698, val:  47.50%, val_best:  52.50%, tr:  99.28%, tr_best:  99.49%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7780%\n",
      "layer   2  Sparsity: 72.1395%\n",
      "layer   3  Sparsity: 64.9618%\n",
      "total_backward_count 176220 real_backward_count 34528  19.594%\n",
      "lif layer 1 self.abs_max_v: 4485.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.563626/  1.842070, val:  40.42%, val_best:  52.50%, tr:  98.98%, tr_best:  99.49%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.8022%\n",
      "layer   2  Sparsity: 72.1951%\n",
      "layer   3  Sparsity: 64.8050%\n",
      "total_backward_count 186010 real_backward_count 36227  19.476%\n",
      "fc layer 2 self.abs_max_out: 2838.0\n",
      "fc layer 2 self.abs_max_out: 2846.0\n",
      "fc layer 2 self.abs_max_out: 2852.0\n",
      "fc layer 2 self.abs_max_out: 2902.0\n",
      "fc layer 2 self.abs_max_out: 2992.0\n",
      "lif layer 1 self.abs_max_v: 5049.5\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.528631/  1.838760, val:  39.17%, val_best:  52.50%, tr:  99.28%, tr_best:  99.49%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7676%\n",
      "layer   2  Sparsity: 72.1165%\n",
      "layer   3  Sparsity: 64.6141%\n",
      "total_backward_count 195800 real_backward_count 37881  19.347%\n",
      "fc layer 1 self.abs_max_out: 3788.0\n",
      "fc layer 1 self.abs_max_out: 3921.0\n",
      "fc layer 2 self.abs_max_out: 3083.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.558648/  1.872891, val:  42.92%, val_best:  52.50%, tr:  98.77%, tr_best:  99.49%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7858%\n",
      "layer   2  Sparsity: 72.0347%\n",
      "layer   3  Sparsity: 65.2941%\n",
      "total_backward_count 205590 real_backward_count 39569  19.247%\n",
      "fc layer 2 self.abs_max_out: 3126.0\n",
      "fc layer 2 self.abs_max_out: 3139.0\n",
      "fc layer 2 self.abs_max_out: 3185.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.548885/  1.836664, val:  52.50%, val_best:  52.50%, tr:  98.77%, tr_best:  99.49%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 96.7824%\n",
      "layer   2  Sparsity: 71.7608%\n",
      "layer   3  Sparsity: 64.4950%\n",
      "total_backward_count 215380 real_backward_count 41259  19.156%\n",
      "fc layer 2 self.abs_max_out: 3229.0\n",
      "lif layer 2 self.abs_max_v: 4475.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.522082/  1.810796, val:  54.17%, val_best:  54.17%, tr:  99.08%, tr_best:  99.49%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 96.7689%\n",
      "layer   2  Sparsity: 72.0870%\n",
      "layer   3  Sparsity: 64.2568%\n",
      "total_backward_count 225170 real_backward_count 42958  19.078%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.495026/  1.846552, val:  35.83%, val_best:  54.17%, tr:  99.18%, tr_best:  99.49%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.8046%\n",
      "layer   2  Sparsity: 72.1803%\n",
      "layer   3  Sparsity: 64.1354%\n",
      "total_backward_count 234960 real_backward_count 44613  18.987%\n",
      "fc layer 2 self.abs_max_out: 3289.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.515788/  1.802861, val:  49.58%, val_best:  54.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.8018%\n",
      "layer   2  Sparsity: 72.0538%\n",
      "layer   3  Sparsity: 64.1097%\n",
      "total_backward_count 244750 real_backward_count 46277  18.908%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.521683/  1.777044, val:  58.75%, val_best:  58.75%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7810%\n",
      "layer   2  Sparsity: 71.9581%\n",
      "layer   3  Sparsity: 64.0213%\n",
      "total_backward_count 254540 real_backward_count 48048  18.876%\n",
      "lif layer 1 self.abs_max_v: 5099.5\n",
      "lif layer 1 self.abs_max_v: 5194.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.505900/  1.812076, val:  51.67%, val_best:  58.75%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7901%\n",
      "layer   2  Sparsity: 72.1468%\n",
      "layer   3  Sparsity: 63.9151%\n",
      "total_backward_count 264330 real_backward_count 49725  18.812%\n",
      "fc layer 1 self.abs_max_out: 3952.0\n",
      "fc layer 1 self.abs_max_out: 4205.0\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "lif layer 1 self.abs_max_v: 5203.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.514076/  1.799676, val:  57.92%, val_best:  58.75%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7913%\n",
      "layer   2  Sparsity: 71.8952%\n",
      "layer   3  Sparsity: 64.6029%\n",
      "total_backward_count 274120 real_backward_count 51429  18.761%\n",
      "lif layer 1 self.abs_max_v: 5266.5\n",
      "lif layer 1 self.abs_max_v: 5288.5\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.515081/  1.815169, val:  44.17%, val_best:  58.75%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7813%\n",
      "layer   2  Sparsity: 72.0247%\n",
      "layer   3  Sparsity: 64.6044%\n",
      "total_backward_count 283910 real_backward_count 53054  18.687%\n",
      "lif layer 1 self.abs_max_v: 5601.5\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.499385/  1.758859, val:  42.08%, val_best:  58.75%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7814%\n",
      "layer   2  Sparsity: 71.7502%\n",
      "layer   3  Sparsity: 64.4441%\n",
      "total_backward_count 293700 real_backward_count 54721  18.632%\n",
      "lif layer 1 self.abs_max_v: 6169.5\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.469981/  1.746237, val:  65.00%, val_best:  65.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7770%\n",
      "layer   2  Sparsity: 71.6598%\n",
      "layer   3  Sparsity: 64.3933%\n",
      "total_backward_count 303490 real_backward_count 56304  18.552%\n",
      "fc layer 1 self.abs_max_out: 4233.0\n",
      "fc layer 1 self.abs_max_out: 4434.0\n",
      "lif layer 2 self.abs_max_v: 4509.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.465275/  1.772065, val:  40.83%, val_best:  65.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7695%\n",
      "layer   2  Sparsity: 71.5226%\n",
      "layer   3  Sparsity: 64.4995%\n",
      "total_backward_count 313280 real_backward_count 57923  18.489%\n",
      "fc layer 2 self.abs_max_out: 3377.0\n",
      "fc layer 1 self.abs_max_out: 4515.0\n",
      "lif layer 2 self.abs_max_v: 4524.0\n",
      "lif layer 1 self.abs_max_v: 6199.5\n",
      "fc layer 3 self.abs_max_out: 632.0\n",
      "fc layer 2 self.abs_max_out: 3382.0\n",
      "lif layer 2 self.abs_max_v: 4578.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.482835/  1.803109, val:  39.17%, val_best:  65.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7821%\n",
      "layer   2  Sparsity: 71.7468%\n",
      "layer   3  Sparsity: 64.2508%\n",
      "total_backward_count 323070 real_backward_count 59509  18.420%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.453653/  1.749387, val:  48.75%, val_best:  65.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7758%\n",
      "layer   2  Sparsity: 71.5712%\n",
      "layer   3  Sparsity: 63.3245%\n",
      "total_backward_count 332860 real_backward_count 61107  18.358%\n",
      "fc layer 3 self.abs_max_out: 707.0\n",
      "lif layer 2 self.abs_max_v: 4610.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.447984/  1.753334, val:  45.83%, val_best:  65.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7784%\n",
      "layer   2  Sparsity: 71.2178%\n",
      "layer   3  Sparsity: 62.9592%\n",
      "total_backward_count 342650 real_backward_count 62677  18.292%\n",
      "fc layer 2 self.abs_max_out: 3441.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.422225/  1.668360, val:  66.25%, val_best:  66.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7866%\n",
      "layer   2  Sparsity: 71.2010%\n",
      "layer   3  Sparsity: 63.3207%\n",
      "total_backward_count 352440 real_backward_count 64284  18.240%\n",
      "fc layer 2 self.abs_max_out: 3500.0\n",
      "fc layer 2 self.abs_max_out: 3546.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.415394/  1.723129, val:  52.50%, val_best:  66.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7969%\n",
      "layer   2  Sparsity: 71.3029%\n",
      "layer   3  Sparsity: 63.8973%\n",
      "total_backward_count 362230 real_backward_count 65837  18.175%\n",
      "lif layer 1 self.abs_max_v: 6253.0\n",
      "fc layer 2 self.abs_max_out: 3594.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.397896/  1.846399, val:  30.83%, val_best:  66.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.8029%\n",
      "layer   2  Sparsity: 71.2366%\n",
      "layer   3  Sparsity: 63.4788%\n",
      "total_backward_count 372020 real_backward_count 67353  18.105%\n",
      "lif layer 1 self.abs_max_v: 6349.5\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.451132/  1.712459, val:  72.50%, val_best:  72.50%, tr:  98.77%, tr_best:  99.69%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7932%\n",
      "layer   2  Sparsity: 71.1146%\n",
      "layer   3  Sparsity: 64.2948%\n",
      "total_backward_count 381810 real_backward_count 68928  18.053%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.446499/  1.717363, val:  58.75%, val_best:  72.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7925%\n",
      "layer   2  Sparsity: 71.0654%\n",
      "layer   3  Sparsity: 64.3480%\n",
      "total_backward_count 391600 real_backward_count 70475  17.997%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.430033/  1.712666, val:  57.92%, val_best:  72.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7776%\n",
      "layer   2  Sparsity: 71.0694%\n",
      "layer   3  Sparsity: 64.0805%\n",
      "total_backward_count 401390 real_backward_count 72016  17.942%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.429685/  1.700131, val:  61.25%, val_best:  72.50%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7907%\n",
      "layer   2  Sparsity: 71.2201%\n",
      "layer   3  Sparsity: 64.0121%\n",
      "total_backward_count 411180 real_backward_count 73573  17.893%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.417224/  1.722178, val:  57.92%, val_best:  72.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7788%\n",
      "layer   2  Sparsity: 71.2316%\n",
      "layer   3  Sparsity: 63.6945%\n",
      "total_backward_count 420970 real_backward_count 75073  17.833%\n",
      "fc layer 1 self.abs_max_out: 4776.0\n",
      "fc layer 1 self.abs_max_out: 4960.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.422092/  1.696621, val:  59.17%, val_best:  72.50%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7742%\n",
      "layer   2  Sparsity: 71.0504%\n",
      "layer   3  Sparsity: 63.6169%\n",
      "total_backward_count 430760 real_backward_count 76558  17.773%\n",
      "lif layer 2 self.abs_max_v: 4802.5\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.425279/  1.696939, val:  48.33%, val_best:  72.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7887%\n",
      "layer   2  Sparsity: 71.0451%\n",
      "layer   3  Sparsity: 63.6171%\n",
      "total_backward_count 440550 real_backward_count 78075  17.722%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.398381/  1.660854, val:  65.83%, val_best:  72.50%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7777%\n",
      "layer   2  Sparsity: 70.9362%\n",
      "layer   3  Sparsity: 63.8360%\n",
      "total_backward_count 450340 real_backward_count 79611  17.678%\n",
      "fc layer 2 self.abs_max_out: 3650.0\n",
      "lif layer 2 self.abs_max_v: 4933.0\n",
      "lif layer 2 self.abs_max_v: 4978.5\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.425231/  1.676305, val:  65.83%, val_best:  72.50%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7867%\n",
      "layer   2  Sparsity: 70.8465%\n",
      "layer   3  Sparsity: 64.2530%\n",
      "total_backward_count 460130 real_backward_count 81208  17.649%\n",
      "lif layer 2 self.abs_max_v: 4995.0\n",
      "lif layer 2 self.abs_max_v: 5062.5\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.418349/  1.700429, val:  60.00%, val_best:  72.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7864%\n",
      "layer   2  Sparsity: 70.8661%\n",
      "layer   3  Sparsity: 64.4010%\n",
      "total_backward_count 469920 real_backward_count 82720  17.603%\n",
      "lif layer 2 self.abs_max_v: 5112.5\n",
      "fc layer 2 self.abs_max_out: 3792.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.409363/  1.714174, val:  53.33%, val_best:  72.50%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.8113%\n",
      "layer   2  Sparsity: 71.0156%\n",
      "layer   3  Sparsity: 64.2979%\n",
      "total_backward_count 479710 real_backward_count 84238  17.560%\n",
      "lif layer 2 self.abs_max_v: 5172.5\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.402057/  1.670933, val:  62.08%, val_best:  72.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7818%\n",
      "layer   2  Sparsity: 70.9881%\n",
      "layer   3  Sparsity: 64.6013%\n",
      "total_backward_count 489500 real_backward_count 85737  17.515%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.407452/  1.655957, val:  60.83%, val_best:  72.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.8017%\n",
      "layer   2  Sparsity: 70.9855%\n",
      "layer   3  Sparsity: 64.6550%\n",
      "total_backward_count 499290 real_backward_count 87254  17.476%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.386375/  1.642049, val:  67.92%, val_best:  72.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7747%\n",
      "layer   2  Sparsity: 70.8253%\n",
      "layer   3  Sparsity: 64.4467%\n",
      "total_backward_count 509080 real_backward_count 88689  17.421%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.386124/  1.673833, val:  55.00%, val_best:  72.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7836%\n",
      "layer   2  Sparsity: 70.9925%\n",
      "layer   3  Sparsity: 64.2654%\n",
      "total_backward_count 518870 real_backward_count 90168  17.378%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.394832/  1.669544, val:  71.67%, val_best:  72.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.8025%\n",
      "layer   2  Sparsity: 70.9731%\n",
      "layer   3  Sparsity: 63.9494%\n",
      "total_backward_count 528660 real_backward_count 91635  17.333%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.397686/  1.659554, val:  59.58%, val_best:  72.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7674%\n",
      "layer   2  Sparsity: 70.6348%\n",
      "layer   3  Sparsity: 63.8786%\n",
      "total_backward_count 538450 real_backward_count 93140  17.298%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.366782/  1.655602, val:  58.75%, val_best:  72.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7720%\n",
      "layer   2  Sparsity: 70.9970%\n",
      "layer   3  Sparsity: 63.8379%\n",
      "total_backward_count 548240 real_backward_count 94623  17.259%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.380295/  1.714436, val:  42.08%, val_best:  72.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 96.7859%\n",
      "layer   2  Sparsity: 71.0216%\n",
      "layer   3  Sparsity: 63.9443%\n",
      "total_backward_count 558030 real_backward_count 96160  17.232%\n",
      "fc layer 3 self.abs_max_out: 755.0\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.369425/  1.644048, val:  61.67%, val_best:  72.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.8004%\n",
      "layer   2  Sparsity: 70.7674%\n",
      "layer   3  Sparsity: 63.3952%\n",
      "total_backward_count 567820 real_backward_count 97606  17.190%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.352912/  1.665407, val:  58.33%, val_best:  72.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7845%\n",
      "layer   2  Sparsity: 70.8637%\n",
      "layer   3  Sparsity: 63.1381%\n",
      "total_backward_count 577610 real_backward_count 99031  17.145%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.363659/  1.690301, val:  51.67%, val_best:  72.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7762%\n",
      "layer   2  Sparsity: 70.8447%\n",
      "layer   3  Sparsity: 63.8324%\n",
      "total_backward_count 587400 real_backward_count 100441  17.099%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.369893/  1.665354, val:  53.75%, val_best:  72.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7924%\n",
      "layer   2  Sparsity: 70.8587%\n",
      "layer   3  Sparsity: 64.6049%\n",
      "total_backward_count 597190 real_backward_count 101853  17.055%\n",
      "fc layer 2 self.abs_max_out: 3835.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.367579/  1.632204, val:  57.92%, val_best:  72.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7835%\n",
      "layer   2  Sparsity: 70.8102%\n",
      "layer   3  Sparsity: 64.1801%\n",
      "total_backward_count 606980 real_backward_count 103291  17.017%\n",
      "fc layer 1 self.abs_max_out: 5068.0\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.331057/  1.660996, val:  58.75%, val_best:  72.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7956%\n",
      "layer   2  Sparsity: 70.5629%\n",
      "layer   3  Sparsity: 63.6221%\n",
      "total_backward_count 616770 real_backward_count 104655  16.968%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.352621/  1.660499, val:  55.83%, val_best:  72.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 96.7834%\n",
      "layer   2  Sparsity: 70.5173%\n",
      "layer   3  Sparsity: 63.6674%\n",
      "total_backward_count 626560 real_backward_count 106059  16.927%\n",
      "lif layer 1 self.abs_max_v: 6491.5\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.374891/  1.633244, val:  68.33%, val_best:  72.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7883%\n",
      "layer   2  Sparsity: 70.7507%\n",
      "layer   3  Sparsity: 63.7421%\n",
      "total_backward_count 636350 real_backward_count 107536  16.899%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.354565/  1.671103, val:  63.33%, val_best:  72.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7929%\n",
      "layer   2  Sparsity: 70.7244%\n",
      "layer   3  Sparsity: 63.8662%\n",
      "total_backward_count 646140 real_backward_count 108916  16.856%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.356335/  1.651379, val:  58.75%, val_best:  72.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7812%\n",
      "layer   2  Sparsity: 70.6437%\n",
      "layer   3  Sparsity: 63.7468%\n",
      "total_backward_count 655930 real_backward_count 110352  16.824%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.328363/  1.583160, val:  72.08%, val_best:  72.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7808%\n",
      "layer   2  Sparsity: 70.5721%\n",
      "layer   3  Sparsity: 63.6887%\n",
      "total_backward_count 665720 real_backward_count 111712  16.781%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.347819/  1.606900, val:  73.75%, val_best:  73.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7875%\n",
      "layer   2  Sparsity: 70.7941%\n",
      "layer   3  Sparsity: 63.8190%\n",
      "total_backward_count 675510 real_backward_count 113177  16.754%\n",
      "lif layer 2 self.abs_max_v: 5252.0\n",
      "fc layer 1 self.abs_max_out: 5223.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.362717/  1.697080, val:  42.08%, val_best:  73.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7677%\n",
      "layer   2  Sparsity: 70.6749%\n",
      "layer   3  Sparsity: 63.7512%\n",
      "total_backward_count 685300 real_backward_count 114577  16.719%\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.347822/  1.622333, val:  61.25%, val_best:  73.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.8026%\n",
      "layer   2  Sparsity: 70.8232%\n",
      "layer   3  Sparsity: 63.8158%\n",
      "total_backward_count 695090 real_backward_count 115945  16.681%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.356940/  1.643675, val:  58.75%, val_best:  73.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7742%\n",
      "layer   2  Sparsity: 70.8792%\n",
      "layer   3  Sparsity: 63.8748%\n",
      "total_backward_count 704880 real_backward_count 117267  16.636%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.335716/  1.567024, val:  75.00%, val_best:  75.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.8063%\n",
      "layer   2  Sparsity: 70.9513%\n",
      "layer   3  Sparsity: 64.0623%\n",
      "total_backward_count 714670 real_backward_count 118668  16.605%\n",
      "fc layer 1 self.abs_max_out: 5353.0\n",
      "fc layer 1 self.abs_max_out: 5399.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.329209/  1.596625, val:  69.58%, val_best:  75.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.8139%\n",
      "layer   2  Sparsity: 70.7513%\n",
      "layer   3  Sparsity: 63.8255%\n",
      "total_backward_count 724460 real_backward_count 120067  16.573%\n",
      "lif layer 1 self.abs_max_v: 6566.5\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.326431/  1.628717, val:  55.42%, val_best:  75.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 96.7845%\n",
      "layer   2  Sparsity: 70.5474%\n",
      "layer   3  Sparsity: 63.7622%\n",
      "total_backward_count 734250 real_backward_count 121504  16.548%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.336028/  1.575240, val:  79.58%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7785%\n",
      "layer   2  Sparsity: 70.5862%\n",
      "layer   3  Sparsity: 63.7516%\n",
      "total_backward_count 744040 real_backward_count 122880  16.515%\n",
      "fc layer 2 self.abs_max_out: 3845.0\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.339911/  1.614049, val:  70.42%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7679%\n",
      "layer   2  Sparsity: 70.6746%\n",
      "layer   3  Sparsity: 63.9217%\n",
      "total_backward_count 753830 real_backward_count 124330  16.493%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.333726/  1.623003, val:  60.42%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7923%\n",
      "layer   2  Sparsity: 70.5479%\n",
      "layer   3  Sparsity: 63.7910%\n",
      "total_backward_count 763620 real_backward_count 125726  16.464%\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "lif layer 1 self.abs_max_v: 6762.5\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.323000/  1.615427, val:  62.08%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7728%\n",
      "layer   2  Sparsity: 70.4758%\n",
      "layer   3  Sparsity: 63.3735%\n",
      "total_backward_count 773410 real_backward_count 127111  16.435%\n",
      "fc layer 2 self.abs_max_out: 3869.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.339261/  1.591040, val:  70.83%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7800%\n",
      "layer   2  Sparsity: 70.5908%\n",
      "layer   3  Sparsity: 63.5501%\n",
      "total_backward_count 783200 real_backward_count 128520  16.410%\n",
      "fc layer 1 self.abs_max_out: 5506.0\n",
      "fc layer 1 self.abs_max_out: 5568.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.334024/  1.617199, val:  67.08%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7849%\n",
      "layer   2  Sparsity: 70.5054%\n",
      "layer   3  Sparsity: 63.2509%\n",
      "total_backward_count 792990 real_backward_count 129888  16.380%\n",
      "lif layer 2 self.abs_max_v: 5355.0\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.338002/  1.668393, val:  50.42%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7731%\n",
      "layer   2  Sparsity: 70.3027%\n",
      "layer   3  Sparsity: 63.7855%\n",
      "total_backward_count 802780 real_backward_count 131242  16.348%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.356049/  1.607554, val:  69.17%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7660%\n",
      "layer   2  Sparsity: 70.3153%\n",
      "layer   3  Sparsity: 64.2390%\n",
      "total_backward_count 812570 real_backward_count 132573  16.315%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.334765/  1.713814, val:  44.17%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7698%\n",
      "layer   2  Sparsity: 70.1265%\n",
      "layer   3  Sparsity: 63.9685%\n",
      "total_backward_count 822360 real_backward_count 133889  16.281%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.341468/  1.590758, val:  60.00%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7833%\n",
      "layer   2  Sparsity: 70.0687%\n",
      "layer   3  Sparsity: 63.5434%\n",
      "total_backward_count 832150 real_backward_count 135265  16.255%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.341691/  1.645948, val:  54.58%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7785%\n",
      "layer   2  Sparsity: 70.1882%\n",
      "layer   3  Sparsity: 63.7526%\n",
      "total_backward_count 841940 real_backward_count 136618  16.227%\n",
      "fc layer 1 self.abs_max_out: 5700.0\n",
      "fc layer 1 self.abs_max_out: 5746.0\n",
      "lif layer 1 self.abs_max_v: 6774.5\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.347089/  1.609316, val:  63.33%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.8042%\n",
      "layer   2  Sparsity: 70.1885%\n",
      "layer   3  Sparsity: 63.1383%\n",
      "total_backward_count 851730 real_backward_count 138014  16.204%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.332202/  1.615832, val:  62.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7876%\n",
      "layer   2  Sparsity: 70.2110%\n",
      "layer   3  Sparsity: 63.6348%\n",
      "total_backward_count 861520 real_backward_count 139336  16.173%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.310427/  1.606809, val:  65.42%, val_best:  79.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7747%\n",
      "layer   2  Sparsity: 70.4112%\n",
      "layer   3  Sparsity: 63.1741%\n",
      "total_backward_count 871310 real_backward_count 140692  16.147%\n",
      "fc layer 1 self.abs_max_out: 5760.0\n",
      "fc layer 1 self.abs_max_out: 5801.0\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.315574/  1.578416, val:  68.33%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7765%\n",
      "layer   2  Sparsity: 70.4468%\n",
      "layer   3  Sparsity: 62.9064%\n",
      "total_backward_count 881100 real_backward_count 142066  16.124%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.308639/  1.585957, val:  67.92%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7955%\n",
      "layer   2  Sparsity: 70.3744%\n",
      "layer   3  Sparsity: 63.4390%\n",
      "total_backward_count 890890 real_backward_count 143378  16.094%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.319943/  1.564925, val:  73.33%, val_best:  79.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7777%\n",
      "layer   2  Sparsity: 70.4582%\n",
      "layer   3  Sparsity: 63.8587%\n",
      "total_backward_count 900680 real_backward_count 144761  16.072%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.328143/  1.635740, val:  53.75%, val_best:  79.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 96.7970%\n",
      "layer   2  Sparsity: 70.4952%\n",
      "layer   3  Sparsity: 64.1820%\n",
      "total_backward_count 910470 real_backward_count 146148  16.052%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.333229/  1.638781, val:  64.58%, val_best:  79.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 96.7848%\n",
      "layer   2  Sparsity: 70.3419%\n",
      "layer   3  Sparsity: 63.5789%\n",
      "total_backward_count 920260 real_backward_count 147481  16.026%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.331376/  1.583912, val:  69.17%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7758%\n",
      "layer   2  Sparsity: 70.3525%\n",
      "layer   3  Sparsity: 63.6043%\n",
      "total_backward_count 930050 real_backward_count 148805  16.000%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.316486/  1.587580, val:  70.83%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7854%\n",
      "layer   2  Sparsity: 70.2356%\n",
      "layer   3  Sparsity: 63.0011%\n",
      "total_backward_count 939840 real_backward_count 150147  15.976%\n",
      "fc layer 2 self.abs_max_out: 3909.0\n",
      "lif layer 1 self.abs_max_v: 6962.5\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.299821/  1.588827, val:  68.75%, val_best:  79.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7611%\n",
      "layer   2  Sparsity: 70.1683%\n",
      "layer   3  Sparsity: 63.2329%\n",
      "total_backward_count 949630 real_backward_count 151495  15.953%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.298763/  1.574371, val:  63.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7971%\n",
      "layer   2  Sparsity: 70.2840%\n",
      "layer   3  Sparsity: 63.7296%\n",
      "total_backward_count 959420 real_backward_count 152807  15.927%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.314299/  1.592991, val:  71.67%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7897%\n",
      "layer   2  Sparsity: 70.1813%\n",
      "layer   3  Sparsity: 63.9549%\n",
      "total_backward_count 969210 real_backward_count 154169  15.907%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.303362/  1.573686, val:  75.42%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7927%\n",
      "layer   2  Sparsity: 70.2087%\n",
      "layer   3  Sparsity: 63.8755%\n",
      "total_backward_count 979000 real_backward_count 155475  15.881%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.315877/  1.567163, val:  69.58%, val_best:  79.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7858%\n",
      "layer   2  Sparsity: 70.2767%\n",
      "layer   3  Sparsity: 63.6205%\n",
      "total_backward_count 988790 real_backward_count 156768  15.855%\n",
      "fc layer 1 self.abs_max_out: 5961.0\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.308573/  1.595360, val:  73.33%, val_best:  79.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7814%\n",
      "layer   2  Sparsity: 70.2389%\n",
      "layer   3  Sparsity: 63.5258%\n",
      "total_backward_count 998580 real_backward_count 158135  15.836%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.320745/  1.583780, val:  79.58%, val_best:  79.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7806%\n",
      "layer   2  Sparsity: 70.4226%\n",
      "layer   3  Sparsity: 64.2048%\n",
      "total_backward_count 1008370 real_backward_count 159423  15.810%\n",
      "fc layer 2 self.abs_max_out: 3914.0\n",
      "lif layer 1 self.abs_max_v: 7089.0\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.327969/  1.608603, val:  50.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7745%\n",
      "layer   2  Sparsity: 70.4744%\n",
      "layer   3  Sparsity: 64.7380%\n",
      "total_backward_count 1018160 real_backward_count 160713  15.785%\n",
      "fc layer 2 self.abs_max_out: 3921.0\n",
      "fc layer 2 self.abs_max_out: 3935.0\n",
      "fc layer 2 self.abs_max_out: 3947.0\n",
      "lif layer 2 self.abs_max_v: 5395.5\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.317790/  1.557875, val:  77.50%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7641%\n",
      "layer   2  Sparsity: 70.1882%\n",
      "layer   3  Sparsity: 64.2987%\n",
      "total_backward_count 1027950 real_backward_count 162048  15.764%\n",
      "fc layer 2 self.abs_max_out: 3983.0\n",
      "lif layer 2 self.abs_max_v: 5477.5\n",
      "lif layer 2 self.abs_max_v: 5497.0\n",
      "fc layer 2 self.abs_max_out: 4030.0\n",
      "lif layer 1 self.abs_max_v: 7193.5\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.310773/  1.625617, val:  52.92%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7927%\n",
      "layer   2  Sparsity: 70.3615%\n",
      "layer   3  Sparsity: 64.2766%\n",
      "total_backward_count 1037740 real_backward_count 163384  15.744%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.331988/  1.641084, val:  65.00%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.8005%\n",
      "layer   2  Sparsity: 70.2996%\n",
      "layer   3  Sparsity: 63.9444%\n",
      "total_backward_count 1047530 real_backward_count 164695  15.722%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.316497/  1.591371, val:  62.08%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7459%\n",
      "layer   2  Sparsity: 70.1438%\n",
      "layer   3  Sparsity: 63.6577%\n",
      "total_backward_count 1057320 real_backward_count 166005  15.701%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.307682/  1.572258, val:  78.75%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 96.7799%\n",
      "layer   2  Sparsity: 70.1961%\n",
      "layer   3  Sparsity: 63.8055%\n",
      "total_backward_count 1067110 real_backward_count 167327  15.680%\n",
      "fc layer 2 self.abs_max_out: 4044.0\n",
      "fc layer 2 self.abs_max_out: 4114.0\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.328429/  1.595330, val:  72.08%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7862%\n",
      "layer   2  Sparsity: 70.1803%\n",
      "layer   3  Sparsity: 63.7276%\n",
      "total_backward_count 1076900 real_backward_count 168625  15.658%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.308275/  1.576853, val:  65.42%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7861%\n",
      "layer   2  Sparsity: 70.1652%\n",
      "layer   3  Sparsity: 63.7629%\n",
      "total_backward_count 1086690 real_backward_count 169929  15.637%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.313232/  1.610597, val:  67.08%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 96.7828%\n",
      "layer   2  Sparsity: 70.1967%\n",
      "layer   3  Sparsity: 63.9062%\n",
      "total_backward_count 1096480 real_backward_count 171263  15.619%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.315097/  1.589957, val:  78.33%, val_best:  79.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.8013%\n",
      "layer   2  Sparsity: 69.9168%\n",
      "layer   3  Sparsity: 63.4046%\n",
      "total_backward_count 1106270 real_backward_count 172582  15.600%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.311938/  1.578352, val:  59.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7988%\n",
      "layer   2  Sparsity: 69.8866%\n",
      "layer   3  Sparsity: 63.6946%\n",
      "total_backward_count 1116060 real_backward_count 173878  15.580%\n",
      "fc layer 2 self.abs_max_out: 4146.0\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.303576/  1.583771, val:  66.25%, val_best:  79.58%, tr:  99.39%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7777%\n",
      "layer   2  Sparsity: 69.9507%\n",
      "layer   3  Sparsity: 63.9074%\n",
      "total_backward_count 1125850 real_backward_count 175205  15.562%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.316390/  1.584563, val:  78.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7814%\n",
      "layer   2  Sparsity: 69.9170%\n",
      "layer   3  Sparsity: 63.6424%\n",
      "total_backward_count 1135640 real_backward_count 176472  15.539%\n",
      "lif layer 1 self.abs_max_v: 7201.0\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.312492/  1.592919, val:  63.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7973%\n",
      "layer   2  Sparsity: 70.0888%\n",
      "layer   3  Sparsity: 63.7039%\n",
      "total_backward_count 1145430 real_backward_count 177772  15.520%\n",
      "fc layer 1 self.abs_max_out: 6011.0\n",
      "lif layer 1 self.abs_max_v: 7248.0\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.288562/  1.563243, val:  80.00%, val_best:  80.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.7850%\n",
      "layer   2  Sparsity: 70.1429%\n",
      "layer   3  Sparsity: 63.8773%\n",
      "total_backward_count 1155220 real_backward_count 179055  15.500%\n",
      "fc layer 2 self.abs_max_out: 4150.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.306679/  1.574403, val:  68.33%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 96.8006%\n",
      "layer   2  Sparsity: 70.2158%\n",
      "layer   3  Sparsity: 64.2350%\n",
      "total_backward_count 1165010 real_backward_count 180290  15.475%\n",
      "fc layer 2 self.abs_max_out: 4159.0\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.299232/  1.558372, val:  63.75%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7606%\n",
      "layer   2  Sparsity: 69.9039%\n",
      "layer   3  Sparsity: 63.8560%\n",
      "total_backward_count 1174800 real_backward_count 181561  15.455%\n",
      "fc layer 2 self.abs_max_out: 4195.0\n",
      "fc layer 2 self.abs_max_out: 4222.0\n",
      "lif layer 1 self.abs_max_v: 7320.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.268473/  1.532005, val:  73.33%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7887%\n",
      "layer   2  Sparsity: 70.0413%\n",
      "layer   3  Sparsity: 64.3361%\n",
      "total_backward_count 1184590 real_backward_count 182847  15.435%\n",
      "fc layer 2 self.abs_max_out: 4336.0\n",
      "lif layer 1 self.abs_max_v: 7456.5\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.275986/  1.545555, val:  70.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.8073%\n",
      "layer   2  Sparsity: 70.1395%\n",
      "layer   3  Sparsity: 64.6612%\n",
      "total_backward_count 1194380 real_backward_count 184124  15.416%\n",
      "fc layer 2 self.abs_max_out: 4418.0\n",
      "fc layer 1 self.abs_max_out: 6239.0\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.274848/  1.551059, val:  72.50%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7710%\n",
      "layer   2  Sparsity: 69.9388%\n",
      "layer   3  Sparsity: 64.1212%\n",
      "total_backward_count 1204170 real_backward_count 185449  15.401%\n",
      "fc layer 1 self.abs_max_out: 6245.0\n",
      "fc layer 1 self.abs_max_out: 6249.0\n",
      "lif layer 1 self.abs_max_v: 7467.5\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.279255/  1.560070, val:  64.17%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7807%\n",
      "layer   2  Sparsity: 69.9422%\n",
      "layer   3  Sparsity: 63.7546%\n",
      "total_backward_count 1213960 real_backward_count 186703  15.380%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.289265/  1.551939, val:  73.33%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7701%\n",
      "layer   2  Sparsity: 69.9258%\n",
      "layer   3  Sparsity: 63.9439%\n",
      "total_backward_count 1223750 real_backward_count 188029  15.365%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.302883/  1.600562, val:  54.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.7794%\n",
      "layer   2  Sparsity: 69.7463%\n",
      "layer   3  Sparsity: 63.9886%\n",
      "total_backward_count 1233540 real_backward_count 189285  15.345%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.280404/  1.616572, val:  47.92%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.8046%\n",
      "layer   2  Sparsity: 69.7813%\n",
      "layer   3  Sparsity: 63.8852%\n",
      "total_backward_count 1243330 real_backward_count 190549  15.326%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.280013/  1.590935, val:  54.17%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7895%\n",
      "layer   2  Sparsity: 69.7548%\n",
      "layer   3  Sparsity: 64.1486%\n",
      "total_backward_count 1253120 real_backward_count 191825  15.308%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.301239/  1.554667, val:  68.75%, val_best:  80.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 96.7951%\n",
      "layer   2  Sparsity: 69.7760%\n",
      "layer   3  Sparsity: 64.6856%\n",
      "total_backward_count 1262910 real_backward_count 193060  15.287%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.291262/  1.571818, val:  59.58%, val_best:  80.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7998%\n",
      "layer   2  Sparsity: 69.5940%\n",
      "layer   3  Sparsity: 64.0112%\n",
      "total_backward_count 1272700 real_backward_count 194314  15.268%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.282668/  1.565604, val:  60.83%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 96.7622%\n",
      "layer   2  Sparsity: 69.7295%\n",
      "layer   3  Sparsity: 63.6413%\n",
      "total_backward_count 1282490 real_backward_count 195578  15.250%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.280036/  1.560693, val:  66.25%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.7778%\n",
      "layer   2  Sparsity: 69.7781%\n",
      "layer   3  Sparsity: 64.5982%\n",
      "total_backward_count 1292280 real_backward_count 196862  15.234%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.302603/  1.578586, val:  70.42%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 96.8032%\n",
      "layer   2  Sparsity: 69.7712%\n",
      "layer   3  Sparsity: 64.0908%\n",
      "total_backward_count 1302070 real_backward_count 198131  15.217%\n",
      "lif layer 2 self.abs_max_v: 5579.5\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.292392/  1.532586, val:  83.33%, val_best:  83.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 96.7767%\n",
      "layer   2  Sparsity: 69.6451%\n",
      "layer   3  Sparsity: 64.5591%\n",
      "total_backward_count 1311860 real_backward_count 199374  15.198%\n",
      "fc layer 2 self.abs_max_out: 4485.0\n",
      "lif layer 2 self.abs_max_v: 5585.0\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.285919/  1.559655, val:  73.75%, val_best:  83.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 96.7797%\n",
      "layer   2  Sparsity: 69.5968%\n",
      "layer   3  Sparsity: 64.2249%\n",
      "total_backward_count 1321650 real_backward_count 200642  15.181%\n",
      "fc layer 2 self.abs_max_out: 4507.0\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.281302/  1.541658, val:  68.33%, val_best:  83.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 96.8082%\n",
      "layer   2  Sparsity: 69.6547%\n",
      "layer   3  Sparsity: 64.3412%\n",
      "total_backward_count 1331440 real_backward_count 201929  15.166%\n",
      "fc layer 2 self.abs_max_out: 4607.0\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [5, 10,]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [5, 10, 15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [12_000, 25_000, 50_000, 75_000, 100_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"2\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'pyz704uj'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
