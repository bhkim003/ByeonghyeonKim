{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33865/4213678604.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "\n",
    "                    timestep_sums_threshold = 15,\n",
    "\n",
    "                    loser_encourage_mode = False, # True # False\n",
    "                    \n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    if which_data == 'n_tidigits_tonic':\n",
    "        assert merge_polarities == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    synapse_fc_out_features = 10\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    # # wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    # ###########################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    # class CustomLossFunction(torch.autograd.Function):\n",
    "    #     @staticmethod\n",
    "    #     def forward(ctx, input, target):\n",
    "    #         ctx.save_for_backward(input, target)\n",
    "    #         return F.cross_entropy(input, target)\n",
    "\n",
    "    #     @staticmethod\n",
    "    #     def backward(ctx, grad_output):\n",
    "    #         # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "    #         input, target = ctx.saved_tensors\n",
    "    #         input_argmax = input.argmax(dim=1)\n",
    "    #         input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "    #         target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "    #         # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "    #         return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "    \n",
    "\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            input, target = ctx.saved_tensors\n",
    "            assert input.shape[0] == 1 and target.shape[0] == 1, \"Batch size must be 1 for this custom loss function.\"\n",
    "            batch_size, num_classes = input.shape\n",
    "\n",
    "            target_0 = [0,1,2,3,4]\n",
    "            target_1 = [5,6,7,8,9]\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "\n",
    "            if (target.item() == 0) and (input_argmax.item() in target_0) or \\\n",
    "                (target.item() == 1) and (input_argmax.item() in target_1):\n",
    "                return input_one_hot - input_one_hot, None  \n",
    "            else:\n",
    "                if target.item() == 0:\n",
    "                    input_slice = input[:, 0:5]\n",
    "                    if loser_encourage_mode:\n",
    "                        input_argmin = input_slice.argmin(dim=1)\n",
    "                    else:\n",
    "                        input_argmin = input_slice.argmax(dim=1)\n",
    "                elif target.item() == 1:\n",
    "                    input_slice = input[:, 5:10] \n",
    "                    if loser_encourage_mode:\n",
    "                        input_argmin = input_slice.argmin(dim=1) + 5\n",
    "                    else:\n",
    "                        input_argmin = input_slice.argmax(dim=1) + 5\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target: {target.item()}\")\n",
    "\n",
    "                # gradient Î∞©Ìñ•ÏùÑ argmin Ï™ΩÏúºÎ°ú\n",
    "                modified_target_one_hot = torch.zeros_like(input).scatter_(1, input_argmin.unsqueeze(1), 1.0)\n",
    "\n",
    "                return input_one_hot - modified_target_one_hot, None\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "            self.additional_dw_weight = 1.0\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                    \n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                            \n",
    "                    \n",
    "                    dw = dw * self.additional_dw_weight\n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    max_activation_accul = 0\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        # optimizer.additional_dw_weight = 1.0 if epoch % 2 ==0 else 0.0\n",
    "        optimizer.additional_dw_weight = 1.0\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        # if epoch %2 == 0:\n",
    "        #     iterator = enumerate(train_loader, 0)\n",
    "        # else:\n",
    "        #     iterator = enumerate(test_loader, 0)\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        train_spike_distribution = []\n",
    "        train_predicted_distribution = []\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                # inputs: [Batch, Time, Channel, Height, Width]\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif (which_data == 'n_tidigits_tonic'):\n",
    "                inputs = inputs.unsqueeze(-1)\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                # labels = torch.tensor(labels) \n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "                            \n",
    "            if i == 1:\n",
    "                # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                        \n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            \n",
    "            \n",
    "                        \n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            hetero_timesteps = True\n",
    "            if hetero_timesteps == True:\n",
    "                assert real_batch == 1\n",
    "                this_data_timesteps = inputs.shape[0]\n",
    "                TIME = this_data_timesteps//temporal_filter\n",
    "                net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            \n",
    "\n",
    "            \n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    # inputs # [Time, Batch, Channel, Height, Width]\n",
    "                    # inputs # [Batch, Channel, Height,Time, Width]\n",
    "                    # inputs # [Batch, Channel, Height,Time * Width]\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "            \n",
    "            # if hetero_timesteps == True:\n",
    "            #     assert real_batch == 1\n",
    "            #     # inputs # [Time, Batch, Channel, Height, Width]\n",
    "            #     # inputs timestpeÎ≥ÑÎ°ú sumÍ∞íÏù¥ 10ÎØ∏ÎßåÏùº Ïãú Ï†úÏô∏\n",
    "            #     # time stepÎ≥Ñ Ìï© Í≥ÑÏÇ∞: shape = [T]\n",
    "            #     timestep_sums = inputs.sum(dim=(1,2,3,4))  # sum over (B, C, H, W)\n",
    "\n",
    "            #     # 10 Ïù¥ÏÉÅÏù∏ ÌÉÄÏûÑÏä§ÌÖùÎßå ÏÑ†ÌÉù\n",
    "            #     valid_timesteps = timestep_sums >= timestep_sums_threshold\n",
    "            #     assert valid_timesteps.sum().item() != 0, \"No valid timesteps found. Check your data preprocessing.\"\n",
    "\n",
    "            #     # Ìï¥Îãπ ÌÉÄÏûÑÏä§ÌÖùÎßå Ï∂îÏ∂ú\n",
    "            #     inputs = inputs[valid_timesteps]\n",
    "            #     TIME = inputs.shape[0] # validÌïú time stepÏùò Í∞úÏàò\n",
    "            #     net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            train_spike_distribution.append(TIME)\n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device).to(torch.float)\n",
    "            labels = labels.to(device).to(torch.long)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            bp_timestep = random.randint(0, TIME - 1)  # 0 ~ TIME-1 Ï§ë ÌïòÎÇò ÏÑ†ÌÉù\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = ((outputs_one_time.detach()).argmax(dim=1) >= 5).long()\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "                    # optimizer.additional_dw_weight = 1.0 if t == bp_timestep else 0.0\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            \n",
    "            # target_0 = [0,1,2,3,4]\n",
    "            # target_1 = [5,6,7,8,9]\n",
    "            predicted = (predicted >= 5).long()\n",
    "            train_predicted_distribution.append(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            # if True :\n",
    "            if i == len(train_loader)-1 :\n",
    "                \n",
    "                \n",
    "                train_predicted_distribution = np.array(train_predicted_distribution)\n",
    "                unique_vals, counts = np.unique(train_predicted_distribution, return_counts=True)\n",
    "                for val, count in zip(unique_vals, counts):\n",
    "                    print(f\"train - Value {val}: {count} occurrences\")\n",
    "\n",
    "                print(f'train_spike_distribution.mean {np.mean(train_spike_distribution):.6f}, min {np.min(train_spike_distribution)}, max {np.max(train_spike_distribution)}')\n",
    "\n",
    "\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "                \n",
    "                test_spike_distribution = []\n",
    "                test_predicted_distribution = []\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    # for data_val in train_loader:\n",
    "                    for data_val in test_loader:\n",
    "                    # for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "                            \n",
    "                        ## batch ÌÅ¨Í∏∞ ######################################\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        ###########################################################\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif (which_data == 'n_tidigits_tonic'):\n",
    "                            inputs_val = inputs_val.unsqueeze(-1)\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                            # labels_val = torch.tensor(labels_val)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "                        \n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        hetero_timesteps = True\n",
    "                        if hetero_timesteps == True:\n",
    "                            assert real_batch == 1\n",
    "                            this_data_timesteps = inputs_val.shape[0]\n",
    "                            TIME = this_data_timesteps//temporal_filter\n",
    "                            net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        \n",
    "\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        \n",
    "                                    \n",
    "                        # if hetero_timesteps == True:\n",
    "                        #     assert real_batch == 1\n",
    "                        #     # inputs_val # [Time, Batch, Channel, Height, Width]\n",
    "                        #     # inputs_val timestpeÎ≥ÑÎ°ú sumÍ∞íÏù¥ 10ÎØ∏ÎßåÏùº Ïãú Ï†úÏô∏\n",
    "                        #     # time stepÎ≥Ñ Ìï© Í≥ÑÏÇ∞: shape = [T]\n",
    "                        #     timestep_sums = inputs_val.sum(dim=(1,2,3,4))  # sum over (B, C, H, W)\n",
    "\n",
    "                        #     # 10 Ïù¥ÏÉÅÏù∏ ÌÉÄÏûÑÏä§ÌÖùÎßå ÏÑ†ÌÉù\n",
    "                        #     valid_timesteps = timestep_sums >= timestep_sums_threshold\n",
    "                        #     assert valid_timesteps.sum().item() != 0, \"No valid timesteps found. Check your data preprocessing.\"\n",
    "\n",
    "                        #     # Ìï¥Îãπ ÌÉÄÏûÑÏä§ÌÖùÎßå Ï∂îÏ∂ú\n",
    "                        #     inputs_val = inputs_val[valid_timesteps]\n",
    "                        #     TIME = inputs_val.shape[0] # validÌïú time stepÏùò Í∞úÏàò\n",
    "                        #     net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        test_spike_distribution.append(TIME)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(torch.float).to(device)\n",
    "                        labels_val = labels_val.to(torch.long).to(device)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                            \n",
    "                            if max_activation_accul < outputs.abs().max().item() * TIME * (2**(-scale_exp[2][0])):\n",
    "                                max_activation_accul = outputs.abs().max().item() * TIME * (2**(-scale_exp[2][0]))\n",
    "                                print(f\"max_activation_accul updated: {max_activation_accul:.2f} at epoch {epoch}, iter {i}\")\n",
    "                       \n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                                    \n",
    "                        predicted = (predicted >= 5).long()\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "                        test_predicted_distribution.append(predicted.cpu().numpy())\n",
    "\n",
    "                    print(f'test_spike_distribution.mean {np.mean(test_spike_distribution):.6f}, min {np.min(test_spike_distribution)}, max {np.max(test_spike_distribution)}')\n",
    "\n",
    "                    test_predicted_distribution = np.array(test_predicted_distribution)\n",
    "                    unique_vals, counts = np.unique(test_predicted_distribution, return_counts=True)\n",
    "                    for val, count in zip(unique_vals, counts):\n",
    "                        print(f\"test - Value {val}: {count} occurrences\")\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251223_132733-wryp8dv0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/wryp8dv0' target=\"_blank\">pretty-river-162</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/wryp8dv0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/wryp8dv0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251223_132731_807', 'my_seed': 42, 'TIME': 4, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 256, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_20250704_185524_987.pth', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 2, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 16.0, 'lif_layer_v_threshold2': 512.0, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 2, 'learning_rate2': 1, 'loser_encourage_mode': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4030, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4030 BATCH: 1 train_data_count: 4030\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4.0, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 16.0, self.v_threshold 512.0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=256, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=512.0, v_reset=10000.0, sg_width=16.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 2\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1450.0\n",
      "lif layer 1 self.abs_max_v: 1450.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "lif layer 2 self.abs_max_v: 571.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "layer   1  Sparsity: 87.4023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 100.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 1519.0\n",
      "lif layer 1 self.abs_max_v: 1519.0\n",
      "lif layer 2 self.abs_max_v: 669.5\n",
      "fc layer 3 self.abs_max_out: 8.0\n",
      "lif layer 1 self.abs_max_v: 1576.0\n",
      "fc layer 1 self.abs_max_out: 1742.0\n",
      "lif layer 1 self.abs_max_v: 1836.5\n",
      "fc layer 1 self.abs_max_out: 1829.0\n",
      "fc layer 2 self.abs_max_out: 579.0\n",
      "fc layer 1 self.abs_max_out: 1878.0\n",
      "lif layer 1 self.abs_max_v: 1837.0\n",
      "fc layer 1 self.abs_max_out: 1940.0\n",
      "lif layer 1 self.abs_max_v: 1968.0\n",
      "fc layer 1 self.abs_max_out: 2075.0\n",
      "lif layer 1 self.abs_max_v: 2078.5\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 2269.0\n",
      "lif layer 1 self.abs_max_v: 2250.5\n",
      "lif layer 1 self.abs_max_v: 2269.0\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "fc layer 1 self.abs_max_out: 2644.0\n",
      "lif layer 1 self.abs_max_v: 2684.0\n",
      "lif layer 2 self.abs_max_v: 757.0\n",
      "fc layer 1 self.abs_max_out: 2746.0\n",
      "lif layer 1 self.abs_max_v: 2746.0\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "fc layer 1 self.abs_max_out: 2793.0\n",
      "lif layer 1 self.abs_max_v: 2793.0\n",
      "fc layer 1 self.abs_max_out: 2888.0\n",
      "lif layer 1 self.abs_max_v: 2888.0\n",
      "fc layer 1 self.abs_max_out: 2906.0\n",
      "lif layer 1 self.abs_max_v: 2906.0\n",
      "fc layer 2 self.abs_max_out: 626.0\n",
      "lif layer 2 self.abs_max_v: 765.0\n",
      "fc layer 1 self.abs_max_out: 3168.0\n",
      "lif layer 1 self.abs_max_v: 3168.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "fc layer 2 self.abs_max_out: 679.0\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "fc layer 2 self.abs_max_out: 808.0\n",
      "lif layer 2 self.abs_max_v: 1009.5\n",
      "fc layer 1 self.abs_max_out: 3304.0\n",
      "lif layer 1 self.abs_max_v: 3304.0\n",
      "fc layer 1 self.abs_max_out: 3460.0\n",
      "lif layer 1 self.abs_max_v: 3526.5\n",
      "fc layer 1 self.abs_max_out: 3467.0\n",
      "fc layer 1 self.abs_max_out: 3771.0\n",
      "lif layer 1 self.abs_max_v: 3771.0\n",
      "fc layer 1 self.abs_max_out: 3931.0\n",
      "lif layer 1 self.abs_max_v: 3931.0\n",
      "fc layer 1 self.abs_max_out: 4006.0\n",
      "lif layer 1 self.abs_max_v: 4041.5\n",
      "fc layer 2 self.abs_max_out: 917.0\n",
      "fc layer 1 self.abs_max_out: 4305.0\n",
      "lif layer 1 self.abs_max_v: 4305.0\n",
      "lif layer 2 self.abs_max_v: 1074.5\n",
      "fc layer 2 self.abs_max_out: 990.0\n",
      "lif layer 2 self.abs_max_v: 1223.0\n",
      "fc layer 2 self.abs_max_out: 1053.0\n",
      "fc layer 1 self.abs_max_out: 4410.0\n",
      "lif layer 1 self.abs_max_v: 4410.0\n",
      "fc layer 2 self.abs_max_out: 1077.0\n",
      "fc layer 3 self.abs_max_out: 27.0\n",
      "fc layer 1 self.abs_max_out: 4575.0\n",
      "lif layer 1 self.abs_max_v: 4575.0\n",
      "lif layer 2 self.abs_max_v: 1247.5\n",
      "fc layer 1 self.abs_max_out: 4793.0\n",
      "lif layer 1 self.abs_max_v: 4905.5\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "fc layer 2 self.abs_max_out: 1126.0\n",
      "lif layer 2 self.abs_max_v: 1269.0\n",
      "lif layer 2 self.abs_max_v: 1289.0\n",
      "fc layer 1 self.abs_max_out: 5050.0\n",
      "lif layer 1 self.abs_max_v: 5050.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 1 self.abs_max_out: 5097.0\n",
      "lif layer 1 self.abs_max_v: 5097.0\n",
      "lif layer 2 self.abs_max_v: 1300.0\n",
      "fc layer 2 self.abs_max_out: 1129.0\n",
      "fc layer 2 self.abs_max_out: 1167.0\n",
      "lif layer 2 self.abs_max_v: 1391.0\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 2 self.abs_max_out: 1190.0\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1413.0\n",
      "fc layer 1 self.abs_max_out: 5262.0\n",
      "lif layer 1 self.abs_max_v: 5262.0\n",
      "fc layer 2 self.abs_max_out: 1233.0\n",
      "fc layer 2 self.abs_max_out: 1263.0\n",
      "fc layer 1 self.abs_max_out: 5497.0\n",
      "lif layer 1 self.abs_max_v: 5497.0\n",
      "fc layer 2 self.abs_max_out: 1271.0\n",
      "fc layer 2 self.abs_max_out: 1334.0\n",
      "fc layer 1 self.abs_max_out: 5554.0\n",
      "lif layer 1 self.abs_max_v: 5554.0\n",
      "fc layer 2 self.abs_max_out: 1380.0\n",
      "fc layer 1 self.abs_max_out: 5594.0\n",
      "lif layer 1 self.abs_max_v: 5594.0\n",
      "lif layer 2 self.abs_max_v: 1486.5\n",
      "lif layer 2 self.abs_max_v: 1499.0\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "lif layer 2 self.abs_max_v: 1522.5\n",
      "fc layer 1 self.abs_max_out: 6109.0\n",
      "lif layer 1 self.abs_max_v: 6109.0\n",
      "fc layer 2 self.abs_max_out: 1459.0\n",
      "lif layer 2 self.abs_max_v: 1655.5\n",
      "fc layer 2 self.abs_max_out: 1501.0\n",
      "fc layer 2 self.abs_max_out: 1509.0\n",
      "fc layer 2 self.abs_max_out: 1534.0\n",
      "fc layer 2 self.abs_max_out: 1540.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 1 self.abs_max_out: 6253.0\n",
      "lif layer 1 self.abs_max_v: 6253.0\n",
      "fc layer 1 self.abs_max_out: 6317.0\n",
      "lif layer 1 self.abs_max_v: 6317.0\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "lif layer 2 self.abs_max_v: 1691.5\n",
      "fc layer 2 self.abs_max_out: 1564.0\n",
      "fc layer 1 self.abs_max_out: 6416.0\n",
      "lif layer 1 self.abs_max_v: 6416.0\n",
      "fc layer 1 self.abs_max_out: 6631.0\n",
      "lif layer 1 self.abs_max_v: 6631.0\n",
      "fc layer 1 self.abs_max_out: 6655.0\n",
      "lif layer 1 self.abs_max_v: 6655.0\n",
      "fc layer 1 self.abs_max_out: 6834.0\n",
      "lif layer 1 self.abs_max_v: 6834.0\n",
      "fc layer 1 self.abs_max_out: 7006.0\n",
      "lif layer 1 self.abs_max_v: 7006.0\n",
      "fc layer 1 self.abs_max_out: 7036.0\n",
      "lif layer 1 self.abs_max_v: 7036.0\n",
      "fc layer 1 self.abs_max_out: 7147.0\n",
      "lif layer 1 self.abs_max_v: 7147.0\n",
      "fc layer 1 self.abs_max_out: 7205.0\n",
      "lif layer 1 self.abs_max_v: 7205.0\n",
      "fc layer 3 self.abs_max_out: 39.0\n",
      "fc layer 3 self.abs_max_out: 40.0\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "fc layer 1 self.abs_max_out: 7281.0\n",
      "lif layer 1 self.abs_max_v: 7281.0\n",
      "fc layer 1 self.abs_max_out: 7356.0\n",
      "lif layer 1 self.abs_max_v: 7356.0\n",
      "fc layer 1 self.abs_max_out: 7451.0\n",
      "lif layer 1 self.abs_max_v: 7451.0\n",
      "train - Value 0: 1723 occurrences\n",
      "train - Value 1: 2307 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 55.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 81.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 85.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 94.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 122.00 at epoch 0, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-0   lr=['2.0000000'], tr/val_loss: 12.915030/ 12.305722, val:  50.22%, val_best:  50.22%, tr:  74.34%, tr_best:  74.34%, epoch time: 147.58 seconds, 2.46 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 16120 real_backward_count 4932  30.596%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1749.0\n",
      "fc layer 1 self.abs_max_out: 7588.0\n",
      "lif layer 1 self.abs_max_v: 7588.0\n",
      "fc layer 2 self.abs_max_out: 1640.0\n",
      "fc layer 1 self.abs_max_out: 7785.0\n",
      "lif layer 1 self.abs_max_v: 7785.0\n",
      "lif layer 1 self.abs_max_v: 8411.5\n",
      "fc layer 1 self.abs_max_out: 8174.0\n",
      "train - Value 0: 1837 occurrences\n",
      "train - Value 1: 2193 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 248 occurrences\n",
      "test - Value 1: 204 occurrences\n",
      "epoch-1   lr=['2.0000000'], tr/val_loss: 10.269512/  9.698833, val:  82.30%, val_best:  82.30%, tr:  82.83%, tr_best:  82.83%, epoch time: 145.30 seconds, 2.42 minutes\n",
      "layer   1  Sparsity: 81.1443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7080%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.5560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32240 real_backward_count 9214  28.579%\n",
      "layer   1  Sparsity: 81.1523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8207.0\n",
      "fc layer 3 self.abs_max_out: 48.0\n",
      "fc layer 1 self.abs_max_out: 8321.0\n",
      "fc layer 1 self.abs_max_out: 8621.0\n",
      "lif layer 1 self.abs_max_v: 8621.0\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "lif layer 1 self.abs_max_v: 8967.5\n",
      "fc layer 1 self.abs_max_out: 8648.0\n",
      "train - Value 0: 1904 occurrences\n",
      "train - Value 1: 2126 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 129.00 at epoch 2, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-2   lr=['2.0000000'], tr/val_loss: 11.946976/ 16.831573, val:  79.87%, val_best:  82.30%, tr:  83.90%, tr_best:  83.90%, epoch time: 147.01 seconds, 2.45 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 98.0778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48360 real_backward_count 13586  28.093%\n",
      "layer   1  Sparsity: 75.6348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8829.0\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "lif layer 2 self.abs_max_v: 1766.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "lif layer 2 self.abs_max_v: 1769.5\n",
      "fc layer 3 self.abs_max_out: 56.0\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "fc layer 2 self.abs_max_out: 1656.0\n",
      "fc layer 2 self.abs_max_out: 1657.0\n",
      "train - Value 0: 1805 occurrences\n",
      "train - Value 1: 2225 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 131.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 149.00 at epoch 3, iter 4029\n",
      "fc layer 2 self.abs_max_out: 1716.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 76 occurrences\n",
      "test - Value 1: 376 occurrences\n",
      "epoch-3   lr=['2.0000000'], tr/val_loss: 19.106367/ 23.331738, val:  65.93%, val_best:  82.30%, tr:  82.78%, tr_best:  83.90%, epoch time: 145.84 seconds, 2.43 minutes\n",
      "layer   1  Sparsity: 81.1459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 96.8336%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64480 real_backward_count 17841  27.669%\n",
      "layer   1  Sparsity: 66.5039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 95.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1815.0\n",
      "lif layer 2 self.abs_max_v: 1843.5\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "lif layer 1 self.abs_max_v: 9064.0\n",
      "fc layer 2 self.abs_max_out: 1719.0\n",
      "fc layer 2 self.abs_max_out: 1758.0\n",
      "lif layer 2 self.abs_max_v: 1847.5\n",
      "lif layer 1 self.abs_max_v: 9663.5\n",
      "lif layer 2 self.abs_max_v: 1874.5\n",
      "fc layer 1 self.abs_max_out: 8882.0\n",
      "fc layer 2 self.abs_max_out: 1792.0\n",
      "fc layer 2 self.abs_max_out: 1848.0\n",
      "fc layer 2 self.abs_max_out: 1967.0\n",
      "lif layer 2 self.abs_max_v: 1967.0\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main'\n",
    "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "wandb.init(project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "\n",
    "my_snn_system(  devices = \"5\",\n",
    "                single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 4, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "                BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 8, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "                # n_tidigits_tonic 8\n",
    "\n",
    "                # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "                which_data = 'n_tidigits_tonic',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','n_tidigits_tonic', 'DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 256,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "                synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "                # pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_20250704_185524_987.pth\",\n",
    "                # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                learning_rate = 2, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 200,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 1, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 2, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "                # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "                # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "                exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 9, \n",
    "\n",
    "                num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "                chaching_on = False, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = False, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 8, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "                quantize_bit_list=[8,8,8],\n",
    "                scale_exp=[[0,0],[0,0],[0,0]], \n",
    "                # quantize_bit_list=[],\n",
    "                # scale_exp=[], \n",
    "                timestep_sums_threshold = 0,\n",
    "\n",
    "                loser_encourage_mode = False,# True # False\n",
    "                \n",
    "                lif_layer_sg_width2 = 16.0,\n",
    "                lif_layer_v_threshold2 = 512.0,\n",
    "                learning_rate2 = 1,\n",
    "                # init_scaling = [10000+ 12,10000+ 12,10000+ 11],\n",
    "                init_scaling = [1/2,1/4,1/16],\n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# average pooling  \n",
    "# Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 176j7st8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251222_211337-176j7st8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/176j7st8' target=\"_blank\">gallant-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/176j7st8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/176j7st8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251222_211345_881', 'my_seed': 42, 'TIME': 4, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 2, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 16, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 8, 'learning_rate2': 4, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4030, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4030 BATCH: 1 train_data_count: 4030\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 16, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 8\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1450.0\n",
      "lif layer 1 self.abs_max_v: 1450.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "lif layer 2 self.abs_max_v: 654.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "layer   1  Sparsity: 87.4023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2166.0\n",
      "lif layer 1 self.abs_max_v: 2166.0\n",
      "lif layer 2 self.abs_max_v: 708.0\n",
      "fc layer 3 self.abs_max_out: 75.0\n",
      "lif layer 2 self.abs_max_v: 861.0\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "lif layer 2 self.abs_max_v: 885.0\n",
      "fc layer 2 self.abs_max_out: 656.0\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "fc layer 2 self.abs_max_out: 682.0\n",
      "lif layer 2 self.abs_max_v: 892.0\n",
      "lif layer 1 self.abs_max_v: 2194.0\n",
      "lif layer 2 self.abs_max_v: 900.5\n",
      "lif layer 2 self.abs_max_v: 940.5\n",
      "lif layer 1 self.abs_max_v: 2372.5\n",
      "fc layer 2 self.abs_max_out: 789.0\n",
      "fc layer 1 self.abs_max_out: 2171.0\n",
      "lif layer 2 self.abs_max_v: 1001.5\n",
      "fc layer 1 self.abs_max_out: 2290.0\n",
      "fc layer 1 self.abs_max_out: 2361.0\n",
      "lif layer 1 self.abs_max_v: 2625.5\n",
      "lif layer 1 self.abs_max_v: 2653.5\n",
      "fc layer 1 self.abs_max_out: 2383.0\n",
      "fc layer 1 self.abs_max_out: 2455.0\n",
      "fc layer 1 self.abs_max_out: 2794.0\n",
      "lif layer 1 self.abs_max_v: 2794.0\n",
      "fc layer 2 self.abs_max_out: 879.0\n",
      "lif layer 2 self.abs_max_v: 1062.0\n",
      "lif layer 1 self.abs_max_v: 2833.0\n",
      "lif layer 2 self.abs_max_v: 1085.5\n",
      "lif layer 1 self.abs_max_v: 2865.5\n",
      "fc layer 1 self.abs_max_out: 3160.0\n",
      "lif layer 1 self.abs_max_v: 3160.0\n",
      "lif layer 2 self.abs_max_v: 1113.0\n",
      "fc layer 1 self.abs_max_out: 3445.0\n",
      "lif layer 1 self.abs_max_v: 3445.0\n",
      "fc layer 2 self.abs_max_out: 892.0\n",
      "lif layer 2 self.abs_max_v: 1141.0\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "lif layer 2 self.abs_max_v: 1213.0\n",
      "lif layer 2 self.abs_max_v: 1345.5\n",
      "fc layer 2 self.abs_max_out: 1097.0\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "fc layer 2 self.abs_max_out: 1119.0\n",
      "fc layer 2 self.abs_max_out: 1129.0\n",
      "lif layer 1 self.abs_max_v: 3621.0\n",
      "fc layer 1 self.abs_max_out: 3682.0\n",
      "lif layer 1 self.abs_max_v: 3682.0\n",
      "lif layer 1 self.abs_max_v: 4407.5\n",
      "lif layer 2 self.abs_max_v: 1365.5\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 1 self.abs_max_out: 3790.0\n",
      "lif layer 1 self.abs_max_v: 4418.5\n",
      "lif layer 1 self.abs_max_v: 4767.5\n",
      "lif layer 2 self.abs_max_v: 1455.0\n",
      "fc layer 1 self.abs_max_out: 3895.0\n",
      "lif layer 2 self.abs_max_v: 1481.0\n",
      "fc layer 1 self.abs_max_out: 3997.0\n",
      "lif layer 2 self.abs_max_v: 1512.0\n",
      "fc layer 2 self.abs_max_out: 1131.0\n",
      "lif layer 2 self.abs_max_v: 1533.5\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "fc layer 1 self.abs_max_out: 4068.0\n",
      "lif layer 1 self.abs_max_v: 4820.0\n",
      "lif layer 2 self.abs_max_v: 1604.0\n",
      "fc layer 2 self.abs_max_out: 1157.0\n",
      "lif layer 2 self.abs_max_v: 1694.5\n",
      "lif layer 2 self.abs_max_v: 1746.5\n",
      "fc layer 2 self.abs_max_out: 1158.0\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "lif layer 1 self.abs_max_v: 4905.5\n",
      "fc layer 1 self.abs_max_out: 4120.0\n",
      "lif layer 1 self.abs_max_v: 5182.0\n",
      "fc layer 2 self.abs_max_out: 1170.0\n",
      "fc layer 1 self.abs_max_out: 4441.0\n",
      "lif layer 1 self.abs_max_v: 5413.0\n",
      "fc layer 2 self.abs_max_out: 1175.0\n",
      "fc layer 2 self.abs_max_out: 1180.0\n",
      "fc layer 2 self.abs_max_out: 1190.0\n",
      "fc layer 2 self.abs_max_out: 1215.0\n",
      "lif layer 1 self.abs_max_v: 5764.5\n",
      "fc layer 2 self.abs_max_out: 1255.0\n",
      "fc layer 1 self.abs_max_out: 4535.0\n",
      "lif layer 1 self.abs_max_v: 5917.0\n",
      "fc layer 1 self.abs_max_out: 4590.0\n",
      "fc layer 1 self.abs_max_out: 4613.0\n",
      "fc layer 2 self.abs_max_out: 1256.0\n",
      "fc layer 2 self.abs_max_out: 1266.0\n",
      "lif layer 2 self.abs_max_v: 1753.0\n",
      "fc layer 2 self.abs_max_out: 1268.0\n",
      "lif layer 2 self.abs_max_v: 1758.0\n",
      "fc layer 2 self.abs_max_out: 1284.0\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "lif layer 1 self.abs_max_v: 5942.0\n",
      "fc layer 1 self.abs_max_out: 4766.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "lif layer 2 self.abs_max_v: 1761.5\n",
      "lif layer 1 self.abs_max_v: 6450.5\n",
      "lif layer 2 self.abs_max_v: 1790.0\n",
      "lif layer 2 self.abs_max_v: 1821.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "fc layer 2 self.abs_max_out: 1395.0\n",
      "fc layer 1 self.abs_max_out: 4993.0\n",
      "fc layer 2 self.abs_max_out: 1413.0\n",
      "lif layer 1 self.abs_max_v: 7053.5\n",
      "fc layer 2 self.abs_max_out: 1452.0\n",
      "fc layer 1 self.abs_max_out: 5037.0\n",
      "fc layer 1 self.abs_max_out: 5106.0\n",
      "fc layer 1 self.abs_max_out: 5147.0\n",
      "fc layer 1 self.abs_max_out: 5152.0\n",
      "fc layer 1 self.abs_max_out: 5403.0\n",
      "fc layer 1 self.abs_max_out: 5583.0\n",
      "lif layer 1 self.abs_max_v: 7607.0\n",
      "lif layer 1 self.abs_max_v: 8330.5\n",
      "fc layer 1 self.abs_max_out: 5634.0\n",
      "lif layer 2 self.abs_max_v: 1845.5\n",
      "fc layer 1 self.abs_max_out: 5791.0\n",
      "lif layer 2 self.abs_max_v: 1864.5\n",
      "lif layer 2 self.abs_max_v: 1920.0\n",
      "lif layer 2 self.abs_max_v: 2005.5\n",
      "fc layer 2 self.abs_max_out: 1470.0\n",
      "fc layer 1 self.abs_max_out: 5894.0\n",
      "lif layer 1 self.abs_max_v: 8530.0\n",
      "fc layer 1 self.abs_max_out: 6005.0\n",
      "lif layer 1 self.abs_max_v: 9049.5\n",
      "fc layer 1 self.abs_max_out: 6164.0\n",
      "fc layer 1 self.abs_max_out: 6607.0\n",
      "lif layer 2 self.abs_max_v: 2060.0\n",
      "lif layer 2 self.abs_max_v: 2076.0\n",
      "lif layer 2 self.abs_max_v: 2084.5\n",
      "lif layer 2 self.abs_max_v: 2125.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "fc layer 2 self.abs_max_out: 1486.0\n",
      "fc layer 1 self.abs_max_out: 6719.0\n",
      "lif layer 2 self.abs_max_v: 2142.5\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "lif layer 1 self.abs_max_v: 9072.5\n",
      "lif layer 2 self.abs_max_v: 2183.0\n",
      "lif layer 2 self.abs_max_v: 2227.5\n",
      "lif layer 2 self.abs_max_v: 2271.5\n",
      "lif layer 2 self.abs_max_v: 2273.5\n",
      "fc layer 2 self.abs_max_out: 1549.0\n",
      "fc layer 2 self.abs_max_out: 1587.0\n",
      "fc layer 2 self.abs_max_out: 1796.0\n",
      "fc layer 2 self.abs_max_out: 1840.0\n",
      "fc layer 1 self.abs_max_out: 6890.0\n",
      "lif layer 2 self.abs_max_v: 2356.0\n",
      "lif layer 2 self.abs_max_v: 2361.5\n",
      "lif layer 2 self.abs_max_v: 2428.5\n",
      "lif layer 2 self.abs_max_v: 2497.0\n",
      "lif layer 2 self.abs_max_v: 2651.5\n",
      "fc layer 1 self.abs_max_out: 7264.0\n",
      "fc layer 1 self.abs_max_out: 7433.0\n",
      "fc layer 2 self.abs_max_out: 1859.0\n",
      "fc layer 2 self.abs_max_out: 1910.0\n",
      "fc layer 2 self.abs_max_out: 1969.0\n",
      "fc layer 2 self.abs_max_out: 2094.0\n",
      "lif layer 1 self.abs_max_v: 9979.5\n",
      "fc layer 3 self.abs_max_out: 144.0\n",
      "train - Value 0: 2085 occurrences\n",
      "train - Value 1: 1945 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 326.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 362.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 371.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 376.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 417.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 418.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 421.00 at epoch 0, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-0   lr=['8.0000000'], tr/val_loss: 46.657547/ 93.652359, val:  50.00%, val_best:  50.00%, tr:  82.33%, tr_best:  82.33%, epoch time: 136.69 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 16120 real_backward_count 3391  21.036%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "lif layer 2 self.abs_max_v: 2690.5\n",
      "fc layer 1 self.abs_max_out: 7592.0\n",
      "fc layer 1 self.abs_max_out: 7814.0\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "lif layer 1 self.abs_max_v: 10328.5\n",
      "lif layer 2 self.abs_max_v: 2701.5\n",
      "lif layer 2 self.abs_max_v: 2781.5\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 1 self.abs_max_out: 7856.0\n",
      "lif layer 2 self.abs_max_v: 2874.5\n",
      "lif layer 2 self.abs_max_v: 2934.0\n",
      "lif layer 1 self.abs_max_v: 10820.5\n",
      "lif layer 1 self.abs_max_v: 11844.5\n",
      "fc layer 1 self.abs_max_out: 7964.0\n",
      "fc layer 1 self.abs_max_out: 8157.0\n",
      "fc layer 1 self.abs_max_out: 8216.0\n",
      "fc layer 1 self.abs_max_out: 8742.0\n",
      "fc layer 1 self.abs_max_out: 8810.0\n",
      "lif layer 2 self.abs_max_v: 3034.5\n",
      "fc layer 1 self.abs_max_out: 9158.0\n",
      "fc layer 1 self.abs_max_out: 9723.0\n",
      "lif layer 1 self.abs_max_v: 12797.5\n",
      "lif layer 1 self.abs_max_v: 14574.0\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "fc layer 2 self.abs_max_out: 2239.0\n",
      "fc layer 2 self.abs_max_out: 2250.0\n",
      "fc layer 2 self.abs_max_out: 2263.0\n",
      "fc layer 2 self.abs_max_out: 2396.0\n",
      "fc layer 1 self.abs_max_out: 9797.0\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 207 occurrences\n",
      "test - Value 1: 245 occurrences\n",
      "epoch-1   lr=['8.0000000'], tr/val_loss: 53.497990/ 25.001268, val:  79.42%, val_best:  79.42%, tr:  88.21%, tr_best:  88.21%, epoch time: 136.83 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32240 real_backward_count 6480  20.099%\n",
      "layer   1  Sparsity: 81.1523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 3090.5\n",
      "lif layer 2 self.abs_max_v: 3201.0\n",
      "fc layer 1 self.abs_max_out: 10199.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 1 self.abs_max_out: 10353.0\n",
      "fc layer 2 self.abs_max_out: 2413.0\n",
      "fc layer 2 self.abs_max_out: 2444.0\n",
      "fc layer 2 self.abs_max_out: 2488.0\n",
      "train - Value 0: 1871 occurrences\n",
      "train - Value 1: 2159 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 425 occurrences\n",
      "test - Value 1: 27 occurrences\n",
      "epoch-2   lr=['8.0000000'], tr/val_loss: 62.017311/ 43.706833, val:  55.09%, val_best:  79.42%, tr:  87.94%, tr_best:  88.21%, epoch time: 136.83 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48360 real_backward_count 9445  19.531%\n",
      "layer   1  Sparsity: 75.6348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 174.0\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 10687.0\n",
      "fc layer 2 self.abs_max_out: 2505.0\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "lif layer 2 self.abs_max_v: 3213.0\n",
      "lif layer 2 self.abs_max_v: 3214.0\n",
      "lif layer 2 self.abs_max_v: 3311.5\n",
      "fc layer 2 self.abs_max_out: 2540.0\n",
      "fc layer 2 self.abs_max_out: 2541.0\n",
      "fc layer 2 self.abs_max_out: 2582.0\n",
      "fc layer 2 self.abs_max_out: 2682.0\n",
      "lif layer 2 self.abs_max_v: 3360.0\n",
      "train - Value 0: 2075 occurrences\n",
      "train - Value 1: 1955 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 3379.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-3   lr=['8.0000000'], tr/val_loss: 50.106586/ 65.081474, val:  66.37%, val_best:  79.42%, tr:  88.24%, tr_best:  88.24%, epoch time: 135.41 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64480 real_backward_count 12502  19.389%\n",
      "layer   1  Sparsity: 66.5039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2772.0\n",
      "lif layer 1 self.abs_max_v: 14803.5\n",
      "fc layer 1 self.abs_max_out: 10779.0\n",
      "fc layer 1 self.abs_max_out: 10915.0\n",
      "fc layer 1 self.abs_max_out: 10936.0\n",
      "fc layer 2 self.abs_max_out: 2788.0\n",
      "fc layer 2 self.abs_max_out: 2811.0\n",
      "lif layer 1 self.abs_max_v: 14847.0\n",
      "lif layer 1 self.abs_max_v: 15369.0\n",
      "lif layer 1 self.abs_max_v: 15774.5\n",
      "fc layer 1 self.abs_max_out: 11096.0\n",
      "lif layer 1 self.abs_max_v: 16144.0\n",
      "fc layer 1 self.abs_max_out: 11117.0\n",
      "fc layer 2 self.abs_max_out: 2860.0\n",
      "fc layer 1 self.abs_max_out: 11323.0\n",
      "lif layer 1 self.abs_max_v: 16454.5\n",
      "fc layer 1 self.abs_max_out: 11362.0\n",
      "fc layer 1 self.abs_max_out: 11521.0\n",
      "train - Value 0: 2061 occurrences\n",
      "train - Value 1: 1969 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 1 self.abs_max_v: 17128.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 436 occurrences\n",
      "test - Value 1: 16 occurrences\n",
      "epoch-4   lr=['8.0000000'], tr/val_loss: 40.693871/ 14.334053, val:  53.54%, val_best:  79.42%, tr:  90.52%, tr_best:  90.52%, epoch time: 134.53 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 80600 real_backward_count 15402  19.109%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 17251.5\n",
      "fc layer 1 self.abs_max_out: 11599.0\n",
      "fc layer 1 self.abs_max_out: 11619.0\n",
      "fc layer 2 self.abs_max_out: 2880.0\n",
      "lif layer 1 self.abs_max_v: 17752.5\n",
      "lif layer 2 self.abs_max_v: 3500.5\n",
      "fc layer 1 self.abs_max_out: 11788.0\n",
      "lif layer 1 self.abs_max_v: 18611.5\n",
      "fc layer 1 self.abs_max_out: 12366.0\n",
      "fc layer 1 self.abs_max_out: 12710.0\n",
      "fc layer 1 self.abs_max_out: 12869.0\n",
      "fc layer 2 self.abs_max_out: 3049.0\n",
      "fc layer 1 self.abs_max_out: 13046.0\n",
      "train - Value 0: 1951 occurrences\n",
      "train - Value 1: 2079 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 13366.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-5   lr=['8.0000000'], tr/val_loss: 42.572224/ 67.005257, val:  50.00%, val_best:  79.42%, tr:  86.25%, tr_best:  90.52%, epoch time: 135.15 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7834%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96720 real_backward_count 18555  19.184%\n",
      "layer   1  Sparsity: 81.0059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 18634.0\n",
      "fc layer 1 self.abs_max_out: 13815.0\n",
      "lif layer 1 self.abs_max_v: 19089.5\n",
      "lif layer 1 self.abs_max_v: 19373.5\n",
      "train - Value 0: 1842 occurrences\n",
      "train - Value 1: 2188 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-6   lr=['8.0000000'], tr/val_loss: 46.117599/ 28.145473, val:  76.33%, val_best:  79.42%, tr:  90.74%, tr_best:  90.74%, epoch time: 134.57 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7238%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 112840 real_backward_count 21258  18.839%\n",
      "layer   1  Sparsity: 80.4199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3092.0\n",
      "fc layer 1 self.abs_max_out: 13925.0\n",
      "lif layer 1 self.abs_max_v: 19658.0\n",
      "fc layer 1 self.abs_max_out: 14100.0\n",
      "fc layer 1 self.abs_max_out: 14161.0\n",
      "fc layer 1 self.abs_max_out: 14490.0\n",
      "fc layer 1 self.abs_max_out: 14844.0\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 37 occurrences\n",
      "test - Value 1: 415 occurrences\n",
      "epoch-7   lr=['8.0000000'], tr/val_loss: 34.309467/ 43.180832, val:  58.19%, val_best:  79.42%, tr:  90.00%, tr_best:  90.74%, epoch time: 132.46 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 128960 real_backward_count 24184  18.753%\n",
      "fc layer 1 self.abs_max_out: 14892.0\n",
      "layer   1  Sparsity: 68.7988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 19783.5\n",
      "fc layer 2 self.abs_max_out: 3103.0\n",
      "train - Value 0: 1963 occurrences\n",
      "train - Value 1: 2067 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 3138.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 291 occurrences\n",
      "test - Value 1: 161 occurrences\n",
      "epoch-8   lr=['8.0000000'], tr/val_loss: 56.357639/ 36.682564, val:  80.75%, val_best:  80.75%, tr:  91.02%, tr_best:  91.02%, epoch time: 133.50 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 145080 real_backward_count 26868  18.519%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 14896.0\n",
      "fc layer 1 self.abs_max_out: 15049.0\n",
      "fc layer 1 self.abs_max_out: 15200.0\n",
      "train - Value 0: 1827 occurrences\n",
      "train - Value 1: 2203 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 15231.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-9   lr=['8.0000000'], tr/val_loss: 55.145454/ 66.887016, val:  81.19%, val_best:  81.19%, tr:  89.63%, tr_best:  91.02%, epoch time: 133.01 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6802%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161200 real_backward_count 29415  18.248%\n",
      "layer   1  Sparsity: 74.4629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 15414.0\n",
      "lif layer 2 self.abs_max_v: 3638.0\n",
      "fc layer 1 self.abs_max_out: 15573.0\n",
      "fc layer 2 self.abs_max_out: 3173.0\n",
      "fc layer 1 self.abs_max_out: 15595.0\n",
      "fc layer 1 self.abs_max_out: 15611.0\n",
      "fc layer 1 self.abs_max_out: 15805.0\n",
      "train - Value 0: 1733 occurrences\n",
      "train - Value 1: 2297 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 1 self.abs_max_out: 15939.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 285 occurrences\n",
      "test - Value 1: 167 occurrences\n",
      "epoch-10  lr=['8.0000000'], tr/val_loss: 50.969437/ 38.695526, val:  81.64%, val_best:  81.64%, tr:  88.83%, tr_best:  91.02%, epoch time: 133.43 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6504%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7415%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 177320 real_backward_count 31909  17.995%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 16061.0\n",
      "lif layer 2 self.abs_max_v: 3711.0\n",
      "lif layer 1 self.abs_max_v: 19973.0\n",
      "lif layer 2 self.abs_max_v: 3740.0\n",
      "lif layer 2 self.abs_max_v: 3757.5\n",
      "fc layer 1 self.abs_max_out: 16408.0\n",
      "lif layer 2 self.abs_max_v: 3865.0\n",
      "lif layer 2 self.abs_max_v: 3888.0\n",
      "lif layer 2 self.abs_max_v: 4157.5\n",
      "fc layer 1 self.abs_max_out: 16821.0\n",
      "fc layer 2 self.abs_max_out: 3471.0\n",
      "train - Value 0: 1932 occurrences\n",
      "train - Value 1: 2098 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 4164.0\n",
      "max_activation_accul updated: 444.00 at epoch 11, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 229 occurrences\n",
      "test - Value 1: 223 occurrences\n",
      "epoch-11  lr=['8.0000000'], tr/val_loss: 65.949852/ 32.569172, val:  85.18%, val_best:  85.18%, tr:  92.13%, tr_best:  92.13%, epoch time: 132.51 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0775%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193440 real_backward_count 34348  17.756%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4199.0\n",
      "lif layer 2 self.abs_max_v: 4215.0\n",
      "lif layer 1 self.abs_max_v: 20486.5\n",
      "lif layer 2 self.abs_max_v: 4532.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 247 occurrences\n",
      "test - Value 1: 205 occurrences\n",
      "epoch-12  lr=['8.0000000'], tr/val_loss: 51.570564/ 20.564339, val:  84.73%, val_best:  85.18%, tr:  88.59%, tr_best:  92.13%, epoch time: 132.92 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4942%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 209560 real_backward_count 37186  17.745%\n",
      "layer   1  Sparsity: 79.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4558.5\n",
      "lif layer 2 self.abs_max_v: 4566.5\n",
      "lif layer 2 self.abs_max_v: 4573.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-13  lr=['8.0000000'], tr/val_loss: 54.913303/ 51.866940, val:  68.58%, val_best:  85.18%, tr:  91.02%, tr_best:  92.13%, epoch time: 134.39 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225680 real_backward_count 39816  17.643%\n",
      "layer   1  Sparsity: 92.9199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "fc layer 1 self.abs_max_out: 16831.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 447.00 at epoch 14, iter 4029\n",
      "max_activation_accul updated: 449.00 at epoch 14, iter 4029\n",
      "max_activation_accul updated: 461.00 at epoch 14, iter 4029\n",
      "max_activation_accul updated: 487.00 at epoch 14, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 58 occurrences\n",
      "test - Value 1: 394 occurrences\n",
      "epoch-14  lr=['8.0000000'], tr/val_loss: 58.588985/ 77.600609, val:  61.95%, val_best:  85.18%, tr:  90.97%, tr_best:  92.13%, epoch time: 134.56 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1420%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 241800 real_backward_count 42346  17.513%\n",
      "layer   1  Sparsity: 84.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "fc layer 1 self.abs_max_out: 17414.0\n",
      "train - Value 0: 1889 occurrences\n",
      "train - Value 1: 2141 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 17 occurrences\n",
      "test - Value 1: 435 occurrences\n",
      "epoch-15  lr=['8.0000000'], tr/val_loss: 61.308002/ 71.421585, val:  53.76%, val_best:  85.18%, tr:  93.70%, tr_best:  93.70%, epoch time: 132.87 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 81.1440%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.0250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1082%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 257920 real_backward_count 44728  17.342%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 17483.0\n",
      "train - Value 0: 1890 occurrences\n",
      "train - Value 1: 2140 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-16  lr=['8.0000000'], tr/val_loss: 51.038807/104.134590, val:  72.79%, val_best:  85.18%, tr:  92.88%, tr_best:  93.70%, epoch time: 134.47 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274040 real_backward_count 47262  17.246%\n",
      "layer   1  Sparsity: 82.3242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1959 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 169 occurrences\n",
      "test - Value 1: 283 occurrences\n",
      "epoch-17  lr=['8.0000000'], tr/val_loss: 59.579475/ 33.732609, val:  77.21%, val_best:  85.18%, tr:  92.21%, tr_best:  93.70%, epoch time: 134.64 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9599%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290160 real_backward_count 49788  17.159%\n",
      "layer   1  Sparsity: 76.3672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 17722.0\n",
      "lif layer 1 self.abs_max_v: 20655.0\n",
      "train - Value 0: 2088 occurrences\n",
      "train - Value 1: 1942 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-18  lr=['8.0000000'], tr/val_loss: 57.794281/ 45.782192, val:  74.34%, val_best:  85.18%, tr:  93.47%, tr_best:  93.70%, epoch time: 134.54 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 306280 real_backward_count 52189  17.040%\n",
      "layer   1  Sparsity: 86.1816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4612.0\n",
      "lif layer 2 self.abs_max_v: 4769.0\n",
      "lif layer 2 self.abs_max_v: 4878.5\n",
      "lif layer 1 self.abs_max_v: 20721.5\n",
      "train - Value 0: 1918 occurrences\n",
      "train - Value 1: 2112 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 147 occurrences\n",
      "test - Value 1: 305 occurrences\n",
      "epoch-19  lr=['8.0000000'], tr/val_loss: 44.639809/ 51.062424, val:  78.10%, val_best:  85.18%, tr:  91.64%, tr_best:  93.70%, epoch time: 134.42 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322400 real_backward_count 54469  16.895%\n",
      "layer   1  Sparsity: 76.7578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 221.0\n",
      "lif layer 2 self.abs_max_v: 4906.5\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 3609.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 191 occurrences\n",
      "test - Value 1: 261 occurrences\n",
      "epoch-20  lr=['8.0000000'], tr/val_loss: 41.739258/ 19.766577, val:  79.87%, val_best:  85.18%, tr:  91.99%, tr_best:  93.70%, epoch time: 135.60 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 338520 real_backward_count 56857  16.796%\n",
      "layer   1  Sparsity: 86.5723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 224.0\n",
      "lif layer 1 self.abs_max_v: 20804.5\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "train - Value 0: 2057 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-21  lr=['8.0000000'], tr/val_loss: 41.762810/ 64.843872, val:  65.71%, val_best:  85.18%, tr:  92.31%, tr_best:  93.70%, epoch time: 134.34 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.9053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7325%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354640 real_backward_count 59187  16.689%\n",
      "layer   1  Sparsity: 81.2012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "lif layer 1 self.abs_max_v: 21438.5\n",
      "fc layer 2 self.abs_max_out: 3730.0\n",
      "fc layer 2 self.abs_max_out: 3902.0\n",
      "lif layer 1 self.abs_max_v: 21679.5\n",
      "lif layer 2 self.abs_max_v: 4909.5\n",
      "lif layer 2 self.abs_max_v: 4921.5\n",
      "fc layer 2 self.abs_max_out: 3905.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-22  lr=['8.0000000'], tr/val_loss: 50.814503/ 37.472038, val:  76.33%, val_best:  85.18%, tr:  93.00%, tr_best:  93.70%, epoch time: 135.89 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0866%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 370760 real_backward_count 61571  16.607%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 22225.0\n",
      "lif layer 1 self.abs_max_v: 22631.5\n",
      "lif layer 1 self.abs_max_v: 24998.0\n",
      "lif layer 2 self.abs_max_v: 5082.0\n",
      "train - Value 0: 2120 occurrences\n",
      "train - Value 1: 1910 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 493.00 at epoch 23, iter 4029\n",
      "max_activation_accul updated: 536.00 at epoch 23, iter 4029\n",
      "max_activation_accul updated: 546.00 at epoch 23, iter 4029\n",
      "max_activation_accul updated: 548.00 at epoch 23, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 308 occurrences\n",
      "test - Value 1: 144 occurrences\n",
      "epoch-23  lr=['8.0000000'], tr/val_loss: 49.373928/ 26.602297, val:  77.88%, val_best:  85.18%, tr:  94.91%, tr_best:  94.91%, epoch time: 134.24 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 386880 real_backward_count 63798  16.490%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3917.0\n",
      "lif layer 1 self.abs_max_v: 25755.0\n",
      "train - Value 0: 2123 occurrences\n",
      "train - Value 1: 1907 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 184 occurrences\n",
      "test - Value 1: 268 occurrences\n",
      "epoch-24  lr=['8.0000000'], tr/val_loss: 54.724892/ 66.337532, val:  78.76%, val_best:  85.18%, tr:  94.74%, tr_best:  94.91%, epoch time: 134.02 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 403000 real_backward_count 65916  16.356%\n",
      "layer   1  Sparsity: 85.6445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5086.5\n",
      "fc layer 2 self.abs_max_out: 3955.0\n",
      "lif layer 1 self.abs_max_v: 25857.5\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-25  lr=['8.0000000'], tr/val_loss: 55.475010/ 57.693710, val:  77.21%, val_best:  85.18%, tr:  93.65%, tr_best:  94.91%, epoch time: 131.94 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419120 real_backward_count 68218  16.276%\n",
      "layer   1  Sparsity: 79.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3990.0\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4256.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 139 occurrences\n",
      "test - Value 1: 313 occurrences\n",
      "epoch-26  lr=['8.0000000'], tr/val_loss: 52.771149/ 43.181110, val:  75.00%, val_best:  85.18%, tr:  93.97%, tr_best:  94.91%, epoch time: 134.49 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 435240 real_backward_count 70471  16.191%\n",
      "layer   1  Sparsity: 79.6875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 233 occurrences\n",
      "test - Value 1: 219 occurrences\n",
      "epoch-27  lr=['8.0000000'], tr/val_loss: 42.882713/ 15.710333, val:  86.95%, val_best:  86.95%, tr:  93.80%, tr_best:  94.91%, epoch time: 135.14 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7885%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451360 real_backward_count 72791  16.127%\n",
      "layer   1  Sparsity: 76.5137%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 25858.5\n",
      "train - Value 0: 2144 occurrences\n",
      "train - Value 1: 1886 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 302 occurrences\n",
      "test - Value 1: 150 occurrences\n",
      "epoch-28  lr=['8.0000000'], tr/val_loss: 40.470417/ 46.953583, val:  78.32%, val_best:  86.95%, tr:  93.52%, tr_best:  94.91%, epoch time: 133.78 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.9273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 467480 real_backward_count 74959  16.035%\n",
      "layer   1  Sparsity: 85.2539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 25912.5\n",
      "train - Value 0: 2108 occurrences\n",
      "train - Value 1: 1922 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 312 occurrences\n",
      "test - Value 1: 140 occurrences\n",
      "epoch-29  lr=['8.0000000'], tr/val_loss: 40.848576/ 10.417954, val:  73.89%, val_best:  86.95%, tr:  93.37%, tr_best:  94.91%, epoch time: 131.97 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1976%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1065%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483600 real_backward_count 77144  15.952%\n",
      "layer   1  Sparsity: 81.9336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2110 occurrences\n",
      "train - Value 1: 1920 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 442 occurrences\n",
      "test - Value 1: 10 occurrences\n",
      "epoch-30  lr=['8.0000000'], tr/val_loss: 41.530746/ 15.282086, val:  52.21%, val_best:  86.95%, tr:  94.37%, tr_best:  94.91%, epoch time: 133.57 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499720 real_backward_count 79297  15.868%\n",
      "layer   1  Sparsity: 79.0039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 25935.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-31  lr=['8.0000000'], tr/val_loss: 43.597500/ 38.110298, val:  78.32%, val_best:  86.95%, tr:  95.48%, tr_best:  95.48%, epoch time: 134.65 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 515840 real_backward_count 81377  15.776%\n",
      "layer   1  Sparsity: 73.5840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 100 occurrences\n",
      "test - Value 1: 352 occurrences\n",
      "epoch-32  lr=['8.0000000'], tr/val_loss: 41.367142/ 87.038055, val:  69.47%, val_best:  86.95%, tr:  94.79%, tr_best:  95.48%, epoch time: 133.16 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 531960 real_backward_count 83530  15.702%\n",
      "layer   1  Sparsity: 78.2715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 17910.0\n",
      "fc layer 1 self.abs_max_out: 18044.0\n",
      "lif layer 1 self.abs_max_v: 25968.0\n",
      "train - Value 0: 1985 occurrences\n",
      "train - Value 1: 2045 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4497.0\n",
      "fc layer 1 self.abs_max_out: 18071.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-33  lr=['8.0000000'], tr/val_loss: 58.499874/ 68.088745, val:  79.42%, val_best:  86.95%, tr:  94.34%, tr_best:  95.48%, epoch time: 133.69 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548080 real_backward_count 85671  15.631%\n",
      "layer   1  Sparsity: 76.8066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 18291.0\n",
      "train - Value 0: 1972 occurrences\n",
      "train - Value 1: 2058 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4545.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 399 occurrences\n",
      "test - Value 1: 53 occurrences\n",
      "epoch-34  lr=['8.0000000'], tr/val_loss: 40.062752/ 29.746624, val:  61.28%, val_best:  86.95%, tr:  91.99%, tr_best:  95.48%, epoch time: 134.10 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 564200 real_backward_count 87954  15.589%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1968 occurrences\n",
      "train - Value 1: 2062 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4567.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-35  lr=['8.0000000'], tr/val_loss: 40.103039/ 37.822342, val:  76.99%, val_best:  86.95%, tr:  91.74%, tr_best:  95.48%, epoch time: 133.56 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580320 real_backward_count 90189  15.541%\n",
      "layer   1  Sparsity: 91.0156%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 18458.0\n",
      "lif layer 2 self.abs_max_v: 5163.0\n",
      "train - Value 0: 1932 occurrences\n",
      "train - Value 1: 2098 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 5370.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-36  lr=['8.0000000'], tr/val_loss: 43.112881/ 24.380087, val:  50.00%, val_best:  86.95%, tr:  91.44%, tr_best:  95.48%, epoch time: 134.19 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9659%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 596440 real_backward_count 92253  15.467%\n",
      "layer   1  Sparsity: 77.6367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 25975.5\n",
      "train - Value 0: 1947 occurrences\n",
      "train - Value 1: 2083 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-37  lr=['8.0000000'], tr/val_loss: 48.863838/103.646675, val:  50.00%, val_best:  86.95%, tr:  93.20%, tr_best:  95.48%, epoch time: 134.16 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612560 real_backward_count 94346  15.402%\n",
      "layer   1  Sparsity: 77.0508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26061.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4731.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 422 occurrences\n",
      "test - Value 1: 30 occurrences\n",
      "epoch-38  lr=['8.0000000'], tr/val_loss: 55.095173/ 47.702160, val:  56.64%, val_best:  86.95%, tr:  94.42%, tr_best:  95.48%, epoch time: 134.11 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2462%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 628680 real_backward_count 96502  15.350%\n",
      "layer   1  Sparsity: 89.5508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26081.0\n",
      "train - Value 0: 1916 occurrences\n",
      "train - Value 1: 2114 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 5469.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 228 occurrences\n",
      "test - Value 1: 224 occurrences\n",
      "epoch-39  lr=['8.0000000'], tr/val_loss: 56.337410/ 49.790890, val:  83.19%, val_best:  86.95%, tr:  92.18%, tr_best:  95.48%, epoch time: 134.37 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9045%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 644800 real_backward_count 98727  15.311%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 18665.0\n",
      "train - Value 0: 1902 occurrences\n",
      "train - Value 1: 2128 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 27 occurrences\n",
      "test - Value 1: 425 occurrences\n",
      "epoch-40  lr=['8.0000000'], tr/val_loss: 55.106384/ 58.379974, val:  55.97%, val_best:  86.95%, tr:  90.99%, tr_best:  95.48%, epoch time: 134.68 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3495%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 660920 real_backward_count 100920  15.270%\n",
      "layer   1  Sparsity: 79.2969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 19057.0\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 240 occurrences\n",
      "test - Value 1: 212 occurrences\n",
      "epoch-41  lr=['8.0000000'], tr/val_loss: 58.612053/ 49.065701, val:  84.51%, val_best:  86.95%, tr:  93.50%, tr_best:  95.48%, epoch time: 133.64 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7257%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5741%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677040 real_backward_count 103027  15.217%\n",
      "layer   1  Sparsity: 84.8633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-42  lr=['8.0000000'], tr/val_loss: 55.259865/ 46.275311, val:  80.75%, val_best:  86.95%, tr:  93.47%, tr_best:  95.48%, epoch time: 133.22 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 693160 real_backward_count 105253  15.185%\n",
      "layer   1  Sparsity: 77.9297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2047 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 236 occurrences\n",
      "test - Value 1: 216 occurrences\n",
      "epoch-43  lr=['8.0000000'], tr/val_loss: 56.345463/ 27.830732, val:  82.74%, val_best:  86.95%, tr:  95.14%, tr_best:  95.48%, epoch time: 132.68 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 81.1453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709280 real_backward_count 107390  15.141%\n",
      "layer   1  Sparsity: 89.3555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-44  lr=['8.0000000'], tr/val_loss: 54.649967/ 69.868401, val:  73.45%, val_best:  86.95%, tr:  94.86%, tr_best:  95.48%, epoch time: 134.51 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9649%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 725400 real_backward_count 109485  15.093%\n",
      "layer   1  Sparsity: 91.2598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1951 occurrences\n",
      "train - Value 1: 2079 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-45  lr=['8.0000000'], tr/val_loss: 59.503433/116.001984, val:  50.66%, val_best:  86.95%, tr:  92.06%, tr_best:  95.48%, epoch time: 134.08 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2752%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741520 real_backward_count 111680  15.061%\n",
      "layer   1  Sparsity: 75.1953%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2081 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4866.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-46  lr=['8.0000000'], tr/val_loss: 61.969196/131.368774, val:  50.22%, val_best:  86.95%, tr:  94.74%, tr_best:  95.48%, epoch time: 133.47 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 757640 real_backward_count 113765  15.016%\n",
      "layer   1  Sparsity: 89.7949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 159 occurrences\n",
      "test - Value 1: 293 occurrences\n",
      "epoch-47  lr=['8.0000000'], tr/val_loss: 80.285164/ 75.016670, val:  81.19%, val_best:  86.95%, tr:  95.41%, tr_best:  95.48%, epoch time: 132.48 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773760 real_backward_count 115723  14.956%\n",
      "layer   1  Sparsity: 90.2832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2047 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-48  lr=['8.0000000'], tr/val_loss: 75.224586/ 87.994850, val:  69.91%, val_best:  86.95%, tr:  95.93%, tr_best:  95.93%, epoch time: 134.60 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8826%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5435%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 789880 real_backward_count 117757  14.908%\n",
      "layer   1  Sparsity: 85.4004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2047 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 557.00 at epoch 49, iter 4029\n",
      "max_activation_accul updated: 565.00 at epoch 49, iter 4029\n",
      "max_activation_accul updated: 584.00 at epoch 49, iter 4029\n",
      "max_activation_accul updated: 628.00 at epoch 49, iter 4029\n",
      "max_activation_accul updated: 640.00 at epoch 49, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 38 occurrences\n",
      "test - Value 1: 414 occurrences\n",
      "epoch-49  lr=['8.0000000'], tr/val_loss: 76.582855/ 94.185112, val:  58.41%, val_best:  86.95%, tr:  97.17%, tr_best:  97.17%, epoch time: 135.34 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4113%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5033%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806000 real_backward_count 119763  14.859%\n",
      "layer   1  Sparsity: 81.5918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 293 occurrences\n",
      "test - Value 1: 159 occurrences\n",
      "epoch-50  lr=['8.0000000'], tr/val_loss: 77.739426/ 44.843731, val:  80.75%, val_best:  86.95%, tr:  96.35%, tr_best:  97.17%, epoch time: 135.16 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3024%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822120 real_backward_count 121769  14.812%\n",
      "layer   1  Sparsity: 87.3047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 19090.0\n",
      "fc layer 1 self.abs_max_out: 19627.0\n",
      "train - Value 0: 2057 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-51  lr=['8.0000000'], tr/val_loss: 69.021980/ 95.802139, val:  50.22%, val_best:  86.95%, tr:  96.38%, tr_best:  97.17%, epoch time: 133.36 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838240 real_backward_count 123837  14.773%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 19675.0\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 186 occurrences\n",
      "test - Value 1: 266 occurrences\n",
      "epoch-52  lr=['8.0000000'], tr/val_loss: 70.008705/ 57.748970, val:  82.30%, val_best:  86.95%, tr:  96.25%, tr_best:  97.17%, epoch time: 134.52 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7850%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 854360 real_backward_count 125866  14.732%\n",
      "layer   1  Sparsity: 85.1562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 66 occurrences\n",
      "test - Value 1: 386 occurrences\n",
      "epoch-53  lr=['8.0000000'], tr/val_loss: 52.459747/ 50.226772, val:  63.72%, val_best:  86.95%, tr:  95.36%, tr_best:  97.17%, epoch time: 134.21 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1937%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870480 real_backward_count 127939  14.698%\n",
      "layer   1  Sparsity: 77.8809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26115.0\n",
      "train - Value 0: 1915 occurrences\n",
      "train - Value 1: 2115 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 103 occurrences\n",
      "test - Value 1: 349 occurrences\n",
      "epoch-54  lr=['8.0000000'], tr/val_loss: 54.911663/ 49.133610, val:  71.02%, val_best:  86.95%, tr:  94.54%, tr_best:  97.17%, epoch time: 133.77 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 886600 real_backward_count 130093  14.673%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26281.5\n",
      "train - Value 0: 2090 occurrences\n",
      "train - Value 1: 1940 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 55 occurrences\n",
      "test - Value 1: 397 occurrences\n",
      "epoch-55  lr=['8.0000000'], tr/val_loss: 47.314461/ 69.473732, val:  62.17%, val_best:  86.95%, tr:  95.21%, tr_best:  97.17%, epoch time: 135.76 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 902720 real_backward_count 132242  14.649%\n",
      "layer   1  Sparsity: 77.4902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 61 occurrences\n",
      "test - Value 1: 391 occurrences\n",
      "epoch-56  lr=['8.0000000'], tr/val_loss: 48.341484/ 77.171486, val:  63.50%, val_best:  86.95%, tr:  97.20%, tr_best:  97.20%, epoch time: 136.21 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 918840 real_backward_count 134237  14.609%\n",
      "layer   1  Sparsity: 79.2480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4916.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 53 occurrences\n",
      "test - Value 1: 399 occurrences\n",
      "epoch-57  lr=['8.0000000'], tr/val_loss: 50.767742/ 78.361038, val:  61.73%, val_best:  86.95%, tr:  97.07%, tr_best:  97.20%, epoch time: 135.92 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 934960 real_backward_count 136236  14.571%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "fc layer 2 self.abs_max_out: 4964.0\n",
      "train - Value 0: 2055 occurrences\n",
      "train - Value 1: 1975 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 303 occurrences\n",
      "test - Value 1: 149 occurrences\n",
      "epoch-58  lr=['8.0000000'], tr/val_loss: 59.383366/ 44.213966, val:  79.42%, val_best:  86.95%, tr:  96.72%, tr_best:  97.20%, epoch time: 135.46 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 951080 real_backward_count 138282  14.539%\n",
      "layer   1  Sparsity: 76.5137%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26299.5\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 196 occurrences\n",
      "test - Value 1: 256 occurrences\n",
      "epoch-59  lr=['8.0000000'], tr/val_loss: 70.144524/ 41.689022, val:  80.53%, val_best:  86.95%, tr:  96.50%, tr_best:  97.20%, epoch time: 134.54 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967200 real_backward_count 140402  14.516%\n",
      "layer   1  Sparsity: 75.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-60  lr=['8.0000000'], tr/val_loss: 66.108398/ 61.338715, val:  78.10%, val_best:  86.95%, tr:  95.48%, tr_best:  97.20%, epoch time: 134.37 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5752%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 983320 real_backward_count 142541  14.496%\n",
      "layer   1  Sparsity: 63.8184%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26336.5\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 231 occurrences\n",
      "test - Value 1: 221 occurrences\n",
      "epoch-61  lr=['8.0000000'], tr/val_loss: 53.157749/ 15.154792, val:  82.96%, val_best:  86.95%, tr:  95.66%, tr_best:  97.20%, epoch time: 132.20 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 81.1485%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7908%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999440 real_backward_count 144580  14.466%\n",
      "layer   1  Sparsity: 77.2461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26362.0\n",
      "train - Value 0: 2087 occurrences\n",
      "train - Value 1: 1943 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 288 occurrences\n",
      "test - Value 1: 164 occurrences\n",
      "epoch-62  lr=['8.0000000'], tr/val_loss: 51.569847/ 24.927134, val:  80.97%, val_best:  86.95%, tr:  93.45%, tr_best:  97.20%, epoch time: 133.35 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1015560 real_backward_count 146657  14.441%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26618.5\n",
      "train - Value 0: 1927 occurrences\n",
      "train - Value 1: 2103 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 137 occurrences\n",
      "test - Value 1: 315 occurrences\n",
      "epoch-63  lr=['8.0000000'], tr/val_loss: 51.328968/ 77.516541, val:  75.88%, val_best:  86.95%, tr:  93.65%, tr_best:  97.20%, epoch time: 136.16 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5883%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1031680 real_backward_count 148681  14.412%\n",
      "layer   1  Sparsity: 72.2656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26802.0\n",
      "train - Value 0: 2039 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 139 occurrences\n",
      "test - Value 1: 313 occurrences\n",
      "epoch-64  lr=['8.0000000'], tr/val_loss: 50.373352/ 73.091782, val:  76.77%, val_best:  86.95%, tr:  96.18%, tr_best:  97.20%, epoch time: 134.67 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047800 real_backward_count 150604  14.373%\n",
      "layer   1  Sparsity: 77.0508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26924.0\n",
      "train - Value 0: 1982 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 146 occurrences\n",
      "test - Value 1: 306 occurrences\n",
      "epoch-65  lr=['8.0000000'], tr/val_loss: 51.781708/ 68.532654, val:  77.88%, val_best:  86.95%, tr:  94.02%, tr_best:  97.20%, epoch time: 132.52 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1063920 real_backward_count 152653  14.348%\n",
      "layer   1  Sparsity: 75.7812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1878 occurrences\n",
      "train - Value 1: 2152 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 161 occurrences\n",
      "test - Value 1: 291 occurrences\n",
      "epoch-66  lr=['8.0000000'], tr/val_loss: 54.153896/ 60.174072, val:  76.33%, val_best:  86.95%, tr:  93.37%, tr_best:  97.20%, epoch time: 133.64 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4809%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1080040 real_backward_count 154734  14.327%\n",
      "layer   1  Sparsity: 91.4551%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 26931.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-67  lr=['8.0000000'], tr/val_loss: 42.937092/ 30.393164, val:  63.05%, val_best:  86.95%, tr:  95.61%, tr_best:  97.20%, epoch time: 135.41 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1423%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3209%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096160 real_backward_count 156719  14.297%\n",
      "layer   1  Sparsity: 73.7793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 27184.5\n",
      "train - Value 0: 2091 occurrences\n",
      "train - Value 1: 1939 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 176 occurrences\n",
      "test - Value 1: 276 occurrences\n",
      "epoch-68  lr=['8.0000000'], tr/val_loss: 48.323513/ 84.824913, val:  82.74%, val_best:  86.95%, tr:  94.49%, tr_best:  97.20%, epoch time: 134.97 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1112280 real_backward_count 158752  14.273%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 27441.0\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2046 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 256 occurrences\n",
      "test - Value 1: 196 occurrences\n",
      "epoch-69  lr=['8.0000000'], tr/val_loss: 65.817734/ 75.548027, val:  85.40%, val_best:  86.95%, tr:  95.71%, tr_best:  97.20%, epoch time: 134.67 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128400 real_backward_count 160848  14.255%\n",
      "layer   1  Sparsity: 77.3438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 27552.5\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 33 occurrences\n",
      "test - Value 1: 419 occurrences\n",
      "epoch-70  lr=['8.0000000'], tr/val_loss: 70.543983/ 90.416962, val:  57.30%, val_best:  86.95%, tr:  95.31%, tr_best:  97.20%, epoch time: 135.46 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0120%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1144520 real_backward_count 162905  14.233%\n",
      "layer   1  Sparsity: 86.2305%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1912 occurrences\n",
      "train - Value 1: 2118 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 189 occurrences\n",
      "test - Value 1: 263 occurrences\n",
      "epoch-71  lr=['8.0000000'], tr/val_loss: 73.539986/ 55.525063, val:  80.31%, val_best:  86.95%, tr:  95.76%, tr_best:  97.20%, epoch time: 135.72 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8618%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1160640 real_backward_count 165050  14.221%\n",
      "layer   1  Sparsity: 77.3438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 27655.5\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-72  lr=['8.0000000'], tr/val_loss: 76.785927/103.178085, val:  71.68%, val_best:  86.95%, tr:  94.86%, tr_best:  97.20%, epoch time: 135.72 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9290%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1176760 real_backward_count 167182  14.207%\n",
      "layer   1  Sparsity: 82.9590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2086 occurrences\n",
      "train - Value 1: 1944 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 139 occurrences\n",
      "test - Value 1: 313 occurrences\n",
      "epoch-73  lr=['8.0000000'], tr/val_loss: 69.559196/ 72.123955, val:  76.77%, val_best:  86.95%, tr:  96.00%, tr_best:  97.20%, epoch time: 135.58 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1442%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1192880 real_backward_count 169197  14.184%\n",
      "layer   1  Sparsity: 83.5449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 27851.5\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 5188.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 226 occurrences\n",
      "test - Value 1: 226 occurrences\n",
      "epoch-74  lr=['8.0000000'], tr/val_loss: 61.624912/ 21.332010, val:  84.07%, val_best:  86.95%, tr:  96.18%, tr_best:  97.20%, epoch time: 135.38 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1441%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1209000 real_backward_count 171221  14.162%\n",
      "layer   1  Sparsity: 86.4746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28038.0\n",
      "train - Value 0: 1989 occurrences\n",
      "train - Value 1: 2041 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 241 occurrences\n",
      "test - Value 1: 211 occurrences\n",
      "epoch-75  lr=['8.0000000'], tr/val_loss: 57.981956/ 65.808014, val:  83.41%, val_best:  86.95%, tr:  95.73%, tr_best:  97.20%, epoch time: 135.89 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0008%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225120 real_backward_count 173447  14.158%\n",
      "layer   1  Sparsity: 89.5020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28095.5\n",
      "fc layer 1 self.abs_max_out: 20154.0\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 239 occurrences\n",
      "test - Value 1: 213 occurrences\n",
      "epoch-76  lr=['8.0000000'], tr/val_loss: 56.149300/ 39.732670, val:  82.08%, val_best:  86.95%, tr:  95.51%, tr_best:  97.20%, epoch time: 135.96 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1241240 real_backward_count 175765  14.160%\n",
      "layer   1  Sparsity: 86.8652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 20293.0\n",
      "fc layer 1 self.abs_max_out: 20388.0\n",
      "lif layer 1 self.abs_max_v: 28277.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 641.00 at epoch 77, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-77  lr=['8.0000000'], tr/val_loss: 69.974319/ 41.199894, val:  50.44%, val_best:  86.95%, tr:  96.05%, tr_best:  97.20%, epoch time: 134.75 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0880%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5638%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257360 real_backward_count 178011  14.158%\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1972 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 5569.0\n",
      "lif layer 2 self.abs_max_v: 5569.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 11 occurrences\n",
      "test - Value 1: 441 occurrences\n",
      "epoch-78  lr=['8.0000000'], tr/val_loss: 67.177803/ 90.053398, val:  52.43%, val_best:  86.95%, tr:  96.70%, tr_best:  97.20%, epoch time: 134.08 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6798%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1273480 real_backward_count 180096  14.142%\n",
      "layer   1  Sparsity: 85.8887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1972 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 243 occurrences\n",
      "test - Value 1: 209 occurrences\n",
      "epoch-79  lr=['8.0000000'], tr/val_loss: 61.739502/ 37.004307, val:  80.75%, val_best:  86.95%, tr:  96.00%, tr_best:  97.20%, epoch time: 134.15 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.9386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1289600 real_backward_count 182236  14.131%\n",
      "layer   1  Sparsity: 82.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2056 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 70 occurrences\n",
      "test - Value 1: 382 occurrences\n",
      "epoch-80  lr=['8.0000000'], tr/val_loss: 51.413879/ 48.663933, val:  64.16%, val_best:  86.95%, tr:  96.60%, tr_best:  97.20%, epoch time: 135.53 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1305720 real_backward_count 184332  14.117%\n",
      "layer   1  Sparsity: 74.5117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2085 occurrences\n",
      "train - Value 1: 1945 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-81  lr=['8.0000000'], tr/val_loss: 55.805767/ 51.543354, val:  72.12%, val_best:  86.95%, tr:  95.83%, tr_best:  97.20%, epoch time: 136.46 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321840 real_backward_count 186553  14.113%\n",
      "layer   1  Sparsity: 84.8633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2101 occurrences\n",
      "train - Value 1: 1929 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 16 occurrences\n",
      "test - Value 1: 436 occurrences\n",
      "epoch-82  lr=['8.0000000'], tr/val_loss: 52.392235/ 90.790375, val:  53.54%, val_best:  86.95%, tr:  95.14%, tr_best:  97.20%, epoch time: 134.33 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2435%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1337960 real_backward_count 188711  14.104%\n",
      "layer   1  Sparsity: 88.5742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2062 occurrences\n",
      "train - Value 1: 1968 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 269 occurrences\n",
      "test - Value 1: 183 occurrences\n",
      "epoch-83  lr=['8.0000000'], tr/val_loss: 62.457161/ 43.385235, val:  74.12%, val_best:  86.95%, tr:  94.52%, tr_best:  97.20%, epoch time: 132.73 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354080 real_backward_count 190949  14.102%\n",
      "layer   1  Sparsity: 65.5762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 20396.0\n",
      "train - Value 0: 2072 occurrences\n",
      "train - Value 1: 1958 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 350 occurrences\n",
      "test - Value 1: 102 occurrences\n",
      "epoch-84  lr=['8.0000000'], tr/val_loss: 64.025940/ 87.580750, val:  71.68%, val_best:  86.95%, tr:  93.08%, tr_best:  97.20%, epoch time: 135.83 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1481%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2672%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370200 real_backward_count 193246  14.103%\n",
      "layer   1  Sparsity: 79.4434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 20420.0\n",
      "fc layer 1 self.abs_max_out: 20570.0\n",
      "train - Value 0: 2049 occurrences\n",
      "train - Value 1: 1981 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-85  lr=['8.0000000'], tr/val_loss: 73.082436/ 85.050301, val:  70.58%, val_best:  86.95%, tr:  94.04%, tr_best:  97.20%, epoch time: 136.19 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8400%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1386320 real_backward_count 195569  14.107%\n",
      "layer   1  Sparsity: 89.1602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-86  lr=['8.0000000'], tr/val_loss: 72.444901/ 54.879082, val:  63.05%, val_best:  86.95%, tr:  94.57%, tr_best:  97.20%, epoch time: 134.30 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1402440 real_backward_count 197851  14.108%\n",
      "layer   1  Sparsity: 89.5996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1909 occurrences\n",
      "train - Value 1: 2121 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 85 occurrences\n",
      "test - Value 1: 367 occurrences\n",
      "epoch-87  lr=['8.0000000'], tr/val_loss: 65.621216/ 73.122757, val:  67.48%, val_best:  86.95%, tr:  91.81%, tr_best:  97.20%, epoch time: 134.12 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1418560 real_backward_count 200061  14.103%\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1972 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 391 occurrences\n",
      "test - Value 1: 61 occurrences\n",
      "epoch-88  lr=['8.0000000'], tr/val_loss: 55.699879/ 28.737732, val:  63.05%, val_best:  86.95%, tr:  93.18%, tr_best:  97.20%, epoch time: 134.47 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0458%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1434680 real_backward_count 202383  14.106%\n",
      "layer   1  Sparsity: 61.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-89  lr=['8.0000000'], tr/val_loss: 63.630573/ 96.193146, val:  50.00%, val_best:  86.95%, tr:  94.54%, tr_best:  97.20%, epoch time: 135.30 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1491%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1450800 real_backward_count 204531  14.098%\n",
      "layer   1  Sparsity: 89.5996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2042 occurrences\n",
      "train - Value 1: 1988 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 220 occurrences\n",
      "test - Value 1: 232 occurrences\n",
      "epoch-90  lr=['8.0000000'], tr/val_loss: 70.069313/ 50.583488, val:  83.19%, val_best:  86.95%, tr:  95.11%, tr_best:  97.20%, epoch time: 134.95 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1466920 real_backward_count 206584  14.083%\n",
      "layer   1  Sparsity: 73.2422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28315.5\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-91  lr=['8.0000000'], tr/val_loss: 61.064934/ 63.515877, val:  68.81%, val_best:  86.95%, tr:  94.12%, tr_best:  97.20%, epoch time: 134.93 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483040 real_backward_count 208740  14.075%\n",
      "layer   1  Sparsity: 78.7598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28369.5\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-92  lr=['8.0000000'], tr/val_loss: 75.080360/ 60.390663, val:  74.78%, val_best:  86.95%, tr:  95.88%, tr_best:  97.20%, epoch time: 136.20 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0492%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1499160 real_backward_count 210757  14.058%\n",
      "layer   1  Sparsity: 81.8848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28398.0\n",
      "lif layer 1 self.abs_max_v: 28448.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 686.00 at epoch 93, iter 4029\n",
      "max_activation_accul updated: 728.00 at epoch 93, iter 4029\n",
      "max_activation_accul updated: 782.00 at epoch 93, iter 4029\n",
      "max_activation_accul updated: 799.00 at epoch 93, iter 4029\n",
      "max_activation_accul updated: 840.00 at epoch 93, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-93  lr=['8.0000000'], tr/val_loss: 77.831581/125.029823, val:  64.82%, val_best:  86.95%, tr:  96.63%, tr_best:  97.20%, epoch time: 136.18 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1515280 real_backward_count 212697  14.037%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5729.5\n",
      "lif layer 1 self.abs_max_v: 28489.5\n",
      "lif layer 2 self.abs_max_v: 5858.0\n",
      "lif layer 2 self.abs_max_v: 5869.0\n",
      "lif layer 2 self.abs_max_v: 5917.5\n",
      "lif layer 2 self.abs_max_v: 5956.0\n",
      "lif layer 2 self.abs_max_v: 5972.0\n",
      "lif layer 2 self.abs_max_v: 5983.5\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 351 occurrences\n",
      "test - Value 1: 101 occurrences\n",
      "epoch-94  lr=['8.0000000'], tr/val_loss: 73.825340/ 47.967255, val:  61.28%, val_best:  86.95%, tr:  95.61%, tr_best:  97.20%, epoch time: 136.62 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3896%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1531400 real_backward_count 214707  14.020%\n",
      "layer   1  Sparsity: 70.3613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 82 occurrences\n",
      "test - Value 1: 370 occurrences\n",
      "epoch-95  lr=['8.0000000'], tr/val_loss: 68.819595/ 63.454666, val:  65.93%, val_best:  86.95%, tr:  95.33%, tr_best:  97.20%, epoch time: 134.92 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1470%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7893%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1547520 real_backward_count 216758  14.007%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6132.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 107 occurrences\n",
      "test - Value 1: 345 occurrences\n",
      "epoch-96  lr=['8.0000000'], tr/val_loss: 68.481400/ 61.167233, val:  71.46%, val_best:  86.95%, tr:  95.68%, tr_best:  97.20%, epoch time: 135.64 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1563640 real_backward_count 218845  13.996%\n",
      "layer   1  Sparsity: 77.7832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28733.5\n",
      "lif layer 1 self.abs_max_v: 28890.0\n",
      "train - Value 0: 1986 occurrences\n",
      "train - Value 1: 2044 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 319 occurrences\n",
      "test - Value 1: 133 occurrences\n",
      "epoch-97  lr=['8.0000000'], tr/val_loss: 70.767639/ 58.716579, val:  72.79%, val_best:  86.95%, tr:  95.86%, tr_best:  97.20%, epoch time: 134.34 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1579760 real_backward_count 220773  13.975%\n",
      "layer   1  Sparsity: 83.2031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28914.5\n",
      "train - Value 0: 1964 occurrences\n",
      "train - Value 1: 2066 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 40 occurrences\n",
      "test - Value 1: 412 occurrences\n",
      "epoch-98  lr=['8.0000000'], tr/val_loss: 54.384792/ 70.665054, val:  58.85%, val_best:  86.95%, tr:  94.32%, tr_best:  97.20%, epoch time: 136.07 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1442%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595880 real_backward_count 222908  13.968%\n",
      "layer   1  Sparsity: 70.1172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6137.5\n",
      "lif layer 2 self.abs_max_v: 6145.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 168 occurrences\n",
      "test - Value 1: 284 occurrences\n",
      "epoch-99  lr=['8.0000000'], tr/val_loss: 61.135162/ 28.923624, val:  78.32%, val_best:  86.95%, tr:  95.11%, tr_best:  97.20%, epoch time: 135.39 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3560%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612000 real_backward_count 225143  13.967%\n",
      "layer   1  Sparsity: 89.6973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6288.0\n",
      "lif layer 2 self.abs_max_v: 6291.5\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-100 lr=['8.0000000'], tr/val_loss: 62.607635/ 75.963982, val:  69.47%, val_best:  86.95%, tr:  95.78%, tr_best:  97.20%, epoch time: 136.13 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5677%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1628120 real_backward_count 227319  13.962%\n",
      "layer   1  Sparsity: 78.4668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2048 occurrences\n",
      "train - Value 1: 1982 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 388 occurrences\n",
      "test - Value 1: 64 occurrences\n",
      "epoch-101 lr=['8.0000000'], tr/val_loss: 63.464569/ 45.267639, val:  63.27%, val_best:  86.95%, tr:  95.71%, tr_best:  97.20%, epoch time: 133.40 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644240 real_backward_count 229356  13.949%\n",
      "layer   1  Sparsity: 73.6816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 20872.0\n",
      "fc layer 1 self.abs_max_out: 21058.0\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1989 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-102 lr=['8.0000000'], tr/val_loss: 66.981979/ 59.214626, val:  66.37%, val_best:  86.95%, tr:  96.13%, tr_best:  97.20%, epoch time: 134.30 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1660360 real_backward_count 231454  13.940%\n",
      "layer   1  Sparsity: 76.7090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2093 occurrences\n",
      "train - Value 1: 1937 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 5993.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 177 occurrences\n",
      "test - Value 1: 275 occurrences\n",
      "epoch-103 lr=['8.0000000'], tr/val_loss: 61.008041/ 40.005032, val:  80.31%, val_best:  86.95%, tr:  95.58%, tr_best:  97.20%, epoch time: 136.62 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7617%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1676480 real_backward_count 233531  13.930%\n",
      "layer   1  Sparsity: 85.1562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "train - Value 0: 2048 occurrences\n",
      "train - Value 1: 1982 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 254 occurrences\n",
      "test - Value 1: 198 occurrences\n",
      "epoch-104 lr=['8.0000000'], tr/val_loss: 69.472527/ 81.390198, val:  84.96%, val_best:  86.95%, tr:  96.95%, tr_best:  97.20%, epoch time: 135.58 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1692600 real_backward_count 235607  13.920%\n",
      "layer   1  Sparsity: 86.3281%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-105 lr=['8.0000000'], tr/val_loss: 78.646645/ 80.893318, val:  58.41%, val_best:  86.95%, tr:  97.22%, tr_best:  97.22%, epoch time: 133.51 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9484%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1708720 real_backward_count 237501  13.899%\n",
      "layer   1  Sparsity: 86.1328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2066 occurrences\n",
      "train - Value 1: 1964 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-106 lr=['8.0000000'], tr/val_loss: 66.750526/ 83.747177, val:  65.71%, val_best:  86.95%, tr:  96.70%, tr_best:  97.22%, epoch time: 135.52 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1724840 real_backward_count 239594  13.891%\n",
      "layer   1  Sparsity: 87.9395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-107 lr=['8.0000000'], tr/val_loss: 41.486153/ 63.295490, val:  71.90%, val_best:  86.95%, tr:  96.63%, tr_best:  97.22%, epoch time: 135.49 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1431%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1740960 real_backward_count 241651  13.880%\n",
      "layer   1  Sparsity: 70.7520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2084 occurrences\n",
      "train - Value 1: 1946 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-108 lr=['8.0000000'], tr/val_loss: 48.721718/ 82.877838, val:  50.00%, val_best:  86.95%, tr:  95.76%, tr_best:  97.22%, epoch time: 135.30 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0395%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1757080 real_backward_count 243810  13.876%\n",
      "layer   1  Sparsity: 91.0156%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 113 occurrences\n",
      "test - Value 1: 339 occurrences\n",
      "epoch-109 lr=['8.0000000'], tr/val_loss: 57.536118/ 71.033089, val:  71.46%, val_best:  86.95%, tr:  96.28%, tr_best:  97.22%, epoch time: 136.56 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9702%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1773200 real_backward_count 245922  13.869%\n",
      "layer   1  Sparsity: 77.8809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28986.0\n",
      "lif layer 2 self.abs_max_v: 6423.0\n",
      "train - Value 0: 2069 occurrences\n",
      "train - Value 1: 1961 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6077.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 281 occurrences\n",
      "test - Value 1: 171 occurrences\n",
      "epoch-110 lr=['8.0000000'], tr/val_loss: 52.413460/ 19.790958, val:  79.87%, val_best:  86.95%, tr:  95.29%, tr_best:  97.22%, epoch time: 135.47 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1789320 real_backward_count 248164  13.869%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 28995.5\n",
      "lif layer 1 self.abs_max_v: 29013.0\n",
      "train - Value 0: 2066 occurrences\n",
      "train - Value 1: 1964 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 84 occurrences\n",
      "test - Value 1: 368 occurrences\n",
      "epoch-111 lr=['8.0000000'], tr/val_loss: 54.975464/ 73.373932, val:  67.26%, val_best:  86.95%, tr:  95.96%, tr_best:  97.22%, epoch time: 136.41 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8058%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1805440 real_backward_count 250387  13.868%\n",
      "layer   1  Sparsity: 80.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29115.5\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 9 occurrences\n",
      "test - Value 1: 443 occurrences\n",
      "epoch-112 lr=['8.0000000'], tr/val_loss: 67.318634/ 92.839668, val:  51.55%, val_best:  86.95%, tr:  96.82%, tr_best:  97.22%, epoch time: 135.94 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1821560 real_backward_count 252519  13.863%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6215.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 89 occurrences\n",
      "test - Value 1: 363 occurrences\n",
      "epoch-113 lr=['8.0000000'], tr/val_loss: 57.630219/ 62.432690, val:  68.36%, val_best:  86.95%, tr:  96.13%, tr_best:  97.22%, epoch time: 136.70 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1837680 real_backward_count 254853  13.868%\n",
      "layer   1  Sparsity: 80.3223%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6423.5\n",
      "lif layer 2 self.abs_max_v: 6457.5\n",
      "lif layer 2 self.abs_max_v: 6595.0\n",
      "train - Value 0: 2056 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6288.0\n",
      "lif layer 2 self.abs_max_v: 6664.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 311 occurrences\n",
      "test - Value 1: 141 occurrences\n",
      "epoch-114 lr=['8.0000000'], tr/val_loss: 61.437641/ 38.891102, val:  75.88%, val_best:  86.95%, tr:  95.51%, tr_best:  97.22%, epoch time: 135.15 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9794%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1853800 real_backward_count 256994  13.863%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6682.5\n",
      "train - Value 0: 2061 occurrences\n",
      "train - Value 1: 1969 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 16 occurrences\n",
      "test - Value 1: 436 occurrences\n",
      "epoch-115 lr=['8.0000000'], tr/val_loss: 56.767311/115.104301, val:  53.54%, val_best:  86.95%, tr:  95.48%, tr_best:  97.22%, epoch time: 133.33 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2273%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869920 real_backward_count 259044  13.853%\n",
      "layer   1  Sparsity: 75.6348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 161 occurrences\n",
      "test - Value 1: 291 occurrences\n",
      "epoch-116 lr=['8.0000000'], tr/val_loss: 64.158752/ 35.331940, val:  75.44%, val_best:  86.95%, tr:  95.53%, tr_best:  97.22%, epoch time: 135.86 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1886040 real_backward_count 261109  13.844%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1993 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 224 occurrences\n",
      "test - Value 1: 228 occurrences\n",
      "epoch-117 lr=['8.0000000'], tr/val_loss: 66.564934/ 74.967056, val:  81.42%, val_best:  86.95%, tr:  96.48%, tr_best:  97.22%, epoch time: 134.57 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1902160 real_backward_count 263193  13.837%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1960 occurrences\n",
      "train - Value 1: 2070 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 266 occurrences\n",
      "test - Value 1: 186 occurrences\n",
      "epoch-118 lr=['8.0000000'], tr/val_loss: 65.919441/ 77.045708, val:  80.53%, val_best:  86.95%, tr:  95.51%, tr_best:  97.22%, epoch time: 134.99 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918280 real_backward_count 265308  13.831%\n",
      "layer   1  Sparsity: 78.5156%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21216.0\n",
      "fc layer 1 self.abs_max_out: 21331.0\n",
      "train - Value 0: 1954 occurrences\n",
      "train - Value 1: 2076 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 234 occurrences\n",
      "test - Value 1: 218 occurrences\n",
      "epoch-119 lr=['8.0000000'], tr/val_loss: 64.542259/ 39.803860, val:  80.97%, val_best:  86.95%, tr:  95.01%, tr_best:  97.22%, epoch time: 133.90 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1934400 real_backward_count 267394  13.823%\n",
      "layer   1  Sparsity: 84.6191%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21422.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 87 occurrences\n",
      "test - Value 1: 365 occurrences\n",
      "epoch-120 lr=['8.0000000'], tr/val_loss: 62.084167/ 68.914474, val:  68.81%, val_best:  86.95%, tr:  96.43%, tr_best:  97.22%, epoch time: 137.01 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2370%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1950520 real_backward_count 269339  13.809%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21424.0\n",
      "fc layer 1 self.abs_max_out: 21435.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 35 occurrences\n",
      "test - Value 1: 417 occurrences\n",
      "epoch-121 lr=['8.0000000'], tr/val_loss: 66.777145/ 78.725632, val:  57.74%, val_best:  86.95%, tr:  96.15%, tr_best:  97.22%, epoch time: 135.05 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.9505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1966640 real_backward_count 271313  13.796%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-122 lr=['8.0000000'], tr/val_loss: 73.621964/155.319336, val:  50.44%, val_best:  86.95%, tr:  96.00%, tr_best:  97.22%, epoch time: 134.12 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1982760 real_backward_count 273306  13.784%\n",
      "layer   1  Sparsity: 88.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21438.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 31 occurrences\n",
      "test - Value 1: 421 occurrences\n",
      "epoch-123 lr=['8.0000000'], tr/val_loss: 71.624435/ 84.406509, val:  56.86%, val_best:  86.95%, tr:  95.86%, tr_best:  97.22%, epoch time: 134.49 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1431%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1998880 real_backward_count 275384  13.777%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 365 occurrences\n",
      "test - Value 1: 87 occurrences\n",
      "epoch-124 lr=['8.0000000'], tr/val_loss: 52.253994/ 56.243706, val:  66.15%, val_best:  86.95%, tr:  96.10%, tr_best:  97.22%, epoch time: 136.19 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2015000 real_backward_count 277560  13.775%\n",
      "layer   1  Sparsity: 74.8047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1982 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 305 occurrences\n",
      "test - Value 1: 147 occurrences\n",
      "epoch-125 lr=['8.0000000'], tr/val_loss: 60.904095/ 26.403992, val:  73.67%, val_best:  86.95%, tr:  95.66%, tr_best:  97.22%, epoch time: 135.19 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2031120 real_backward_count 279658  13.769%\n",
      "layer   1  Sparsity: 88.9648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1943 occurrences\n",
      "train - Value 1: 2087 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 274 occurrences\n",
      "test - Value 1: 178 occurrences\n",
      "epoch-126 lr=['8.0000000'], tr/val_loss: 64.017105/ 76.465675, val:  75.22%, val_best:  86.95%, tr:  94.89%, tr_best:  97.22%, epoch time: 136.39 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1429%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2047240 real_backward_count 281778  13.764%\n",
      "layer   1  Sparsity: 73.2422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 64 occurrences\n",
      "test - Value 1: 388 occurrences\n",
      "epoch-127 lr=['8.0000000'], tr/val_loss: 68.755066/ 62.468819, val:  63.72%, val_best:  86.95%, tr:  95.76%, tr_best:  97.22%, epoch time: 135.59 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2063360 real_backward_count 283907  13.759%\n",
      "layer   1  Sparsity: 86.4258%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2056 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-128 lr=['8.0000000'], tr/val_loss: 62.294907/ 44.998253, val:  76.11%, val_best:  86.95%, tr:  95.36%, tr_best:  97.22%, epoch time: 136.08 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.9229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7701%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2079480 real_backward_count 285988  13.753%\n",
      "layer   1  Sparsity: 70.8496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2081 occurrences\n",
      "train - Value 1: 1949 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 344 occurrences\n",
      "test - Value 1: 108 occurrences\n",
      "epoch-129 lr=['8.0000000'], tr/val_loss: 53.326160/ 33.055515, val:  71.68%, val_best:  86.95%, tr:  93.90%, tr_best:  97.22%, epoch time: 135.44 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2095600 real_backward_count 288037  13.745%\n",
      "layer   1  Sparsity: 89.5020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2105 occurrences\n",
      "train - Value 1: 1925 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 110 occurrences\n",
      "test - Value 1: 342 occurrences\n",
      "epoch-130 lr=['8.0000000'], tr/val_loss: 52.303261/ 35.115303, val:  69.91%, val_best:  86.95%, tr:  92.56%, tr_best:  97.22%, epoch time: 135.88 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.9360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2111720 real_backward_count 290160  13.740%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2057 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 104 occurrences\n",
      "test - Value 1: 348 occurrences\n",
      "epoch-131 lr=['8.0000000'], tr/val_loss: 65.235893/ 66.583458, val:  71.24%, val_best:  86.95%, tr:  95.43%, tr_best:  97.22%, epoch time: 134.97 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2127840 real_backward_count 292185  13.732%\n",
      "layer   1  Sparsity: 89.1602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 160 occurrences\n",
      "test - Value 1: 292 occurrences\n",
      "epoch-132 lr=['8.0000000'], tr/val_loss: 60.526421/ 34.463840, val:  79.20%, val_best:  86.95%, tr:  95.21%, tr_best:  97.22%, epoch time: 135.65 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2143960 real_backward_count 294252  13.725%\n",
      "layer   1  Sparsity: 80.9570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2074 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 178 occurrences\n",
      "test - Value 1: 274 occurrences\n",
      "epoch-133 lr=['8.0000000'], tr/val_loss: 72.874252/ 84.304077, val:  81.42%, val_best:  86.95%, tr:  96.95%, tr_best:  97.22%, epoch time: 133.90 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0118%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2160080 real_backward_count 296214  13.713%\n",
      "layer   1  Sparsity: 77.1484%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 425 occurrences\n",
      "test - Value 1: 27 occurrences\n",
      "epoch-134 lr=['8.0000000'], tr/val_loss: 43.199722/ 21.244753, val:  55.97%, val_best:  86.95%, tr:  92.63%, tr_best:  97.22%, epoch time: 135.95 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2176200 real_backward_count 298526  13.718%\n",
      "layer   1  Sparsity: 85.3516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2097 occurrences\n",
      "train - Value 1: 1933 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 246 occurrences\n",
      "test - Value 1: 206 occurrences\n",
      "epoch-135 lr=['8.0000000'], tr/val_loss: 39.837673/ 23.958975, val:  80.97%, val_best:  86.95%, tr:  93.15%, tr_best:  97.22%, epoch time: 135.63 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3582%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0968%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2192320 real_backward_count 300749  13.718%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1929 occurrences\n",
      "train - Value 1: 2101 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-136 lr=['8.0000000'], tr/val_loss: 56.459419/ 64.498985, val:  71.46%, val_best:  86.95%, tr:  93.45%, tr_best:  97.22%, epoch time: 134.43 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2172%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2208440 real_backward_count 302930  13.717%\n",
      "layer   1  Sparsity: 71.3379%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 280 occurrences\n",
      "test - Value 1: 172 occurrences\n",
      "epoch-137 lr=['8.0000000'], tr/val_loss: 50.581917/ 49.538639, val:  78.76%, val_best:  86.95%, tr:  95.66%, tr_best:  97.22%, epoch time: 135.02 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2224560 real_backward_count 305115  13.716%\n",
      "layer   1  Sparsity: 68.6523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2050 occurrences\n",
      "train - Value 1: 1980 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 243 occurrences\n",
      "test - Value 1: 209 occurrences\n",
      "epoch-138 lr=['8.0000000'], tr/val_loss: 72.516090/ 10.907048, val:  81.64%, val_best:  86.95%, tr:  94.37%, tr_best:  97.22%, epoch time: 136.89 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2240680 real_backward_count 307240  13.712%\n",
      "layer   1  Sparsity: 73.9258%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2066 occurrences\n",
      "train - Value 1: 1964 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 223 occurrences\n",
      "test - Value 1: 229 occurrences\n",
      "epoch-139 lr=['8.0000000'], tr/val_loss: 52.434425/ 12.297324, val:  76.77%, val_best:  86.95%, tr:  92.33%, tr_best:  97.22%, epoch time: 135.53 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1462%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2408%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2256800 real_backward_count 309441  13.711%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 237 occurrences\n",
      "test - Value 1: 215 occurrences\n",
      "epoch-140 lr=['8.0000000'], tr/val_loss: 48.972919/ 20.810968, val:  82.08%, val_best:  86.95%, tr:  92.98%, tr_best:  97.22%, epoch time: 135.24 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2272920 real_backward_count 311447  13.703%\n",
      "layer   1  Sparsity: 86.6211%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-141 lr=['8.0000000'], tr/val_loss: 67.295807/ 67.953423, val:  66.37%, val_best:  86.95%, tr:  92.83%, tr_best:  97.22%, epoch time: 134.71 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1592%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2289040 real_backward_count 313592  13.700%\n",
      "layer   1  Sparsity: 76.8066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6685.0\n",
      "train - Value 0: 1938 occurrences\n",
      "train - Value 1: 2092 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 172 occurrences\n",
      "test - Value 1: 280 occurrences\n",
      "epoch-142 lr=['8.0000000'], tr/val_loss: 65.987518/ 45.570923, val:  78.32%, val_best:  86.95%, tr:  94.07%, tr_best:  97.22%, epoch time: 136.48 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1511%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2305160 real_backward_count 315752  13.698%\n",
      "layer   1  Sparsity: 82.9102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1948 occurrences\n",
      "train - Value 1: 2082 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 223 occurrences\n",
      "test - Value 1: 229 occurrences\n",
      "epoch-143 lr=['8.0000000'], tr/val_loss: 75.198608/ 16.917709, val:  83.85%, val_best:  86.95%, tr:  94.12%, tr_best:  97.22%, epoch time: 136.24 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1442%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2668%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2321280 real_backward_count 317855  13.693%\n",
      "layer   1  Sparsity: 82.1777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6968.0\n",
      "train - Value 0: 2057 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 330 occurrences\n",
      "test - Value 1: 122 occurrences\n",
      "epoch-144 lr=['8.0000000'], tr/val_loss: 52.436188/ 16.209206, val:  74.78%, val_best:  86.95%, tr:  94.84%, tr_best:  97.22%, epoch time: 136.22 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9844%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2337400 real_backward_count 319937  13.688%\n",
      "layer   1  Sparsity: 74.4141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2074 occurrences\n",
      "train - Value 1: 1956 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6383.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 334 occurrences\n",
      "test - Value 1: 118 occurrences\n",
      "epoch-145 lr=['8.0000000'], tr/val_loss: 45.951725/ 19.355684, val:  72.57%, val_best:  86.95%, tr:  92.48%, tr_best:  97.22%, epoch time: 136.12 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5182%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3855%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2353520 real_backward_count 322124  13.687%\n",
      "layer   1  Sparsity: 84.8633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1948 occurrences\n",
      "train - Value 1: 2082 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-146 lr=['8.0000000'], tr/val_loss: 55.231140/ 77.011726, val:  52.88%, val_best:  86.95%, tr:  92.83%, tr_best:  97.22%, epoch time: 136.84 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2369640 real_backward_count 324230  13.683%\n",
      "layer   1  Sparsity: 87.9883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6546.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 319 occurrences\n",
      "test - Value 1: 133 occurrences\n",
      "epoch-147 lr=['8.0000000'], tr/val_loss: 53.469055/ 26.223377, val:  75.44%, val_best:  86.95%, tr:  94.86%, tr_best:  97.22%, epoch time: 135.82 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1431%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3653%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2385760 real_backward_count 326292  13.677%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 46 occurrences\n",
      "test - Value 1: 406 occurrences\n",
      "epoch-148 lr=['8.0000000'], tr/val_loss: 54.566467/ 93.232780, val:  60.18%, val_best:  86.95%, tr:  96.82%, tr_best:  97.22%, epoch time: 136.05 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2401880 real_backward_count 328318  13.669%\n",
      "layer   1  Sparsity: 68.7988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 155 occurrences\n",
      "test - Value 1: 297 occurrences\n",
      "epoch-149 lr=['8.0000000'], tr/val_loss: 50.104015/ 59.562443, val:  77.21%, val_best:  86.95%, tr:  95.19%, tr_best:  97.22%, epoch time: 134.79 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2418000 real_backward_count 330287  13.660%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2046 occurrences\n",
      "train - Value 1: 1984 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 259 occurrences\n",
      "test - Value 1: 193 occurrences\n",
      "epoch-150 lr=['8.0000000'], tr/val_loss: 44.329151/ 33.023689, val:  83.85%, val_best:  86.95%, tr:  93.67%, tr_best:  97.22%, epoch time: 136.46 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2434120 real_backward_count 332629  13.665%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7006.5\n",
      "lif layer 2 self.abs_max_v: 7016.0\n",
      "lif layer 2 self.abs_max_v: 7026.0\n",
      "lif layer 2 self.abs_max_v: 7096.0\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 243 occurrences\n",
      "test - Value 1: 209 occurrences\n",
      "epoch-151 lr=['8.0000000'], tr/val_loss: 48.945450/ 61.813873, val:  82.52%, val_best:  86.95%, tr:  92.83%, tr_best:  97.22%, epoch time: 134.39 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8594%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2450240 real_backward_count 334819  13.665%\n",
      "layer   1  Sparsity: 86.7676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1918 occurrences\n",
      "train - Value 1: 2112 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-152 lr=['8.0000000'], tr/val_loss: 62.582108/ 64.242195, val:  69.03%, val_best:  86.95%, tr:  94.67%, tr_best:  97.22%, epoch time: 135.18 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2466360 real_backward_count 336770  13.655%\n",
      "layer   1  Sparsity: 84.8145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2076 occurrences\n",
      "train - Value 1: 1954 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 260 occurrences\n",
      "test - Value 1: 192 occurrences\n",
      "epoch-153 lr=['8.0000000'], tr/val_loss: 48.482704/ 21.194407, val:  82.30%, val_best:  86.95%, tr:  94.81%, tr_best:  97.22%, epoch time: 136.52 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2482480 real_backward_count 338876  13.651%\n",
      "layer   1  Sparsity: 87.2070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 283 occurrences\n",
      "test - Value 1: 169 occurrences\n",
      "epoch-154 lr=['8.0000000'], tr/val_loss: 48.332569/ 35.684189, val:  83.41%, val_best:  86.95%, tr:  94.00%, tr_best:  97.22%, epoch time: 134.28 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2498600 real_backward_count 341225  13.657%\n",
      "layer   1  Sparsity: 77.0020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 223 occurrences\n",
      "test - Value 1: 229 occurrences\n",
      "epoch-155 lr=['8.0000000'], tr/val_loss: 53.650051/ 43.934792, val:  86.50%, val_best:  86.95%, tr:  94.71%, tr_best:  97.22%, epoch time: 133.98 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7542%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2514720 real_backward_count 343339  13.653%\n",
      "layer   1  Sparsity: 65.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 93 occurrences\n",
      "test - Value 1: 359 occurrences\n",
      "epoch-156 lr=['8.0000000'], tr/val_loss: 57.881474/ 79.752419, val:  70.13%, val_best:  86.95%, tr:  95.83%, tr_best:  97.22%, epoch time: 136.33 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1482%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2530840 real_backward_count 345412  13.648%\n",
      "layer   1  Sparsity: 86.5234%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2042 occurrences\n",
      "train - Value 1: 1988 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 419 occurrences\n",
      "test - Value 1: 33 occurrences\n",
      "epoch-157 lr=['8.0000000'], tr/val_loss: 52.829319/ 55.226063, val:  55.97%, val_best:  86.95%, tr:  95.46%, tr_best:  97.22%, epoch time: 136.14 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2546960 real_backward_count 347692  13.651%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2039 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 411 occurrences\n",
      "test - Value 1: 41 occurrences\n",
      "epoch-158 lr=['8.0000000'], tr/val_loss: 57.679157/ 25.523848, val:  59.07%, val_best:  86.95%, tr:  95.88%, tr_best:  97.22%, epoch time: 133.94 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1111%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3933%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2563080 real_backward_count 349834  13.649%\n",
      "layer   1  Sparsity: 85.2539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29117.5\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6591.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 241 occurrences\n",
      "test - Value 1: 211 occurrences\n",
      "epoch-159 lr=['8.0000000'], tr/val_loss: 57.691998/ 27.872177, val:  82.96%, val_best:  86.95%, tr:  95.98%, tr_best:  97.22%, epoch time: 134.90 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2579200 real_backward_count 351823  13.641%\n",
      "layer   1  Sparsity: 76.6602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29145.5\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 107 occurrences\n",
      "test - Value 1: 345 occurrences\n",
      "epoch-160 lr=['8.0000000'], tr/val_loss: 58.208862/ 59.468872, val:  72.79%, val_best:  86.95%, tr:  96.53%, tr_best:  97.22%, epoch time: 135.94 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2595320 real_backward_count 353846  13.634%\n",
      "layer   1  Sparsity: 87.3047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29147.5\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 92 occurrences\n",
      "test - Value 1: 360 occurrences\n",
      "epoch-161 lr=['8.0000000'], tr/val_loss: 65.784164/ 70.143829, val:  69.91%, val_best:  86.95%, tr:  95.93%, tr_best:  97.22%, epoch time: 135.61 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2492%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2611440 real_backward_count 355994  13.632%\n",
      "layer   1  Sparsity: 72.2656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29287.5\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 247 occurrences\n",
      "test - Value 1: 205 occurrences\n",
      "epoch-162 lr=['8.0000000'], tr/val_loss: 82.708488/ 80.995888, val:  77.65%, val_best:  86.95%, tr:  97.15%, tr_best:  97.22%, epoch time: 135.90 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2627560 real_backward_count 358004  13.625%\n",
      "layer   1  Sparsity: 91.4062%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29293.5\n",
      "train - Value 0: 2067 occurrences\n",
      "train - Value 1: 1963 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6762.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 188 occurrences\n",
      "test - Value 1: 264 occurrences\n",
      "epoch-163 lr=['8.0000000'], tr/val_loss: 79.069672/ 84.085442, val:  83.19%, val_best:  86.95%, tr:  95.58%, tr_best:  97.22%, epoch time: 136.29 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1423%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4035%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2643680 real_backward_count 359984  13.617%\n",
      "layer   1  Sparsity: 75.8301%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29378.5\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1972 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 280 occurrences\n",
      "test - Value 1: 172 occurrences\n",
      "epoch-164 lr=['8.0000000'], tr/val_loss: 82.531654/ 77.152634, val:  80.53%, val_best:  86.95%, tr:  96.85%, tr_best:  97.22%, epoch time: 135.75 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2659800 real_backward_count 362018  13.611%\n",
      "layer   1  Sparsity: 75.6836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1982 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 280 occurrences\n",
      "test - Value 1: 172 occurrences\n",
      "epoch-165 lr=['8.0000000'], tr/val_loss: 78.732025/ 59.404217, val:  80.09%, val_best:  86.95%, tr:  96.55%, tr_best:  97.22%, epoch time: 136.01 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7154%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2675920 real_backward_count 364078  13.606%\n",
      "layer   1  Sparsity: 76.6602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1993 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6797.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 172 occurrences\n",
      "test - Value 1: 280 occurrences\n",
      "epoch-166 lr=['8.0000000'], tr/val_loss: 73.260201/ 48.538734, val:  78.76%, val_best:  86.95%, tr:  96.18%, tr_best:  97.22%, epoch time: 135.79 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3503%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2692040 real_backward_count 366333  13.608%\n",
      "layer   1  Sparsity: 89.6973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 100 occurrences\n",
      "test - Value 1: 352 occurrences\n",
      "epoch-167 lr=['8.0000000'], tr/val_loss: 61.810989/ 48.809826, val:  69.91%, val_best:  86.95%, tr:  96.15%, tr_best:  97.22%, epoch time: 136.48 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2708160 real_backward_count 368372  13.602%\n",
      "layer   1  Sparsity: 76.0254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29461.5\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 210 occurrences\n",
      "test - Value 1: 242 occurrences\n",
      "epoch-168 lr=['8.0000000'], tr/val_loss: 69.540474/ 36.585751, val:  75.66%, val_best:  86.95%, tr:  93.15%, tr_best:  97.22%, epoch time: 135.17 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2724280 real_backward_count 370642  13.605%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1962 occurrences\n",
      "train - Value 1: 2068 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 36 occurrences\n",
      "test - Value 1: 416 occurrences\n",
      "epoch-169 lr=['8.0000000'], tr/val_loss: 71.236153/ 88.971542, val:  57.96%, val_best:  86.95%, tr:  94.76%, tr_best:  97.22%, epoch time: 134.03 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2740400 real_backward_count 372898  13.607%\n",
      "layer   1  Sparsity: 54.7363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6936.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 163 occurrences\n",
      "test - Value 1: 289 occurrences\n",
      "epoch-170 lr=['8.0000000'], tr/val_loss: 67.012894/ 53.183956, val:  76.77%, val_best:  86.95%, tr:  95.46%, tr_best:  97.22%, epoch time: 135.00 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1505%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.9857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2756520 real_backward_count 375115  13.608%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21447.0\n",
      "lif layer 1 self.abs_max_v: 29606.5\n",
      "fc layer 1 self.abs_max_out: 21615.0\n",
      "train - Value 0: 2060 occurrences\n",
      "train - Value 1: 1970 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 62 occurrences\n",
      "test - Value 1: 390 occurrences\n",
      "epoch-171 lr=['8.0000000'], tr/val_loss: 63.401165/ 82.630745, val:  63.27%, val_best:  86.95%, tr:  94.81%, tr_best:  97.22%, epoch time: 135.23 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6220%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2772640 real_backward_count 377406  13.612%\n",
      "layer   1  Sparsity: 79.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1947 occurrences\n",
      "train - Value 1: 2083 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 349 occurrences\n",
      "test - Value 1: 103 occurrences\n",
      "epoch-172 lr=['8.0000000'], tr/val_loss: 70.376022/ 83.989243, val:  68.81%, val_best:  86.95%, tr:  95.33%, tr_best:  97.22%, epoch time: 134.37 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2788760 real_backward_count 379647  13.613%\n",
      "layer   1  Sparsity: 73.1934%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 231 occurrences\n",
      "test - Value 1: 221 occurrences\n",
      "epoch-173 lr=['8.0000000'], tr/val_loss: 79.615486/ 50.138443, val:  79.42%, val_best:  86.95%, tr:  97.02%, tr_best:  97.22%, epoch time: 135.26 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.9872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2804880 real_backward_count 381673  13.607%\n",
      "layer   1  Sparsity: 91.2598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 351 occurrences\n",
      "test - Value 1: 101 occurrences\n",
      "epoch-174 lr=['8.0000000'], tr/val_loss: 84.987137/ 92.514961, val:  70.58%, val_best:  86.95%, tr:  96.35%, tr_best:  97.22%, epoch time: 136.38 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2821000 real_backward_count 383718  13.602%\n",
      "layer   1  Sparsity: 77.6367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21617.0\n",
      "lif layer 2 self.abs_max_v: 7194.0\n",
      "lif layer 2 self.abs_max_v: 7249.0\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1989 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 155 occurrences\n",
      "test - Value 1: 297 occurrences\n",
      "epoch-175 lr=['8.0000000'], tr/val_loss: 74.343727/ 61.285500, val:  79.87%, val_best:  86.95%, tr:  95.68%, tr_best:  97.22%, epoch time: 136.97 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2837120 real_backward_count 385786  13.598%\n",
      "layer   1  Sparsity: 84.3262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21638.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 6992.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 192 occurrences\n",
      "test - Value 1: 260 occurrences\n",
      "epoch-176 lr=['8.0000000'], tr/val_loss: 56.280025/ 36.254276, val:  79.20%, val_best:  86.95%, tr:  95.76%, tr_best:  97.22%, epoch time: 134.14 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3678%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2853240 real_backward_count 387907  13.595%\n",
      "layer   1  Sparsity: 85.3516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21693.0\n",
      "lif layer 2 self.abs_max_v: 7413.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 7502.5\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 234 occurrences\n",
      "test - Value 1: 218 occurrences\n",
      "epoch-177 lr=['8.0000000'], tr/val_loss: 67.141014/ 51.535652, val:  78.76%, val_best:  86.95%, tr:  94.84%, tr_best:  97.22%, epoch time: 135.78 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2869360 real_backward_count 390003  13.592%\n",
      "layer   1  Sparsity: 82.3242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2090 occurrences\n",
      "train - Value 1: 1940 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 7177.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-178 lr=['8.0000000'], tr/val_loss: 76.667923/ 73.563400, val:  63.50%, val_best:  86.95%, tr:  95.16%, tr_best:  97.22%, epoch time: 136.78 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.9975%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2885480 real_backward_count 392009  13.586%\n",
      "layer   1  Sparsity: 76.2207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 7190.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 167 occurrences\n",
      "test - Value 1: 285 occurrences\n",
      "epoch-179 lr=['8.0000000'], tr/val_loss: 66.435196/ 47.399094, val:  77.65%, val_best:  86.95%, tr:  94.27%, tr_best:  97.22%, epoch time: 135.94 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2901600 real_backward_count 394093  13.582%\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7608.5\n",
      "train - Value 0: 2054 occurrences\n",
      "train - Value 1: 1976 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 182 occurrences\n",
      "test - Value 1: 270 occurrences\n",
      "epoch-180 lr=['8.0000000'], tr/val_loss: 67.017296/ 42.802837, val:  82.74%, val_best:  86.95%, tr:  94.86%, tr_best:  97.22%, epoch time: 136.24 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2022%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2917720 real_backward_count 396212  13.580%\n",
      "layer   1  Sparsity: 79.3945%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7655.5\n",
      "fc layer 1 self.abs_max_out: 21813.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 279 occurrences\n",
      "test - Value 1: 173 occurrences\n",
      "epoch-181 lr=['8.0000000'], tr/val_loss: 59.082150/ 77.215958, val:  81.64%, val_best:  86.95%, tr:  95.36%, tr_best:  97.22%, epoch time: 136.87 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2933840 real_backward_count 398323  13.577%\n",
      "layer   1  Sparsity: 75.8789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 103 occurrences\n",
      "test - Value 1: 349 occurrences\n",
      "epoch-182 lr=['8.0000000'], tr/val_loss: 67.446175/ 50.873310, val:  70.13%, val_best:  86.95%, tr:  96.48%, tr_best:  97.22%, epoch time: 135.78 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1129%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2949960 real_backward_count 400386  13.573%\n",
      "layer   1  Sparsity: 73.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 250 occurrences\n",
      "test - Value 1: 202 occurrences\n",
      "epoch-183 lr=['8.0000000'], tr/val_loss: 73.413498/ 86.840645, val:  84.96%, val_best:  86.95%, tr:  97.67%, tr_best:  97.67%, epoch time: 135.28 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2537%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2966080 real_backward_count 402314  13.564%\n",
      "layer   1  Sparsity: 73.7793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-184 lr=['8.0000000'], tr/val_loss: 82.829735/ 98.845604, val:  66.15%, val_best:  86.95%, tr:  97.30%, tr_best:  97.67%, epoch time: 137.43 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 81.1463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2982200 real_backward_count 404192  13.553%\n",
      "layer   1  Sparsity: 84.5215%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7664.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-185 lr=['8.0000000'], tr/val_loss: 81.065147/172.050171, val:  52.88%, val_best:  86.95%, tr:  97.62%, tr_best:  97.67%, epoch time: 136.63 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2998320 real_backward_count 406123  13.545%\n",
      "layer   1  Sparsity: 76.2207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-186 lr=['8.0000000'], tr/val_loss: 78.104614/ 82.967415, val:  72.35%, val_best:  86.95%, tr:  96.58%, tr_best:  97.67%, epoch time: 135.17 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3014440 real_backward_count 408046  13.536%\n",
      "layer   1  Sparsity: 72.5098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 100 occurrences\n",
      "test - Value 1: 352 occurrences\n",
      "epoch-187 lr=['8.0000000'], tr/val_loss: 75.861443/ 74.202904, val:  69.91%, val_best:  86.95%, tr:  96.60%, tr_best:  97.67%, epoch time: 133.90 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4529%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3030560 real_backward_count 410219  13.536%\n",
      "layer   1  Sparsity: 75.6348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7740.5\n",
      "train - Value 0: 1955 occurrences\n",
      "train - Value 1: 2075 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 221 occurrences\n",
      "test - Value 1: 231 occurrences\n",
      "epoch-188 lr=['8.0000000'], tr/val_loss: 84.059677/101.119118, val:  79.87%, val_best:  86.95%, tr:  96.82%, tr_best:  97.67%, epoch time: 135.38 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3046680 real_backward_count 412277  13.532%\n",
      "layer   1  Sparsity: 68.5059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21941.0\n",
      "lif layer 1 self.abs_max_v: 29704.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-189 lr=['8.0000000'], tr/val_loss: 79.677521/136.065094, val:  50.44%, val_best:  86.95%, tr:  96.70%, tr_best:  97.67%, epoch time: 135.70 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1475%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3062800 real_backward_count 414232  13.525%\n",
      "layer   1  Sparsity: 60.7422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7920.0\n",
      "fc layer 1 self.abs_max_out: 21950.0\n",
      "lif layer 1 self.abs_max_v: 29920.5\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 340 occurrences\n",
      "test - Value 1: 112 occurrences\n",
      "epoch-190 lr=['8.0000000'], tr/val_loss: 80.885742/ 93.929718, val:  73.01%, val_best:  86.95%, tr:  96.23%, tr_best:  97.67%, epoch time: 132.74 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 81.1492%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3078920 real_backward_count 416181  13.517%\n",
      "layer   1  Sparsity: 86.2793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2046 occurrences\n",
      "train - Value 1: 1984 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-191 lr=['8.0000000'], tr/val_loss: 79.622643/ 69.479523, val:  63.05%, val_best:  86.95%, tr:  96.40%, tr_best:  97.67%, epoch time: 135.61 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9211%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3095040 real_backward_count 418159  13.511%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 38 occurrences\n",
      "test - Value 1: 414 occurrences\n",
      "epoch-192 lr=['8.0000000'], tr/val_loss: 85.206711/130.615921, val:  58.41%, val_best:  86.95%, tr:  95.43%, tr_best:  97.67%, epoch time: 134.80 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3111160 real_backward_count 420251  13.508%\n",
      "layer   1  Sparsity: 87.3047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 311.0\n",
      "fc layer 1 self.abs_max_out: 22063.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "lif layer 2 self.abs_max_v: 7936.5\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 45 occurrences\n",
      "test - Value 1: 407 occurrences\n",
      "epoch-193 lr=['8.0000000'], tr/val_loss:102.304375/120.858559, val:  59.51%, val_best:  86.95%, tr:  96.10%, tr_best:  97.67%, epoch time: 134.30 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3127280 real_backward_count 422340  13.505%\n",
      "layer   1  Sparsity: 85.8398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7969.5\n",
      "lif layer 1 self.abs_max_v: 29965.0\n",
      "fc layer 1 self.abs_max_out: 22233.0\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 168 occurrences\n",
      "test - Value 1: 284 occurrences\n",
      "epoch-194 lr=['8.0000000'], tr/val_loss: 91.001701/ 73.816231, val:  79.20%, val_best:  86.95%, tr:  96.65%, tr_best:  97.67%, epoch time: 134.23 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3143400 real_backward_count 424353  13.500%\n",
      "layer   1  Sparsity: 73.0469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 29966.0\n",
      "fc layer 1 self.abs_max_out: 22386.0\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 271 occurrences\n",
      "test - Value 1: 181 occurrences\n",
      "epoch-195 lr=['8.0000000'], tr/val_loss: 80.922623/ 32.781963, val:  78.54%, val_best:  86.95%, tr:  96.63%, tr_best:  97.67%, epoch time: 131.59 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3159520 real_backward_count 426541  13.500%\n",
      "layer   1  Sparsity: 88.3789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8141.0\n",
      "fc layer 1 self.abs_max_out: 22483.0\n",
      "lif layer 1 self.abs_max_v: 30003.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 32 occurrences\n",
      "test - Value 1: 420 occurrences\n",
      "epoch-196 lr=['8.0000000'], tr/val_loss: 87.451775/161.244965, val:  57.08%, val_best:  86.95%, tr:  95.83%, tr_best:  97.67%, epoch time: 135.33 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6395%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3175640 real_backward_count 428755  13.501%\n",
      "layer   1  Sparsity: 79.8340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1972 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 128 occurrences\n",
      "test - Value 1: 324 occurrences\n",
      "epoch-197 lr=['8.0000000'], tr/val_loss: 83.804657/102.340515, val:  76.11%, val_best:  86.95%, tr:  95.61%, tr_best:  97.67%, epoch time: 134.77 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.9537%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3191760 real_backward_count 430942  13.502%\n",
      "layer   1  Sparsity: 77.3438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2053 occurrences\n",
      "train - Value 1: 1977 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 435 occurrences\n",
      "test - Value 1: 17 occurrences\n",
      "epoch-198 lr=['8.0000000'], tr/val_loss: 89.893143/ 96.257004, val:  53.76%, val_best:  86.95%, tr:  95.98%, tr_best:  97.67%, epoch time: 131.18 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4738%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3207880 real_backward_count 433052  13.500%\n",
      "layer   1  Sparsity: 91.9434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 19 occurrences\n",
      "test - Value 1: 433 occurrences\n",
      "epoch-199 lr=['8.0000000'], tr/val_loss: 86.860756/ 83.538528, val:  54.20%, val_best:  86.95%, tr:  95.41%, tr_best:  97.67%, epoch time: 135.03 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0979a627104898a2842e7c9146dcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñá‚ñÅ‚ñà‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñá‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÇ‚ñà‚ñÖ‚ñá‚ñá‚ñà‚ñÖ‚ñÉ‚ñÇ</td></tr><tr><td>tr_acc</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÜ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñá‚ñÅ‚ñà‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñá‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÇ‚ñà‚ñÖ‚ñá‚ñá‚ñà‚ñÖ‚ñÉ‚ñÇ</td></tr><tr><td>val_loss</td><td>‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.95409</td></tr><tr><td>tr_epoch_loss</td><td>86.86076</td></tr><tr><td>val_acc_best</td><td>0.86947</td></tr><tr><td>val_acc_now</td><td>0.54204</td></tr><tr><td>val_loss</td><td>83.53853</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gallant-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/176j7st8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/176j7st8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251222_211337-176j7st8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x1uq1mn6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251223_044425-x1uq1mn6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/x1uq1mn6' target=\"_blank\">major-sweep-31</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/x1uq1mn6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/x1uq1mn6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251223_044434_875', 'my_seed': 42, 'TIME': 4, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 2, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 1, 'learning_rate2': 2, 'loser_encourage_mode': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4030, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4030 BATCH: 1 train_data_count: 4030\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 1, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1450.0\n",
      "lif layer 1 self.abs_max_v: 1450.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "lif layer 2 self.abs_max_v: 654.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 84.0\n",
      "lif layer 2 self.abs_max_v: 718.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "layer   1  Sparsity: 87.4023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 1519.0\n",
      "lif layer 1 self.abs_max_v: 1519.0\n",
      "lif layer 2 self.abs_max_v: 731.0\n",
      "lif layer 1 self.abs_max_v: 1576.0\n",
      "lif layer 2 self.abs_max_v: 828.5\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "fc layer 3 self.abs_max_out: 160.0\n",
      "fc layer 1 self.abs_max_out: 1742.0\n",
      "lif layer 1 self.abs_max_v: 1836.5\n",
      "fc layer 2 self.abs_max_out: 667.0\n",
      "fc layer 1 self.abs_max_out: 1810.0\n",
      "lif layer 1 self.abs_max_v: 1837.0\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "fc layer 1 self.abs_max_out: 2147.0\n",
      "lif layer 1 self.abs_max_v: 2147.0\n",
      "lif layer 2 self.abs_max_v: 855.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "lif layer 2 self.abs_max_v: 911.5\n",
      "lif layer 1 self.abs_max_v: 2269.0\n",
      "lif layer 1 self.abs_max_v: 2321.0\n",
      "lif layer 2 self.abs_max_v: 912.0\n",
      "lif layer 1 self.abs_max_v: 2519.5\n",
      "fc layer 2 self.abs_max_out: 677.0\n",
      "fc layer 2 self.abs_max_out: 775.0\n",
      "lif layer 2 self.abs_max_v: 1094.5\n",
      "lif layer 2 self.abs_max_v: 1127.5\n",
      "fc layer 1 self.abs_max_out: 2182.0\n",
      "lif layer 1 self.abs_max_v: 2540.0\n",
      "fc layer 2 self.abs_max_out: 779.0\n",
      "fc layer 3 self.abs_max_out: 195.0\n",
      "fc layer 1 self.abs_max_out: 2222.0\n",
      "fc layer 2 self.abs_max_out: 788.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "fc layer 2 self.abs_max_out: 834.0\n",
      "fc layer 3 self.abs_max_out: 260.0\n",
      "fc layer 2 self.abs_max_out: 897.0\n",
      "fc layer 1 self.abs_max_out: 2293.0\n",
      "lif layer 1 self.abs_max_v: 2691.0\n",
      "lif layer 2 self.abs_max_v: 1171.0\n",
      "lif layer 2 self.abs_max_v: 1209.5\n",
      "lif layer 2 self.abs_max_v: 1255.0\n",
      "fc layer 2 self.abs_max_out: 912.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "lif layer 2 self.abs_max_v: 1284.0\n",
      "lif layer 2 self.abs_max_v: 1370.5\n",
      "lif layer 2 self.abs_max_v: 1535.5\n",
      "fc layer 2 self.abs_max_out: 982.0\n",
      "lif layer 1 self.abs_max_v: 2816.0\n",
      "fc layer 1 self.abs_max_out: 2336.0\n",
      "fc layer 1 self.abs_max_out: 2461.0\n",
      "fc layer 1 self.abs_max_out: 2607.0\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "lif layer 2 self.abs_max_v: 1542.5\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "fc layer 3 self.abs_max_out: 293.0\n",
      "fc layer 2 self.abs_max_out: 1062.0\n",
      "fc layer 1 self.abs_max_out: 2704.0\n",
      "lif layer 2 self.abs_max_v: 1594.5\n",
      "lif layer 2 self.abs_max_v: 1704.5\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "fc layer 1 self.abs_max_out: 2839.0\n",
      "lif layer 1 self.abs_max_v: 2839.0\n",
      "fc layer 1 self.abs_max_out: 3113.0\n",
      "lif layer 1 self.abs_max_v: 3113.0\n",
      "fc layer 2 self.abs_max_out: 1124.0\n",
      "fc layer 1 self.abs_max_out: 3146.0\n",
      "lif layer 1 self.abs_max_v: 3146.0\n",
      "lif layer 2 self.abs_max_v: 1715.0\n",
      "fc layer 1 self.abs_max_out: 3416.0\n",
      "lif layer 1 self.abs_max_v: 3416.0\n",
      "fc layer 1 self.abs_max_out: 3418.0\n",
      "lif layer 1 self.abs_max_v: 3482.0\n",
      "lif layer 1 self.abs_max_v: 3518.0\n",
      "fc layer 2 self.abs_max_out: 1153.0\n",
      "lif layer 1 self.abs_max_v: 3589.5\n",
      "fc layer 2 self.abs_max_out: 1182.0\n",
      "fc layer 3 self.abs_max_out: 304.0\n",
      "fc layer 2 self.abs_max_out: 1298.0\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "fc layer 3 self.abs_max_out: 317.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "lif layer 1 self.abs_max_v: 3650.5\n",
      "lif layer 2 self.abs_max_v: 1719.5\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "fc layer 1 self.abs_max_out: 3601.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "lif layer 1 self.abs_max_v: 3728.5\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "lif layer 1 self.abs_max_v: 4093.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "lif layer 2 self.abs_max_v: 1781.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "lif layer 2 self.abs_max_v: 1815.0\n",
      "lif layer 2 self.abs_max_v: 1906.5\n",
      "lif layer 2 self.abs_max_v: 1908.5\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "lif layer 2 self.abs_max_v: 1914.5\n",
      "lif layer 2 self.abs_max_v: 2050.5\n",
      "fc layer 1 self.abs_max_out: 3690.0\n",
      "lif layer 2 self.abs_max_v: 2064.5\n",
      "lif layer 2 self.abs_max_v: 2100.5\n",
      "fc layer 3 self.abs_max_out: 520.0\n",
      "fc layer 2 self.abs_max_out: 1440.0\n",
      "fc layer 2 self.abs_max_out: 1484.0\n",
      "fc layer 2 self.abs_max_out: 1550.0\n",
      "fc layer 2 self.abs_max_out: 1680.0\n",
      "lif layer 1 self.abs_max_v: 4106.0\n",
      "lif layer 1 self.abs_max_v: 4328.5\n",
      "lif layer 1 self.abs_max_v: 4880.5\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "lif layer 2 self.abs_max_v: 2292.5\n",
      "fc layer 2 self.abs_max_out: 1713.0\n",
      "fc layer 2 self.abs_max_out: 1774.0\n",
      "fc layer 2 self.abs_max_out: 1775.0\n",
      "fc layer 2 self.abs_max_out: 2023.0\n",
      "fc layer 1 self.abs_max_out: 3721.0\n",
      "lif layer 1 self.abs_max_v: 5085.0\n",
      "fc layer 1 self.abs_max_out: 4068.0\n",
      "lif layer 1 self.abs_max_v: 5756.0\n",
      "lif layer 1 self.abs_max_v: 6852.0\n",
      "lif layer 1 self.abs_max_v: 6919.0\n",
      "lif layer 2 self.abs_max_v: 2422.5\n",
      "lif layer 2 self.abs_max_v: 2479.5\n",
      "lif layer 2 self.abs_max_v: 2485.5\n",
      "lif layer 2 self.abs_max_v: 2686.5\n",
      "fc layer 1 self.abs_max_out: 4107.0\n",
      "fc layer 1 self.abs_max_out: 4204.0\n",
      "lif layer 2 self.abs_max_v: 2708.0\n",
      "fc layer 1 self.abs_max_out: 4455.0\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "lif layer 2 self.abs_max_v: 2710.5\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "lif layer 2 self.abs_max_v: 2725.0\n",
      "lif layer 2 self.abs_max_v: 2727.5\n",
      "lif layer 2 self.abs_max_v: 2748.5\n",
      "lif layer 2 self.abs_max_v: 2762.0\n",
      "lif layer 2 self.abs_max_v: 2791.5\n",
      "lif layer 2 self.abs_max_v: 2805.0\n",
      "lif layer 2 self.abs_max_v: 2810.5\n",
      "lif layer 2 self.abs_max_v: 2853.5\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 940.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1408.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1445.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1478.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1545.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1693.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1751.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1938.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 1952.00 at epoch 0, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 6 occurrences\n",
      "test - Value 1: 446 occurrences\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:195.086411/306.572296, val:  51.33%, val_best:  51.33%, tr:  87.87%, tr_best:  87.87%, epoch time: 137.46 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.1827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.4962%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 16120 real_backward_count 3200  19.851%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2923.0\n",
      "lif layer 2 self.abs_max_v: 3101.0\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "fc layer 1 self.abs_max_out: 4523.0\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 1 self.abs_max_out: 4787.0\n",
      "lif layer 1 self.abs_max_v: 6974.0\n",
      "lif layer 1 self.abs_max_v: 7189.5\n",
      "fc layer 1 self.abs_max_out: 4837.0\n",
      "fc layer 1 self.abs_max_out: 5398.0\n",
      "train - Value 0: 1968 occurrences\n",
      "train - Value 1: 2062 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 23 occurrences\n",
      "test - Value 1: 429 occurrences\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:285.083679/530.746765, val:  55.09%, val_best:  55.09%, tr:  90.60%, tr_best:  90.60%, epoch time: 136.13 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5784%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32240 real_backward_count 6118  18.976%\n",
      "layer   1  Sparsity: 81.1523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5405.0\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "fc layer 3 self.abs_max_out: 702.0\n",
      "fc layer 3 self.abs_max_out: 708.0\n",
      "fc layer 3 self.abs_max_out: 727.0\n",
      "fc layer 1 self.abs_max_out: 5669.0\n",
      "fc layer 3 self.abs_max_out: 755.0\n",
      "lif layer 2 self.abs_max_v: 3117.5\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "fc layer 1 self.abs_max_out: 5786.0\n",
      "fc layer 1 self.abs_max_out: 5872.0\n",
      "fc layer 1 self.abs_max_out: 5898.0\n",
      "lif layer 1 self.abs_max_v: 7503.5\n",
      "lif layer 1 self.abs_max_v: 7862.0\n",
      "lif layer 1 self.abs_max_v: 8103.0\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "fc layer 2 self.abs_max_out: 2073.0\n",
      "fc layer 2 self.abs_max_out: 2111.0\n",
      "fc layer 2 self.abs_max_out: 2237.0\n",
      "fc layer 2 self.abs_max_out: 2277.0\n",
      "fc layer 2 self.abs_max_out: 2344.0\n",
      "fc layer 2 self.abs_max_out: 2362.0\n",
      "fc layer 2 self.abs_max_out: 2479.0\n",
      "train - Value 0: 1989 occurrences\n",
      "train - Value 1: 2041 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1969.00 at epoch 2, iter 4029\n",
      "max_activation_accul updated: 2008.00 at epoch 2, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 418 occurrences\n",
      "test - Value 1: 34 occurrences\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:561.805115/541.736938, val:  57.52%, val_best:  57.52%, tr:  92.16%, tr_best:  92.16%, epoch time: 136.61 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.9193%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48360 real_backward_count 8855  18.311%\n",
      "layer   1  Sparsity: 75.6348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 795.0\n",
      "lif layer 2 self.abs_max_v: 3129.0\n",
      "fc layer 3 self.abs_max_out: 820.0\n",
      "fc layer 3 self.abs_max_out: 830.0\n",
      "fc layer 3 self.abs_max_out: 871.0\n",
      "fc layer 1 self.abs_max_out: 5975.0\n",
      "lif layer 2 self.abs_max_v: 3561.5\n",
      "lif layer 2 self.abs_max_v: 3693.5\n",
      "lif layer 1 self.abs_max_v: 8129.0\n",
      "lif layer 2 self.abs_max_v: 3814.5\n",
      "lif layer 2 self.abs_max_v: 3818.0\n",
      "lif layer 1 self.abs_max_v: 8130.0\n",
      "lif layer 1 self.abs_max_v: 8628.5\n",
      "fc layer 1 self.abs_max_out: 6024.0\n",
      "lif layer 1 self.abs_max_v: 9296.5\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 22 occurrences\n",
      "test - Value 1: 430 occurrences\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:538.430969/524.880981, val:  53.98%, val_best:  57.52%, tr:  92.90%, tr_best:  92.90%, epoch time: 135.23 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64480 real_backward_count 11576  17.953%\n",
      "layer   1  Sparsity: 66.5039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6204.0\n",
      "lif layer 2 self.abs_max_v: 3883.5\n",
      "fc layer 3 self.abs_max_out: 873.0\n",
      "fc layer 2 self.abs_max_out: 2539.0\n",
      "lif layer 2 self.abs_max_v: 3922.0\n",
      "lif layer 2 self.abs_max_v: 4016.0\n",
      "lif layer 2 self.abs_max_v: 4238.5\n",
      "fc layer 1 self.abs_max_out: 6216.0\n",
      "train - Value 0: 1987 occurrences\n",
      "train - Value 1: 2043 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 2604.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 396 occurrences\n",
      "test - Value 1: 56 occurrences\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:483.716064/488.441833, val:  61.95%, val_best:  61.95%, tr:  93.35%, tr_best:  93.35%, epoch time: 135.22 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 80600 real_backward_count 14219  17.641%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2628.0\n",
      "fc layer 1 self.abs_max_out: 6218.0\n",
      "fc layer 2 self.abs_max_out: 2639.0\n",
      "fc layer 2 self.abs_max_out: 2652.0\n",
      "fc layer 2 self.abs_max_out: 2668.0\n",
      "fc layer 1 self.abs_max_out: 6243.0\n",
      "fc layer 2 self.abs_max_out: 2719.0\n",
      "fc layer 1 self.abs_max_out: 6256.0\n",
      "fc layer 1 self.abs_max_out: 6367.0\n",
      "fc layer 2 self.abs_max_out: 2750.0\n",
      "fc layer 3 self.abs_max_out: 888.0\n",
      "fc layer 2 self.abs_max_out: 2781.0\n",
      "fc layer 2 self.abs_max_out: 2821.0\n",
      "fc layer 2 self.abs_max_out: 2863.0\n",
      "fc layer 2 self.abs_max_out: 2936.0\n",
      "fc layer 1 self.abs_max_out: 6455.0\n",
      "fc layer 2 self.abs_max_out: 2976.0\n",
      "fc layer 1 self.abs_max_out: 6460.0\n",
      "fc layer 1 self.abs_max_out: 6516.0\n",
      "fc layer 2 self.abs_max_out: 2997.0\n",
      "fc layer 2 self.abs_max_out: 3002.0\n",
      "fc layer 1 self.abs_max_out: 6618.0\n",
      "fc layer 1 self.abs_max_out: 6667.0\n",
      "fc layer 1 self.abs_max_out: 6736.0\n",
      "fc layer 2 self.abs_max_out: 3049.0\n",
      "fc layer 1 self.abs_max_out: 6915.0\n",
      "lif layer 1 self.abs_max_v: 9327.5\n",
      "fc layer 2 self.abs_max_out: 3139.0\n",
      "fc layer 2 self.abs_max_out: 3183.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 2252.00 at epoch 5, iter 4029\n",
      "max_activation_accul updated: 2295.00 at epoch 5, iter 4029\n",
      "max_activation_accul updated: 2359.00 at epoch 5, iter 4029\n",
      "max_activation_accul updated: 2441.00 at epoch 5, iter 4029\n",
      "fc layer 2 self.abs_max_out: 3198.0\n",
      "fc layer 2 self.abs_max_out: 3250.0\n",
      "max_activation_accul updated: 2457.00 at epoch 5, iter 4029\n",
      "lif layer 1 self.abs_max_v: 9520.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 34 occurrences\n",
      "test - Value 1: 418 occurrences\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:536.948059/593.465637, val:  57.52%, val_best:  61.95%, tr:  93.90%, tr_best:  93.90%, epoch time: 135.09 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96720 real_backward_count 16791  17.360%\n",
      "layer   1  Sparsity: 81.0059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3337.0\n",
      "lif layer 1 self.abs_max_v: 9535.0\n",
      "fc layer 1 self.abs_max_out: 6929.0\n",
      "fc layer 1 self.abs_max_out: 6946.0\n",
      "lif layer 1 self.abs_max_v: 9807.5\n",
      "lif layer 1 self.abs_max_v: 9872.0\n",
      "fc layer 1 self.abs_max_out: 7241.0\n",
      "fc layer 3 self.abs_max_out: 894.0\n",
      "train - Value 0: 1981 occurrences\n",
      "train - Value 1: 2049 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:601.321960/507.422546, val:  74.12%, val_best:  74.12%, tr:  94.69%, tr_best:  94.69%, epoch time: 136.30 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.4139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 112840 real_backward_count 19287  17.092%\n",
      "layer   1  Sparsity: 80.4199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7420.0\n",
      "fc layer 1 self.abs_max_out: 7483.0\n",
      "fc layer 1 self.abs_max_out: 7587.0\n",
      "lif layer 1 self.abs_max_v: 10352.5\n",
      "fc layer 3 self.abs_max_out: 946.0\n",
      "fc layer 3 self.abs_max_out: 978.0\n",
      "fc layer 3 self.abs_max_out: 1081.0\n",
      "train - Value 0: 1968 occurrences\n",
      "train - Value 1: 2062 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 2667.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 2676.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 2741.00 at epoch 7, iter 4029\n",
      "max_activation_accul updated: 2812.00 at epoch 7, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 101 occurrences\n",
      "test - Value 1: 351 occurrences\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:632.815796/764.037292, val:  70.13%, val_best:  74.12%, tr:  94.32%, tr_best:  94.69%, epoch time: 135.29 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.4559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 128960 real_backward_count 21644  16.783%\n",
      "layer   1  Sparsity: 68.7988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7622.0\n",
      "fc layer 3 self.abs_max_out: 1149.0\n",
      "lif layer 1 self.abs_max_v: 10427.0\n",
      "lif layer 1 self.abs_max_v: 10432.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 2816.00 at epoch 8, iter 4029\n",
      "max_activation_accul updated: 2903.00 at epoch 8, iter 4029\n",
      "max_activation_accul updated: 3127.00 at epoch 8, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 203 occurrences\n",
      "test - Value 1: 249 occurrences\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:801.675903/746.418701, val:  82.52%, val_best:  82.52%, tr:  96.15%, tr_best:  96.15%, epoch time: 133.33 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4547%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 145080 real_backward_count 23882  16.461%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 10602.5\n",
      "fc layer 3 self.abs_max_out: 1158.0\n",
      "fc layer 1 self.abs_max_out: 7693.0\n",
      "lif layer 1 self.abs_max_v: 10625.5\n",
      "lif layer 1 self.abs_max_v: 10949.0\n",
      "fc layer 1 self.abs_max_out: 7715.0\n",
      "fc layer 1 self.abs_max_out: 7722.0\n",
      "fc layer 1 self.abs_max_out: 7827.0\n",
      "lif layer 1 self.abs_max_v: 11049.0\n",
      "fc layer 1 self.abs_max_out: 7988.0\n",
      "fc layer 1 self.abs_max_out: 8084.0\n",
      "lif layer 1 self.abs_max_v: 11073.0\n",
      "lif layer 1 self.abs_max_v: 11692.5\n",
      "train - Value 0: 1980 occurrences\n",
      "train - Value 1: 2050 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 233 occurrences\n",
      "test - Value 1: 219 occurrences\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:807.388794/765.461792, val:  83.85%, val_best:  83.85%, tr:  95.56%, tr_best:  96.15%, epoch time: 134.85 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161200 real_backward_count 26067  16.171%\n",
      "layer   1  Sparsity: 74.4629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 11733.0\n",
      "fc layer 1 self.abs_max_out: 8433.0\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 3236.00 at epoch 10, iter 4029\n",
      "max_activation_accul updated: 3554.00 at epoch 10, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 394 occurrences\n",
      "test - Value 1: 58 occurrences\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:800.696594/779.159973, val:  62.39%, val_best:  83.85%, tr:  96.38%, tr_best:  96.38%, epoch time: 135.89 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 177320 real_backward_count 28175  15.889%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 306 occurrences\n",
      "test - Value 1: 146 occurrences\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:808.578613/775.681396, val:  79.20%, val_best:  83.85%, tr:  96.95%, tr_best:  96.95%, epoch time: 133.28 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193440 real_backward_count 30193  15.608%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1172.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 86 occurrences\n",
      "test - Value 1: 366 occurrences\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:828.711670/788.750183, val:  69.03%, val_best:  83.85%, tr:  97.37%, tr_best:  97.37%, epoch time: 133.56 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.4122%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.9986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 209560 real_backward_count 32186  15.359%\n",
      "layer   1  Sparsity: 79.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 11824.5\n",
      "fc layer 3 self.abs_max_out: 1173.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 4413.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 152 occurrences\n",
      "test - Value 1: 300 occurrences\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:815.514038/784.410278, val:  78.32%, val_best:  83.85%, tr:  97.64%, tr_best:  97.64%, epoch time: 137.17 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.9056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225680 real_backward_count 34097  15.109%\n",
      "layer   1  Sparsity: 92.9199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12031.0\n",
      "lif layer 2 self.abs_max_v: 4569.5\n",
      "lif layer 2 self.abs_max_v: 4679.5\n",
      "fc layer 1 self.abs_max_out: 8439.0\n",
      "fc layer 3 self.abs_max_out: 1174.0\n",
      "lif layer 2 self.abs_max_v: 4862.5\n",
      "fc layer 2 self.abs_max_out: 3406.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 3486.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 58 occurrences\n",
      "test - Value 1: 394 occurrences\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:825.907227/836.119690, val:  62.83%, val_best:  83.85%, tr:  97.59%, tr_best:  97.64%, epoch time: 134.87 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1420%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.4686%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 241800 real_backward_count 35995  14.886%\n",
      "layer   1  Sparsity: 84.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8576.0\n",
      "lif layer 2 self.abs_max_v: 4898.5\n",
      "fc layer 2 self.abs_max_out: 3556.0\n",
      "fc layer 2 self.abs_max_out: 3743.0\n",
      "fc layer 2 self.abs_max_out: 3744.0\n",
      "fc layer 2 self.abs_max_out: 3853.0\n",
      "lif layer 2 self.abs_max_v: 4914.5\n",
      "lif layer 2 self.abs_max_v: 4988.5\n",
      "fc layer 3 self.abs_max_out: 1210.0\n",
      "lif layer 2 self.abs_max_v: 5101.0\n",
      "lif layer 2 self.abs_max_v: 5288.0\n",
      "fc layer 2 self.abs_max_out: 3890.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:829.604553/804.917297, val:  74.12%, val_best:  83.85%, tr:  97.42%, tr_best:  97.64%, epoch time: 133.52 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1440%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.4313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.9685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 257920 real_backward_count 37863  14.680%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3900.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 153 occurrences\n",
      "test - Value 1: 299 occurrences\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:826.182861/780.750610, val:  79.87%, val_best:  83.85%, tr:  97.57%, tr_best:  97.64%, epoch time: 134.91 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274040 real_backward_count 39634  14.463%\n",
      "layer   1  Sparsity: 82.3242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 200 occurrences\n",
      "test - Value 1: 252 occurrences\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:829.145203/772.692749, val:  83.19%, val_best:  83.85%, tr:  97.99%, tr_best:  97.99%, epoch time: 135.48 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290160 real_backward_count 41316  14.239%\n",
      "layer   1  Sparsity: 76.3672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5380.5\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:801.981201/784.431274, val:  82.74%, val_best:  83.85%, tr:  98.04%, tr_best:  98.04%, epoch time: 135.69 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.9360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 306280 real_backward_count 43074  14.064%\n",
      "layer   1  Sparsity: 86.1816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5417.0\n",
      "lif layer 1 self.abs_max_v: 12073.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 69 occurrences\n",
      "test - Value 1: 383 occurrences\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:805.768311/753.171570, val:  65.27%, val_best:  83.85%, tr:  97.79%, tr_best:  98.04%, epoch time: 135.30 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322400 real_backward_count 44774  13.888%\n",
      "layer   1  Sparsity: 76.7578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12506.5\n",
      "fc layer 3 self.abs_max_out: 1296.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 84 occurrences\n",
      "test - Value 1: 368 occurrences\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:811.496887/800.363220, val:  68.14%, val_best:  83.85%, tr:  98.29%, tr_best:  98.29%, epoch time: 135.78 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 338520 real_backward_count 46446  13.720%\n",
      "layer   1  Sparsity: 86.5723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5422.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 150 occurrences\n",
      "test - Value 1: 302 occurrences\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:820.071960/778.333557, val:  80.09%, val_best:  83.85%, tr:  98.21%, tr_best:  98.29%, epoch time: 136.40 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354640 real_backward_count 48090  13.560%\n",
      "layer   1  Sparsity: 81.2012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 51 occurrences\n",
      "test - Value 1: 401 occurrences\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:826.607300/827.524780, val:  61.28%, val_best:  83.85%, tr:  98.24%, tr_best:  98.29%, epoch time: 134.07 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0268%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 370760 real_backward_count 49707  13.407%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1300.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 3636.00 at epoch 23, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 220 occurrences\n",
      "test - Value 1: 232 occurrences\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:833.847473/789.026611, val:  85.40%, val_best:  85.40%, tr:  98.41%, tr_best:  98.41%, epoch time: 133.59 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.9584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 386880 real_backward_count 51223  13.240%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8629.0\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 201 occurrences\n",
      "test - Value 1: 251 occurrences\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:843.554749/788.786194, val:  85.62%, val_best:  85.62%, tr:  98.26%, tr_best:  98.41%, epoch time: 136.86 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 403000 real_backward_count 52760  13.092%\n",
      "layer   1  Sparsity: 85.6445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 210 occurrences\n",
      "test - Value 1: 242 occurrences\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:831.397217/783.837952, val:  83.63%, val_best:  85.62%, tr:  98.54%, tr_best:  98.54%, epoch time: 135.02 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1754%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419120 real_backward_count 54297  12.955%\n",
      "layer   1  Sparsity: 79.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5453.0\n",
      "lif layer 2 self.abs_max_v: 5467.5\n",
      "lif layer 2 self.abs_max_v: 5636.5\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4001.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 104 occurrences\n",
      "test - Value 1: 348 occurrences\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:841.040894/794.523865, val:  71.24%, val_best:  85.62%, tr:  98.49%, tr_best:  98.54%, epoch time: 134.73 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 435240 real_backward_count 55757  12.811%\n",
      "layer   1  Sparsity: 79.6875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1319.0\n",
      "lif layer 2 self.abs_max_v: 5775.5\n",
      "fc layer 1 self.abs_max_out: 8745.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4061.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 99 occurrences\n",
      "test - Value 1: 353 occurrences\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:815.776062/817.063354, val:  71.02%, val_best:  85.62%, tr:  98.41%, tr_best:  98.54%, epoch time: 135.35 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451360 real_backward_count 57230  12.679%\n",
      "layer   1  Sparsity: 76.5137%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8897.0\n",
      "lif layer 2 self.abs_max_v: 5800.0\n",
      "lif layer 2 self.abs_max_v: 6056.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 3680.00 at epoch 28, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 205 occurrences\n",
      "test - Value 1: 247 occurrences\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:843.121460/854.450439, val:  84.29%, val_best:  85.62%, tr:  98.93%, tr_best:  98.93%, epoch time: 135.09 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 467480 real_backward_count 58686  12.554%\n",
      "layer   1  Sparsity: 85.2539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1343.0\n",
      "fc layer 2 self.abs_max_out: 4165.0\n",
      "fc layer 2 self.abs_max_out: 4273.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 146 occurrences\n",
      "test - Value 1: 306 occurrences\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:904.988953/862.744812, val:  79.65%, val_best:  85.62%, tr:  98.81%, tr_best:  98.93%, epoch time: 135.20 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483600 real_backward_count 60174  12.443%\n",
      "layer   1  Sparsity: 81.9336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1344.0\n",
      "fc layer 3 self.abs_max_out: 1386.0\n",
      "fc layer 3 self.abs_max_out: 1398.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 124 occurrences\n",
      "test - Value 1: 328 occurrences\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:914.338257/860.263733, val:  76.11%, val_best:  85.62%, tr:  98.86%, tr_best:  98.93%, epoch time: 135.65 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7741%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499720 real_backward_count 61579  12.323%\n",
      "layer   1  Sparsity: 79.0039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 3778.00 at epoch 31, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:907.566956/886.464172, val:  67.26%, val_best:  85.62%, tr:  98.93%, tr_best:  98.93%, epoch time: 135.84 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8709%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 515840 real_backward_count 62893  12.192%\n",
      "layer   1  Sparsity: 73.5840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8923.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4048.00 at epoch 32, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 74 occurrences\n",
      "test - Value 1: 378 occurrences\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:911.277039/901.442566, val:  65.93%, val_best:  85.62%, tr:  98.91%, tr_best:  98.93%, epoch time: 136.84 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8152%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 531960 real_backward_count 64240  12.076%\n",
      "layer   1  Sparsity: 78.2715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 140 occurrences\n",
      "test - Value 1: 312 occurrences\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:915.817505/863.211182, val:  79.20%, val_best:  85.62%, tr:  98.83%, tr_best:  98.93%, epoch time: 136.83 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548080 real_backward_count 65632  11.975%\n",
      "layer   1  Sparsity: 76.8066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 196 occurrences\n",
      "test - Value 1: 256 occurrences\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:911.089111/853.430298, val:  84.96%, val_best:  85.62%, tr:  99.03%, tr_best:  99.03%, epoch time: 136.44 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 564200 real_backward_count 66937  11.864%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 181 occurrences\n",
      "test - Value 1: 271 occurrences\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:911.260925/851.749756, val:  84.29%, val_best:  85.62%, tr:  99.23%, tr_best:  99.23%, epoch time: 136.22 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9795%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580320 real_backward_count 68224  11.756%\n",
      "layer   1  Sparsity: 91.0156%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6074.5\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 268 occurrences\n",
      "test - Value 1: 184 occurrences\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:910.306763/862.092346, val:  84.51%, val_best:  85.62%, tr:  99.16%, tr_best:  99.23%, epoch time: 135.55 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0734%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 596440 real_backward_count 69486  11.650%\n",
      "layer   1  Sparsity: 77.6367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4108.00 at epoch 37, iter 4029\n",
      "max_activation_accul updated: 4310.00 at epoch 37, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 36 occurrences\n",
      "test - Value 1: 416 occurrences\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:920.116638/935.673462, val:  57.96%, val_best:  85.62%, tr:  98.98%, tr_best:  99.23%, epoch time: 135.83 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612560 real_backward_count 70757  11.551%\n",
      "layer   1  Sparsity: 77.0508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 173 occurrences\n",
      "test - Value 1: 279 occurrences\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:926.447571/865.528320, val:  82.52%, val_best:  85.62%, tr:  99.21%, tr_best:  99.23%, epoch time: 136.11 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 628680 real_backward_count 71980  11.449%\n",
      "layer   1  Sparsity: 89.5508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6132.5\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 129 occurrences\n",
      "test - Value 1: 323 occurrences\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:924.316345/890.127869, val:  77.21%, val_best:  85.62%, tr:  99.35%, tr_best:  99.35%, epoch time: 135.62 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2794%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 644800 real_backward_count 73200  11.352%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4346.00 at epoch 40, iter 4029\n",
      "max_activation_accul updated: 4566.00 at epoch 40, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 45 occurrences\n",
      "test - Value 1: 407 occurrences\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:930.753357/970.208740, val:  59.96%, val_best:  85.62%, tr:  99.13%, tr_best:  99.35%, epoch time: 134.46 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0356%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6841%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 660920 real_backward_count 74452  11.265%\n",
      "layer   1  Sparsity: 79.2969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 244 occurrences\n",
      "test - Value 1: 208 occurrences\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:933.200195/861.551880, val:  85.84%, val_best:  85.84%, tr:  99.40%, tr_best:  99.40%, epoch time: 133.68 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677040 real_backward_count 75588  11.164%\n",
      "layer   1  Sparsity: 84.8633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 141 occurrences\n",
      "test - Value 1: 311 occurrences\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:928.768677/877.665588, val:  79.87%, val_best:  85.84%, tr:  99.08%, tr_best:  99.40%, epoch time: 134.85 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 693160 real_backward_count 76808  11.081%\n",
      "layer   1  Sparsity: 77.9297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:933.825317/862.984741, val:  84.73%, val_best:  85.84%, tr:  99.38%, tr_best:  99.40%, epoch time: 134.67 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709280 real_backward_count 77987  10.995%\n",
      "layer   1  Sparsity: 89.3555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:932.367432/888.730774, val:  74.78%, val_best:  85.84%, tr:  99.33%, tr_best:  99.40%, epoch time: 134.76 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 725400 real_backward_count 79193  10.917%\n",
      "layer   1  Sparsity: 91.2598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:937.186340/909.670837, val:  71.90%, val_best:  85.84%, tr:  99.28%, tr_best:  99.40%, epoch time: 135.79 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741520 real_backward_count 80378  10.840%\n",
      "layer   1  Sparsity: 75.1953%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:942.864136/928.188110, val:  67.04%, val_best:  85.84%, tr:  99.38%, tr_best:  99.40%, epoch time: 135.24 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7408%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 757640 real_backward_count 81563  10.765%\n",
      "layer   1  Sparsity: 89.7949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 183 occurrences\n",
      "test - Value 1: 269 occurrences\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:936.111816/870.068970, val:  82.52%, val_best:  85.84%, tr:  99.23%, tr_best:  99.40%, epoch time: 133.77 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.9284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773760 real_backward_count 82707  10.689%\n",
      "layer   1  Sparsity: 90.2832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:943.690063/912.760986, val:  70.13%, val_best:  85.84%, tr:  99.50%, tr_best:  99.50%, epoch time: 134.57 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 789880 real_backward_count 83851  10.616%\n",
      "layer   1  Sparsity: 85.4004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 25 occurrences\n",
      "test - Value 1: 427 occurrences\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:944.923096/974.186157, val:  55.53%, val_best:  85.84%, tr:  99.60%, tr_best:  99.60%, epoch time: 136.24 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806000 real_backward_count 84885  10.532%\n",
      "layer   1  Sparsity: 81.5918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 118 occurrences\n",
      "test - Value 1: 334 occurrences\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:954.169556/909.038452, val:  75.22%, val_best:  85.84%, tr:  99.63%, tr_best:  99.63%, epoch time: 135.79 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822120 real_backward_count 86059  10.468%\n",
      "layer   1  Sparsity: 87.3047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1423.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4669.00 at epoch 51, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 35 occurrences\n",
      "test - Value 1: 417 occurrences\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:955.226929/978.890381, val:  57.74%, val_best:  85.84%, tr:  99.35%, tr_best:  99.63%, epoch time: 135.46 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838240 real_backward_count 87215  10.405%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:951.860596/899.892456, val:  75.66%, val_best:  85.84%, tr:  99.53%, tr_best:  99.63%, epoch time: 135.43 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 854360 real_backward_count 88293  10.334%\n",
      "layer   1  Sparsity: 85.1562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12593.0\n",
      "lif layer 1 self.abs_max_v: 12747.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 110 occurrences\n",
      "test - Value 1: 342 occurrences\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:958.865967/920.638672, val:  73.45%, val_best:  85.84%, tr:  99.53%, tr_best:  99.63%, epoch time: 135.37 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7175%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870480 real_backward_count 89318  10.261%\n",
      "layer   1  Sparsity: 77.8809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 68 occurrences\n",
      "test - Value 1: 384 occurrences\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:958.517822/959.657471, val:  65.04%, val_best:  85.84%, tr:  99.53%, tr_best:  99.63%, epoch time: 135.36 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 886600 real_backward_count 90430  10.200%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 69 occurrences\n",
      "test - Value 1: 383 occurrences\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:965.565430/945.231140, val:  65.27%, val_best:  85.84%, tr:  99.65%, tr_best:  99.65%, epoch time: 135.00 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 902720 real_backward_count 91445  10.130%\n",
      "layer   1  Sparsity: 77.4902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:961.569275/922.277832, val:  71.02%, val_best:  85.84%, tr:  99.70%, tr_best:  99.70%, epoch time: 133.64 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6718%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 918840 real_backward_count 92494  10.066%\n",
      "layer   1  Sparsity: 79.2480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 75 occurrences\n",
      "test - Value 1: 377 occurrences\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:951.896423/940.934326, val:  66.15%, val_best:  85.84%, tr:  99.78%, tr_best:  99.78%, epoch time: 134.19 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 934960 real_backward_count 93445   9.995%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 146 occurrences\n",
      "test - Value 1: 306 occurrences\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:957.689209/903.553528, val:  79.20%, val_best:  85.84%, tr:  99.75%, tr_best:  99.78%, epoch time: 134.40 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 951080 real_backward_count 94417   9.927%\n",
      "layer   1  Sparsity: 76.5137%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 132 occurrences\n",
      "test - Value 1: 320 occurrences\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:966.151550/933.817200, val:  76.55%, val_best:  85.84%, tr:  99.53%, tr_best:  99.78%, epoch time: 134.03 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6890%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967200 real_backward_count 95469   9.871%\n",
      "layer   1  Sparsity: 75.0488%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 189 occurrences\n",
      "test - Value 1: 263 occurrences\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:969.975769/908.544434, val:  85.18%, val_best:  85.84%, tr:  99.43%, tr_best:  99.78%, epoch time: 135.35 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 983320 real_backward_count 96509   9.815%\n",
      "layer   1  Sparsity: 63.8184%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 89 occurrences\n",
      "test - Value 1: 363 occurrences\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:973.070618/947.238159, val:  69.69%, val_best:  85.84%, tr:  99.53%, tr_best:  99.78%, epoch time: 133.68 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1485%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999440 real_backward_count 97537   9.759%\n",
      "layer   1  Sparsity: 77.2461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:969.580444/924.255981, val:  80.75%, val_best:  85.84%, tr:  99.45%, tr_best:  99.78%, epoch time: 133.06 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1015560 real_backward_count 98580   9.707%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:970.058960/912.543335, val:  84.29%, val_best:  85.84%, tr:  99.58%, tr_best:  99.78%, epoch time: 134.66 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1031680 real_backward_count 99583   9.653%\n",
      "layer   1  Sparsity: 72.2656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4699.00 at epoch 64, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 66 occurrences\n",
      "test - Value 1: 386 occurrences\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:978.752014/972.990967, val:  64.60%, val_best:  85.84%, tr:  99.75%, tr_best:  99.78%, epoch time: 133.17 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9081%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7036%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047800 real_backward_count 100525   9.594%\n",
      "layer   1  Sparsity: 77.0508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 73 occurrences\n",
      "test - Value 1: 379 occurrences\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:983.048401/962.664185, val:  66.15%, val_best:  85.84%, tr:  99.80%, tr_best:  99.80%, epoch time: 133.45 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1063920 real_backward_count 101522   9.542%\n",
      "layer   1  Sparsity: 75.7812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 181 occurrences\n",
      "test - Value 1: 271 occurrences\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:994.368225/928.944397, val:  84.73%, val_best:  85.84%, tr:  99.65%, tr_best:  99.80%, epoch time: 136.01 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5145%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1080040 real_backward_count 102515   9.492%\n",
      "layer   1  Sparsity: 91.4551%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1434.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4803.00 at epoch 67, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 65 occurrences\n",
      "test - Value 1: 387 occurrences\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:991.074890/997.095581, val:  64.38%, val_best:  85.84%, tr:  99.83%, tr_best:  99.83%, epoch time: 134.87 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1423%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096160 real_backward_count 103493   9.441%\n",
      "layer   1  Sparsity: 73.7793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 82 occurrences\n",
      "test - Value 1: 370 occurrences\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:992.480530/966.343628, val:  68.14%, val_best:  85.84%, tr:  99.68%, tr_best:  99.83%, epoch time: 134.87 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1112280 real_backward_count 104448   9.390%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:992.307800/929.906921, val:  80.31%, val_best:  85.84%, tr:  99.73%, tr_best:  99.83%, epoch time: 136.74 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128400 real_backward_count 105416   9.342%\n",
      "layer   1  Sparsity: 77.3438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:982.044067/927.309143, val:  77.21%, val_best:  85.84%, tr:  99.75%, tr_best:  99.83%, epoch time: 135.47 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1144520 real_backward_count 106342   9.291%\n",
      "layer   1  Sparsity: 86.2305%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:988.348389/930.784668, val:  75.88%, val_best:  85.84%, tr:  99.73%, tr_best:  99.83%, epoch time: 135.87 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1160640 real_backward_count 107351   9.249%\n",
      "layer   1  Sparsity: 77.3438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 154 occurrences\n",
      "test - Value 1: 298 occurrences\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:985.005066/923.774963, val:  80.53%, val_best:  85.84%, tr:  99.70%, tr_best:  99.83%, epoch time: 136.36 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1176760 real_backward_count 108340   9.207%\n",
      "layer   1  Sparsity: 82.9590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:993.816650/953.128052, val:  66.37%, val_best:  85.84%, tr:  99.78%, tr_best:  99.83%, epoch time: 135.71 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1442%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1192880 real_backward_count 109293   9.162%\n",
      "layer   1  Sparsity: 83.5449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 141 occurrences\n",
      "test - Value 1: 311 occurrences\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:985.124939/918.646423, val:  79.87%, val_best:  85.84%, tr:  99.70%, tr_best:  99.83%, epoch time: 134.67 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1441%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8807%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1209000 real_backward_count 110237   9.118%\n",
      "layer   1  Sparsity: 86.4746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1455.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 69 occurrences\n",
      "test - Value 1: 383 occurrences\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:984.846985/966.676880, val:  64.82%, val_best:  85.84%, tr:  99.78%, tr_best:  99.83%, epoch time: 136.60 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9396%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225120 real_backward_count 111141   9.072%\n",
      "layer   1  Sparsity: 89.5020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 155 occurrences\n",
      "test - Value 1: 297 occurrences\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:988.635254/922.327209, val:  82.08%, val_best:  85.84%, tr:  99.75%, tr_best:  99.83%, epoch time: 133.72 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1241240 real_backward_count 112069   9.029%\n",
      "layer   1  Sparsity: 86.8652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1492.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 141 occurrences\n",
      "test - Value 1: 311 occurrences\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:983.237610/924.868347, val:  79.42%, val_best:  85.84%, tr:  99.73%, tr_best:  99.83%, epoch time: 133.66 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257360 real_backward_count 113006   8.988%\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6300.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4840.00 at epoch 78, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:988.271912/981.752380, val:  63.50%, val_best:  85.84%, tr:  99.88%, tr_best:  99.88%, epoch time: 136.02 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1273480 real_backward_count 113904   8.944%\n",
      "layer   1  Sparsity: 85.8887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8934.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 92 occurrences\n",
      "test - Value 1: 360 occurrences\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:994.832886/956.907654, val:  69.91%, val_best:  85.84%, tr:  99.70%, tr_best:  99.88%, epoch time: 133.07 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1289600 real_backward_count 114819   8.903%\n",
      "layer   1  Sparsity: 82.0312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6313.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 136 occurrences\n",
      "test - Value 1: 316 occurrences\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:992.680969/925.654480, val:  77.43%, val_best:  85.84%, tr:  99.85%, tr_best:  99.88%, epoch time: 134.32 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1305720 real_backward_count 115701   8.861%\n",
      "layer   1  Sparsity: 74.5117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9046.0\n",
      "fc layer 3 self.abs_max_out: 1508.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 60 occurrences\n",
      "test - Value 1: 392 occurrences\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:996.147583/985.014160, val:  63.27%, val_best:  85.84%, tr:  99.83%, tr_best:  99.88%, epoch time: 135.87 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321840 real_backward_count 116591   8.820%\n",
      "layer   1  Sparsity: 84.8633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4845.00 at epoch 82, iter 4029\n",
      "max_activation_accul updated: 4848.00 at epoch 82, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 45 occurrences\n",
      "test - Value 1: 407 occurrences\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:994.570862/1003.478455, val:  59.96%, val_best:  85.84%, tr:  99.85%, tr_best:  99.88%, epoch time: 133.69 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1337960 real_backward_count 117489   8.781%\n",
      "layer   1  Sparsity: 88.5742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4849.00 at epoch 83, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:994.695068/998.679871, val:  59.73%, val_best:  85.84%, tr:  99.78%, tr_best:  99.88%, epoch time: 134.19 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354080 real_backward_count 118374   8.742%\n",
      "layer   1  Sparsity: 65.5762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 169 occurrences\n",
      "test - Value 1: 283 occurrences\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:999.358032/926.602356, val:  83.41%, val_best:  85.84%, tr:  99.85%, tr_best:  99.88%, epoch time: 134.95 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1481%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370200 real_backward_count 119303   8.707%\n",
      "layer   1  Sparsity: 79.4434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1535.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 4885.00 at epoch 85, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 68 occurrences\n",
      "test - Value 1: 384 occurrences\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:1011.302429/1000.570007, val:  65.04%, val_best:  85.84%, tr:  99.85%, tr_best:  99.88%, epoch time: 135.10 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1386320 real_backward_count 120171   8.668%\n",
      "layer   1  Sparsity: 89.1602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 5035.00 at epoch 86, iter 4029\n",
      "fc layer 3 self.abs_max_out: 1543.0\n",
      "max_activation_accul updated: 5158.00 at epoch 86, iter 4029\n",
      "max_activation_accul updated: 5495.00 at epoch 86, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 27 occurrences\n",
      "test - Value 1: 425 occurrences\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:1010.190918/1074.310425, val:  55.97%, val_best:  85.84%, tr:  99.78%, tr_best:  99.88%, epoch time: 136.11 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6968%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1402440 real_backward_count 121054   8.632%\n",
      "layer   1  Sparsity: 89.5996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 116 occurrences\n",
      "test - Value 1: 336 occurrences\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:1014.733765/966.586609, val:  74.34%, val_best:  85.84%, tr:  99.88%, tr_best:  99.88%, epoch time: 134.87 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8298%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1418560 real_backward_count 121911   8.594%\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 130 occurrences\n",
      "test - Value 1: 322 occurrences\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:1018.831055/955.419739, val:  77.43%, val_best:  85.84%, tr:  99.90%, tr_best:  99.90%, epoch time: 134.07 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1434680 real_backward_count 122850   8.563%\n",
      "layer   1  Sparsity: 61.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 105 occurrences\n",
      "test - Value 1: 347 occurrences\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:1022.915039/977.113647, val:  72.35%, val_best:  85.84%, tr:  99.85%, tr_best:  99.90%, epoch time: 135.87 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1491%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1450800 real_backward_count 123777   8.532%\n",
      "layer   1  Sparsity: 89.5996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1567.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 155 occurrences\n",
      "test - Value 1: 297 occurrences\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:1013.544678/937.972717, val:  81.64%, val_best:  85.84%, tr:  99.78%, tr_best:  99.90%, epoch time: 136.21 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1466920 real_backward_count 124660   8.498%\n",
      "layer   1  Sparsity: 73.2422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 159 occurrences\n",
      "test - Value 1: 293 occurrences\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:1009.852905/937.008789, val:  81.64%, val_best:  85.84%, tr:  99.88%, tr_best:  99.90%, epoch time: 135.76 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6592%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483040 real_backward_count 125479   8.461%\n",
      "layer   1  Sparsity: 78.7598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:1009.524841/982.278625, val:  63.94%, val_best:  85.84%, tr:  99.90%, tr_best:  99.90%, epoch time: 135.96 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6264%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1499160 real_backward_count 126344   8.428%\n",
      "layer   1  Sparsity: 81.8848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 98 occurrences\n",
      "test - Value 1: 354 occurrences\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:1016.681580/975.445984, val:  71.24%, val_best:  85.84%, tr:  99.80%, tr_best:  99.90%, epoch time: 135.91 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1515280 real_backward_count 127213   8.395%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 130 occurrences\n",
      "test - Value 1: 322 occurrences\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:1014.176392/939.830933, val:  77.43%, val_best:  85.84%, tr:  99.90%, tr_best:  99.90%, epoch time: 135.19 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1531400 real_backward_count 128060   8.362%\n",
      "layer   1  Sparsity: 70.3613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:1011.146729/1015.494141, val:  59.73%, val_best:  85.84%, tr:  99.85%, tr_best:  99.90%, epoch time: 135.01 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1470%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7151%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1547520 real_backward_count 128902   8.330%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 47 occurrences\n",
      "test - Value 1: 405 occurrences\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:1013.370667/986.662048, val:  60.40%, val_best:  85.84%, tr:  99.80%, tr_best:  99.90%, epoch time: 135.46 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5718%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1563640 real_backward_count 129777   8.300%\n",
      "layer   1  Sparsity: 77.7832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9173.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:1013.327209/938.520203, val:  80.97%, val_best:  85.84%, tr:  99.95%, tr_best:  99.95%, epoch time: 135.09 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9195%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1579760 real_backward_count 130627   8.269%\n",
      "layer   1  Sparsity: 83.2031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 93 occurrences\n",
      "test - Value 1: 359 occurrences\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:1012.700500/964.664612, val:  70.58%, val_best:  85.84%, tr:  99.80%, tr_best:  99.95%, epoch time: 135.77 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1442%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595880 real_backward_count 131496   8.240%\n",
      "layer   1  Sparsity: 70.1172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 95 occurrences\n",
      "test - Value 1: 357 occurrences\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:1000.015503/951.052734, val:  69.69%, val_best:  85.84%, tr:  99.85%, tr_best:  99.95%, epoch time: 136.89 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612000 real_backward_count 132361   8.211%\n",
      "layer   1  Sparsity: 89.6973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 62 occurrences\n",
      "test - Value 1: 390 occurrences\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:1001.509033/1001.581848, val:  63.27%, val_best:  85.84%, tr:  99.83%, tr_best:  99.95%, epoch time: 134.81 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1628120 real_backward_count 133251   8.184%\n",
      "layer   1  Sparsity: 78.4668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 194 occurrences\n",
      "test - Value 1: 258 occurrences\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:1005.347168/927.845459, val:  83.19%, val_best:  85.84%, tr:  99.78%, tr_best:  99.95%, epoch time: 135.26 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644240 real_backward_count 134132   8.158%\n",
      "layer   1  Sparsity: 73.6816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:1003.075256/992.174438, val:  63.94%, val_best:  85.84%, tr:  99.93%, tr_best:  99.95%, epoch time: 135.83 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1660360 real_backward_count 135009   8.131%\n",
      "layer   1  Sparsity: 76.7090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4329.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 55 occurrences\n",
      "test - Value 1: 397 occurrences\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:1012.505798/988.629761, val:  61.73%, val_best:  85.84%, tr:  99.85%, tr_best:  99.95%, epoch time: 136.29 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1676480 real_backward_count 135861   8.104%\n",
      "layer   1  Sparsity: 85.1562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 145 occurrences\n",
      "test - Value 1: 307 occurrences\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:1010.012939/941.000671, val:  79.42%, val_best:  85.84%, tr:  99.90%, tr_best:  99.95%, epoch time: 135.59 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1692600 real_backward_count 136712   8.077%\n",
      "layer   1  Sparsity: 86.3281%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:1005.636780/989.705078, val:  64.82%, val_best:  85.84%, tr:  99.90%, tr_best:  99.95%, epoch time: 135.64 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1708720 real_backward_count 137490   8.046%\n",
      "layer   1  Sparsity: 86.1328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4371.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:1018.981934/987.788879, val:  66.37%, val_best:  85.84%, tr:  99.90%, tr_best:  99.95%, epoch time: 136.62 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1724840 real_backward_count 138326   8.020%\n",
      "layer   1  Sparsity: 87.9395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 85 occurrences\n",
      "test - Value 1: 367 occurrences\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:1023.118042/991.982971, val:  68.36%, val_best:  85.84%, tr:  99.88%, tr_best:  99.95%, epoch time: 135.76 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1431%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1740960 real_backward_count 139178   7.994%\n",
      "layer   1  Sparsity: 70.7520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1572.0\n",
      "fc layer 3 self.abs_max_out: 1574.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 144 occurrences\n",
      "test - Value 1: 308 occurrences\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:1025.549438/953.025391, val:  77.88%, val_best:  85.84%, tr:  99.88%, tr_best:  99.95%, epoch time: 135.70 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1757080 real_backward_count 139984   7.967%\n",
      "layer   1  Sparsity: 91.0156%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:1029.995117/987.346680, val:  71.02%, val_best:  85.84%, tr:  99.88%, tr_best:  99.95%, epoch time: 136.62 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1773200 real_backward_count 140810   7.941%\n",
      "layer   1  Sparsity: 77.8809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 92 occurrences\n",
      "test - Value 1: 360 occurrences\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:1025.655762/983.498535, val:  69.91%, val_best:  85.84%, tr:  99.88%, tr_best:  99.95%, epoch time: 135.34 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7305%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6083%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1789320 real_backward_count 141591   7.913%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 3 self.abs_max_out: 1578.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 27 occurrences\n",
      "test - Value 1: 425 occurrences\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:1024.761719/1052.100708, val:  55.97%, val_best:  85.84%, tr:  99.95%, tr_best:  99.95%, epoch time: 136.41 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5211%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1805440 real_backward_count 142411   7.888%\n",
      "layer   1  Sparsity: 80.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1590.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 91 occurrences\n",
      "test - Value 1: 361 occurrences\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:1030.622314/958.878113, val:  69.25%, val_best:  85.84%, tr:  99.95%, tr_best:  99.95%, epoch time: 134.15 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1821560 real_backward_count 143250   7.864%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 115 occurrences\n",
      "test - Value 1: 337 occurrences\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:1018.502930/941.722107, val:  74.12%, val_best:  85.84%, tr:  99.90%, tr_best:  99.95%, epoch time: 133.40 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1837680 real_backward_count 144071   7.840%\n",
      "layer   1  Sparsity: 80.3223%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:1018.314270/976.350464, val:  69.91%, val_best:  85.84%, tr:  99.95%, tr_best:  99.95%, epoch time: 135.37 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1853800 real_backward_count 144927   7.818%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:1021.465698/1002.231506, val:  63.94%, val_best:  85.84%, tr:  99.93%, tr_best:  99.95%, epoch time: 135.55 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869920 real_backward_count 145708   7.792%\n",
      "layer   1  Sparsity: 75.6348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:1019.079895/969.160278, val:  73.01%, val_best:  85.84%, tr:  99.83%, tr_best:  99.95%, epoch time: 135.62 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1886040 real_backward_count 146513   7.768%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 86 occurrences\n",
      "test - Value 1: 366 occurrences\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:1021.382141/993.099121, val:  69.03%, val_best:  85.84%, tr:  99.90%, tr_best:  99.95%, epoch time: 135.24 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8698%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1902160 real_backward_count 147372   7.748%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:1027.520630/975.996338, val:  66.15%, val_best:  85.84%, tr:  99.80%, tr_best:  99.95%, epoch time: 134.63 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0159%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6889%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918280 real_backward_count 148141   7.723%\n",
      "layer   1  Sparsity: 78.5156%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:1018.878235/980.190430, val:  69.47%, val_best:  85.84%, tr:  99.85%, tr_best:  99.95%, epoch time: 135.13 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6892%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1934400 real_backward_count 148996   7.702%\n",
      "layer   1  Sparsity: 84.6191%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9247.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 108 occurrences\n",
      "test - Value 1: 344 occurrences\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:1030.547974/973.078125, val:  72.57%, val_best:  85.84%, tr:  99.88%, tr_best:  99.95%, epoch time: 134.48 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1950520 real_backward_count 149809   7.680%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 103 occurrences\n",
      "test - Value 1: 349 occurrences\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:1032.789551/982.359863, val:  72.35%, val_best:  85.84%, tr:  99.90%, tr_best:  99.95%, epoch time: 135.44 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9072%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1966640 real_backward_count 150634   7.659%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9256.0\n",
      "fc layer 3 self.abs_max_out: 1621.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 75 occurrences\n",
      "test - Value 1: 377 occurrences\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:1035.480103/997.827454, val:  66.15%, val_best:  85.84%, tr: 100.00%, tr_best: 100.00%, epoch time: 135.74 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1982760 real_backward_count 151417   7.637%\n",
      "layer   1  Sparsity: 88.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9269.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:1036.815430/1007.602295, val:  67.26%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 135.41 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1431%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1998880 real_backward_count 152178   7.613%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 75 occurrences\n",
      "test - Value 1: 377 occurrences\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:1034.942505/1005.841064, val:  66.59%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 135.47 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2015000 real_backward_count 153011   7.594%\n",
      "layer   1  Sparsity: 74.8047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9279.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 165 occurrences\n",
      "test - Value 1: 287 occurrences\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:1033.385010/945.633972, val:  82.52%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.57 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2031120 real_backward_count 153818   7.573%\n",
      "layer   1  Sparsity: 88.9648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12785.5\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 184 occurrences\n",
      "test - Value 1: 268 occurrences\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:1037.414307/941.078857, val:  84.96%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 136.78 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1429%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2047240 real_backward_count 154605   7.552%\n",
      "layer   1  Sparsity: 73.2422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:1037.333618/1013.090881, val:  66.59%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 136.45 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2063360 real_backward_count 155433   7.533%\n",
      "layer   1  Sparsity: 86.4258%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 108 occurrences\n",
      "test - Value 1: 344 occurrences\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:1034.046387/971.616699, val:  72.57%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 135.47 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2079480 real_backward_count 156203   7.512%\n",
      "layer   1  Sparsity: 70.8496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 123 occurrences\n",
      "test - Value 1: 329 occurrences\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:1045.653198/966.932678, val:  75.00%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.28 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2095600 real_backward_count 156990   7.491%\n",
      "layer   1  Sparsity: 89.5020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:1047.101318/967.449768, val:  72.57%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 134.14 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5370%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2111720 real_backward_count 157857   7.475%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1628.0\n",
      "fc layer 3 self.abs_max_out: 1796.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 70 occurrences\n",
      "test - Value 1: 382 occurrences\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:1040.554810/1006.148987, val:  65.49%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 134.30 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6239%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2127840 real_backward_count 158703   7.458%\n",
      "layer   1  Sparsity: 89.1602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 68 occurrences\n",
      "test - Value 1: 384 occurrences\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:1040.502563/1013.865417, val:  64.60%, val_best:  85.84%, tr:  99.88%, tr_best: 100.00%, epoch time: 135.02 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5220%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2143960 real_backward_count 159492   7.439%\n",
      "layer   1  Sparsity: 80.9570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 105 occurrences\n",
      "test - Value 1: 347 occurrences\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:1031.691772/966.361938, val:  71.90%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 133.88 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2160080 real_backward_count 160282   7.420%\n",
      "layer   1  Sparsity: 77.1484%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 128 occurrences\n",
      "test - Value 1: 324 occurrences\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:1042.560059/967.120361, val:  77.43%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 136.25 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2176200 real_backward_count 161053   7.401%\n",
      "layer   1  Sparsity: 85.3516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 206 occurrences\n",
      "test - Value 1: 246 occurrences\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:1037.445190/956.217529, val:  84.51%, val_best:  85.84%, tr:  99.78%, tr_best: 100.00%, epoch time: 135.62 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8816%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2192320 real_backward_count 161813   7.381%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 5566.00 at epoch 136, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 23 occurrences\n",
      "test - Value 1: 429 occurrences\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:1043.327637/1108.358032, val:  55.09%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.70 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6611%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2208440 real_backward_count 162573   7.361%\n",
      "layer   1  Sparsity: 71.3379%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:1035.569702/965.267151, val:  79.87%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 136.14 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2224560 real_backward_count 163314   7.341%\n",
      "layer   1  Sparsity: 68.6523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12920.5\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:1036.152832/964.468445, val:  79.87%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 136.30 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2240680 real_backward_count 164119   7.325%\n",
      "layer   1  Sparsity: 73.9258%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13054.5\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 72 occurrences\n",
      "test - Value 1: 380 occurrences\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:1046.946655/1025.151611, val:  65.93%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 135.12 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1462%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2256800 real_backward_count 164898   7.307%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 103 occurrences\n",
      "test - Value 1: 349 occurrences\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:1048.421753/975.710815, val:  71.46%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.05 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2272920 real_backward_count 165719   7.291%\n",
      "layer   1  Sparsity: 86.6211%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 70 occurrences\n",
      "test - Value 1: 382 occurrences\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:1047.931274/1018.102417, val:  65.49%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.42 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2289040 real_backward_count 166493   7.273%\n",
      "layer   1  Sparsity: 76.8066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 99 occurrences\n",
      "test - Value 1: 353 occurrences\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:1053.230103/996.042114, val:  71.46%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 135.08 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2305160 real_backward_count 167290   7.257%\n",
      "layer   1  Sparsity: 82.9102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9302.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:1051.557739/1002.504028, val:  66.59%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.10 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1442%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2321280 real_backward_count 168035   7.239%\n",
      "layer   1  Sparsity: 82.1777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:1059.583374/988.895386, val:  75.44%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.14 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2337400 real_backward_count 168828   7.223%\n",
      "layer   1  Sparsity: 74.4141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9409.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 118 occurrences\n",
      "test - Value 1: 334 occurrences\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:1051.544556/968.081604, val:  74.78%, val_best:  85.84%, tr:  99.80%, tr_best: 100.00%, epoch time: 135.91 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2353520 real_backward_count 169607   7.207%\n",
      "layer   1  Sparsity: 84.8633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 111 occurrences\n",
      "test - Value 1: 341 occurrences\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:1057.698730/993.787048, val:  73.67%, val_best:  85.84%, tr: 100.00%, tr_best: 100.00%, epoch time: 135.95 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2369640 real_backward_count 170381   7.190%\n",
      "layer   1  Sparsity: 87.9883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:1053.436401/989.031616, val:  71.24%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 136.24 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1431%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2385760 real_backward_count 171134   7.173%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 58 occurrences\n",
      "test - Value 1: 394 occurrences\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:1050.531250/1014.292297, val:  62.83%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 134.75 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2401880 real_backward_count 171891   7.157%\n",
      "layer   1  Sparsity: 68.7988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 153 occurrences\n",
      "test - Value 1: 299 occurrences\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:1045.395752/968.445374, val:  79.87%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 132.97 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8284%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2418000 real_backward_count 172645   7.140%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 79 occurrences\n",
      "test - Value 1: 373 occurrences\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:1044.063232/986.443542, val:  67.48%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.99 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2434120 real_backward_count 173435   7.125%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 157 occurrences\n",
      "test - Value 1: 295 occurrences\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:1050.083618/974.247681, val:  80.31%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 135.09 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8012%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2450240 real_backward_count 174239   7.111%\n",
      "layer   1  Sparsity: 86.7676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:1049.863647/1041.437256, val:  64.82%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.31 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2466360 real_backward_count 175012   7.096%\n",
      "layer   1  Sparsity: 84.8145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:1047.867920/976.290527, val:  73.67%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.10 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2482480 real_backward_count 175778   7.081%\n",
      "layer   1  Sparsity: 87.2070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:1056.186279/979.977356, val:  74.56%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.60 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8282%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2498600 real_backward_count 176539   7.066%\n",
      "layer   1  Sparsity: 77.0020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9544.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 68 occurrences\n",
      "test - Value 1: 384 occurrences\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:1057.930298/1044.150757, val:  65.04%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.78 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9126%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2514720 real_backward_count 177303   7.051%\n",
      "layer   1  Sparsity: 65.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 69 occurrences\n",
      "test - Value 1: 383 occurrences\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:1060.435547/1025.387817, val:  65.27%, val_best:  85.84%, tr:  99.88%, tr_best: 100.00%, epoch time: 135.47 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1482%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2530840 real_backward_count 178024   7.034%\n",
      "layer   1  Sparsity: 86.5234%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 166 occurrences\n",
      "test - Value 1: 286 occurrences\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:1050.813477/958.578613, val:  80.97%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.33 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2546960 real_backward_count 178762   7.019%\n",
      "layer   1  Sparsity: 84.4727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 107 occurrences\n",
      "test - Value 1: 345 occurrences\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:1048.390503/988.129578, val:  73.23%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 137.45 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 81.1439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2563080 real_backward_count 179535   7.005%\n",
      "layer   1  Sparsity: 85.2539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:1052.067749/1042.196167, val:  64.82%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.17 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2579200 real_backward_count 180311   6.991%\n",
      "layer   1  Sparsity: 76.6602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 144 occurrences\n",
      "test - Value 1: 308 occurrences\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:1060.268921/968.495300, val:  80.09%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.05 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2595320 real_backward_count 181055   6.976%\n",
      "layer   1  Sparsity: 87.3047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9584.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 53 occurrences\n",
      "test - Value 1: 399 occurrences\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:1051.294678/1037.395996, val:  61.73%, val_best:  85.84%, tr:  99.88%, tr_best: 100.00%, epoch time: 136.10 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2611440 real_backward_count 181856   6.964%\n",
      "layer   1  Sparsity: 72.2656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9693.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 85 occurrences\n",
      "test - Value 1: 367 occurrences\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:1053.975220/1018.227844, val:  68.81%, val_best:  85.84%, tr: 100.00%, tr_best: 100.00%, epoch time: 135.27 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7976%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2627560 real_backward_count 182660   6.952%\n",
      "layer   1  Sparsity: 91.4062%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 176 occurrences\n",
      "test - Value 1: 276 occurrences\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:1054.673340/967.247803, val:  83.19%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.72 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1423%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2643680 real_backward_count 183404   6.937%\n",
      "layer   1  Sparsity: 75.8301%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 153 occurrences\n",
      "test - Value 1: 299 occurrences\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:1057.354736/975.537170, val:  78.54%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 135.79 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2659800 real_backward_count 184187   6.925%\n",
      "layer   1  Sparsity: 75.6836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9826.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 159 occurrences\n",
      "test - Value 1: 293 occurrences\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:1056.325073/966.447205, val:  81.19%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.13 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2675920 real_backward_count 184938   6.911%\n",
      "layer   1  Sparsity: 76.6602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:1055.390381/973.220703, val:  79.42%, val_best:  85.84%, tr: 100.00%, tr_best: 100.00%, epoch time: 134.45 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2692040 real_backward_count 185695   6.898%\n",
      "layer   1  Sparsity: 89.6973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:1053.998047/999.787659, val:  69.47%, val_best:  85.84%, tr:  99.88%, tr_best: 100.00%, epoch time: 134.86 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6040%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2708160 real_backward_count 186449   6.885%\n",
      "layer   1  Sparsity: 76.0254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 131 occurrences\n",
      "test - Value 1: 321 occurrences\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:1056.186401/973.470093, val:  76.77%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 135.09 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2724280 real_backward_count 187187   6.871%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:1056.694458/981.504700, val:  74.56%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 134.33 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2740400 real_backward_count 187902   6.857%\n",
      "layer   1  Sparsity: 54.7363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 30 occurrences\n",
      "test - Value 1: 422 occurrences\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:1058.559692/1101.436401, val:  56.64%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.78 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1505%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2756520 real_backward_count 188671   6.845%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 105 occurrences\n",
      "test - Value 1: 347 occurrences\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:1056.746582/995.887268, val:  72.79%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.18 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6807%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2772640 real_backward_count 189432   6.832%\n",
      "layer   1  Sparsity: 79.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 107 occurrences\n",
      "test - Value 1: 345 occurrences\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:1055.817871/992.637451, val:  72.35%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.64 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2788760 real_backward_count 190181   6.820%\n",
      "layer   1  Sparsity: 73.1934%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:1056.696411/999.489746, val:  70.80%, val_best:  85.84%, tr:  99.85%, tr_best: 100.00%, epoch time: 136.78 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5965%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2804880 real_backward_count 190939   6.807%\n",
      "layer   1  Sparsity: 91.2598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 87 occurrences\n",
      "test - Value 1: 365 occurrences\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:1054.444824/1005.887573, val:  68.81%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.33 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1424%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2821000 real_backward_count 191690   6.795%\n",
      "layer   1  Sparsity: 77.6367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 53 occurrences\n",
      "test - Value 1: 399 occurrences\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:1048.438110/1045.592529, val:  61.73%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 136.01 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2837120 real_backward_count 192459   6.784%\n",
      "layer   1  Sparsity: 84.3262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 79 occurrences\n",
      "test - Value 1: 373 occurrences\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:1052.294189/1010.524353, val:  67.04%, val_best:  85.84%, tr: 100.00%, tr_best: 100.00%, epoch time: 136.04 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.7970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5860%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2853240 real_backward_count 193220   6.772%\n",
      "layer   1  Sparsity: 85.3516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:1052.870728/974.551941, val:  75.22%, val_best:  85.84%, tr:  99.88%, tr_best: 100.00%, epoch time: 135.97 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8734%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2869360 real_backward_count 194005   6.761%\n",
      "layer   1  Sparsity: 82.3242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 93 occurrences\n",
      "test - Value 1: 359 occurrences\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:1054.277222/1022.921326, val:  70.13%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 137.72 seconds, 2.30 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2885480 real_backward_count 194796   6.751%\n",
      "layer   1  Sparsity: 76.2207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 143 occurrences\n",
      "test - Value 1: 309 occurrences\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:1056.341919/967.334045, val:  78.54%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.50 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2901600 real_backward_count 195570   6.740%\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 72 occurrences\n",
      "test - Value 1: 380 occurrences\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:1061.575439/1040.571777, val:  65.49%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 136.50 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2917720 real_backward_count 196308   6.728%\n",
      "layer   1  Sparsity: 79.3945%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 111 occurrences\n",
      "test - Value 1: 341 occurrences\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:1066.984131/991.634216, val:  73.67%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 136.81 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2933840 real_backward_count 197046   6.716%\n",
      "layer   1  Sparsity: 75.8789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:1067.682617/1021.873230, val:  72.12%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 137.29 seconds, 2.29 minutes\n",
      "layer   1  Sparsity: 81.1458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2949960 real_backward_count 197768   6.704%\n",
      "layer   1  Sparsity: 73.1445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 79 occurrences\n",
      "test - Value 1: 373 occurrences\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:1066.329956/1030.748535, val:  67.04%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 137.10 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2966080 real_backward_count 198545   6.694%\n",
      "layer   1  Sparsity: 73.7793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 177 occurrences\n",
      "test - Value 1: 275 occurrences\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:1063.575562/966.427673, val:  83.41%, val_best:  85.84%, tr:  99.93%, tr_best: 100.00%, epoch time: 135.30 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2982200 real_backward_count 199284   6.682%\n",
      "layer   1  Sparsity: 84.5215%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 71 occurrences\n",
      "test - Value 1: 381 occurrences\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:1058.987061/1034.163452, val:  65.71%, val_best:  85.84%, tr: 100.00%, tr_best: 100.00%, epoch time: 134.08 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8428%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5850%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2998320 real_backward_count 200047   6.672%\n",
      "layer   1  Sparsity: 76.2207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:1062.512207/962.069336, val:  81.64%, val_best:  85.84%, tr: 100.00%, tr_best: 100.00%, epoch time: 134.55 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3014440 real_backward_count 200782   6.661%\n",
      "layer   1  Sparsity: 72.5098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:1054.769775/1036.026123, val:  67.04%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 133.50 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3030560 real_backward_count 201555   6.651%\n",
      "layer   1  Sparsity: 75.6348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 75 occurrences\n",
      "test - Value 1: 377 occurrences\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:1075.200073/1050.395264, val:  66.59%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 135.77 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3046680 real_backward_count 202313   6.640%\n",
      "layer   1  Sparsity: 68.5059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:1085.276245/1005.823669, val:  71.24%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 133.24 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1475%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5990%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3062800 real_backward_count 203022   6.629%\n",
      "layer   1  Sparsity: 60.7422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 123 occurrences\n",
      "test - Value 1: 329 occurrences\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:1080.467651/990.218262, val:  76.33%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 134.76 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1492%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0700%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7065%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3078920 real_backward_count 203751   6.618%\n",
      "layer   1  Sparsity: 86.2793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:1074.494385/987.904053, val:  77.21%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 129.96 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3095040 real_backward_count 204486   6.607%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 159 occurrences\n",
      "test - Value 1: 293 occurrences\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:1072.977661/978.240906, val:  80.75%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 135.32 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9378%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3111160 real_backward_count 205226   6.596%\n",
      "layer   1  Sparsity: 87.3047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:1067.321655/973.350342, val:  80.31%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 134.71 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3127280 real_backward_count 205949   6.586%\n",
      "layer   1  Sparsity: 85.8398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:1074.861206/1029.161499, val:  67.04%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 134.62 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3143400 real_backward_count 206669   6.575%\n",
      "layer   1  Sparsity: 73.0469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:1069.971069/974.236572, val:  82.52%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 134.17 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3159520 real_backward_count 207334   6.562%\n",
      "layer   1  Sparsity: 88.3789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 116 occurrences\n",
      "test - Value 1: 336 occurrences\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:1077.104126/1005.357910, val:  74.34%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 135.35 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3175640 real_backward_count 208048   6.551%\n",
      "layer   1  Sparsity: 79.8340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 66 occurrences\n",
      "test - Value 1: 386 occurrences\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:1072.209106/1010.178955, val:  64.60%, val_best:  85.84%, tr:  99.90%, tr_best: 100.00%, epoch time: 135.47 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8929%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3191760 real_backward_count 208787   6.541%\n",
      "layer   1  Sparsity: 77.3438%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 85 occurrences\n",
      "test - Value 1: 367 occurrences\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:1064.932129/1045.608765, val:  68.36%, val_best:  85.84%, tr:  99.95%, tr_best: 100.00%, epoch time: 135.40 seconds, 2.26 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.4598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3207880 real_backward_count 209515   6.531%\n",
      "layer   1  Sparsity: 91.9434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:1073.602661/1028.486938, val:  67.70%, val_best:  85.84%, tr:  99.98%, tr_best: 100.00%, epoch time: 129.56 seconds, 2.16 minutes\n",
      "layer   1  Sparsity: 81.1422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07530786b12449219c2e195f48fad444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÑ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÑ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÑ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÑ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99975</td></tr><tr><td>tr_epoch_loss</td><td>1073.60266</td></tr><tr><td>val_acc_best</td><td>0.85841</td></tr><tr><td>val_acc_now</td><td>0.67699</td></tr><tr><td>val_loss</td><td>1028.48694</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-sweep-31</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/x1uq1mn6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/x1uq1mn6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251223_044425-x1uq1mn6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3rh3p0ow with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251223_121617-3rh3p0ow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/3rh3p0ow' target=\"_blank\">stoic-sweep-46</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/xlkrlgkj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/3rh3p0ow' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/3rh3p0ow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251223_121626_560', 'my_seed': 42, 'TIME': 4, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 2, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 16, 'lif_layer_v_threshold2': 256, 'init_scaling': [0.5, 0.25, 0.0625], 'learning_rate': 4, 'learning_rate2': 4, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 2\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4030, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4030 BATCH: 1 train_data_count: 4030\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 1, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB3eklEQVR4nO3deVxUVf8H8M/MMAyLgALJgKHimgpumHuiKZC7WVpqpmZKbrk+mllJ7pqppbmVCWmo/UptMRc0cQlNxcytxzbFJYhUAmWb7fz+4Jmb4wyrjLPweb9e9+Wde8+999xzjvd+OXeTCSEEiIiIiJyU3NYZICIiIrImBjtERETk1BjsEBERkVNjsENEREROjcEOEREROTUGO0REROTUGOwQERGRU2OwQ0RERE6NwQ4RERE5NQY75NTi4uIgk8ksDtOmTTNJW1BQgFWrVqFjx46oVq0aXF1dUaNGDQwcOBCHDh2S0qWkpGDcuHEICwuDl5cXAgIC0K1bN3z33Xcl5ufzzz+HTCbDtm3bzOY1a9YMMpkMe/fuNZtXt25dtGzZskz7Pnz4cNSuXbtMyxjFxsZCJpPh5s2bJaZdsGABdu7cWep131sHCoUC1apVQ7NmzRATE4Pjx4+bpb9y5QpkMhni4uLKsAdAQkICVqxYUaZlLG2rLGVRWhcvXkRsbCyuXLliNu9B6q0i/P7771CpVDh27Jg0rXPnzggNDS3V8jKZDLGxsdLv4va1vIQQ+PDDDxEeHg5vb2/4+fkhIiICu3btMkn3yy+/wNXVFadPn66wbZODEkRObOPGjQKA2Lhxozh27JjJkJqaKqX7+++/RXh4uFAqlSImJkbs3LlTHD58WGzZskU8//zzQqFQiDNnzgghhJg6dapo1aqVWLZsmThw4ID46quvRI8ePQQAER8fX2x+/v77byGTyURMTIzJ9Fu3bgmZTCY8PT3FjBkzTOZdu3ZNABBTpkwp077/9ttv4vTp02Vaxmj27NkCgPj7779LTOvp6SmGDRtW6nUDEM8++6w4duyYSE5OFnv27BFLly4VTZs2FQDEq6++apI+Pz9fHDt2TGRkZJRpH3r27Clq1apVpmUsbassZVFa//d//ycAiIMHD5rNe5B6qwj9+vUTPXv2NJkWEREhmjRpUqrljx07Jq5duyb9Lm5fy+vNN98UAMQrr7wi9u3bJ7766isRGRkpAIgvvvjCJO3w4cNFp06dKmzb5JgY7JBTMwY7J0+eLDZd9+7dhYuLizhw4IDF+SdOnJCCo7/++stsvk6nE02bNhV169YtMU9hYWGiYcOGJtO2b98ulEqlePXVV0Xr1q1N5n3yyScCgPj6669LXHdFsXawM27cOLPpOp1OvPTSSwKAWL16dVmya1FZgh2dTify8/MtznvYwY4tXbx4UQAQe/bsMZlelmDnftbY1xo1aoiOHTuaTMvLyxM+Pj6iT58+JtNPnTolAIjvv/++wrZPjoeXsajSS0lJwe7duzFy5Eg8+eSTFtM8/vjjqFmzJgCgevXqZvMVCgXCw8Nx7dq1ErfXpUsXXLp0CWlpadK0pKQkPP744+jRowdSUlJw584dk3kKhQJPPPEEgMIu/NWrV6N58+Zwd3dHtWrV8Oyzz+KPP/4w2Y6lyyH//PMPRo4cCV9fX1SpUgU9e/bEH3/8YXbpweivv/7CoEGD4OPjg4CAALz00kvIysqS5stkMuTk5CA+Pl66NNW5c+cSy8AShUKBVatWwd/fH++884403dKlpb///hujR49GcHAwVCoVHnnkEXTo0AH79+8HUHjZZdeuXUhNTTW5bHbv+pYsWYJ58+YhJCQEKpUKBw8eLPaS2bVr19C/f394e3vDx8cHL7zwAv7++2+TNEWVY+3atTF8+HAAhZdWBwwYAKCwLRjzZtympXrLz8/HzJkzERISIl1eHTduHP755x+z7fTq1Qt79uxBy5Yt4e7ujsceewwff/xxCaVfaM2aNVCr1YiMjLQ4/8iRI2jbti3c3d1Ro0YNvPnmm9Dr9UWWQUn7Wl5KpRI+Pj4m09zc3KThXuHh4WjUqBHWrl37QNskx8ZghyoFvV4PnU5nMhjt27cPANCvX79yr1+n0+HIkSNo0qRJiWm7dOkCoDCIMTp48CAiIiLQoUMHyGQyHDlyxGRey5YtpYN7TEwMJk2ahG7dumHnzp1YvXo1Lly4gPbt2+Ovv/4qcrsGgwG9e/dGQkICZsyYgR07dqBNmzZ46qmnilzmmWeeQYMGDfDFF1/gtddeQ0JCAiZPnizNP3bsGNzd3dGjRw8cO3YMx44dw+rVq0ssg6K4u7ujW7duuHz5Mq5fv15kuqFDh2Lnzp146623sG/fPnz00Ufo1q0bbt26BQBYvXo1OnToALVaLeXr3ntQAOD999/Hd999h6VLl2L37t147LHHis3b008/jXr16uHzzz9HbGwsdu7ciejoaGi12jLtY8+ePbFgwQIAwAcffCDlrWfPnhbTCyHQr18/LF26FEOHDsWuXbswZcoUxMfH48knn0RBQYFJ+p9++glTp07F5MmT8eWXX6Jp06YYOXIkDh8+XGLedu3ahU6dOkEuNz81pKen4/nnn8eQIUPw5Zdf4tlnn8W8efMwceLEcu+rwWAw+39pabg/oJo4cSL27NmDDRs2IDMzE2lpaZgyZQqysrLw6quvmuWjc+fO2L17N4QQJZYBOSkb9ywRWZXxMpalQavVCiGEeOWVVwQA8d///rfc25k1a5YAIHbu3Fli2tu3bwu5XC5Gjx4thBDi5s2bQiaTSZcOWrduLaZNmyaEEOLq1asCgJg+fboQovB+CADi3XffNVnntWvXhLu7u5ROCCGGDRtmchln165dAoBYs2aNybILFy4UAMTs2bOlacZLN0uWLDFJO3bsWOHm5iYMBoM0raIuYxnNmDFDABA//PCDEEKIy5cvS/ddGVWpUkVMmjSp2O0UdRnLuL66desKjUZjcd692zKWxeTJk03SfvrppwKA2Lx5s8m+3VuORrVq1TIpo+Iu7dxfb3v27LFYF9u2bRMAxPr160224+bmZnI/Wl5envD19TW7T+x+f/31lwAgFi1aZDYvIiJCABBffvmlyfRRo0YJuVxusr37y6C4fTWWbUmDpXpcu3atUKlUUhpfX1+RmJhocd8+/PBDAUD8/PPPxZYBOS/27FCl8Mknn+DkyZMmg4uLS4Ws+6OPPsL8+fMxdepU9O3bt8T0xqePjD07hw4dgkKhQIcOHQAAEREROHjwIABI/xp7g7755hvIZDK88MILJn/5qtVqk3VaYnyibODAgSbTBw0aVOQyffr0MfndtGlT5OfnIyMjo8T9LC9Rir++W7dujbi4OMybNw/Hjx8vc+8KULhvSqWy1OmHDBli8nvgwIFwcXGR6shajE/5GS+DGQ0YMACenp44cOCAyfTmzZtLl1yBwss7DRo0QGpqarHb+fPPPwFYvkwLAF5eXmbtYfDgwTAYDKXqNbJk9OjRZv8vLQ1ff/21yXIbN27ExIkTMX78eOzfvx/ffvstoqKi0LdvX4tPMxr36caNG+XKJzm+ijnaE9m5Ro0aoVWrVhbnGU8Mly9fRsOGDcu03o0bNyImJgajR482uc+kJF26dMGyZcvw559/4uDBgwgPD0eVKlUAFAY77777LrKysnDw4EG4uLigY8eOAArvoRFCICAgwOJ669SpU+Q2b926BRcXF/j6+ppML2pdAODn52fyW6VSAQDy8vJK3slyMp6Ug4KCikyzbds2zJs3Dx999BHefPNNVKlSBU8//TSWLFkCtVpdqu0EBgaWKV/3r9fFxQV+fn7SpTNrMdbbI488YjJdJpNBrVabbf/+OgMK662kOjPOv/+eFyNL7cRYJuUtA7VaXWRwdS/j/VYAkJmZiXHjxuHll1/G0qVLpendu3dH586d8corr+Dy5csmyxv3yZrtluwbe3ao0ouOjgaAMr0rBigMdF5++WUMGzYMa9euNTkgl+Te+3aSkpIQEREhzTMGNocPH5ZuXDYGQv7+/pDJZDh69KjFv4CL2wc/Pz/odDrcvn3bZHp6enqp821teXl52L9/P+rWrYtHH320yHT+/v5YsWIFrly5gtTUVCxcuBDbt2836/0oTlnqCzAvJ51Oh1u3bpkEFyqVyuweGqD8wQDwb73dfzO0EALp6enw9/cv97rvZVzP/e3DyNL9YMYysRRglcacOXOgVCpLHOrWrSstc+nSJeTl5eHxxx83W1+rVq1w5coV3L1712S6cZ8qqqzI8TDYoUqvZcuW6N69OzZs2FDkiwFPnTqFq1evSr/j4uLw8ssv44UXXsBHH31U5hNnp06doFAo8Pnnn+PChQsmTzD5+PigefPmiI+Px5UrV6TACAB69eoFIQRu3LiBVq1amQ1hYWFFbtMYUN3/QsOtW7eWKe/3K02vQWno9XqMHz8et27dwowZM0q9XM2aNTF+/HhERkaavDyuovJl9Omnn5r8/uyzz6DT6Uzqrnbt2jh79qxJuu+++87s5FuWHrKuXbsCADZv3mwy/YsvvkBOTo40/0HVqlUL7u7u+P333y3Ov3PnDr766iuTaQkJCZDL5ejUqVOR6y1uX8tzGcvY43f/CyiFEDh+/DiqVasGT09Pk3l//PEH5HJ5mXtuyXnwMhYRCu/peeqpp9C9e3e89NJL6N69O6pVq4a0tDR8/fXX2LJlC1JSUlCzZk383//9H0aOHInmzZsjJiYGJ06cMFlXixYtpAN8Uby9vdGyZUvs3LkTcrlcul/HKCIiQnr7773BTocOHTB69GiMGDECp06dQqdOneDp6Ym0tDQcPXoUYWFhGDNmjMVtPvXUU+jQoQOmTp2K7OxshIeH49ixY/jkk08AwOITOKURFhaGpKQkfP311wgMDISXl1eJJ5W//voLx48fhxACd+7cwfnz5/HJJ5/gp59+wuTJkzFq1Kgil83KykKXLl0wePBgPPbYY/Dy8sLJkyexZ88e9O/f3yRf27dvx5o1axAeHg65XF7kpczS2L59O1xcXBAZGYkLFy7gzTffRLNmzUzugRo6dCjefPNNvPXWW4iIiMDFixexatUqs8ekjW8jXr9+Pby8vODm5oaQkBCLPSSRkZGIjo7GjBkzkJ2djQ4dOuDs2bOYPXs2WrRogaFDh5Z7n+7l6uqKdu3aWXyLNVDYezNmzBhcvXoVDRo0wLfffosPP/wQY8aMMblH6H7F7WtQUFCxlystqVmzJvr374/169dDpVKhR48eKCgoQHx8PL7//nvMnTvX7I+P48ePo3nz5qhWrVqZtkVOxJZ3RxNZW2lfKihE4VMr77//vmjXrp3w9vYWLi4uIigoSPTv31/s2rVLSjds2LBinxy5fPlyqfI2ffp0AUC0atXKbN7OnTsFAOHq6ipycnLM5n/88ceiTZs2wtPTU7i7u4u6deuKF198UZw6dcokn/c/xXL79m0xYsQIUbVqVeHh4SEiIyPF8ePHBQDx3nvvSemKepGesTzv3cczZ86IDh06CA8PDwFAREREFLvf95aVXC4X3t7eIiwsTIwePVocO3bMLP39T0jl5+eLV155RTRt2lR4e3sLd3d30bBhQzF79myTsrp9+7Z49tlnRdWqVYVMJhPGw51xfe+8806J27q3LFJSUkTv3r1FlSpVhJeXlxg0aJDZCyYLCgrE9OnTRXBwsHB3dxcRERHizJkzZk9jCSHEihUrREhIiFAoFCbbtFRveXl5YsaMGaJWrVpCqVSKwMBAMWbMGJGZmWmSrlatWmZvPxai8GmqkupFCCE2bNggFAqF+PPPP82Wb9KkiUhKShKtWrUSKpVKBAYGitdff116qtEIFp5IK2pfyysvL0+88847omnTpsLLy0v4+vqKtm3bis2bN5s8KSiEEHfu3BEeHh5mTzBS5SITgi8eIKrMEhISMGTIEHz//fdo3769rbNDNpSfn4+aNWti6tSpZbqUaM82bNiAiRMn4tq1a+zZqcQY7BBVIlu2bMGNGzcQFhYGuVyO48eP45133kGLFi1MPnZKldeaNWsQGxuLP/74w+zeF0ej0+nQuHFjDBs2DLNmzbJ1dsiGeM8OUSXi5eWFrVu3Yt68ecjJyUFgYCCGDx+OefPm2TprZCdGjx6Nf/75B3/88UexN7w7gmvXruGFF17A1KlTbZ0VsjH27BAREZFT46PnRERE5NQY7BAREZFTs2mwU7t2bchkMrNh3LhxAApfEhUbG4ugoCC4u7ujc+fOuHDhgsk6CgoKMGHCBPj7+8PT0xN9+vQp9mvJREREVLnY9J6dv//+G3q9Xvp9/vx5REZG4uDBg+jcuTMWL16M+fPnIy4uDg0aNMC8efNw+PBhXLp0CV5eXgCAMWPG4Ouvv0ZcXBz8/PwwdepU3L59GykpKVAoFKXKh8FgwJ9//gkvL68yvwmXiIiIbEP878WkQUFBxb8Y1VYv+LFk4sSJom7dusJgMAiDwSDUarVYtGiRND8/P1/4+PiItWvXCiGE+Oeff4RSqRRbt26V0ty4cUPI5XKxZ8+eUm/32rVrxb4kjgMHDhw4cOBgv8O1a9eKPc/bzaPnGo0GmzdvxpQpUyCTyfDHH38gPT0dUVFRUhqVSoWIiAgkJycjJiYGKSkp0Gq1JmmCgoIQGhqK5ORk6QOPJTH2El27dg3e3t4Vsj+5Gh1azz8AADgxqys8XO2mqMtMq9Vi3759iIqKglKptHV2nA7L1/pYxtbF8rU+Ry1ja58Ls7OzERwcLJ3Hi2I3Z+CdO3fin3/+kb5abPyabkBAgEm6gIAApKamSmlcXV3N3ooZEBBQ7JecCwoKTL5MfOfOHQCAu7s73N3dH3hfAEAodJCrPP5drwMHOy4uLvDw8IC7u7tD/SdzFCxf62MZWxfL1/octYytfS7UarUAUOItKHZzBt6wYQO6d+9u9lG4+3dACFHiTpWUZuHChXj77bfNpu/btw8eHh5lyHXRCvSAsXj37t0HVeluH7JriYmJts6CU7NF+WoNwOZfC69zv1DfAKWTP5/pjG3YnurQGcvX3jhaGVv7XJibm1uqdHYR7KSmpmL//v3Yvn27NE2tVgMo7L0JDAyUpmdkZEi9PWq1GhqNBpmZmSa9OxkZGcV+42fmzJmYMmWK9NvYDRYVFVWhl7Gmn/gOABAdHeXwl7ESExMRGRnpUH9ROApblm+uRodpPxS20/iobg7dTovjzG3YHurQmcvXXjhqGVv7XJidnV2qdHZxZNu4cSOqV6+Onj17StNCQkKgVquRmJiIFi1aACi8r+fQoUNYvHgxACA8PBxKpRKJiYkYOHAgACAtLQ3nz5/HkiVLityeSqWCSqUym65UKiusEbnJ5Him5aOF4ypXKF0cv2unIsuHzNmifJXi3x7Qwu3bxSHBapyxDdtTHTpj+dobRytja58LS1sWNj+yGQwGbNy4EcOGDYOLy7/ZkclkmDRpEhYsWID69eujfv36WLBgATw8PDB48GAAgI+PD0aOHImpU6fCz88Pvr6+mDZtGsLCwtCtWzdb7RIAQOWiwLsDm9k0D0RE1qDX66V7JYDCXgcXFxfk5+ebvE6EKo4jl/H8Pg0BAEKnRb5OW0JqU0qlstSvkSmOzYOd/fv34+rVq3jppZfM5k2fPh15eXkYO3YsMjMz0aZNG+zbt8/kruvly5fDxcUFAwcORF5eHrp27Yq4uLgKKRwiIvqXEALp6en4559/zKar1Wpcu3aN7yqzkspcxlWrVoVarX6g/bZ5sBMVFQVRxHsNZTIZYmNjERsbW+Tybm5uWLlyJVauXGmlHJaPEAJ52sLo212pqHSNk4icjzHQqV69Ojw8PKTjmsFgwN27d1GlSpXiX+xG5eaoZSyEgOF/p3i5rOSnpu5fNjc3FxkZGQBgcv9uWdk82HFWeVo9Gr+1FwBwcU600974SUSVg16vlwIdPz8/k3kGgwEajQZubm4OdSJ2JI5axnqDwIU/swAATYJ8oJCX7Q9/4+tgMjIyUL169XJftXGcEiMiIpsx3qNTUa/nICotY5u79z6xsmKwQ0REpcZL8vSwVUSbY7BDRERETo3BDhERUSV169YtVK9eHVeuXHno2542bRpeffXVh7ItBjtEROS0hg8fjn79+pn8lslkWLRokUm6nTt3SpdLjGmKGwBAp9PhjTfeQEhICNzd3VGnTh3MmTMHBoPhoe3fg1q4cCF69+6N2rVrS9MmTpyI8PBwqFQqNG/e3GyZpKQk9O3bF4GBgfD09ETz5s3x6aefmqQxlqGLQo5mwdXQLLgaXBRyNGnSREozffp0bNy4EZcvX7bW7kkY7BARUaXi5uaGxYsXIzMz0+L89957D2lpadIAFL7p//5pixcvxtq1a7Fq1Sr8/PPPWLJkCd555x27exVKUfLy8rBhwwa8/PLLJtOFEHjppZfw3HPPWVwuOTkZTZs2xRdffIGzZ8/ipZdewosvvoivv/5aSmMsw+s3/sSBlP9i34nz8PX1xYABA6Q01atXR1RUFNauXWudHbwHgx0rkctk6BGmRo8wNeS8oY/sFNup42Mdll23bt2gVquxcOFCi/N9fHygVqulAfj3xXb3Tjt27Bj69u2Lnj17onbt2nj22WcRFRWFU6dOFbnt2NhYNG/eHB9//DFq1qyJKlWqYMyYMdDr9ViyZAnUajWqV6+O+fPnmyy3fPlytG/fHl5eXggODsbYsWNx9+5daf5LL72Epk2boqCgAEDhk0vh4eEYMmRIkXnZvXs3XFxc0K5dO5Pp77//PsaNG4c6depYXO7111/H3Llz0b59e9StWxevvvoqnnrqKezYscOsDAPVatSt9Sgu//ccMjMzMWLECJN19enTB1u2bCkyjxWFwY6VuCkVWD0kHKuHhMNNybc5k31iO3V8tq7DXI0OuRod8jR6adw45Gv1FtNaGkqbtiIoFAosWLAAK1euxPXr18u9no4dO+LAgQP45ZdfAAA//fQTjh49ih49ehS73O+//47du3djz5492LJlCz7++GP07NkT169fl77/+MYbb+D48ePSMnK5HIsXL8bZs2cRHx+P7777DtOnT5fmv//++8jJycFrr70GAHjzzTdx8+ZNrF69ush8HD58GK1atSr3/t8rKysLvr6+ZtPlchlq+Xni688+Rbdu3VCrVi2T+a1bt8a1a9eQmppaIfkoCt90R0RE5WZ8eaolXRo+go0jWku/w+ful94sf782Ib7YFvNvD0PHxQdxO0djlu7Kop5m08rj6aefRvPmzTF79mxs2LChXOuYMWMGsrKy8Nhjj0GhUECv12P+/PkYNGhQscsZDAZ8/PHH8PLyQuPGjdGlSxdcunQJ3377LeRyORo2bIjFixcjKSkJbdu2BVB4H012dja8vb1Rt25dzJ07F2PGjJGCmSpVqmDz5s2IiIiAl5cX3n33XRw4cAA+Pj5F5uPKlSsICgoq177f6/PPP8fJkyexbt06i/PT0tKwe/duJCQkmM2rUaOGlJf7A6GKxJ4dIqJSqv3aLltngSrQ4sWLER8fj4sXL5Zr+W3btmHz5s1ISEjA6dOnER8fj6VLlyI+Pr7Y5WrXrm3yjceAgAA0btzY5M3IAQEB0mcSAODgwYN4+umnERwcDC8vL7z44ou4desWcnJypDTt2rXDtGnTMHfuXEydOhWdOnUqNh95eXlwc3Mr626bSEpKwvDhw/Hhhx+a3Hx8r7i4OFStWtXkRnEj4xuSc3NzHygfJWHPjpXkanT8XATZPbZTx2frOrw4JxoGgwF3su/Ay9vL5IR9/z1EKW92K3I996c9OqNLxWbUgk6dOiE6Ohqvv/46hg8fXubl//Of/+C1117D888/DwAICwtDamoqFi5ciGHDhhW5nFKpNPktk8ksTjM+1ZWamopevXphxIgRmD9/Pvz9/XH06FGMHDnS5K3CBoMB33//PRQKBX799dcS8+/v71/kTdqlcejQIfTu3RvLli3Diy++aDGNTm/A2vUfoXu/gVC4KM3m3759GwDwyCOPlDsfpcEjGxERlZuHqwsMBgN0rgp4uLoU+92msgRiDytoW7RoEZo3b44GDRqUednc3Fyz/VUoFBX+6PmpU6eg0+kwb948VK1aFXK5HJ999plZunfeeQc///wzDh06hOjoaGzcuNHshuB7tWjRAps3by5XnpKSktCrVy8sXrwYo0ePLjLdoUOHcPXKH+j3/AsW558/fx5KpbLIXqGKwmCHqBJzVyqQ8kY3aZwcD+vwwYSFhWHIkCHlely8d+/emD9/PmrWrIkmTZrgxx9/xLJly/DSSy9VaB7r1q0LnU6H9evX49lnn8WxY8fMHtc+c+YM3nrrLXz++efo0KED3nvvPUycOBERERFFPlUVHR2NmTNnIjMzE9WqVZOm//bbb7h79y7S09ORl5eHM2fOAAAaN24MV1dXJCUloWfPnpg4cSKeeeYZpKenAwBcXV3NblLe+PHHCGvRCvUfa2wxD0eOHMETTzwhXc6yFt6zQ1SJyWQy+FVRwa+Kyu6+ecT7Y0rHnuvQUcydOxdCiDIvt3LlSjz77LMYO3YsGjVqhGnTpiEmJgZz586t0Pw1b94c7777Lt577z00bdoUn376qclj8/n5+RgyZAiGDx+O3r17AwBGjhyJbt26YejQodDrLd8UHhYWhlatWpn1Er388sto0aIF1q1bh19++QUtWrRAixYt8OeffwIovAcnNzcXCxcuRGBgoDT079/fZD1ZWVnYvv0LPF1Erw4AbNmyBaNGjSpXuZSFTJSnhp1MdnY2fHx8kJWVBW9v7wpZp62vo1ckrVaLb7/9Fj169DC7rkwPjuVrWe3XdlXYkzcVVcYVmSdHk5+fj8uXLyMkJMTsplaDwSA9KVTcZSwqP2uV8bfffotp06bh/PnzVqk7vUHgwp9ZAIAmQT5QyP8NyHft2oX//Oc/OHv2LFxcij5HFtf2Snv+ZqskqsQKdHq8ufM83tx5HgU6y3/9sYfFvpWmDomK0qNHD8TExODGjRsPfds5OTnYuHFjsYFORWGwQ1SJ6Q0Cm46nYtPxVOgN5e/kZUBkOxVVh1R5TZw4EcHBwQ99uwMHDkSbNm0eyrYc99qKnZPLZOjS8BFpnIiIqLKRAfByU0rjtsKeHStxUyqwcURrbBzRmq/hJ6pglb0nqbLvPzkOuVyGEH9PhPh7Qi63XbjDYIeIyo0n3aKxbIjsB4MdIiIicmoMdqwkV6NDozf3oNGbeyrsS71ERESORG8QOH8jC+dvZNn0BnreoGxFRX3dl4iIqLIw2MHr/NizQ0SVFu+rIaocGOwQEVUgBlBkDQqFArt2PXjb+u677/DYY49V+MdKy6OgoAA1a9ZESkqK1bfFYIeIiJzW8OHD0a9fP5PfMpkMixYtMkm3c+dO6dtixjTFDQCg0+nwxhtvICQkBO7u7qhTpw7mzJljlUDixo0b6Nat2wOvZ/r06Zg1a1axn4a4cOECnnnmGdSuXRsymQwrVqwwS7Nw4UI8/vjj8PLyQvXq1dGvXz9cunTJJM3du3fx6oTxiHy8CVrXC0Rok8ZYs2aNNF+lUmHatGmYMWPGA+9XSRjsEBFRpeLm5obFixcjMzPT4vz33nsPaWlp0gAAGzduNJu2ePFirF27FqtWrcLPP/+MJUuW4J133inXF9RLolaroVKpHmgdycnJ+PXXXzFgwIBi0+Xm5qJOnTpYtGgR1Gq1xTSHDh3CuHHjcPz4cSQmJkKn0yEqKgo5OTlSmsmTJ2Pv3r1Y8P467Dj4AyZOnIQJEybgyy+/lNIMGTIER44cwc8///xA+1YSBjtERFSpdOvWDWq12uTL4ffy8fGBWq2WBgCoWrWq2bRjx46hb9++6NmzJ2rXro1nn30WUVFROHXqVJHbjo2NRfPmzfHxxx+jZs2aqFKlCsaMGQO9Xo8lS5ZArVajevXqmD9/vsly917GunLlCmQyGbZv344uXbrAw8MDzZo1w7Fjx4rd761btyIqKsrsY5r3e/zxx/HOO+/g+eefLzLA2rNnD4YPH44mTZqgWbNm2LhxI65evWpySerYsWMY+uKLeLxdR9QIrolRo0ejWbNmJuXj5+eH9u3bY8uWLcXm6UEx2LESuUyGNiG+aBPiy89FkN1iO3V8tq7DXI0OuRod8jR6adw45N/3ROr988uTtiIoFAosWLAAK1euxPXr18u9no4dO+LAgQP45ZdfAAA//fQTjh49ih49ehS73O+//47du3djz5492LJlCz7++GP07NkT169fx6FDh7B48WK88cYbOH78eLHrmTVrFqZNm4YzZ86gQYMGGDRoEHS6osvo8OHDaNWqVdl3tBSysgq/bO7r6ytN69ixI775+mvcuZ0BD1cFkg4exC+//ILo6GiTZVu3bo0jR45YJV9GfPTcStyUCmyLaWfrbBAVyxHaae3XduHKop62zsZDU9b9tXUdNn5rb5HzujR8BBtHtJZ+h8/dX+QrOdqE+JrsR8fFB3E7R2OWrqLawtNPP43mzZtj9uzZ2LBhQ7nWMWPGDGRlZeGxxx6DQqGAXq/H/PnzMWjQoGKXMxgM+Pjjj+Hl5YXGjRujS5cuuHTpEr799lvI5XI0bNgQixcvRlJSEtq2bVvkeqZNm4aePQvL4+2330aTJk3w22+/4bHHHrOY/sqVKwgKCirXvhZHCIEpU6agY8eOCA0Nlaa///77GDVqFDo2awgXFxfI5XJ89NFH6Nixo8nyNWrUwJUrVyo8X/dizw4R0T34NFXlsXjxYsTHx+PixYvlWn7btm3YvHkzEhIScPr0acTHx2Pp0qWIj48vdrnatWvDy8tL+h0QEIDGjRub3DQcEBCAjIyMYtfTtGlTaTwwMBAAil0mLy/P5BLW1atXUaVKFWlYsGBBsdsryvjx43H27FmzS1Hvv/8+jh8/jq+++gopKSl49913MXbsWOzfv98knbu7O3Jzc8u17dJizw4REZXbxTnRMBgMuJN9B17eXiYn7Psvq6W8WfTTRPenPTqjS8Vm1IJOnTohOjoar7/+OoYPH17m5f/zn//gtddew/PPPw8ACAsLQ2pqKhYuXIhhw4YVuZxSqTT5LZPJLE4r6amue5cxPiFW3DL+/v4mN2UHBQXhzJkz0u97L0GV1oQJE/DVV1/h8OHDePTRR6XpeXl5eP3117Fjxw6p96lp06Y4c+YMli5davJk2e3bt/HII4+UedtlwWDHSnI1OnRcfBBA4X9aD1cWNdmfh9FOK9tlqIfN1scaD1cXGAwG6FwV8HB1KfaR5rLk7WHtx6JFi9C8eXM0aNCgzMvm5uaa7a9CobCLd9hY0qJFC5NeLBcXF9SrV69c6xJCYMKECdixYweSkpIQEhJiMl+r1UKr1UJAhot/ZgMAGqq9LJbP+fPn0aJFi3Llo7R4GcuKbudoLF5zJrInztpOK9PlKGetw4chLCwMQ4YMKdfj4r1798b8+fOxa9cuXLlyBTt27MCyZcvw9NNPWyGnDy46OhpHjx4tMZ1Go8GZM2dw5swZaDQa3LhxA2fOnMFvv/0mpRk3bpx0Cc/Lywvp6elIT09HXl4eAMDb2xsRERF4bcZ0HPv+MK5cuYz4uDh88sknZuVz5MgRREVFVezO3ofBDlEl5uaiwL7JnbBvcie4uSgqdN2VKdiwJWvWYWUxd+5ciHJ8v2nlypV49tlnMXbsWDRq1AjTpk1DTEwM5s6da4VcPrgXXngBFy9eNHv53/3+/PNPtGjRAi1atEBaWhqWLl2KFi1a4OWXX5bSrFmzBllZWejcuTMCAwOlYdu2bVKarVu3olWrxzFzwmj0f7ItlixZjPnz5+OVV16R0hw7dgxZWVl49tlnK36H78FrK0SVmFwuQ4MAr5ITkt1iHRYvLi6u2N8AUKtWLeTn5xe5jqICIS8vL6xYscLiG4aLEhsbi9jY2BLzlJSUZPJbr9cjO7vwclDt2rXN8lS1atUSA7Zq1aph/PjxWLZsGdatW1dkOkvrv19pgkO1Wo0NH3+MC38WPpbeJMgHCrnpvVnLli3Df/7zH7i7u5e4vgfBnh0icljsPSIqm1mzZqFWrVrQ6y2/AuBhKigoQLNmzTB58mSrb4vBDlElptEZsDzxFyxP/AUanX3eVOmMKjJIYx1SWfj4+OD111+HQmH7S54qlQpvvPGG1Xt1AF7GIqrUdAYD3jvwKwAgJqIOXPn3j8NhHRKVjP8rrEQuk6Hpoz5o+qgPX8NPdB9bX36y9fbLy1HzTZWXDIC7qwLurgrY8kzInh0rcVMq8NX4jiUnJCKHw3cHEZWOXC5D/eq2v4He5j07N27cwAsvvAA/Pz94eHigefPmJl9NFUIgNjYWQUFBcHd3R+fOnXHhwgWTdRQUFGDChAnw9/eHp6cn+vTp80AfdyMiIiLnYdNgJzMzEx06dIBSqcTu3btx8eJFvPvuu6hataqUZsmSJVi2bBlWrVqFkydPQq1WIzIyEnfu3JHSTJo0CTt27MDWrVtx9OhR3L17F7169bKLu82JiIx4GYrINmx6GWvx4sUIDg7Gxo0bpWm1a9eWxoUQWLFiBWbNmoX+/fsDAOLj4xEQEICEhATExMQgKysLGzZswKZNm6RvbWzevBnBwcHYv3+/2afkH5Y8jR7dlh0CAOyfEgF3V9vf+U5kb3g5iMi5GQwCv/xV2DnRIMALcrlt7tyxabDz1VdfITo6GgMGDMChQ4dQo0YNjB07FqNGjQIAXL58Genp6SavkVapVIiIiEBycjJiYmKQkpICrVZrkiYoKAihoaFITk62GOwUFBSgoKBA+m18UZPxWx4VQaPV4cY/ef8b18BF5ri3RxnLpKLKhkzZsny1Wp1JPrQy8xeFqRSiyLwZ51lKU955Fb28cd/u/bc06w6N3YvzsdEW5z2sfJdm3aWpw4qg1WohhIDBYDD7tpHxBXPG+VTxHLWMDQLQ6A3/GxeFE8q6DoMBQhS2+fsfmS/tcVMmyvOO7Api/NT8lClTMGDAAJw4cQKTJk3CunXr8OKLLyI5ORkdOnTAjRs3EBQUJC03evRopKamYu/evUhISMCIESNMghcAiIqKQkhIiMW3RMbGxuLtt982m56QkAAPD48K2bcCPTD9RGGAs6S1Dip27JAdYjt1fA+rDl1cXKBWqxEcHAxXV1frbIScjkEA13MKxx/1BMrTsaPRaHDt2jWkp6dDp9OZzMvNzcXgwYORlZUFb2/volcibEipVIp27dqZTJswYYJo27atEEKI77//XgAQf/75p0mal19+WURHRwshhPj000+Fq6ur2bq7desmYmJiLG43Pz9fZGVlScO1a9cEAHHz5k2h0WgqZPjnbq6oNeMbUWvGN+Kfu7kVtl5bDDk5OWLnzp0iJyfH5nlxxsGW5Vuadtrg9a+LXN44z1Ka8s6r6OUtlbGj5Ls0yz+sY012dra4cOGCyMnJEXq93mTQ6XQiMzNT6HQ6s3kcKmbQ6XRixowZomHDhsLDw0NUrVpVdO3aVSQnJ0tp/v77bzFu3DjRoEED4e7uLoKDg8X48ePF7du3S1z/qlWrRO3atYVKpRItW7YUSUlJZtt/6623RGBgoHBzcxMRERHi7NmzJa5Xq9OLn65lip+uZQqtrnz7npOTIy5cuCCys7PN2uXNmzcFAJGVlVVsvGHTayuBgYFo3LixybRGjRrhiy++AFD4XQ0ASE9PR2BgoJQmIyMDAQEBUhqNRoPMzExUq1bNJE379u0tblelUkGlUplNVyqVUCqVD7ZTxnWJf8PXwvU67mUso4osHzJni/ItTTst0MuKzJdxnqU05Z1X0cub7O//ytge8228f6ms635Yxxq9Xg+ZTAa5XA653PTZFuNlFeN8qngGgwF169bF+++/j3r16iEvLw/Lly/HU089hd9++w2PPPII0tPTpQ93Nm7cGKmpqXjllVeQlpaGzz//vMh1b9u2DZMnT8bq1avRoUMHrFu3Dj179sTFixdRs2ZNAIX32C5fvhxxcXFo0KAB5s2bh+joaFy6dAleXkU/Wi7uuWxV2D7K3rUjl8shk8ksHiNLe8y0aavs0KGD2ddXf/nlF9SqVQsAEBISArVajcTERGm+RqPBoUOHpEAmPDwcSqXSJE1aWhrOnz9fZLBDRESVQ+fOnTFhwgRMmjQJ1apVQ0BAANavX4+cnByMGDECXl5eqFu3Lnbv3i0to9frMXLkSISEhMDd3R0NGzbEe++9J83Pz89HkyZNMHr0aGna5cuX4ePjgw8//NBq+zJgwAB069YNderUQZMmTbBs2TJkZ2fj7NmzAIDQ0FB88cUX6N27N+rWrYsnn3wS8+fPx9dff212+edey5Ytw8iRI/Hyyy+jUaNGWLFiBYKDg7FmzRoA5g8LhYaGIj4+Hrm5uUhISLDa/lYkmwY7kydPxvHjx7FgwQL89ttvSEhIwPr16zFu3DgAhVHgpEmTsGDBAuzYsQPnz5/H8OHD4eHhgcGDBwMo/M7HyJEjMXXqVBw4cAA//vgjXnjhBYSFhUlPZxERkXXkanTI1eiQp9FL4yUNOv2/N9jq9AbkanTI1+otrvf+oTzi4+Ph7++PEydOYMKECRgzZgwGDBiA9u3b4/Tp04iOjsbQoUORm5sLoLAX5dFHH8Vnn32Gixcv4q233sLrr7+Ozz77DEDh/aaffvop4uPjsXPnTuj1egwdOhRdunSRHrCxpHv37qhSpUqxQ2lpNBqsX78ePj4+aNasWZHpjPeyuLhY7vHTaDRISUkxecgHKLzvNTk5GUDJDws5ApteW3n88cexY8cOzJw5E3PmzEFISAhWrFiBIUOGSGmmT5+OvLw8jB07FpmZmWjTpg327dtn0m22fPlyuLi4YODAgcjLy0PXrl0RFxdn0w+dySBD/epVpHEie8R26vhsXYeN39pb5mU+GNwSPZsW3pqw98JfGJdwGm1CfLEtpp2UpuPig7idozFbtjyvKmjWrBneeOMNAMDMmTOxaNEi+Pv7S4HJW2+9hTVr1uDs2bNo27YtlEqlyUMsISEhSE5OxmeffYaBAwcCAJo3b4558+Zh1KhRGDRoEH7//Xfs3Lmz2Hx89NFHyMvLK3P+7/XNN99g8ODByM3NRWBgIBITE+Hv728x7a1btzB37lzExMQUub6bN29Cr9dLt4YYBQQEID09HQCkfy2lSU1NLTa/MgBuLgpp3FZsfiNJr1690KtXryLny2QyxMbGIjY2tsg0bm5uWLlyJVauXGmFHJaPu6sCiVMibJ0NomKxnTo+1mHJmjZtKo0rFAr4+fkhLCxMmmY8iWdkZEjT1q5di48++gipqanIy8uDRqNB8+bNTdY7depUfPnll1i5ciV2795dZNBhVKNGjQfely5duuDMmTO4efMmPvzwQwwcOBA//PADqlevbpIuOzsbPXv2ROPGjTF79uwS1yu77xuOQgizaaVJcz+5XIYGatt/LsLmwQ4RETmui3OiYTAYcCf7Dry8vUp1g7Kr4t800U0CcHFOtNkHk4/O6FJhebz/Jlbjza73/gb+vdH6s88+w+TJk/Huu++iXbt28PLywjvvvIMffvjBZD0ZGRm4dOkSFAoFfv31Vzz11FPF5qN79+44cuRIsWnu3r1b7HxPT0/Uq1cP9erVQ9u2bVG/fn1s2LABM2fOlNLcuXMHTz31FKpUqYIdO3YUexOvv78/FAqF1Htz777d+yAQUPzDQvaOwQ4REZWbh6sLDAYDdK4KeLi6lPlpLBeFHC4K82U8XG13ejpy5Ajat2+PsWPHStN+//13s3QvvfQSQkNDMWrUKIwcORJdu3Y1e8L4XhVxGet+Qgizl+RGR0dDpVLhq6++kt5nVxRXV1eEh4cjMTERTz/9tDQ9MTERffv2BWD6sFCLFi0A/Puw0OLFiyt0f6yFwY6V5Gn06LPqKADgq/Ed+bkIskvO2E4r2yconLEOba1evXr45JNPsHfvXoSEhGDTpk04efIkQkJCpDQffPABjh07hrNnzyI4OBi7d+/GkCFD8MMPPxT50sUHuYyVk5ODOXPm4Nlnn0WNGjVw69YtrF69GtevX8eAAQMAFPboREVFITc3F5s3b0Z2drb0hYBHHnlEuo+1a9euePrppzF+/HgAhS/2HTp0KFq1aoV27dph/fr1uHr1Kl555RUApg8L1a9fH/Xr18eCBQtMHhYqisEg8FtGYW9VvepVKufnIpyZgMCv/6tgAZu9pJqoWGynjo91WPFeeeUVnDlzBs899xxkMhkGDRqEsWPHSo+n//e//8V//vMfbNiwAcHBwQAKg59mzZrhzTfftEpvh/FS2YABA3Dz5k34+fnh8ccfx5EjR9CkSRMAQEpKinSprV69eibLX758Wfr25O+//46bN29K85577jncunULc+bMQVpaGkJDQ/Htt99Kr4EBSvewkCUCQL5OL43bCoMdokpM5aLAllFtpXFyPKzD4iUlJZlNu3Llitk0cc+Xk1QqFTZu3GjykWoAWLhwIQDgsccekx5TN/L29sbly5cfPMNFcHNzw6ZNm+Dt7V3kpcLOnTub7EdRLO3/2LFjTS7b3a80DwvZMwY7RJWYQi5Du7p+ts4GPQDWIVHJ+F5vIiIicmrs2SGqxLR6A7acuAoAGNS6JpQWnooh+8Y6JCoZgx2iSkyrN+CtLy8AAJ4Nf5QnSgfEOiQqGYMdK5FBhhpV3aVxIiKiykaGf18iWak/F+Gs3F0V+P61J22dDSIiIpuRy2V4LNDb1tngDcpERETk3BjsEBERkVPjZSwrydfqMXDdMQDAZzHt4Kbky76IiKhyMRgEfr9Z+Ibvuv62+1wEe3asxCAEzl7PwtnrWTCU4o2WRERkH5KSkiCTyfDPP//YOisOT6Dw+215Gr1NPxfBYIeIiOge7du3R1paGnx8fGydFRO3b99G9+7dERQUBJVKheDgYIwfP1762CdQGKj17dsXgYGB8PT0RPPmzfHpp5+WuO7MzEwMHToUPj4+8PHxwdChQ82CvatXr6J3797w9PSEv78/Xn31VWg0moreTatgsENERHQPV1dXqNVqyGT29doQuVyOPn364KuvvsIvv/yCuLg47N+/X/o6OQAkJyejadOm+OKLL3D27Fm89NJLePHFF/H1118Xu+7BgwfjzJkz2LNnD/bs2YMzZ85g6NCh0ny9Xo+ePXsiJycHR48exdatW/HFF19g6tSpVtvfisRgh4iInFbnzp0xYcIETJo0CdWqVUNAQADWr1+PnJwcjBgxAl5eXqhbt670RXPA/DJWXFwcqlatir1796JRo0aoUqUKnnrqKaSlpT3UfalatSrGjBmDVq1aoVatWujatSvGjh2LI0eOSGlef/11zJ07F+3bt0fdunXx6quv4qmnnsKOHTuKXO/PP/+MPXv24KOPPkK7du3Qrl07fPjhh/jmm29w6dIlAMC+fftw8eJFbN68GS1atEC3bt3w7rvv4sMPPzTpWbJXDHaIiKjccjU65Gp0yNPopfGSBp3eIC2v0xuQq9EhX6u3uN77h/KIj4+Hv78/Tpw4gQkTJmDMmDEYMGAA2rdvj9OnTyM6OhpDhw41+5K5SX5yc7F06VJs2rQJhw8fxtWrVzFt2rRit1ulSpVih+7du5drf4z+/PNPbN++HREREcWmy8rKgq+vb5Hzjx07Bh8fH7Rp00aa1rZtW/j4+CA5OVlKExoaiqCgIClNdHQ0CgoKkJKS8kD78TDwaSwiIiq3xm/tLfMyHwxuiZ5NAwEAey/8hXEJp9EmxBfbYtpJaTouPojbOeb3g1xZ1LPM22vWrBneeOMNAMDMmTOxaNEi+Pv7Y9SoUQCAt956C2vWrMHZs2fRtm1bi+vQarVYu3Yt6tatCwAYP3485syZU+x2z5w5U+x8d3f3Mu5JoUGDBuHLL79EXl4eevfujY8++qjItJ9//jlOnjyJdevWFZkmPT0d1atXN5tevXp1pKenS2kCAgJM5lerVg2urq5SGnvGYMeKfD1dbZ0FohKxnTo+1mHxmjZtKo0rFAr4+fkhLCxMmmY8iWdkZBS5Dg8PDynQAYDAwMBi0wNAvXr1yptldO/eXbo8VatWLZw7d06at3z5csyePRuXLl3C66+/jilTpmD16tVm60hKSsLw4cPx4YcfokmTJsVuz9L9SUIIk+mlSWOJi9z2F5EY7FiJh6sLTr8ZaetsEBWL7dTx2boOL86JhsFgwJ3sO/Dy9oK8FCc213s+VhrdJAAX50RDft8J8+iMLhWWR6VSafJbJpOZTDOerA0GA4piaR2ihNeKVKlSpdj5TzzxhMm9Qvf66KOPkJeXZ3HbarUaarUajz32GPz8/PDEE0/gzTffRGBgoJTm0KFD6N27N5YtW4YXX3yx2Hyo1Wr89ddfZtP//vtvKRBUq9X44YcfTOZnZmZCq9Wa9fjcSyGXoXGQ7T8XwWCHiIjKzcPVBQaDATpXBTxcXUoV7NzLRSGHi4UvtXu4Ov7p6UEuY9WoUcPkd1GBmDHgKigokKYlJSWhV69eWLx4MUaPHl1iPtu1a4esrCycOHECrVu3BgD88MMPyMrKQvv27aU08+fPR1pamhRU7du3DyqVCuHh4SVuw9YcvzURERHZoQe5jGXJvn37cOfOHbRp0wZVqlTBxYsXMX36dHTo0AG1a9cGUBjo9OzZExMnTsQzzzwj3U/j6uoq3aR84sQJvPjiizhw4ABq1KiBRo0a4amnnsKoUaOke3tGjx6NXr16oWHDhgCAqKgoNG7cGEOHDsU777yD27dvY9q0aRg1ahS8vW3fc1MS219Ic1L5Wj2eW3cMz607ZvaUAZG9YDt1fKzDysPd3R0bNmxAx44d0ahRI0yaNAm9evXCN998I6WJi4tDbm4uFi5ciMDAQGno37+/lCY3NxeXLl2CVquVpn366acICwtDVFQUoqKi0LRpU2zatEmar1AosGvXLri5uaFDhw4YOHAg+vXrh6VLlxabZ4NB4Pe/7+L3v+/CYLDdO5TZs2MlBiHww+Xb0jiRPWI7dXysw+IlJSWZTbty5YrZtHvvv+ncubPJ7+HDh2P48OEm6fv161fiPTsV7YknnkDPnj2LvVQYFxeHuLi4Ytdz//4BgK+vLzZv3lzscjVr1jQJrEpDAMgp0EnjtsJgh6gSc1XI8cHgltI4OR7WIVHJGOwQVWIuCrn0vhNyTKxDopLxzwAiIiJyauzZIarEdHoD9l4ofL9GdJMAi48Ak31jHRKVjMEOUSWm0RswLuE0gMKXw/FE6Xgedh0+7JtyiSqizfHIZkXuSgXclQpbZ4OI6IEZ3+Jb3McyiSyRy2Rmb8guC2Obu/9N0mXBnh0r8XB1wc9zn7J1NoiIKoRCoUDVqlWl70F5eHiYfGZBo9EgPz+/zG9QptJx5DKu56cCAGg1BdCWkPZeQgjk5uYiIyMDVatWhUJR/s4DBjtERFQqarUagPkHM4UQyMvLg7u7e4kfhaTyqcxlXLVqVantlReDHSIiKhWZTIbAwEBUr17d5O27Wq0Whw8fRqdOnR7oUgMVrbKWsVKpfKAeHSMGO1aSr9VjzOYUAMCaF8Lhxnt3iMhJKBQKkxOQQqGATqeDm5tbpToRP0yOWsb2ci5ksGMlBiFw8NLf0jgREVFlYy/nQse6y4mIiIiojBjsEBERkVNjsENEREROjcEOEREROTWbBjuxsbGQyWQmw73P0gshEBsbi6CgILi7u6Nz5864cOGCyToKCgowYcIE+Pv7w9PTE3369MH169cf9q4QERGRnbJ5z06TJk2QlpYmDefOnZPmLVmyBMuWLcOqVatw8uRJqNVqREZG4s6dO1KaSZMmYceOHdi6dSuOHj2Ku3fvolevXtDr9bbYHSIiIrIzNn/03MXFxeKbEYUQWLFiBWbNmoX+/fsDAOLj4xEQEICEhATExMQgKysLGzZswKZNm9CtWzcAwObNmxEcHIz9+/cjOjr6oe7LvTxcXXBlUU+bbZ+oNNhOHR/rkOyZvbRPmwc7v/76K4KCgqBSqdCmTRssWLAAderUweXLl5Geno6oqCgprUqlQkREBJKTkxETE4OUlBRotVqTNEFBQQgNDUVycnKRwU5BQQEKCgqk39nZ2QAK31B571tBqZCxTFg21mHv5atSiCLzZpxnKU1551lj3ff/6yj5Lsu6bcne27AzYBlbVtrykImK+HZ6Oe3evRu5ublo0KAB/vrrL8ybNw///e9/ceHCBVy6dAkdOnTAjRs3EBQUJC0zevRopKamYu/evUhISMCIESNMAhcAiIqKQkhICNatW2dxu7GxsXj77bfNpickJMDDw6Nid5KIiIisIjc3F4MHD0ZWVha8vb2LTGfTnp3u3btL42FhYWjXrh3q1q2L+Ph4tG3bFgDMPngmhCjxI2glpZk5cyamTJki/c7OzkZwcDCioqKKLayyKNDqMe2L8wCApc+EQuXAn4vQarVITExEZGSkQ72m3FHYsnxL005DY/fifKzlXlLjPEtpyjvPGuu+v4wdJd+lWbc9HGt4jLA+Ry1ja7dP45WZktj8Mta9PD09ERYWhl9//RX9+vUDAKSnpyMwMFBKk5GRgYCAAACFX+DVaDTIzMxEtWrVTNK0b9++yO2oVCqoVCqz6UqlssIakVbIsOfCXwCAZc81h1JpV0VdLhVZPmTOFuVbmnZaoJcVmS/jPEtpyjvPmus2lrGj5bu4NPZ0rOExwvocrYyt3T5LWxY2fxrrXgUFBfj5558RGBiIkJAQqNVqJCYmSvM1Gg0OHTokBTLh4eFQKpUmadLS0nD+/Pligx0iKqRUyDGnbxPM6dsESoVdHQ6olFiHRCWzaXfDtGnT0Lt3b9SsWRMZGRmYN28esrOzMWzYMMhkMkyaNAkLFixA/fr1Ub9+fSxYsAAeHh4YPHgwAMDHxwcjR47E1KlT4efnB19fX0ybNg1hYWHS01lEVDSlQo4X29W2dTboAbAOiUpm02Dn+vXrGDRoEG7evIlHHnkEbdu2xfHjx1GrVi0AwPTp05GXl4exY8ciMzMTbdq0wb59++Dl5SWtY/ny5XBxccHAgQORl5eHrl27Ii4uDgqF494jQ0RERBXHpsHO1q1bi50vk8kQGxuL2NjYItO4ublh5cqVWLlyZQXnjsj56Q0CJy7fBgC0DvGFQl78zf9kf1iHRCVz/LtmiajcCnR6DPrwOADg4pxoeLjykOBoWIdEJePdbEREROTU+CeAlbgrFbg4J1oaJyIiqmzs5VzIYMdKZDIZu5OJiKhSs5dzIS9jERERkVNjsGMlBTo9pn72E6Z+9hMKdHpbZ4eIiOihs5dzIYMdK9EbBL44fR1fnL4OvcFm31olIiKyGXs5FzLYISIiIqfGYIeIiIicGoMdIiIicmoMdoiIiMipMdghIiIip8Zgh4iIiJya7V9r6KTclQqkvNFNGieyR2ynjo91SPbMXtongx0rkclk8KuisnU2iIrFdur4WIdkz+ylffIyFhERETk19uxYSYFOj3nf/AwAeKNXI6hc2L1M9oft1PGxDsme2Uv7ZM+OlegNApuOp2LT8VR+LoLsFtup42Mdkj2zl/bJnh2iSsxFLsfErvWlcXI8rEOikjHYIarEXF3kmBzZwNbZoAfAOiQqGf8MICIiIqfGnh2iSsxgEPjt77sAgHqPVIFcLrNxjqisWIdEJWOwQ1SJ5ev0iFp+GABwcU40PFx5SHA0rEOikvEyFhERETk1/glgJW4uChyZ3kUaJyIiqmzs5VzIYMdK5HIZgn09bJ0NIiIim7GXcyEvYxEREZFTY8+OlWh0BizddwkAMC2qIVxdGFcSEVHlYi/nQp6BrURnMGD94T+w/vAf0BkMts4OERHRQ2cv50IGO0REROTUGOwQERGRU2OwQ0RERE6NwQ4RERE5NQY7RERE5NQY7BAREZFT43t2rMTNRYF9kztJ40T2iO3U8bEOyZ7ZS/tksGMlcrkMDQK8bJ0NomKxnTo+1iHZM3tpn7yMRURERE6NPTtWotEZ8MHB3wAA47rU4+ciyC6xnTo+1iHZM3tpnwx2rERnMOC9A78CAGIi6sCVnWhkh9hOHR/rkOyZvbRPBjtElZhCLsPQtrWkcXI8rEOikjHYIarEVC4KzO0Xauts0ANgHRKVzG76OxcuXAiZTIZJkyZJ04QQiI2NRVBQENzd3dG5c2dcuHDBZLmCggJMmDAB/v7+8PT0RJ8+fXD9+vWHnHsiIiKyV3YR7Jw8eRLr169H06ZNTaYvWbIEy5Ytw6pVq3Dy5Emo1WpERkbizp07UppJkyZhx44d2Lp1K44ePYq7d++iV69e0Ov1D3s3iByOEAK37hbg1t0CCCFsnR0qB9YhUclsHuzcvXsXQ4YMwYcffohq1apJ04UQWLFiBWbNmoX+/fsjNDQU8fHxyM3NRUJCAgAgKysLGzZswLvvvotu3bqhRYsW2Lx5M86dO4f9+/fbapeIHEaeVo/wefsRPm8/8rT8A8ERsQ6JSmbze3bGjRuHnj17olu3bpg3b540/fLly0hPT0dUVJQ0TaVSISIiAsnJyYiJiUFKSgq0Wq1JmqCgIISGhiI5ORnR0dEWt1lQUICCggLpd3Z2NgBAq9VCq9VWyH5ptbp7xrXQyhz3Ly5jmVRU2ZApW5ZvadqpSiGKzJtxnqU05Z1njXXf/6+j5Ls067aHYw2PEdbnqGVs7fZZ2vKQCRv2e27duhXz58/HyZMn4ebmhs6dO6N58+ZYsWIFkpOT0aFDB9y4cQNBQUHSMqNHj0Zqair27t2LhIQEjBgxwiRwAYCoqCiEhIRg3bp1FrcbGxuLt99+22x6QkICPDw8KmTfDAK4llM4HuwJ8CEJskcFemD6icK/eZa01kHFrw04HNYh2TNrnwtzc3MxePBgZGVlwdvbu8h0NuvZuXbtGiZOnIh9+/bBzc2tyHQymWnJCCHMpt2vpDQzZ87ElClTpN/Z2dkIDg5GVFRUsYVVWWm1WiQmJiIyMhJKpdLW2XE6tizfXI0O0098BwCIjo6Ch6v5ISE0di/Ox1ruJTXOs5SmvPOsse77y9hR8l2adZemDq2NxwjrYxlbZrwyUxKbBTspKSnIyMhAeHi4NE2v1+Pw4cNYtWoVLl26BABIT09HYGCglCYjIwMBAQEAALVaDY1Gg8zMTJP7fTIyMtC+ffsit61SqaBSqcymK5VKNqJisHysyxblqxT//lFQuH3zQ0KBXlZkvozzLKUp7zxrrttYxo6W7+LSlKYOHxYeI6yPZWyqtGVhsxuUu3btinPnzuHMmTPS0KpVKwwZMgRnzpxBnTp1oFarkZiYKC2j0Whw6NAhKZAJDw+HUqk0SZOWlobz588XG+w8DBqdAesO/Y51h36HRmewaV6IiIhswV7OhTb7E8DLywuhoaYvwvL09ISfn580fdKkSViwYAHq16+P+vXrY8GCBfDw8MDgwYMBAD4+Phg5ciSmTp0KPz8/+Pr6Ytq0aQgLC0O3bt0e+j7dS2cwYOHu/wIAhrarxVe4ExFRpWMv50KbP41VnOnTpyMvLw9jx45FZmYm2rRpg3379sHL69/PxS9fvhwuLi4YOHAg8vLy0LVrV8TFxUGh4F16REREZGfBTlJSkslvmUyG2NhYxMbGFrmMm5sbVq5ciZUrV1o3c0REROSQytWfVKdOHdy6dcts+j///IM6deo8cKaIiIiIKkq5gp0rV65Y/BxDQUEBbty48cCZIiIiIqooZbqM9dVXX0nje/fuhY+Pj/Rbr9fjwIEDqF27doVljoiIiOhBlSnY6devH4DCe2mGDRtmMk+pVKJ27dp49913KyxzRERERA+qTMGOwVD4jHxISAhOnjwJf39/q2TKGahcFNgyqq00TmSP2E4dH+uQ7Jm9tM9yPY11+fLlis6H01HIZWhX18/W2SAqFtup42Mdkj2zl/ZZ7kfPDxw4gAMHDiAjI0Pq8TH6+OOPHzhjRERERBWhXMHO22+/jTlz5qBVq1YIDAws8cOclZFWb8CWE1cBAINa14RSwTcok/1hO3V8rEOyZ/bSPssV7KxduxZxcXEYOnRoRefHaWj1Brz15QUAwLPhj/IARHaJ7dTxsQ7JntlL+yxXsKPRaGz+oU0ienBymQw9wtTSODke1iFRycoV7Lz88stISEjAm2++WdH5IaKHyE2pwOoh4bbOBj0A1iFRycoV7OTn52P9+vXYv38/mjZtCqVSaTJ/2bJlFZI5IiIiogdVrmDn7NmzaN68OQDg/PnzJvN4szIRERHZk3IFOwcPHqzofBCRDeRqdGj81l4AwMU50fBwLffbKMhGWIdEJeNt+0REROTUyvUnQJcuXYq9XPXdd9+VO0POwlUhx8fDW0njRERElY29nAvLFewY79cx0mq1OHPmDM6fP2/2gdDKykUhx5OPBdg6G0RERDZjL+fCcgU7y5cvtzg9NjYWd+/efaAMEREREVWkCu1TeuGFF/hdrP/R6g34v1PX8H+nrkGrN5S8ABERkZOxl3Nhhd62f+zYMbi5uVXkKh2WVm/Afz4/CwDo2TSQr3AnIqJKx17OheUKdvr372/yWwiBtLQ0nDp1im9VJiIiIrtSrmDHx8fH5LdcLkfDhg0xZ84cREVFVUjGiIiIiCpCuYKdjRs3VnQ+iIiIiKzige7ZSUlJwc8//wyZTIbGjRujRYsWFZUvIiIiogpRrmAnIyMDzz//PJKSklC1alUIIZCVlYUuXbpg69ateOSRRyo6n0RERETlUq7boidMmIDs7GxcuHABt2/fRmZmJs6fP4/s7Gy8+uqrFZ1HIiIionIrV8/Onj17sH//fjRq1Eia1rhxY3zwwQe8Qfl/XBVyfDC4pTROZI/YTh0f65Dsmb20z3IFOwaDAUql0my6UqmEwcAX6AGFr8ju2TTQ1tkgKhbbqeNjHZI9s5f2Wa4w68knn8TEiRPx559/StNu3LiByZMno2vXrhWWOSIiIqIHVa5gZ9WqVbhz5w5q166NunXrol69eggJCcGdO3ewcuXKis6jQ9LpDdh1Ng27zqZBx89FkJ1iO3V8rEOyZ/bSPst1GSs4OBinT59GYmIi/vvf/0IIgcaNG6Nbt24VnT+HpdEbMC7hNADg4pxouPBaOtkhtlPHxzoke2Yv7bNMwc53332H8ePH4/jx4/D29kZkZCQiIyMBAFlZWWjSpAnWrl2LJ554wiqZJaKKJZfJ0CbEVxonx8M6JCpZmYKdFStWYNSoUfD29jab5+Pjg5iYGCxbtozBDpGDcFMqsC2mna2zQQ+AdUhUsjL1J/3000946qmnipwfFRWFlJSUB84UERERUUUpU7Dz119/WXzk3MjFxQV///33A2eKiIiIqKKUKdipUaMGzp07V+T8s2fPIjDQ9s/TE1Hp5Gp0aDk3ES3nJiJXo7N1dqgcWIdEJStTsNOjRw+89dZbyM/PN5uXl5eH2bNno1evXhWWOSKyvts5GtzO0dg6G/QAWIdExSvTDcpvvPEGtm/fjgYNGmD8+PFo2LAhZDIZfv75Z3zwwQfQ6/WYNWuWtfLqUJQKOd55tqk0TkREVNnYy7mwTMFOQEAAkpOTMWbMGMycORNCCACATCZDdHQ0Vq9ejYCAAKtk1NEoFXIMaBVs62wQERHZjL2cC8v8UsFatWrh22+/RWZmJn777TcIIVC/fn1Uq1bNGvkjIiIieiDleoMyAFSrVg2PP/54RebFqej0Bhz+tfDJtE71H+FbTYmIqNKxl3OhTc/Aa9asQdOmTeHt7Q1vb2+0a9cOu3fvluYLIRAbG4ugoCC4u7ujc+fOuHDhgsk6CgoKMGHCBPj7+8PT0xN9+vTB9evXH/aumNHoDXgp7hReijsFDb9XQ0RElZC9nAttGuw8+uijWLRoEU6dOoVTp07hySefRN++faWAZsmSJVi2bBlWrVqFkydPQq1WIzIyEnfu3JHWMWnSJOzYsQNbt27F0aNHcffuXfTq1Qt6vd5Wu0VERER2xKbBTu/evdGjRw80aNAADRo0wPz581GlShUcP34cQgisWLECs2bNQv/+/REaGor4+Hjk5uYiISEBQOH3uDZs2IB3330X3bp1Q4sWLbB582acO3cO+/fvt+WuERERkZ0o9z07FU2v1+P//u//kJOTg3bt2uHy5ctIT09HVFSUlEalUiEiIgLJycmIiYlBSkoKtFqtSZqgoCCEhoYiOTkZ0dHRFrdVUFCAgoIC6Xd2djYAQKvVQqvVVsj+aLW6e8a10MpEhazXFoxlUlFlQ6ZsWb6laacqhSgyb8Z5ltKUd5411n3/v46S79Ks2x6ONTxGWJ+jlrG122dpy0MmjM+P28i5c+fQrl075Ofno0qVKkhISECPHj2QnJyMDh064MaNGwgKCpLSjx49Gqmpqdi7dy8SEhIwYsQIk8AFKPxGV0hICNatW2dxm7GxsXj77bfNpickJMDDw6NC9qtAD0w/URhLLmmtg0pRIaslqlBsp46PdUj2zNrtMzc3F4MHD0ZWVpbFj5Qb2bxnp2HDhjhz5gz++ecffPHFFxg2bBgOHTokzZfJZCbphRBm0+5XUpqZM2diypQp0u/s7GwEBwcjKiqq2MIqi1yNDtNPfAcAiI6OgoerzYu63LRaLRITExEZGVnst9GofGxZvqVpp6Gxe3E+1nIvqXGepTTlnWeNdd9fxo6S79Ks2x6ONTxGWJ+jlrG126fxykxJbH4GdnV1Rb169QAArVq1wsmTJ/Hee+9hxowZAID09HST721lZGRILy5Uq9XQaDTIzMw0ec9PRkYG2rdvX+Q2VSoVVCqV2XSlUllhjUgp/g22Ctdr86J+YBVZPmTOFuVbmnZaoJcVmS/jPEtpyjvPmus2lrGj5bu4NPZ0rOExwvocrYyt3T5LWxZ29/IXIQQKCgoQEhICtVqNxMREaZ5Go8GhQ4ekQCY8PBxKpdIkTVpaGs6fP19ssPMwKBVyzOnbBHP6NuHnIshuGdupcZwcD481ZM/spX3atLvh9ddfR/fu3REcHIw7d+5g69atSEpKwp49eyCTyTBp0iQsWLAA9evXR/369bFgwQJ4eHhg8ODBAAAfHx+MHDkSU6dOhZ+fH3x9fTFt2jSEhYWhW7duttw1KBVyvNiutk3zQFQSYzt968sLPFE6KB5ryJ7ZS/u0abDz119/YejQoUhLS4OPjw+aNm2KPXv2IDIyEgAwffp05OXlYezYscjMzESbNm2wb98+eHl5SetYvnw5XFxcMHDgQOTl5aFr166Ii4uDQsG79IiIiMjGwc6GDRuKnS+TyRAbG4vY2Ngi07i5uWHlypVYuXJlBefuwegNAicu3wYAtA7xhUJe/E3VRLZwbzvVGwTbqQPisYbsmb20T/ZbW0mBTo9BHx7HoA+Po0DHtzmTfTK2U+M4OR4ea8ie2Uv7ZLBDVInJIEP96lWkcXI8xjqsX70K65CoCI7/PDQRlZu7qwKJUyJQ+7VdcHflfW6OyFiHRFQ09uwQERGRU2OwQ0RERE6NwQ5RJZan0SNy2SFpnByPsQ4jlx1iHRIVgffsEFViAgK/ZtyVxsnxsA6JSsZgx0pc5HLM7P6YNE5ERFTZ2Mu5kMGOlbi6yBETUdfW2SAiIrIZezkXssuBiIiInBp7dqxEbxA4fyMLABBaw4evcCciokrHXs6F7NmxkgKdHn0/+B59P/ier3AnIqJKyV7OhQx2iIiIyKkx2CEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfG9+xYiYtcjold60vjRPbI2E7fO/Ar26mD4rGG7Jm9tE8GO1bi6iLH5MgGts4GUbGM7fS9A7/C1YUnSkfEYw3ZM3tpnzy6ERERkVNjz46VGAwCv/19FwBQ75EqkPNzEWSH7m2nBoNgO3VAPNaQPbOX9smeHSvJ1+kRtfwwopYfRj4/F0F2ythOjePkeHisIXtmL+2TwQ5RJefr6WrrLNAD8vV0ZT0SFYOXsYgqMQ9XF5x+MxK1X9sFD1ceDhyRsQ6JqGjs2SEiIiKnxmCHiIiInBqDHaJKLF+rx3Prjknj5HiMdfjcumOsQ6Ii8CI9USVmEAI/XL4tjZPjYR0SlYzBjpW4yOUY3amONE5ERFTZ2Mu5kMGOlbi6yPF6j0a2zgYREZHN2Mu5kF0ORERE5NTYs2MlBoPAjX/yAAA1qrrzFe5ERFTp2Mu5kD07VpKv0+OJJQfxxJKDfIU7ERFVSvZyLmSwQ0RERE6NwQ4RERE5NQY7RERE5NQY7BAREZFTY7BDRERETo3BDhERETk1vmfHShRyGYa2rSWNE9kjYzvddDyV7dRB8VhD9sxe2ieDHStRuSgwt1+orbNBVCxjO910PBUqF4Wts0PlwGMN2TN7aZ82vYy1cOFCPP744/Dy8kL16tXRr18/XLp0ySSNEAKxsbEICgqCu7s7OnfujAsXLpikKSgowIQJE+Dv7w9PT0/06dMH169ff5i7QkRERHbKpsHOoUOHMG7cOBw/fhyJiYnQ6XSIiopCTk6OlGbJkiVYtmwZVq1ahZMnT0KtViMyMhJ37tyR0kyaNAk7duzA1q1bcfToUdy9exe9evWCXm+7tzUKIXDrbgFu3S2AEMJm+SAqjrGdGsfJ8fBYQ/bMXtqnTS9j7dmzx+T3xo0bUb16daSkpKBTp04QQmDFihWYNWsW+vfvDwCIj49HQEAAEhISEBMTg6ysLGzYsAGbNm1Ct27dAACbN29GcHAw9u/fj+jo6Ie+XwCQp9UjfN5+AMDFOdHwcOUVQ7I/97bTPK2e7dQB8VhD9sxe2qdd/a/IysoCAPj6+gIALl++jPT0dERFRUlpVCoVIiIikJycjJiYGKSkpECr1ZqkCQoKQmhoKJKTky0GOwUFBSgoKJB+Z2dnAwC0Wi20Wm2F7ItWq7tnXAutzHH/4jKWSUWVDZmyZfmWpp2qFKLIvBnnWUpT3nnWWPf9/zpKvkuzbns41vAYYX2OWsbWbp+lLQ+ZsJN+TyEE+vbti8zMTBw5cgQAkJycjA4dOuDGjRsICgqS0o4ePRqpqanYu3cvEhISMGLECJPgBQCioqIQEhKCdevWmW0rNjYWb7/9ttn0hIQEeHh4VMj+FOiB6ScKY8klrXVQ8d5PIiKqZKx9LszNzcXgwYORlZUFb2/vItPZTc/O+PHjcfbsWRw9etRsnkxm+riaEMJs2v2KSzNz5kxMmTJF+p2dnY3g4GBERUUVW1hlkavRYfqJ7wAA0dFRDt21rNVqkZiYiMjISCiVSltnx+nYQ/mGxu7F+VjLl3xLM89SmvLOs8a67y9jR8l3WdZtS/bQhp2do5axtc+FxiszJbGLM/CECRPw1Vdf4fDhw3j00Uel6Wq1GgCQnp6OwMBAaXpGRgYCAgKkNBqNBpmZmahWrZpJmvbt21vcnkqlgkqlMpuuVCorrBEpxb+BVuF67aKoH0hFlg+Zs2X5FuhlRW67NPMspSnvPGuu21jGjpbvsqSxJR4jrM/Rytja58LSloVNn8YSQmD8+PHYvn07vvvuO4SEhJjMDwkJgVqtRmJiojRNo9Hg0KFDUiATHh4OpVJpkiYtLQ3nz58vMtghokL5Wj3GfpoijZPjMdbh2E9TWIdERbBpd8O4ceOQkJCAL7/8El5eXkhPTwcA+Pj4wN3dHTKZDJMmTcKCBQtQv3591K9fHwsWLICHhwcGDx4spR05ciSmTp0KPz8/+Pr6Ytq0aQgLC5OeziIiywxC4Ntz6dI4OZ5763DpANYhkSU2DXbWrFkDAOjcubPJ9I0bN2L48OEAgOnTpyMvLw9jx45FZmYm2rRpg3379sHLy0tKv3z5cri4uGDgwIHIy8tD165dERcXB4XCdncFK+QyPNPyUWmciIiosrGXc6FNg53SPAgmk8kQGxuL2NjYItO4ublh5cqVWLlyZQXm7sGoXBR4d2AzW2eDiIjIZuzlXMivnhMREZFTc/xHhOyUEAJ5/7tZ0F2pKPFReSIiImdjL+dC9uxYSZ5Wj8Zv7UXjt/ZKFU1ERFSZ2Mu5kMEOEREROTUGO0REROTUGOwQERGRU2OwQ0RERE6NwQ4RERE5NQY7RERE5NT4nh0rkctk6BGmlsaJ7JGxnX57Lp3t1EHxWEP2zF7aJ4MdK3FTKrB6SLits0FULGM7rf3aLrgpbfctOSo/HmvIntlL++RlLCIiInJqDHaIiIjIqTHYsZJcjQ61X9uF2q/tQq5GZ+vsEFlkbKfGcXI8PNaQPbOX9slgh4iIiJwagx2iSsxdqUDKG92kcXI8xjpMeaMb65CoCHwai6gSk8lk8KuiksbJ8dxbh0RkGXt2iIiIyKmxZ4eoEivQ6THvm5+lcZULL4M4mnvr8I1ejViHRBawZ4eoEtMbBDYdT5XGyfEY63DT8VTWIVER2LNjJXKZDF0aPiKNExERVTb2ci5ksGMlbkoFNo5obetsEBER2Yy9nAt5GYuIiIicGoMdIiIicmoMdqwkV6NDozf3oNGbe/gKdyIiqpTs5VzIe3asKE+rt3UWiIiIbMoezoXs2SEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfGp7GsRC6ToU2IrzROZI+M7fSHy7fZTh0UjzVkz+ylfTLYsRI3pQLbYtrZOhtExTK209qv7YKbkl/LdkQ81pA9s5f2yctYRERE5NQY7BAREZFTY7BjJbkaHVrOTUTLuYn8XATZLWM7NY6T4+GxhuyZvbRP3rNjRbdzNLbOAlGJ2E4dH+uQ7Jk9tE/27BBVYm4uCuyb3EkaJ8djrMN9kzuxDomKwJ4dokpMLpehQYCXNE6O5946JCLL2LNDRERETo09O0SVmEZnwAcHf5PGXV3494+jubcOx3WpxzokssCm/ysOHz6M3r17IygoCDKZDDt37jSZL4RAbGwsgoKC4O7ujs6dO+PChQsmaQoKCjBhwgT4+/vD09MTffr0wfXr1x/iXhA5Lp3BgPcO/CqNk+Mx1uF7B35lHRIVwabBTk5ODpo1a4ZVq1ZZnL9kyRIsW7YMq1atwsmTJ6FWqxEZGYk7d+5IaSZNmoQdO3Zg69atOHr0KO7evYtevXpBr9c/rN2wSC6ToemjPmj6qA9f4U5ERJWSvZwLbXoZq3v37ujevbvFeUIIrFixArNmzUL//v0BAPHx8QgICEBCQgJiYmKQlZWFDRs2YNOmTejWrRsAYPPmzQgODsb+/fsRHR390Pblfm5KBb4a39Fm2yciIrI1ezkX2u09O5cvX0Z6ejqioqKkaSqVChEREUhOTkZMTAxSUlKg1WpN0gQFBSE0NBTJyclFBjsFBQUoKCiQfmdnZwMAtFottFqtlfbIcRnLhGVjHbYsX61Wd8+4FlqZMEujUogi82acZylNeedZY933/+so+S7NuktTh9bGY4T1sYwtK215yIQQD/9/hgUymQw7duxAv379AADJycno0KEDbty4gaCgICnd6NGjkZqair179yIhIQEjRowwCVwAICoqCiEhIVi3bp3FbcXGxuLtt982m56QkAAPD4+K2ykiO1egB6afKPybZ0lrHVR8TYvDYR1SZZabm4vBgwcjKysL3t7eRaaz254dI9l91/iEEGbT7ldSmpkzZ2LKlCnS7+zsbAQHByMqKqrYwiqLPI0e3Vd+DwDYPaED3F0d9wik1WqRmJiIyMhIKJVKW2fH6diyfHM1Okw/8R0AIDo6Ch6u5oeE0Ni9OB9ruZfUOM9SmvLOs8a67y9jR8l3adZdmjq0Nh4jrM9Ry9ja50LjlZmS2G2wo1arAQDp6ekIDAyUpmdkZCAgIEBKo9FokJmZiWrVqpmkad++fZHrVqlUUKlUZtOVSmWFNSKtkOHGP/kAABelC5RKuy3qUqvI8iFztihfpfj3j4LC7Zu30wK9rMh8GedZSlPeedZct7GMHS3fxaUpTR0+LDxGWJ+jlbG1z4WlLQu7fSFDSEgI1Go1EhMTpWkajQaHDh2SApnw8HAolUqTNGlpaTh//nyxwQ4RERFVHjbtbrh79y5+++036ffly5dx5swZ+Pr6ombNmpg0aRIWLFiA+vXro379+liwYAE8PDwwePBgAICPjw9GjhyJqVOnws/PD76+vpg2bRrCwsKkp7OIiIiocrNpsHPq1Cl06dJF+m28j2bYsGGIi4vD9OnTkZeXh7FjxyIzMxNt2rTBvn374OX173dgli9fDhcXFwwcOBB5eXno2rUr4uLioFA47j0yREREVHFsGux07twZxT0MJpPJEBsbi9jY2CLTuLm5YeXKlVi5cqUVckhERESOzm7v2SEiIiKqCI7/iJCdkkGG+tWrSONE9sjYTn/NuMt26qB4rCF7Zi/tk8GOlbi7KpA4JcLW2SAqlrGd1n5tl0O/C6oy47GG7Jm9tE9exiIiIiKnxmCHiIiInBqDHSvJ0+gRuewQIpcdQp5Gb+vsEFlkbKfGcXI8PNaQPbOX9sl7dqxEQODXjLvSOJE9Yjt1fKxDsmf20j7Zs0NUialcFNgyqq00To7HWIdbRrVlHRIVgT07RJWYQi5Du7p+0jg5nnvrkIgsY88OEREROTX27BBVYlq9AVtOXJXGlQr+/eNo7q3DQa1rsg6JLGCwQ1SJafUGvPXlBWmcJ0rHc28dPhv+KOuQyAIGO1Yigww1qrpL40RERJWNvZwLGexYiburAt+/9qSts0FERGQz9nIuZH8nEREROTUGO0REROTUGOxYSb5Wjz6rjqLPqqPI1/IV7kREVPnYy7mQ9+xYiUEInL2eJY0TERFVNvZyLmTPDhERETk1BjtERETk1BjsEBERkVNjsENEREROjcEOEREROTU+jWVFvp6uts4CUYl8PV1xO0dj62zQA+CxhuyZPbRPBjtW4uHqgtNvRto6G0TFMrbT2q/tgocrDweOiMcasmf20j55GYuIiIicGoMdIiIicmoMdqwkX6vHc+uO4bl1x/i5CLJbxnZqHCfHw2MN2TN7aZ+8SG8lBiHww+Xb0jiRPWI7dXysQ7Jn9tI+2bNDVIm5KuT4YHBLaZwcj7EOPxjcknVIVAT+zyCqxFwUcvRsGiiNk+Mx1mHPpoGsQ6Ii8H8GEREROTXes0NUien0Buy98Jc0zp4Bx3NvHUY3CWAdElnAYIeoEtPoDRiXcFoa54nS8dxbhxfnRLMOiSxgsGNF7kqFrbNARERkU/ZwLmSwYyUeri74ee5Tts4GERGRzdjLuZD9nUREROTUGOwQERGRU2OwYyX5Wj1GbDyBERtP8BXuRERUKdnLuZD37FiJQQgcvPS3NE5ERFTZ2Mu5kD07RERE5NScJthZvXo1QkJC4ObmhvDwcBw5csTWWSIiIiI74BTBzrZt2zBp0iTMmjULP/74I5544gl0794dV69etXXWiIiIyMacIthZtmwZRo4ciZdffhmNGjXCihUrEBwcjDVr1tg6a0RERGRjDh/saDQapKSkICoqymR6VFQUkpOTbZQrIiIishcO/zTWzZs3odfrERAQYDI9ICAA6enpFpcpKChAQUGB9DsrKwsAcPv2bWi12grJV65GB0NBLgDg1q1byHN13KLWarXIzc3FrVu3oFQqbZ0dp2PL8i1NO3XR5eDWrVsWlzfOs5SmvPOsse77y9hR8l2addvDsYbHCOtz1DK2dvu8c+cOAECU9KSXcHA3btwQAERycrLJ9Hnz5omGDRtaXGb27NkCAAcOHDhw4MDBCYZr164VGys4bnfD//j7+0OhUJj14mRkZJj19hjNnDkTU6ZMkX4bDAbcvn0bfn5+kMlkVs2vI8rOzkZwcDCuXbsGb29vW2fH6bB8rY9lbF0sX+tjGVsmhMCdO3cQFBRUbDqHD3ZcXV0RHh6OxMREPP3009L0xMRE9O3b1+IyKpUKKpXKZFrVqlWtmU2n4O3tzf9kVsTytT6WsXWxfK2PZWzOx8enxDQOH+wAwJQpUzB06FC0atUK7dq1w/r163H16lW88sorts4aERER2ZhTBDvPPfccbt26hTlz5iAtLQ2hoaH49ttvUatWLVtnjYiIiGzMKYIdABg7dizGjh1r62w4JZVKhdmzZ5td+qOKwfK1PpaxdbF8rY9l/GBkQvArlUREROS8HP6lgkRERETFYbBDRERETo3BDhERETk1BjtERETk1BjskGT+/Plo3749PDw8inzJ4tWrV9G7d294enrC398fr776KjQajUmac+fOISIiAu7u7qhRowbmzJlT8ndLKqnatWtDJpOZDK+99ppJmtKUORVt9erVCAkJgZubG8LDw3HkyBFbZ8khxcbGmrVVtVotzRdCIDY2FkFBQXB3d0fnzp1x4cIFG+bY/h0+fBi9e/dGUFAQZDIZdu7caTK/NGVaUFCACRMmwN/fH56enujTpw+uX7/+EPfCMTDYIYlGo8GAAQMwZswYi/P1ej169uyJnJwcHD16FFu3bsUXX3yBqVOnSmmys7MRGRmJoKAgnDx5EitXrsTSpUuxbNmyh7UbDsf4fijj8MYbb0jzSlPmVLRt27Zh0qRJmDVrFn788Uc88cQT6N69O65evWrrrDmkJk2amLTVc+fOSfOWLFmCZcuWYdWqVTh58iTUajUiIyOlDzWSuZycHDRr1gyrVq2yOL80ZTpp0iTs2LEDW7duxdGjR3H37l306tULer3+Ye2GY6iAb3GSk9m4caPw8fExm/7tt98KuVwubty4IU3bsmWLUKlUIisrSwghxOrVq4WPj4/Iz8+X0ixcuFAEBQUJg8Fg9bw7mlq1aonly5cXOb80ZU5Fa926tXjllVdMpj322GPitddes1GOHNfs2bNFs2bNLM4zGAxCrVaLRYsWSdPy8/OFj4+PWLt27UPKoWMDIHbs2CH9Lk2Z/vPPP0KpVIqtW7dKaW7cuCHkcrnYs2fPQ8u7I2DPDpXasWPHEBoaavLBtejoaBQUFCAlJUVKExERYfLiq+joaPz555+4cuXKw86yQ1i8eDH8/PzQvHlzzJ8/3+QSVWnKnCzTaDRISUlBVFSUyfSoqCgkJyfbKFeO7ddff0VQUBBCQkLw/PPP448//gAAXL58Genp6SZlrVKpEBERwbIup9KUaUpKCrRarUmaoKAghIaGstzv4zRvUCbrS09PN/uSfLVq1eDq6ip9dT49PR21a9c2SWNcJj09HSEhIQ8lr45i4sSJaNmyJapVq4YTJ05g5syZuHz5Mj766CMApStzsuzmzZvQ6/Vm5RcQEMCyK4c2bdrgk08+QYMGDfDXX39h3rx5aN++PS5cuCCVp6WyTk1NtUV2HV5pyjQ9PR2urq6oVq2aWRq2cVPs2XFylm4qvH84depUqdcnk8nMpgkhTKbfn0b87+ZkS8s6o7KU+eTJkxEREYGmTZvi5Zdfxtq1a7FhwwbcunVLWl9pypyKZqk9suzKrnv37njmmWcQFhaGbt26YdeuXQCA+Ph4KQ3LuuKVp0xZ7ubYs+Pkxo8fj+eff77YNPf3xBRFrVbjhx9+MJmWmZkJrVYr/fWhVqvN/qLIyMgAYP4XirN6kDJv27YtAOC3336Dn59fqcqcLPP394dCobDYHll2D87T0xNhYWH49ddf0a9fPwCFPQ2BgYFSGpZ1+RmfdCuuTNVqNTQaDTIzM016dzIyMtC+ffuHm2E7x54dJ+fv74/HHnus2MHNza1U62rXrh3Onz+PtLQ0adq+ffugUqkQHh4upTl8+LDJfSf79u1DUFBQqYMqR/cgZf7jjz8CgHRwK02Zk2Wurq4IDw9HYmKiyfTExESeCCpAQUEBfv75ZwQGBiIkJARqtdqkrDUaDQ4dOsSyLqfSlGl4eDiUSqVJmrS0NJw/f57lfj8b3hxNdiY1NVX8+OOP4u233xZVqlQRP/74o/jxxx/FnTt3hBBC6HQ6ERoaKrp27SpOnz4t9u/fLx599FExfvx4aR3//POPCAgIEIMGDRLnzp0T27dvF97e3mLp0qW22i27lZycLJYtWyZ+/PFH8ccff4ht27aJoKAg0adPHylNacqcirZ161ahVCrFhg0bxMWLF8WkSZOEp6enuHLliq2z5nCmTp0qkpKSxB9//CGOHz8uevXqJby8vKSyXLRokfDx8RHbt28X586dE4MGDRKBgYEiOzvbxjm3X3fu3JGOswCk40FqaqoQonRl+sorr4hHH31U7N+/X5w+fVo8+eSTolmzZkKn09lqt+wSgx2SDBs2TAAwGw4ePCilSU1NFT179hTu7u7C19dXjB8/3uQxcyGEOHv2rHjiiSeESqUSarVaxMbG8rFzC1JSUkSbNm2Ej4+PcHNzEw0bNhSzZ88WOTk5JulKU+ZUtA8++EDUqlVLuLq6ipYtW4pDhw7ZOksO6bnnnhOBgYFCqVSKoKAg0b9/f3HhwgVpvsFgELNnzxZqtVqoVCrRqVMnce7cORvm2P4dPHjQ4jF32LBhQojSlWleXp4YP3688PX1Fe7u7qJXr17i6tWrNtgb+yYTgq+2JSIiIufFe3aIiIjIqTHYISIiIqfGYIeIiIicGoMdIiIicmoMdoiIiMipMdghIiIip8Zgh4iIiJwagx0isgtxcXGoWrVqmZYZPny49F0mW7ty5QpkMhnOnDlj66wQ0X0Y7BBRmaxduxZeXl7Q6XTStLt370KpVOKJJ54wSXvkyBHIZDL88ssvJa73ueeeK1W6sqpduzZWrFhR4eslIsfBYIeIyqRLly64e/cuTp06JU07cuQI1Go1Tp48idzcXGl6UlISgoKC0KBBgxLX6+7ujurVq1slz0RUuTHYIaIyadiwIYKCgpCUlCRNS0pKQt++fVG3bl0kJyebTO/SpQuAwi82T58+HTVq1ICnpyfatGljsg5Ll7HmzZuH6tWrw8vLCy+//DJee+01NG/e3CxPS5cuRWBgIPz8/DBu3DhotVoAQOfOnZGamorJkydDJpNBJpNZ3KdBgwbh+eefN5mm1Wrh7++PjRs3AgD27NmDjh07omrVqvDz80OvXr3w+++/F1lOlvZn586dZnn4+uuvER4eDjc3N9SpUwdvv/22Sa8ZET04BjtEVGadO3fGwYMHpd8HDx5E586dERERIU3XaDQ4duyYFOyMGDEC33//PbZu3YqzZ89iwIABeOqpp/Drr79a3Mann36K+fPnY/HixUhJSUHNmjWxZs0as3QHDx7E77//joMHDyI+Ph5xcXGIi4sDAGzfvh2PPvoo5syZg7S0NKSlpVnc1pAhQ/DVV1/h7t270rS9e/ciJycHzzzzDAAgJycHU6ZMwcmTJ3HgwAHI5XI8/fTTMBgMZS/Ae7bxwgsv4NVXX8XFixexbt06xMXFYf78+eVeJxFZYOsvkRKR41m/fr3w9PQUWq1WZGdnCxcXF/HXX3+JrVu3ivbt2wshhDh06JAAIH7//Xfx22+/CZlMJm7cuGGynq5du4qZM2cKIYTYuHGj8PHxkea1adNGjBs3ziR9hw4dRLNmzaTfw4YNE7Vq1RI6nU6aNmDAAPHcc89Jv2vVqiWWL19e7P5oNBrh7+8vPvnkE2naoEGDxIABA4pcJiMjQwCQvkJ9+fJlAUD8+OOPFvdHCCF27Ngh7j3sPvHEE2LBggUmaTZt2iQCAwOLzS8RlQ17doiozLp06YKcnBycPHkSR44cQYMGDVC9enVERETg5MmTyMnJQVJSEmrWrIk6derg9OnTEEKgQYMGqFKlijQcOnSoyEtBly5dQuvWrU2m3f8bAJo0aQKFQiH9DgwMREZGRpn2R6lUYsCAAfj0008BFPbifPnllxgyZIiU5vfff8fgwYNRp04deHt7IyQkBABw9erVMm3rXikpKZgzZ45JmYwaNQppaWkm9z4R0YNxsXUGiMjx1KtXD48++igOHjyIzMxMREREAADUajVCQkLw/fff4+DBg3jyyScBAAaDAQqFAikpKSaBCQBUqVKlyO3cf3+LEMIsjVKpNFumPJeWhgwZgoiICGRkZCAxMRFubm7o3r27NL93794IDg7Ghx9+iKCgIBgMBoSGhkKj0Vhcn1wuN8uv8V4iI4PBgLfffhv9+/c3W97Nza3M+0BEljHYIaJy6dKlC5KSkpCZmYn//Oc/0vSIiAjs3bsXx48fx4gRIwAALVq0gF6vR0ZGhtnj6UVp2LAhTpw4gaFDh0rT7n0CrLRcXV2h1+tLTNe+fXsEBwdj27Zt2L17NwYMGABXV1cAwK1bt/Dzzz9j3bp1Uv6PHj1a7PoeeeQR3LlzBzk5OfD09AQAs3fwtGzZEpcuXUK9evXKvF9EVHoMdoioXLp06SI9+WTs2QEKg50xY8YgPz9fujm5QYMGGDJkCF588UW8++67aNGiBW7evInvvvsOYWFh6NGjh9n6J0yYgFGjRqFVq1Zo3749tm3bhrNnz6JOnTplymft2rVx+PBhPP/881CpVPD397eYTiaTYfDgwVi7di1++eUXkxuwq1WrBj8/P6xfvx6BgYG4evUqXnvttWK326ZNG3h4eOD111/HhAkTcOLECenGaaO33noLvXr1QnBwMAYMGAC5XI6zZ8/i3LlzmDdvXpn2k4iKxnt2iKhcunTpgry8PNSrVw8BAQHS9IiICNy5cwd169ZFcHCwNH3jxo148cUXMXXqVDRs2BB9+vTBDz/8YJLmXkOGDMHMmTMxbdo0tGzZEpcvX8bw4cPLfHlnzpw5uHLlCurWrYtHHnmk2LRDhgzBxYsXUaNGDXTo0EGaLpfLsXXrVqSkpCA0NBSTJ0/GO++8U+y6fH19sXnzZnz77bcICwvDli1bEBsba5ImOjoa33zzDRITE/H444+jbdu2WLZsGWrVqlWmfSSi4smEpYvgRER2KDIyEmq1Gps2bbJ1VojIgfAyFhHZpdzcXKxduxbR0dFQKBTYsmUL9u/fj8TERFtnjYgcDHt2iMgu5eXloXfv3jh9+jQKCgrQsGFDvPHGGxafXCIiKg6DHSIiInJqvEGZiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfGYIeIiIicGoMdIiIicmoMdoiIiMipMdghIiIip8Zgh4iIiJza/wNbSPyhnD903QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 16, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.25, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 4\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1450.0\n",
      "lif layer 1 self.abs_max_v: 1450.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "lif layer 2 self.abs_max_v: 654.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "layer   1  Sparsity: 87.4023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 94.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 1519.0\n",
      "lif layer 1 self.abs_max_v: 1519.0\n",
      "lif layer 2 self.abs_max_v: 691.5\n",
      "fc layer 3 self.abs_max_out: 55.0\n",
      "lif layer 1 self.abs_max_v: 1576.0\n",
      "lif layer 2 self.abs_max_v: 839.0\n",
      "fc layer 1 self.abs_max_out: 1742.0\n",
      "lif layer 1 self.abs_max_v: 1836.5\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 2 self.abs_max_out: 655.0\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "fc layer 1 self.abs_max_out: 2089.0\n",
      "lif layer 1 self.abs_max_v: 2041.5\n",
      "fc layer 2 self.abs_max_out: 669.0\n",
      "fc layer 2 self.abs_max_out: 740.0\n",
      "lif layer 2 self.abs_max_v: 857.5\n",
      "fc layer 2 self.abs_max_out: 748.0\n",
      "lif layer 1 self.abs_max_v: 2063.5\n",
      "lif layer 1 self.abs_max_v: 2308.5\n",
      "fc layer 1 self.abs_max_out: 2413.0\n",
      "lif layer 1 self.abs_max_v: 2413.0\n",
      "lif layer 1 self.abs_max_v: 2743.5\n",
      "fc layer 2 self.abs_max_out: 801.0\n",
      "lif layer 1 self.abs_max_v: 2868.0\n",
      "lif layer 2 self.abs_max_v: 860.0\n",
      "fc layer 1 self.abs_max_out: 2515.0\n",
      "lif layer 2 self.abs_max_v: 869.0\n",
      "fc layer 1 self.abs_max_out: 2645.0\n",
      "lif layer 2 self.abs_max_v: 929.0\n",
      "fc layer 2 self.abs_max_out: 825.0\n",
      "fc layer 1 self.abs_max_out: 2704.0\n",
      "lif layer 1 self.abs_max_v: 3048.5\n",
      "fc layer 2 self.abs_max_out: 838.0\n",
      "lif layer 2 self.abs_max_v: 1008.5\n",
      "fc layer 2 self.abs_max_out: 866.0\n",
      "fc layer 1 self.abs_max_out: 2800.0\n",
      "lif layer 1 self.abs_max_v: 3085.5\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "lif layer 1 self.abs_max_v: 3206.0\n",
      "fc layer 1 self.abs_max_out: 2806.0\n",
      "fc layer 1 self.abs_max_out: 2885.0\n",
      "fc layer 1 self.abs_max_out: 2973.0\n",
      "fc layer 1 self.abs_max_out: 3172.0\n",
      "lif layer 1 self.abs_max_v: 3325.5\n",
      "fc layer 1 self.abs_max_out: 3337.0\n",
      "lif layer 1 self.abs_max_v: 3337.0\n",
      "lif layer 1 self.abs_max_v: 3698.0\n",
      "fc layer 1 self.abs_max_out: 3460.0\n",
      "lif layer 1 self.abs_max_v: 3881.5\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "fc layer 2 self.abs_max_out: 944.0\n",
      "fc layer 2 self.abs_max_out: 980.0\n",
      "lif layer 2 self.abs_max_v: 1013.5\n",
      "fc layer 2 self.abs_max_out: 1034.0\n",
      "lif layer 2 self.abs_max_v: 1034.0\n",
      "lif layer 2 self.abs_max_v: 1059.5\n",
      "lif layer 1 self.abs_max_v: 4025.0\n",
      "fc layer 1 self.abs_max_out: 3583.0\n",
      "fc layer 1 self.abs_max_out: 3800.0\n",
      "fc layer 2 self.abs_max_out: 1041.0\n",
      "fc layer 2 self.abs_max_out: 1100.0\n",
      "lif layer 1 self.abs_max_v: 4392.5\n",
      "lif layer 2 self.abs_max_v: 1090.0\n",
      "fc layer 1 self.abs_max_out: 3895.0\n",
      "lif layer 1 self.abs_max_v: 5034.0\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "lif layer 2 self.abs_max_v: 1113.0\n",
      "fc layer 2 self.abs_max_out: 1146.0\n",
      "lif layer 2 self.abs_max_v: 1146.0\n",
      "fc layer 2 self.abs_max_out: 1164.0\n",
      "lif layer 2 self.abs_max_v: 1164.0\n",
      "fc layer 1 self.abs_max_out: 4068.0\n",
      "lif layer 2 self.abs_max_v: 1221.5\n",
      "fc layer 1 self.abs_max_out: 4150.0\n",
      "fc layer 1 self.abs_max_out: 4571.0\n",
      "fc layer 2 self.abs_max_out: 1176.0\n",
      "fc layer 2 self.abs_max_out: 1213.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "lif layer 2 self.abs_max_v: 1234.0\n",
      "lif layer 2 self.abs_max_v: 1241.5\n",
      "lif layer 2 self.abs_max_v: 1254.5\n",
      "fc layer 2 self.abs_max_out: 1239.0\n",
      "lif layer 2 self.abs_max_v: 1264.5\n",
      "fc layer 2 self.abs_max_out: 1256.0\n",
      "fc layer 2 self.abs_max_out: 1261.0\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "lif layer 2 self.abs_max_v: 1289.0\n",
      "fc layer 2 self.abs_max_out: 1322.0\n",
      "lif layer 2 self.abs_max_v: 1322.0\n",
      "lif layer 1 self.abs_max_v: 5302.5\n",
      "lif layer 2 self.abs_max_v: 1331.5\n",
      "lif layer 2 self.abs_max_v: 1342.0\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "lif layer 2 self.abs_max_v: 1350.0\n",
      "fc layer 2 self.abs_max_out: 1356.0\n",
      "lif layer 2 self.abs_max_v: 1356.0\n",
      "fc layer 1 self.abs_max_out: 4625.0\n",
      "fc layer 2 self.abs_max_out: 1406.0\n",
      "lif layer 2 self.abs_max_v: 1406.0\n",
      "lif layer 1 self.abs_max_v: 5965.5\n",
      "lif layer 2 self.abs_max_v: 1422.0\n",
      "fc layer 1 self.abs_max_out: 4930.0\n",
      "fc layer 2 self.abs_max_out: 1489.0\n",
      "lif layer 2 self.abs_max_v: 1489.0\n",
      "fc layer 2 self.abs_max_out: 1500.0\n",
      "lif layer 2 self.abs_max_v: 1500.0\n",
      "lif layer 1 self.abs_max_v: 6021.0\n",
      "lif layer 2 self.abs_max_v: 1540.5\n",
      "fc layer 2 self.abs_max_out: 1551.0\n",
      "lif layer 2 self.abs_max_v: 1551.0\n",
      "fc layer 1 self.abs_max_out: 4949.0\n",
      "fc layer 1 self.abs_max_out: 5112.0\n",
      "fc layer 2 self.abs_max_out: 1558.0\n",
      "lif layer 2 self.abs_max_v: 1558.0\n",
      "lif layer 1 self.abs_max_v: 6186.5\n",
      "fc layer 2 self.abs_max_out: 1582.0\n",
      "lif layer 2 self.abs_max_v: 1582.0\n",
      "fc layer 2 self.abs_max_out: 1684.0\n",
      "lif layer 2 self.abs_max_v: 1684.0\n",
      "fc layer 1 self.abs_max_out: 5122.0\n",
      "fc layer 1 self.abs_max_out: 5196.0\n",
      "fc layer 1 self.abs_max_out: 5453.0\n",
      "lif layer 1 self.abs_max_v: 6243.5\n",
      "fc layer 1 self.abs_max_out: 5626.0\n",
      "fc layer 1 self.abs_max_out: 5687.0\n",
      "fc layer 1 self.abs_max_out: 5746.0\n",
      "fc layer 1 self.abs_max_out: 5843.0\n",
      "fc layer 1 self.abs_max_out: 5936.0\n",
      "fc layer 1 self.abs_max_out: 6230.0\n",
      "lif layer 1 self.abs_max_v: 6547.0\n",
      "fc layer 2 self.abs_max_out: 1708.0\n",
      "lif layer 2 self.abs_max_v: 1708.0\n",
      "fc layer 2 self.abs_max_out: 1714.0\n",
      "lif layer 2 self.abs_max_v: 1714.0\n",
      "fc layer 2 self.abs_max_out: 1796.0\n",
      "lif layer 2 self.abs_max_v: 1796.0\n",
      "lif layer 1 self.abs_max_v: 6786.0\n",
      "lif layer 1 self.abs_max_v: 6953.0\n",
      "fc layer 2 self.abs_max_out: 1802.0\n",
      "lif layer 2 self.abs_max_v: 1802.0\n",
      "fc layer 1 self.abs_max_out: 6287.0\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "lif layer 2 self.abs_max_v: 1884.0\n",
      "fc layer 1 self.abs_max_out: 6351.0\n",
      "fc layer 2 self.abs_max_out: 1973.0\n",
      "lif layer 2 self.abs_max_v: 1973.0\n",
      "fc layer 1 self.abs_max_out: 6421.0\n",
      "fc layer 1 self.abs_max_out: 6537.0\n",
      "lif layer 1 self.abs_max_v: 8008.5\n",
      "fc layer 1 self.abs_max_out: 6571.0\n",
      "fc layer 1 self.abs_max_out: 6814.0\n",
      "fc layer 1 self.abs_max_out: 6948.0\n",
      "fc layer 1 self.abs_max_out: 6976.0\n",
      "fc layer 1 self.abs_max_out: 7505.0\n",
      "lif layer 1 self.abs_max_v: 9234.0\n",
      "lif layer 1 self.abs_max_v: 10179.5\n",
      "fc layer 1 self.abs_max_out: 7820.0\n",
      "fc layer 2 self.abs_max_out: 1975.0\n",
      "lif layer 2 self.abs_max_v: 1975.0\n",
      "lif layer 1 self.abs_max_v: 10377.5\n",
      "fc layer 2 self.abs_max_out: 2003.0\n",
      "lif layer 2 self.abs_max_v: 2003.0\n",
      "fc layer 1 self.abs_max_out: 7874.0\n",
      "fc layer 1 self.abs_max_out: 7928.0\n",
      "fc layer 2 self.abs_max_out: 2015.0\n",
      "lif layer 2 self.abs_max_v: 2015.0\n",
      "fc layer 1 self.abs_max_out: 8284.0\n",
      "fc layer 1 self.abs_max_out: 8364.0\n",
      "lif layer 2 self.abs_max_v: 2365.5\n",
      "lif layer 2 self.abs_max_v: 2526.0\n",
      "fc layer 2 self.abs_max_out: 2083.0\n",
      "lif layer 2 self.abs_max_v: 2587.0\n",
      "fc layer 2 self.abs_max_out: 2096.0\n",
      "lif layer 2 self.abs_max_v: 2640.5\n",
      "lif layer 2 self.abs_max_v: 2960.5\n",
      "lif layer 2 self.abs_max_v: 3045.5\n",
      "fc layer 1 self.abs_max_out: 8635.0\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "fc layer 1 self.abs_max_out: 8926.0\n",
      "fc layer 2 self.abs_max_out: 2122.0\n",
      "fc layer 2 self.abs_max_out: 2147.0\n",
      "fc layer 2 self.abs_max_out: 2155.0\n",
      "lif layer 1 self.abs_max_v: 11165.5\n",
      "fc layer 2 self.abs_max_out: 2204.0\n",
      "fc layer 2 self.abs_max_out: 2219.0\n",
      "fc layer 2 self.abs_max_out: 2246.0\n",
      "train - Value 0: 1895 occurrences\n",
      "train - Value 1: 2135 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 105.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 118.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 121.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 138.00 at epoch 0, iter 4029\n",
      "max_activation_accul updated: 142.00 at epoch 0, iter 4029\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "fc layer 2 self.abs_max_out: 2306.0\n",
      "max_activation_accul updated: 145.00 at epoch 0, iter 4029\n",
      "fc layer 2 self.abs_max_out: 2347.0\n",
      "max_activation_accul updated: 147.00 at epoch 0, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-0   lr=['4.0000000'], tr/val_loss: 19.813864/ 29.859106, val:  50.44%, val_best:  50.44%, tr:  80.89%, tr_best:  80.89%, epoch time: 136.45 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 84.4962%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 16120 real_backward_count 3837  23.803%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 82.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2349.0\n",
      "fc layer 2 self.abs_max_out: 2377.0\n",
      "fc layer 2 self.abs_max_out: 2386.0\n",
      "fc layer 2 self.abs_max_out: 2403.0\n",
      "fc layer 2 self.abs_max_out: 2409.0\n",
      "fc layer 2 self.abs_max_out: 2445.0\n",
      "fc layer 2 self.abs_max_out: 2453.0\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 2 self.abs_max_out: 2565.0\n",
      "fc layer 2 self.abs_max_out: 2609.0\n",
      "fc layer 3 self.abs_max_out: 68.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "lif layer 1 self.abs_max_v: 13302.5\n",
      "fc layer 1 self.abs_max_out: 9053.0\n",
      "fc layer 1 self.abs_max_out: 9131.0\n",
      "fc layer 2 self.abs_max_out: 2615.0\n",
      "fc layer 2 self.abs_max_out: 2632.0\n",
      "fc layer 2 self.abs_max_out: 2677.0\n",
      "fc layer 2 self.abs_max_out: 2709.0\n",
      "fc layer 1 self.abs_max_out: 9356.0\n",
      "fc layer 2 self.abs_max_out: 2714.0\n",
      "fc layer 1 self.abs_max_out: 9458.0\n",
      "fc layer 1 self.abs_max_out: 9513.0\n",
      "fc layer 3 self.abs_max_out: 79.0\n",
      "fc layer 3 self.abs_max_out: 82.0\n",
      "fc layer 1 self.abs_max_out: 9561.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "lif layer 2 self.abs_max_v: 3217.5\n",
      "lif layer 2 self.abs_max_v: 3257.5\n",
      "lif layer 2 self.abs_max_v: 3310.0\n",
      "lif layer 2 self.abs_max_v: 3409.0\n",
      "lif layer 2 self.abs_max_v: 3509.5\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "fc layer 2 self.abs_max_out: 2736.0\n",
      "fc layer 2 self.abs_max_out: 2769.0\n",
      "fc layer 2 self.abs_max_out: 2825.0\n",
      "fc layer 1 self.abs_max_out: 9900.0\n",
      "fc layer 1 self.abs_max_out: 9919.0\n",
      "fc layer 2 self.abs_max_out: 2846.0\n",
      "lif layer 2 self.abs_max_v: 3582.0\n",
      "lif layer 2 self.abs_max_v: 3621.5\n",
      "fc layer 1 self.abs_max_out: 9966.0\n",
      "fc layer 2 self.abs_max_out: 2983.0\n",
      "lif layer 2 self.abs_max_v: 4037.0\n",
      "fc layer 2 self.abs_max_out: 2996.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 3011.0\n",
      "fc layer 2 self.abs_max_out: 3013.0\n",
      "fc layer 2 self.abs_max_out: 3075.0\n",
      "max_activation_accul updated: 152.00 at epoch 1, iter 4029\n",
      "fc layer 1 self.abs_max_out: 10477.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 292 occurrences\n",
      "test - Value 1: 160 occurrences\n",
      "epoch-1   lr=['4.0000000'], tr/val_loss: 27.271431/ 16.197798, val:  76.11%, val_best:  76.11%, tr:  86.87%, tr_best:  86.87%, epoch time: 134.16 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1443%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.5212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32240 real_backward_count 7153  22.187%\n",
      "layer   1  Sparsity: 81.1523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3155.0\n",
      "lif layer 2 self.abs_max_v: 4374.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 3258.0\n",
      "lif layer 2 self.abs_max_v: 4391.0\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "fc layer 1 self.abs_max_out: 10664.0\n",
      "fc layer 1 self.abs_max_out: 11074.0\n",
      "fc layer 1 self.abs_max_out: 11233.0\n",
      "fc layer 1 self.abs_max_out: 11249.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "lif layer 2 self.abs_max_v: 4441.5\n",
      "fc layer 2 self.abs_max_out: 3277.0\n",
      "lif layer 1 self.abs_max_v: 14097.0\n",
      "lif layer 1 self.abs_max_v: 14642.0\n",
      "train - Value 0: 1929 occurrences\n",
      "train - Value 1: 2101 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 165.00 at epoch 2, iter 4029\n",
      "max_activation_accul updated: 174.00 at epoch 2, iter 4029\n",
      "max_activation_accul updated: 190.00 at epoch 2, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 284 occurrences\n",
      "test - Value 1: 168 occurrences\n",
      "epoch-2   lr=['4.0000000'], tr/val_loss: 31.231323/ 29.900511, val:  77.43%, val_best:  77.43%, tr:  89.48%, tr_best:  89.48%, epoch time: 133.32 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.2101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48360 real_backward_count 10301  21.301%\n",
      "layer   1  Sparsity: 75.6348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3315.0\n",
      "fc layer 2 self.abs_max_out: 3357.0\n",
      "fc layer 2 self.abs_max_out: 3367.0\n",
      "fc layer 2 self.abs_max_out: 3388.0\n",
      "fc layer 2 self.abs_max_out: 3428.0\n",
      "fc layer 2 self.abs_max_out: 3451.0\n",
      "fc layer 2 self.abs_max_out: 3517.0\n",
      "fc layer 2 self.abs_max_out: 3521.0\n",
      "fc layer 2 self.abs_max_out: 3538.0\n",
      "fc layer 2 self.abs_max_out: 3610.0\n",
      "fc layer 2 self.abs_max_out: 3679.0\n",
      "lif layer 1 self.abs_max_v: 15199.0\n",
      "fc layer 2 self.abs_max_out: 3720.0\n",
      "fc layer 2 self.abs_max_out: 3726.0\n",
      "fc layer 2 self.abs_max_out: 3804.0\n",
      "fc layer 2 self.abs_max_out: 3847.0\n",
      "fc layer 1 self.abs_max_out: 11271.0\n",
      "fc layer 1 self.abs_max_out: 11353.0\n",
      "fc layer 1 self.abs_max_out: 11430.0\n",
      "fc layer 1 self.abs_max_out: 11725.0\n",
      "fc layer 1 self.abs_max_out: 11736.0\n",
      "lif layer 2 self.abs_max_v: 4550.0\n",
      "lif layer 2 self.abs_max_v: 4551.0\n",
      "lif layer 2 self.abs_max_v: 4567.0\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 1 self.abs_max_out: 11837.0\n",
      "fc layer 1 self.abs_max_out: 11963.0\n",
      "fc layer 1 self.abs_max_out: 12214.0\n",
      "fc layer 1 self.abs_max_out: 12219.0\n",
      "fc layer 1 self.abs_max_out: 12233.0\n",
      "train - Value 0: 1955 occurrences\n",
      "train - Value 1: 2075 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 281.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 286.00 at epoch 3, iter 4029\n",
      "max_activation_accul updated: 291.00 at epoch 3, iter 4029\n",
      "fc layer 1 self.abs_max_out: 12363.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 32 occurrences\n",
      "test - Value 1: 420 occurrences\n",
      "epoch-3   lr=['4.0000000'], tr/val_loss: 28.695677/ 35.237225, val:  57.08%, val_best:  77.43%, tr:  90.22%, tr_best:  90.22%, epoch time: 133.48 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.1534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.1578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64480 real_backward_count 13320  20.658%\n",
      "layer   1  Sparsity: 66.5039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 35.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12397.0\n",
      "fc layer 3 self.abs_max_out: 132.0\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "lif layer 1 self.abs_max_v: 16068.0\n",
      "lif layer 2 self.abs_max_v: 4580.5\n",
      "train - Value 0: 1933 occurrences\n",
      "train - Value 1: 2097 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 366 occurrences\n",
      "test - Value 1: 86 occurrences\n",
      "epoch-4   lr=['4.0000000'], tr/val_loss: 33.582817/ 23.363440, val:  69.03%, val_best:  77.43%, tr:  91.96%, tr_best:  91.96%, epoch time: 133.08 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.5435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 80600 real_backward_count 16189  20.086%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 44.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4631.0\n",
      "lif layer 2 self.abs_max_v: 4689.0\n",
      "lif layer 2 self.abs_max_v: 4701.0\n",
      "lif layer 2 self.abs_max_v: 4815.5\n",
      "lif layer 2 self.abs_max_v: 4827.0\n",
      "fc layer 1 self.abs_max_out: 12701.0\n",
      "lif layer 2 self.abs_max_v: 4941.5\n",
      "fc layer 1 self.abs_max_out: 13111.0\n",
      "lif layer 2 self.abs_max_v: 4958.0\n",
      "fc layer 1 self.abs_max_out: 13510.0\n",
      "fc layer 1 self.abs_max_out: 13627.0\n",
      "train - Value 0: 2039 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 100 occurrences\n",
      "test - Value 1: 352 occurrences\n",
      "epoch-5   lr=['4.0000000'], tr/val_loss: 27.560186/ 36.937080, val:  69.47%, val_best:  77.43%, tr:  90.72%, tr_best:  91.96%, epoch time: 133.82 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 44.5130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.8210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96720 real_backward_count 19105  19.753%\n",
      "layer   1  Sparsity: 81.0059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 13675.0\n",
      "lif layer 1 self.abs_max_v: 16181.0\n",
      "lif layer 1 self.abs_max_v: 16556.0\n",
      "lif layer 1 self.abs_max_v: 16894.0\n",
      "fc layer 1 self.abs_max_out: 14140.0\n",
      "train - Value 0: 1896 occurrences\n",
      "train - Value 1: 2134 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 90 occurrences\n",
      "test - Value 1: 362 occurrences\n",
      "epoch-6   lr=['4.0000000'], tr/val_loss: 37.352161/ 29.182461, val:  68.14%, val_best:  77.43%, tr:  91.54%, tr_best:  91.96%, epoch time: 134.45 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1447%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 44.4602%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.8898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 112840 real_backward_count 21856  19.369%\n",
      "layer   1  Sparsity: 80.4199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5127.0\n",
      "lif layer 2 self.abs_max_v: 5191.0\n",
      "lif layer 2 self.abs_max_v: 5220.0\n",
      "lif layer 1 self.abs_max_v: 18009.5\n",
      "lif layer 2 self.abs_max_v: 5249.0\n",
      "fc layer 2 self.abs_max_out: 3893.0\n",
      "lif layer 2 self.abs_max_v: 5403.0\n",
      "lif layer 2 self.abs_max_v: 5445.0\n",
      "lif layer 2 self.abs_max_v: 5457.5\n",
      "lif layer 2 self.abs_max_v: 5630.0\n",
      "lif layer 2 self.abs_max_v: 5656.5\n",
      "fc layer 2 self.abs_max_out: 3904.0\n",
      "fc layer 2 self.abs_max_out: 3977.0\n",
      "fc layer 2 self.abs_max_out: 4092.0\n",
      "lif layer 2 self.abs_max_v: 5724.5\n",
      "fc layer 2 self.abs_max_out: 4121.0\n",
      "fc layer 2 self.abs_max_out: 4141.0\n",
      "train - Value 0: 1962 occurrences\n",
      "train - Value 1: 2068 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 100 occurrences\n",
      "test - Value 1: 352 occurrences\n",
      "epoch-7   lr=['4.0000000'], tr/val_loss: 39.909538/ 25.470383, val:  69.91%, val_best:  77.43%, tr:  93.13%, tr_best:  93.13%, epoch time: 134.22 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1448%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 44.1449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 128960 real_backward_count 24547  19.035%\n",
      "layer   1  Sparsity: 68.7988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 42.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5948.5\n",
      "fc layer 2 self.abs_max_out: 4242.0\n",
      "fc layer 2 self.abs_max_out: 4284.0\n",
      "fc layer 2 self.abs_max_out: 4307.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 308.00 at epoch 8, iter 4029\n",
      "max_activation_accul updated: 317.00 at epoch 8, iter 4029\n",
      "max_activation_accul updated: 319.00 at epoch 8, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-8   lr=['4.0000000'], tr/val_loss: 37.837166/ 48.871571, val:  72.35%, val_best:  77.43%, tr:  92.73%, tr_best:  93.13%, epoch time: 134.89 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.6357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 145080 real_backward_count 27311  18.825%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 39.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1965 occurrences\n",
      "train - Value 1: 2065 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4340.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 213 occurrences\n",
      "test - Value 1: 239 occurrences\n",
      "epoch-9   lr=['4.0000000'], tr/val_loss: 33.608154/ 15.709629, val:  83.85%, val_best:  83.85%, tr:  93.60%, tr_best:  93.60%, epoch time: 133.96 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.4630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7690%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161200 real_backward_count 29942  18.574%\n",
      "layer   1  Sparsity: 74.4629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 40.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 14274.0\n",
      "train - Value 0: 1931 occurrences\n",
      "train - Value 1: 2099 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 320.00 at epoch 10, iter 4029\n",
      "max_activation_accul updated: 327.00 at epoch 10, iter 4029\n",
      "max_activation_accul updated: 336.00 at epoch 10, iter 4029\n",
      "max_activation_accul updated: 345.00 at epoch 10, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 447 occurrences\n",
      "test - Value 1: 5 occurrences\n",
      "epoch-10  lr=['4.0000000'], tr/val_loss: 34.285824/ 45.880360, val:  51.11%, val_best:  83.85%, tr:  93.35%, tr_best:  93.60%, epoch time: 134.61 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.0367%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 177320 real_backward_count 32572  18.369%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 41.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 19069.5\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4386.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 228 occurrences\n",
      "test - Value 1: 224 occurrences\n",
      "epoch-11  lr=['4.0000000'], tr/val_loss: 38.060673/ 37.142258, val:  82.30%, val_best:  83.85%, tr:  94.12%, tr_best:  94.12%, epoch time: 134.65 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.8921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193440 real_backward_count 35131  18.161%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1938 occurrences\n",
      "train - Value 1: 2092 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 6077.5\n",
      "fc layer 2 self.abs_max_out: 4388.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-12  lr=['4.0000000'], tr/val_loss: 40.038532/ 38.010178, val:  80.53%, val_best:  83.85%, tr:  94.57%, tr_best:  94.57%, epoch time: 134.44 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.8859%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6586%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 209560 real_backward_count 37662  17.972%\n",
      "layer   1  Sparsity: 79.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4466.0\n",
      "fc layer 2 self.abs_max_out: 4631.0\n",
      "train - Value 0: 1927 occurrences\n",
      "train - Value 1: 2103 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 68 occurrences\n",
      "test - Value 1: 384 occurrences\n",
      "epoch-13  lr=['4.0000000'], tr/val_loss: 36.948402/ 31.360214, val:  64.16%, val_best:  83.85%, tr:  93.80%, tr_best:  94.57%, epoch time: 135.17 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.8814%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225680 real_backward_count 40094  17.766%\n",
      "layer   1  Sparsity: 92.9199%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4825.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2052 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-14  lr=['4.0000000'], tr/val_loss: 39.992680/ 20.094912, val:  79.65%, val_best:  83.85%, tr:  93.82%, tr_best:  94.57%, epoch time: 135.19 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1420%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.9967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 241800 real_backward_count 42658  17.642%\n",
      "layer   1  Sparsity: 84.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4890.0\n",
      "train - Value 0: 1961 occurrences\n",
      "train - Value 1: 2069 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 178 occurrences\n",
      "test - Value 1: 274 occurrences\n",
      "epoch-15  lr=['4.0000000'], tr/val_loss: 40.527065/ 46.219189, val:  83.19%, val_best:  83.85%, tr:  93.95%, tr_best:  94.57%, epoch time: 135.22 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1440%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.3409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 257920 real_backward_count 45206  17.527%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5123.0\n",
      "train - Value 0: 1936 occurrences\n",
      "train - Value 1: 2094 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 58 occurrences\n",
      "test - Value 1: 394 occurrences\n",
      "epoch-16  lr=['4.0000000'], tr/val_loss: 43.887909/ 42.865314, val:  62.83%, val_best:  83.85%, tr:  94.52%, tr_best:  94.57%, epoch time: 134.96 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.1822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3775%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274040 real_backward_count 47634  17.382%\n",
      "layer   1  Sparsity: 82.3242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 42.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 14971.0\n",
      "lif layer 2 self.abs_max_v: 6177.0\n",
      "lif layer 2 self.abs_max_v: 6307.0\n",
      "lif layer 1 self.abs_max_v: 19272.0\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 319 occurrences\n",
      "test - Value 1: 133 occurrences\n",
      "epoch-17  lr=['4.0000000'], tr/val_loss: 42.235779/ 28.539345, val:  76.77%, val_best:  83.85%, tr:  95.98%, tr_best:  95.98%, epoch time: 136.25 seconds, 2.27 minutes\n",
      "layer   1  Sparsity: 81.1444%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.3779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290160 real_backward_count 50009  17.235%\n",
      "layer   1  Sparsity: 76.3672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 42.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 15753.0\n",
      "fc layer 1 self.abs_max_out: 15921.0\n",
      "lif layer 1 self.abs_max_v: 19381.5\n",
      "lif layer 2 self.abs_max_v: 6754.5\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 5306.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 62 occurrences\n",
      "test - Value 1: 390 occurrences\n",
      "epoch-18  lr=['4.0000000'], tr/val_loss: 37.576611/ 61.493767, val:  62.83%, val_best:  83.85%, tr:  95.26%, tr_best:  95.98%, epoch time: 135.09 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.3312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7494%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 306280 real_backward_count 52384  17.103%\n",
      "layer   1  Sparsity: 86.1816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 81.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5871.0\n",
      "fc layer 1 self.abs_max_out: 16039.0\n",
      "lif layer 2 self.abs_max_v: 6764.5\n",
      "train - Value 0: 1969 occurrences\n",
      "train - Value 1: 2061 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-19  lr=['4.0000000'], tr/val_loss: 32.945187/ 47.796112, val:  59.73%, val_best:  83.85%, tr:  93.90%, tr_best:  95.98%, epoch time: 136.62 seconds, 2.28 minutes\n",
      "layer   1  Sparsity: 81.1435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.9344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322400 real_backward_count 54863  17.017%\n",
      "layer   1  Sparsity: 76.7578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 42.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-20  lr=['4.0000000'], tr/val_loss: 36.117836/ 36.037041, val:  77.43%, val_best:  83.85%, tr:  95.38%, tr_best:  95.98%, epoch time: 132.95 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.6442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.2485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 338520 real_backward_count 57172  16.889%\n",
      "layer   1  Sparsity: 86.5723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 19792.0\n",
      "lif layer 2 self.abs_max_v: 6897.5\n",
      "train - Value 0: 1943 occurrences\n",
      "train - Value 1: 2087 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 351.00 at epoch 21, iter 4029\n",
      "lif layer 2 self.abs_max_v: 6929.0\n",
      "max_activation_accul updated: 368.00 at epoch 21, iter 4029\n",
      "max_activation_accul updated: 372.00 at epoch 21, iter 4029\n",
      "max_activation_accul updated: 395.00 at epoch 21, iter 4029\n",
      "max_activation_accul updated: 401.00 at epoch 21, iter 4029\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 33 occurrences\n",
      "test - Value 1: 419 occurrences\n",
      "epoch-21  lr=['4.0000000'], tr/val_loss: 40.535542/ 69.674545, val:  57.30%, val_best:  83.85%, tr:  96.23%, tr_best:  96.23%, epoch time: 133.65 seconds, 2.23 minutes\n",
      "layer   1  Sparsity: 81.1434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.8909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5933%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354640 real_backward_count 59336  16.731%\n",
      "layer   1  Sparsity: 81.2012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7051.5\n",
      "train - Value 0: 1973 occurrences\n",
      "train - Value 1: 2057 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 33 occurrences\n",
      "test - Value 1: 419 occurrences\n",
      "epoch-22  lr=['4.0000000'], tr/val_loss: 40.293388/ 67.867561, val:  57.30%, val_best:  83.85%, tr:  95.53%, tr_best:  96.23%, epoch time: 132.97 seconds, 2.22 minutes\n",
      "layer   1  Sparsity: 81.1446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.0006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.4273%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 370760 real_backward_count 61599  16.614%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7142.0\n",
      "lif layer 2 self.abs_max_v: 7240.5\n",
      "lif layer 2 self.abs_max_v: 7307.0\n",
      "lif layer 2 self.abs_max_v: 7394.5\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2046 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 402 occurrences\n",
      "test - Value 1: 50 occurrences\n",
      "epoch-23  lr=['4.0000000'], tr/val_loss: 45.549541/ 23.451635, val:  61.06%, val_best:  83.85%, tr:  96.80%, tr_best:  96.80%, epoch time: 134.67 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.2542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 386880 real_backward_count 63836  16.500%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7557.0\n",
      "lif layer 2 self.abs_max_v: 7621.5\n",
      "lif layer 2 self.abs_max_v: 7716.5\n",
      "train - Value 0: 1976 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 243 occurrences\n",
      "test - Value 1: 209 occurrences\n",
      "epoch-24  lr=['4.0000000'], tr/val_loss: 42.431435/ 24.842463, val:  84.73%, val_best:  84.73%, tr:  96.30%, tr_best:  96.80%, epoch time: 135.23 seconds, 2.25 minutes\n",
      "layer   1  Sparsity: 81.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.6976%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.5491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 403000 real_backward_count 66122  16.407%\n",
      "layer   1  Sparsity: 85.6445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 20075.5\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2081 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 203 occurrences\n",
      "test - Value 1: 249 occurrences\n",
      "epoch-25  lr=['4.0000000'], tr/val_loss: 35.400249/ 18.809938, val:  85.62%, val_best:  85.62%, tr:  95.73%, tr_best:  96.80%, epoch time: 134.46 seconds, 2.24 minutes\n",
      "layer   1  Sparsity: 81.1436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.2093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.3432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419120 real_backward_count 68349  16.308%\n",
      "layer   1  Sparsity: 79.1504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 42.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "target_word=2\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_targetword{target_word}_new251129',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [4]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [8]},\n",
    "        \"which_data\": {\"values\": ['n_tidigits_tonic']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [2048.0,1024.0,512.0,256.0,128.0,64.0,32.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [2048.0,1024.0,512.0,256.0,128.0,64.0,32.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [0.25, 0.5,1.0, 2.0, 4.0]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [4.0, 8.0, 16.0,32.0,64.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1.0, 2.0, 4.0, 8.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0, 2.0, 4.0, 8.0]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [1]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [target_word]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [False]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [False]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [8]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "\n",
    "        \"timestep_sums_threshold\": {\"values\": [0]},\n",
    "\n",
    "        \"loser_encourage_mode\": {\"values\": [True, False]},\n",
    "        \n",
    "        \"init_scaling_0\": {\"values\": [1/2]},\n",
    "        \"init_scaling_1\": {\"values\": [1/4]},\n",
    "        \"init_scaling_2\": {\"values\": [1/16]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"1\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        timestep_sums_threshold  =  wandb.config.timestep_sums_threshold,\n",
    "        loser_encourage_mode  =  wandb.config.loser_encourage_mode,\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'xlkrlgkj'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
