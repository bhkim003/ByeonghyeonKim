{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19658/4213678604.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "\n",
    "                    timestep_sums_threshold = 15,\n",
    "\n",
    "                    loser_encourage_mode = False, # True # False\n",
    "                    \n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    if which_data == 'n_tidigits_tonic':\n",
    "        assert merge_polarities == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    synapse_fc_out_features = 10\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    # # wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    # ###########################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    # class CustomLossFunction(torch.autograd.Function):\n",
    "    #     @staticmethod\n",
    "    #     def forward(ctx, input, target):\n",
    "    #         ctx.save_for_backward(input, target)\n",
    "    #         return F.cross_entropy(input, target)\n",
    "\n",
    "    #     @staticmethod\n",
    "    #     def backward(ctx, grad_output):\n",
    "    #         # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "    #         input, target = ctx.saved_tensors\n",
    "    #         input_argmax = input.argmax(dim=1)\n",
    "    #         input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "    #         target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "    #         # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "    #         return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "    \n",
    "\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    print(\"ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\")\n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            input, target = ctx.saved_tensors\n",
    "            assert input.shape[0] == 1 and target.shape[0] == 1, \"Batch size must be 1 for this custom loss function.\"\n",
    "            batch_size, num_classes = input.shape\n",
    "\n",
    "            target_0 = [0,1,2,3,4]\n",
    "            target_1 = [5,6,7,8,9]\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "\n",
    "            if (target.item() == 0) and (input_argmax.item() in target_0) or \\\n",
    "                (target.item() == 1) and (input_argmax.item() in target_1):\n",
    "                return input_one_hot - input_one_hot, None  \n",
    "            else:\n",
    "                if target.item() == 0:\n",
    "                    input_slice = input[:, 0:5]\n",
    "                    if loser_encourage_mode:\n",
    "                        input_argmin = input_slice.argmin(dim=1)\n",
    "                    else:\n",
    "                        input_argmin = input_slice.argmax(dim=1)\n",
    "                elif target.item() == 1:\n",
    "                    input_slice = input[:, 5:10] \n",
    "                    if loser_encourage_mode:\n",
    "                        input_argmin = input_slice.argmin(dim=1) + 5\n",
    "                    else:\n",
    "                        input_argmin = input_slice.argmax(dim=1) + 5\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target: {target.item()}\")\n",
    "\n",
    "                # gradient Î∞©Ìñ•ÏùÑ argmin Ï™ΩÏúºÎ°ú\n",
    "                modified_target_one_hot = torch.zeros_like(input).scatter_(1, input_argmin.unsqueeze(1), 1.0)\n",
    "\n",
    "                return input_one_hot - modified_target_one_hot, None\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "            self.additional_dw_weight = 1.0\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                    \n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                            \n",
    "                    \n",
    "                    dw = dw * self.additional_dw_weight\n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    max_activation_accul = 0\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        # optimizer.additional_dw_weight = 1.0 if epoch % 2 ==0 else 0.0\n",
    "        optimizer.additional_dw_weight = 1.0\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        # if epoch %2 == 0:\n",
    "        #     iterator = enumerate(train_loader, 0)\n",
    "        # else:\n",
    "        #     iterator = enumerate(test_loader, 0)\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        train_spike_distribution = []\n",
    "        train_predicted_distribution = []\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                # inputs: [Batch, Time, Channel, Height, Width]\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif (which_data == 'n_tidigits_tonic'):\n",
    "                inputs = inputs.unsqueeze(-1)\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                # labels = torch.tensor(labels) \n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "                            \n",
    "            if i == 1:\n",
    "                # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                        \n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            \n",
    "            \n",
    "                        \n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            hetero_timesteps = True\n",
    "            if hetero_timesteps == True:\n",
    "                assert real_batch == 1\n",
    "                this_data_timesteps = inputs.shape[0]\n",
    "                TIME = this_data_timesteps//temporal_filter\n",
    "                net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            \n",
    "\n",
    "            \n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    # inputs # [Time, Batch, Channel, Height, Width]\n",
    "                    # inputs # [Batch, Channel, Height,Time, Width]\n",
    "                    # inputs # [Batch, Channel, Height,Time * Width]\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "            \n",
    "            # if hetero_timesteps == True:\n",
    "            #     assert real_batch == 1\n",
    "            #     # inputs # [Time, Batch, Channel, Height, Width]\n",
    "            #     # inputs timestpeÎ≥ÑÎ°ú sumÍ∞íÏù¥ 10ÎØ∏ÎßåÏùº Ïãú Ï†úÏô∏\n",
    "            #     # time stepÎ≥Ñ Ìï© Í≥ÑÏÇ∞: shape = [T]\n",
    "            #     timestep_sums = inputs.sum(dim=(1,2,3,4))  # sum over (B, C, H, W)\n",
    "\n",
    "            #     # 10 Ïù¥ÏÉÅÏù∏ ÌÉÄÏûÑÏä§ÌÖùÎßå ÏÑ†ÌÉù\n",
    "            #     valid_timesteps = timestep_sums >= timestep_sums_threshold\n",
    "            #     assert valid_timesteps.sum().item() != 0, \"No valid timesteps found. Check your data preprocessing.\"\n",
    "\n",
    "            #     # Ìï¥Îãπ ÌÉÄÏûÑÏä§ÌÖùÎßå Ï∂îÏ∂ú\n",
    "            #     inputs = inputs[valid_timesteps]\n",
    "            #     TIME = inputs.shape[0] # validÌïú time stepÏùò Í∞úÏàò\n",
    "            #     net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            train_spike_distribution.append(TIME)\n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device).to(torch.float)\n",
    "            labels = labels.to(device).to(torch.long)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            bp_timestep = random.randint(0, TIME - 1)  # 0 ~ TIME-1 Ï§ë ÌïòÎÇò ÏÑ†ÌÉù\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = ((outputs_one_time.detach()).argmax(dim=1) >= 5).long()\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "                    # optimizer.additional_dw_weight = 1.0 if t == bp_timestep else 0.0\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            \n",
    "            # target_0 = [0,1,2,3,4]\n",
    "            # target_1 = [5,6,7,8,9]\n",
    "            predicted = (predicted >= 5).long()\n",
    "            train_predicted_distribution.append(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            # if True :\n",
    "            if i == len(train_loader)-1 :\n",
    "                \n",
    "                \n",
    "                train_predicted_distribution = np.array(train_predicted_distribution)\n",
    "                unique_vals, counts = np.unique(train_predicted_distribution, return_counts=True)\n",
    "                for val, count in zip(unique_vals, counts):\n",
    "                    print(f\"train - Value {val}: {count} occurrences\")\n",
    "\n",
    "                print(f'train_spike_distribution.mean {np.mean(train_spike_distribution):.6f}, min {np.min(train_spike_distribution)}, max {np.max(train_spike_distribution)}')\n",
    "\n",
    "\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "                \n",
    "                test_spike_distribution = []\n",
    "                test_predicted_distribution = []\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    # for data_val in train_loader:\n",
    "                    for data_val in test_loader:\n",
    "                    # for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "                            \n",
    "                        ## batch ÌÅ¨Í∏∞ ######################################\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        ###########################################################\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif (which_data == 'n_tidigits_tonic'):\n",
    "                            inputs_val = inputs_val.unsqueeze(-1)\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                            # labels_val = torch.tensor(labels_val)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "                        \n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        hetero_timesteps = True\n",
    "                        if hetero_timesteps == True:\n",
    "                            assert real_batch == 1\n",
    "                            this_data_timesteps = inputs_val.shape[0]\n",
    "                            TIME = this_data_timesteps//temporal_filter\n",
    "                            net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        \n",
    "\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        \n",
    "                                    \n",
    "                        # if hetero_timesteps == True:\n",
    "                        #     assert real_batch == 1\n",
    "                        #     # inputs_val # [Time, Batch, Channel, Height, Width]\n",
    "                        #     # inputs_val timestpeÎ≥ÑÎ°ú sumÍ∞íÏù¥ 10ÎØ∏ÎßåÏùº Ïãú Ï†úÏô∏\n",
    "                        #     # time stepÎ≥Ñ Ìï© Í≥ÑÏÇ∞: shape = [T]\n",
    "                        #     timestep_sums = inputs_val.sum(dim=(1,2,3,4))  # sum over (B, C, H, W)\n",
    "\n",
    "                        #     # 10 Ïù¥ÏÉÅÏù∏ ÌÉÄÏûÑÏä§ÌÖùÎßå ÏÑ†ÌÉù\n",
    "                        #     valid_timesteps = timestep_sums >= timestep_sums_threshold\n",
    "                        #     assert valid_timesteps.sum().item() != 0, \"No valid timesteps found. Check your data preprocessing.\"\n",
    "\n",
    "                        #     # Ìï¥Îãπ ÌÉÄÏûÑÏä§ÌÖùÎßå Ï∂îÏ∂ú\n",
    "                        #     inputs_val = inputs_val[valid_timesteps]\n",
    "                        #     TIME = inputs_val.shape[0] # validÌïú time stepÏùò Í∞úÏàò\n",
    "                        #     net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                        test_spike_distribution.append(TIME)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(torch.float).to(device)\n",
    "                        labels_val = labels_val.to(torch.long).to(device)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                            \n",
    "                            if max_activation_accul < outputs.abs().max().item() * TIME * (2**(-scale_exp[2][0])):\n",
    "                                max_activation_accul = outputs.abs().max().item() * TIME * (2**(-scale_exp[2][0]))\n",
    "                                print(f\"max_activation_accul updated: {max_activation_accul:.2f} at epoch {epoch}, iter {i}\")\n",
    "                       \n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                                    \n",
    "                        predicted = (predicted >= 5).long()\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "                        test_predicted_distribution.append(predicted.cpu().numpy())\n",
    "\n",
    "                    print(f'test_spike_distribution.mean {np.mean(test_spike_distribution):.6f}, min {np.min(test_spike_distribution)}, max {np.max(test_spike_distribution)}')\n",
    "\n",
    "                    test_predicted_distribution = np.array(test_predicted_distribution)\n",
    "                    unique_vals, counts = np.unique(test_predicted_distribution, return_counts=True)\n",
    "                    for val, count in zip(unique_vals, counts):\n",
    "                        print(f\"test - Value {val}: {count} occurrences\")\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 4, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 8, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                 # n_tidigits_tonic 8\n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'n_tidigits_tonic',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','n_tidigits_tonic', 'DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.03125,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 # pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_20250704_185524_987.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 1000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 1, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 0, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = False, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 8, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "#                 # quantize_bit_list=[],\n",
    "#                 # scale_exp=[], \n",
    "#                 timestep_sums_threshold = 0,\n",
    "\n",
    "#                 loser_encourage_mode = True,\n",
    "                \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/4,1/4,1/4],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1k2ukckh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251220_050313-1k2ukckh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/1k2ukckh' target=\"_blank\">lilac-sweep-61</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/1k2ukckh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/1k2ukckh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251220_050320_786', 'my_seed': 42, 'TIME': 4, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1024, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 0, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 0.125, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.125, 1, 0.25], 'learning_rate': 8, 'learning_rate2': 1, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4032, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4032 BATCH: 1 train_data_count: 4032\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4, self.v_threshold 1024\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.125, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.125, 1, 0.25])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1024, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.125, 1, 0.25])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=0.125, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=4, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=4, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.125, 1, 0.25])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 8\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 279.0\n",
      "lif layer 1 self.abs_max_v: 279.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 428.0\n",
      "lif layer 1 self.abs_max_v: 514.0\n",
      "lif layer 1 self.abs_max_v: 598.0\n",
      "layer   1  Sparsity: 71.2891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 100.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 100.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 455.0\n",
      "fc layer 1 self.abs_max_out: 786.0\n",
      "lif layer 1 self.abs_max_v: 791.5\n",
      "fc layer 1 self.abs_max_out: 1683.0\n",
      "lif layer 1 self.abs_max_v: 1715.5\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "lif layer 2 self.abs_max_v: 232.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 1 self.abs_max_out: 2188.0\n",
      "lif layer 1 self.abs_max_v: 2624.5\n",
      "fc layer 2 self.abs_max_out: 427.0\n",
      "lif layer 2 self.abs_max_v: 427.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 2 self.abs_max_out: 489.0\n",
      "lif layer 2 self.abs_max_v: 489.0\n",
      "lif layer 2 self.abs_max_v: 505.5\n",
      "fc layer 1 self.abs_max_out: 2281.0\n",
      "fc layer 1 self.abs_max_out: 2569.0\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "lif layer 2 self.abs_max_v: 648.0\n",
      "fc layer 1 self.abs_max_out: 2767.0\n",
      "lif layer 1 self.abs_max_v: 2767.0\n",
      "fc layer 2 self.abs_max_out: 709.0\n",
      "lif layer 2 self.abs_max_v: 791.0\n",
      "fc layer 1 self.abs_max_out: 3261.0\n",
      "lif layer 1 self.abs_max_v: 3613.5\n",
      "fc layer 2 self.abs_max_out: 762.0\n",
      "lif layer 2 self.abs_max_v: 884.0\n",
      "fc layer 1 self.abs_max_out: 3402.0\n",
      "fc layer 1 self.abs_max_out: 3919.0\n",
      "lif layer 1 self.abs_max_v: 3919.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 1 self.abs_max_out: 4440.0\n",
      "lif layer 1 self.abs_max_v: 4569.0\n",
      "lif layer 2 self.abs_max_v: 953.5\n",
      "fc layer 2 self.abs_max_out: 983.0\n",
      "lif layer 2 self.abs_max_v: 1246.5\n",
      "fc layer 1 self.abs_max_out: 4627.0\n",
      "lif layer 1 self.abs_max_v: 5035.0\n",
      "lif layer 2 self.abs_max_v: 1293.0\n",
      "fc layer 2 self.abs_max_out: 1144.0\n",
      "lif layer 2 self.abs_max_v: 1432.0\n",
      "fc layer 1 self.abs_max_out: 4911.0\n",
      "fc layer 1 self.abs_max_out: 4914.0\n",
      "fc layer 1 self.abs_max_out: 5123.0\n",
      "lif layer 1 self.abs_max_v: 5123.0\n",
      "fc layer 1 self.abs_max_out: 5197.0\n",
      "lif layer 1 self.abs_max_v: 5197.0\n",
      "lif layer 2 self.abs_max_v: 1443.0\n",
      "lif layer 2 self.abs_max_v: 1486.0\n",
      "lif layer 2 self.abs_max_v: 1540.0\n",
      "fc layer 1 self.abs_max_out: 5542.0\n",
      "lif layer 1 self.abs_max_v: 5542.0\n",
      "lif layer 2 self.abs_max_v: 1559.0\n",
      "fc layer 1 self.abs_max_out: 5602.0\n",
      "lif layer 1 self.abs_max_v: 5919.5\n",
      "fc layer 1 self.abs_max_out: 6064.0\n",
      "lif layer 1 self.abs_max_v: 6064.0\n",
      "fc layer 2 self.abs_max_out: 1163.0\n",
      "lif layer 2 self.abs_max_v: 1661.5\n",
      "lif layer 2 self.abs_max_v: 1823.0\n",
      "fc layer 2 self.abs_max_out: 1174.0\n",
      "fc layer 2 self.abs_max_out: 1263.0\n",
      "lif layer 2 self.abs_max_v: 1982.0\n",
      "lif layer 2 self.abs_max_v: 2098.5\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "fc layer 2 self.abs_max_out: 1328.0\n",
      "fc layer 2 self.abs_max_out: 1419.0\n",
      "fc layer 2 self.abs_max_out: 1461.0\n",
      "lif layer 2 self.abs_max_v: 2356.0\n",
      "lif layer 2 self.abs_max_v: 2408.0\n",
      "fc layer 1 self.abs_max_out: 6770.0\n",
      "lif layer 1 self.abs_max_v: 6770.0\n",
      "fc layer 1 self.abs_max_out: 7027.0\n",
      "lif layer 1 self.abs_max_v: 7027.0\n",
      "fc layer 1 self.abs_max_out: 8049.0\n",
      "lif layer 1 self.abs_max_v: 8049.0\n",
      "fc layer 2 self.abs_max_out: 1535.0\n",
      "fc layer 1 self.abs_max_out: 8072.0\n",
      "lif layer 1 self.abs_max_v: 8072.0\n",
      "fc layer 1 self.abs_max_out: 8261.0\n",
      "lif layer 1 self.abs_max_v: 8261.0\n",
      "fc layer 1 self.abs_max_out: 8675.0\n",
      "lif layer 1 self.abs_max_v: 8675.0\n",
      "fc layer 1 self.abs_max_out: 8869.0\n",
      "lif layer 1 self.abs_max_v: 8869.0\n",
      "fc layer 1 self.abs_max_out: 9800.0\n",
      "lif layer 1 self.abs_max_v: 9800.0\n",
      "fc layer 1 self.abs_max_out: 9945.0\n",
      "lif layer 1 self.abs_max_v: 9945.0\n",
      "lif layer 1 self.abs_max_v: 10101.0\n",
      "fc layer 1 self.abs_max_out: 10467.0\n",
      "lif layer 1 self.abs_max_v: 10467.0\n",
      "fc layer 2 self.abs_max_out: 1549.0\n",
      "fc layer 1 self.abs_max_out: 11243.0\n",
      "lif layer 1 self.abs_max_v: 11243.0\n",
      "fc layer 1 self.abs_max_out: 11519.0\n",
      "lif layer 1 self.abs_max_v: 11519.0\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "fc layer 2 self.abs_max_out: 1569.0\n",
      "fc layer 2 self.abs_max_out: 1596.0\n",
      "lif layer 2 self.abs_max_v: 2595.0\n",
      "fc layer 2 self.abs_max_out: 1611.0\n",
      "fc layer 2 self.abs_max_out: 1628.0\n",
      "fc layer 2 self.abs_max_out: 1639.0\n",
      "lif layer 2 self.abs_max_v: 2629.5\n",
      "lif layer 2 self.abs_max_v: 2734.0\n",
      "lif layer 2 self.abs_max_v: 2857.5\n",
      "fc layer 1 self.abs_max_out: 11962.0\n",
      "lif layer 1 self.abs_max_v: 11962.0\n",
      "fc layer 2 self.abs_max_out: 1696.0\n",
      "fc layer 1 self.abs_max_out: 12563.0\n",
      "lif layer 1 self.abs_max_v: 12563.0\n",
      "fc layer 2 self.abs_max_out: 1760.0\n",
      "lif layer 2 self.abs_max_v: 2932.5\n",
      "fc layer 1 self.abs_max_out: 13456.0\n",
      "lif layer 1 self.abs_max_v: 13456.0\n",
      "fc layer 2 self.abs_max_out: 1764.0\n",
      "lif layer 2 self.abs_max_v: 3165.0\n",
      "fc layer 1 self.abs_max_out: 13459.0\n",
      "lif layer 1 self.abs_max_v: 13459.0\n",
      "fc layer 1 self.abs_max_out: 13851.0\n",
      "lif layer 1 self.abs_max_v: 13851.0\n",
      "fc layer 2 self.abs_max_out: 1772.0\n",
      "fc layer 2 self.abs_max_out: 1775.0\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "fc layer 1 self.abs_max_out: 13854.0\n",
      "lif layer 1 self.abs_max_v: 13854.0\n",
      "fc layer 2 self.abs_max_out: 1999.0\n",
      "fc layer 1 self.abs_max_out: 14522.0\n",
      "lif layer 1 self.abs_max_v: 14522.0\n",
      "fc layer 1 self.abs_max_out: 15344.0\n",
      "lif layer 1 self.abs_max_v: 15344.0\n",
      "fc layer 2 self.abs_max_out: 2015.0\n",
      "fc layer 1 self.abs_max_out: 15436.0\n",
      "lif layer 1 self.abs_max_v: 15436.0\n",
      "lif layer 2 self.abs_max_v: 3390.5\n",
      "fc layer 2 self.abs_max_out: 2069.0\n",
      "fc layer 2 self.abs_max_out: 2144.0\n",
      "fc layer 1 self.abs_max_out: 15951.0\n",
      "lif layer 1 self.abs_max_v: 15951.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 306.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 331.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 348.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 384.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 512.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 523.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 598.00 at epoch 0, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 418 occurrences\n",
      "test - Value 1: 34 occurrences\n",
      "epoch-0   lr=['8.0000000'], tr/val_loss: 68.367638/ 61.431675, val:  57.52%, val_best:  57.52%, tr:  72.02%, tr_best:  72.02%, epoch time: 132.52 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.9584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 16128 real_backward_count 5866  36.372%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 73.3398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 16691.0\n",
      "lif layer 1 self.abs_max_v: 16691.0\n",
      "fc layer 1 self.abs_max_out: 16852.0\n",
      "lif layer 1 self.abs_max_v: 16852.0\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "fc layer 2 self.abs_max_out: 2209.0\n",
      "fc layer 1 self.abs_max_out: 17164.0\n",
      "lif layer 1 self.abs_max_v: 17164.0\n",
      "fc layer 2 self.abs_max_out: 2221.0\n",
      "fc layer 1 self.abs_max_out: 17630.0\n",
      "lif layer 1 self.abs_max_v: 17630.0\n",
      "fc layer 2 self.abs_max_out: 2412.0\n",
      "fc layer 2 self.abs_max_out: 2463.0\n",
      "fc layer 1 self.abs_max_out: 17638.0\n",
      "lif layer 1 self.abs_max_v: 17638.0\n",
      "fc layer 1 self.abs_max_out: 17860.0\n",
      "lif layer 1 self.abs_max_v: 17860.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 621.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 640.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 679.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 749.00 at epoch 1, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 400 occurrences\n",
      "test - Value 1: 52 occurrences\n",
      "epoch-1   lr=['8.0000000'], tr/val_loss: 55.828255/ 29.185068, val:  60.62%, val_best:  60.62%, tr:  82.02%, tr_best:  82.02%, epoch time: 132.32 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.5761%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32256 real_backward_count 10510  32.583%\n",
      "layer   1  Sparsity: 81.4453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 18045.0\n",
      "lif layer 1 self.abs_max_v: 18045.0\n",
      "fc layer 1 self.abs_max_out: 18213.0\n",
      "lif layer 1 self.abs_max_v: 18213.0\n",
      "fc layer 1 self.abs_max_out: 18322.0\n",
      "lif layer 1 self.abs_max_v: 18322.0\n",
      "fc layer 1 self.abs_max_out: 18492.0\n",
      "lif layer 1 self.abs_max_v: 18492.0\n",
      "fc layer 1 self.abs_max_out: 18513.0\n",
      "lif layer 1 self.abs_max_v: 18513.0\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 799.00 at epoch 2, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 10 occurrences\n",
      "test - Value 1: 442 occurrences\n",
      "epoch-2   lr=['8.0000000'], tr/val_loss: 55.362289/ 75.270172, val:  51.77%, val_best:  60.62%, tr:  85.49%, tr_best:  85.49%, epoch time: 132.45 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.6583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48384 real_backward_count 14728  30.440%\n",
      "layer   1  Sparsity: 72.9980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 19294.0\n",
      "lif layer 1 self.abs_max_v: 19294.0\n",
      "fc layer 1 self.abs_max_out: 19450.0\n",
      "lif layer 1 self.abs_max_v: 19450.0\n",
      "lif layer 2 self.abs_max_v: 3439.5\n",
      "fc layer 1 self.abs_max_out: 19506.0\n",
      "lif layer 1 self.abs_max_v: 19506.0\n",
      "fc layer 1 self.abs_max_out: 19674.0\n",
      "lif layer 1 self.abs_max_v: 19674.0\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2076 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 39 occurrences\n",
      "test - Value 1: 413 occurrences\n",
      "epoch-3   lr=['8.0000000'], tr/val_loss: 57.358280/ 60.920681, val:  56.86%, val_best:  60.62%, tr:  87.40%, tr_best:  87.40%, epoch time: 131.38 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64512 real_backward_count 18587  28.812%\n",
      "layer   1  Sparsity: 73.9258%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 19711.0\n",
      "lif layer 1 self.abs_max_v: 19711.0\n",
      "fc layer 1 self.abs_max_out: 19758.0\n",
      "lif layer 1 self.abs_max_v: 19758.0\n",
      "fc layer 1 self.abs_max_out: 20018.0\n",
      "lif layer 1 self.abs_max_v: 20018.0\n",
      "fc layer 1 self.abs_max_out: 20179.0\n",
      "lif layer 1 self.abs_max_v: 20179.0\n",
      "fc layer 1 self.abs_max_out: 20198.0\n",
      "lif layer 1 self.abs_max_v: 20198.0\n",
      "lif layer 2 self.abs_max_v: 3518.5\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 253 occurrences\n",
      "test - Value 1: 199 occurrences\n",
      "epoch-4   lr=['8.0000000'], tr/val_loss: 57.857243/ 34.456203, val:  79.87%, val_best:  79.87%, tr:  89.46%, tr_best:  89.46%, epoch time: 131.36 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2408%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 80640 real_backward_count 22399  27.777%\n",
      "layer   1  Sparsity: 82.3730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 20571.0\n",
      "lif layer 1 self.abs_max_v: 20571.0\n",
      "lif layer 2 self.abs_max_v: 3569.5\n",
      "lif layer 2 self.abs_max_v: 3644.0\n",
      "fc layer 1 self.abs_max_out: 20611.0\n",
      "lif layer 1 self.abs_max_v: 20611.0\n",
      "lif layer 2 self.abs_max_v: 3725.5\n",
      "fc layer 1 self.abs_max_out: 20798.0\n",
      "lif layer 1 self.abs_max_v: 20798.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1002.00 at epoch 5, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 83 occurrences\n",
      "test - Value 1: 369 occurrences\n",
      "epoch-5   lr=['8.0000000'], tr/val_loss: 55.874557/ 46.736523, val:  65.27%, val_best:  79.87%, tr:  88.74%, tr_best:  89.46%, epoch time: 131.68 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3280%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96768 real_backward_count 26153  27.026%\n",
      "layer   1  Sparsity: 82.8613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2478.0\n",
      "fc layer 1 self.abs_max_out: 20849.0\n",
      "lif layer 1 self.abs_max_v: 20849.0\n",
      "fc layer 1 self.abs_max_out: 20867.0\n",
      "lif layer 1 self.abs_max_v: 20867.0\n",
      "fc layer 1 self.abs_max_out: 21111.0\n",
      "lif layer 1 self.abs_max_v: 21111.0\n",
      "train - Value 0: 1969 occurrences\n",
      "train - Value 1: 2063 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1150.00 at epoch 6, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 15 occurrences\n",
      "test - Value 1: 437 occurrences\n",
      "epoch-6   lr=['8.0000000'], tr/val_loss: 58.610432/ 84.084656, val:  53.32%, val_best:  79.87%, tr:  90.15%, tr_best:  90.15%, epoch time: 131.18 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.0281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 112896 real_backward_count 29847  26.438%\n",
      "layer   1  Sparsity: 84.5215%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2545.0\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "fc layer 2 self.abs_max_out: 2560.0\n",
      "fc layer 1 self.abs_max_out: 21370.0\n",
      "lif layer 1 self.abs_max_v: 21370.0\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2044 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 270 occurrences\n",
      "test - Value 1: 182 occurrences\n",
      "epoch-7   lr=['8.0000000'], tr/val_loss: 60.987675/ 26.883160, val:  84.96%, val_best:  84.96%, tr:  90.77%, tr_best:  90.77%, epoch time: 130.69 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 129024 real_backward_count 33318  25.823%\n",
      "layer   1  Sparsity: 78.8086%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2571.0\n",
      "fc layer 2 self.abs_max_out: 2600.0\n",
      "fc layer 2 self.abs_max_out: 2688.0\n",
      "train - Value 0: 1993 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 332 occurrences\n",
      "test - Value 1: 120 occurrences\n",
      "epoch-8   lr=['8.0000000'], tr/val_loss: 60.703701/ 31.171738, val:  75.66%, val_best:  84.96%, tr:  90.65%, tr_best:  90.77%, epoch time: 131.66 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 145152 real_backward_count 36784  25.342%\n",
      "layer   1  Sparsity: 81.1523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 21623.0\n",
      "lif layer 1 self.abs_max_v: 21623.0\n",
      "train - Value 0: 1987 occurrences\n",
      "train - Value 1: 2045 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 45 occurrences\n",
      "test - Value 1: 407 occurrences\n",
      "epoch-9   lr=['8.0000000'], tr/val_loss: 56.944218/ 62.607651, val:  59.07%, val_best:  84.96%, tr:  91.54%, tr_best:  91.54%, epoch time: 131.17 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0227%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161280 real_backward_count 40161  24.901%\n",
      "layer   1  Sparsity: 76.7090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2771.0\n",
      "fc layer 1 self.abs_max_out: 21885.0\n",
      "lif layer 1 self.abs_max_v: 21885.0\n",
      "fc layer 2 self.abs_max_out: 2859.0\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2053 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 273 occurrences\n",
      "test - Value 1: 179 occurrences\n",
      "epoch-10  lr=['8.0000000'], tr/val_loss: 57.460976/ 39.293751, val:  82.52%, val_best:  84.96%, tr:  91.94%, tr_best:  91.94%, epoch time: 130.08 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 177408 real_backward_count 43566  24.557%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2889.0\n",
      "fc layer 2 self.abs_max_out: 2903.0\n",
      "lif layer 2 self.abs_max_v: 3813.5\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-11  lr=['8.0000000'], tr/val_loss: 60.766052/ 56.986870, val:  58.85%, val_best:  84.96%, tr:  92.31%, tr_best:  92.31%, epoch time: 131.87 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3594%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193536 real_backward_count 46940  24.254%\n",
      "layer   1  Sparsity: 82.2754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2908.0\n",
      "fc layer 2 self.abs_max_out: 2913.0\n",
      "fc layer 1 self.abs_max_out: 22060.0\n",
      "lif layer 1 self.abs_max_v: 22060.0\n",
      "train - Value 0: 1981 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-12  lr=['8.0000000'], tr/val_loss: 64.730621/ 42.646782, val:  73.89%, val_best:  84.96%, tr:  93.73%, tr_best:  93.73%, epoch time: 130.44 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 209664 real_backward_count 50174  23.931%\n",
      "layer   1  Sparsity: 87.1094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2917.0\n",
      "fc layer 1 self.abs_max_out: 22225.0\n",
      "lif layer 1 self.abs_max_v: 22225.0\n",
      "fc layer 1 self.abs_max_out: 22238.0\n",
      "lif layer 1 self.abs_max_v: 22238.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-13  lr=['8.0000000'], tr/val_loss: 61.182549/ 64.060150, val:  66.37%, val_best:  84.96%, tr:  93.80%, tr_best:  93.80%, epoch time: 130.94 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225792 real_backward_count 53328  23.618%\n",
      "layer   1  Sparsity: 89.1602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3051.0\n",
      "fc layer 1 self.abs_max_out: 22258.0\n",
      "lif layer 1 self.abs_max_v: 22258.0\n",
      "lif layer 2 self.abs_max_v: 3901.5\n",
      "fc layer 2 self.abs_max_out: 3145.0\n",
      "fc layer 1 self.abs_max_out: 22457.0\n",
      "lif layer 1 self.abs_max_v: 22457.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 141 occurrences\n",
      "test - Value 1: 311 occurrences\n",
      "epoch-14  lr=['8.0000000'], tr/val_loss: 60.194000/ 48.039169, val:  78.10%, val_best:  84.96%, tr:  93.77%, tr_best:  93.80%, epoch time: 130.65 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6280%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 241920 real_backward_count 56471  23.343%\n",
      "layer   1  Sparsity: 92.3340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 86.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3163.0\n",
      "fc layer 1 self.abs_max_out: 22625.0\n",
      "lif layer 1 self.abs_max_v: 22625.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 161 occurrences\n",
      "test - Value 1: 291 occurrences\n",
      "epoch-15  lr=['8.0000000'], tr/val_loss: 61.931248/ 62.702457, val:  77.65%, val_best:  84.96%, tr:  95.06%, tr_best:  95.06%, epoch time: 130.89 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4782%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 258048 real_backward_count 59425  23.029%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 3935.0\n",
      "lif layer 2 self.abs_max_v: 4193.0\n",
      "fc layer 1 self.abs_max_out: 22885.0\n",
      "lif layer 1 self.abs_max_v: 22885.0\n",
      "lif layer 2 self.abs_max_v: 4301.0\n",
      "fc layer 2 self.abs_max_out: 3191.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 217 occurrences\n",
      "test - Value 1: 235 occurrences\n",
      "epoch-16  lr=['8.0000000'], tr/val_loss: 62.443195/ 37.554817, val:  80.31%, val_best:  84.96%, tr:  93.97%, tr_best:  95.06%, epoch time: 131.74 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274176 real_backward_count 62497  22.794%\n",
      "layer   1  Sparsity: 72.9004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 22942.0\n",
      "lif layer 1 self.abs_max_v: 22942.0\n",
      "lif layer 2 self.abs_max_v: 4307.5\n",
      "lif layer 2 self.abs_max_v: 4642.5\n",
      "fc layer 2 self.abs_max_out: 3353.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 61 occurrences\n",
      "test - Value 1: 391 occurrences\n",
      "epoch-17  lr=['8.0000000'], tr/val_loss: 62.380161/ 73.834419, val:  62.17%, val_best:  84.96%, tr:  94.92%, tr_best:  95.06%, epoch time: 130.45 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290304 real_backward_count 65423  22.536%\n",
      "layer   1  Sparsity: 72.6562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 23079.0\n",
      "lif layer 1 self.abs_max_v: 23079.0\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 136 occurrences\n",
      "test - Value 1: 316 occurrences\n",
      "epoch-18  lr=['8.0000000'], tr/val_loss: 65.130196/ 48.743851, val:  76.11%, val_best:  84.96%, tr:  94.79%, tr_best:  95.06%, epoch time: 130.52 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1008%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 306432 real_backward_count 68331  22.299%\n",
      "layer   1  Sparsity: 86.9141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 23177.0\n",
      "lif layer 1 self.abs_max_v: 23177.0\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 320 occurrences\n",
      "test - Value 1: 132 occurrences\n",
      "epoch-19  lr=['8.0000000'], tr/val_loss: 62.838512/ 50.319675, val:  75.66%, val_best:  84.96%, tr:  95.61%, tr_best:  95.61%, epoch time: 130.83 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7245%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322560 real_backward_count 71134  22.053%\n",
      "layer   1  Sparsity: 78.4180%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 23221.0\n",
      "lif layer 1 self.abs_max_v: 23221.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 281 occurrences\n",
      "test - Value 1: 171 occurrences\n",
      "epoch-20  lr=['8.0000000'], tr/val_loss: 60.777348/ 64.011971, val:  78.98%, val_best:  84.96%, tr:  96.01%, tr_best:  96.01%, epoch time: 130.93 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6058%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 338688 real_backward_count 73952  21.835%\n",
      "layer   1  Sparsity: 84.8633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 23393.0\n",
      "lif layer 1 self.abs_max_v: 23393.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-21  lr=['8.0000000'], tr/val_loss: 66.455528/ 73.332397, val:  74.56%, val_best:  84.96%, tr:  95.01%, tr_best:  96.01%, epoch time: 131.45 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354816 real_backward_count 76772  21.637%\n",
      "layer   1  Sparsity: 75.5371%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 23544.0\n",
      "lif layer 1 self.abs_max_v: 23544.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-22  lr=['8.0000000'], tr/val_loss: 64.035393/ 67.865356, val:  64.82%, val_best:  84.96%, tr:  95.56%, tr_best:  96.01%, epoch time: 131.02 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 370944 real_backward_count 79683  21.481%\n",
      "layer   1  Sparsity: 74.2188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4644.5\n",
      "lif layer 2 self.abs_max_v: 4684.0\n",
      "fc layer 1 self.abs_max_out: 23683.0\n",
      "lif layer 1 self.abs_max_v: 23683.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 334 occurrences\n",
      "test - Value 1: 118 occurrences\n",
      "epoch-23  lr=['8.0000000'], tr/val_loss: 58.784515/ 22.890057, val:  74.78%, val_best:  84.96%, tr:  93.53%, tr_best:  96.01%, epoch time: 130.53 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 387072 real_backward_count 82809  21.394%\n",
      "layer   1  Sparsity: 72.9492%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 23728.0\n",
      "lif layer 1 self.abs_max_v: 23728.0\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-24  lr=['8.0000000'], tr/val_loss: 64.330215/ 59.835052, val:  78.76%, val_best:  84.96%, tr:  95.36%, tr_best:  96.01%, epoch time: 130.79 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 403200 real_backward_count 85724  21.261%\n",
      "layer   1  Sparsity: 70.5078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 23755.0\n",
      "lif layer 1 self.abs_max_v: 23755.0\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 175 occurrences\n",
      "test - Value 1: 277 occurrences\n",
      "epoch-25  lr=['8.0000000'], tr/val_loss: 62.351433/ 35.738560, val:  77.65%, val_best:  84.96%, tr:  95.19%, tr_best:  96.01%, epoch time: 131.05 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419328 real_backward_count 88685  21.149%\n",
      "layer   1  Sparsity: 75.4395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 23955.0\n",
      "lif layer 1 self.abs_max_v: 23955.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 148 occurrences\n",
      "test - Value 1: 304 occurrences\n",
      "epoch-26  lr=['8.0000000'], tr/val_loss: 62.600460/ 75.118011, val:  76.55%, val_best:  84.96%, tr:  95.21%, tr_best:  96.01%, epoch time: 131.54 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2225%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 435456 real_backward_count 91627  21.042%\n",
      "layer   1  Sparsity: 70.8984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3382.0\n",
      "fc layer 1 self.abs_max_out: 24001.0\n",
      "lif layer 1 self.abs_max_v: 24001.0\n",
      "fc layer 2 self.abs_max_out: 3436.0\n",
      "lif layer 2 self.abs_max_v: 4739.5\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 246 occurrences\n",
      "test - Value 1: 206 occurrences\n",
      "epoch-27  lr=['8.0000000'], tr/val_loss: 61.180946/ 66.639572, val:  84.07%, val_best:  84.96%, tr:  95.04%, tr_best:  96.01%, epoch time: 131.36 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451584 real_backward_count 94547  20.937%\n",
      "layer   1  Sparsity: 78.0273%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4837.0\n",
      "lif layer 2 self.abs_max_v: 5114.5\n",
      "fc layer 2 self.abs_max_out: 3557.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 258 occurrences\n",
      "test - Value 1: 194 occurrences\n",
      "epoch-28  lr=['8.0000000'], tr/val_loss: 63.347660/ 75.991028, val:  82.30%, val_best:  84.96%, tr:  95.11%, tr_best:  96.01%, epoch time: 130.73 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 467712 real_backward_count 97392  20.823%\n",
      "layer   1  Sparsity: 78.2227%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3595.0\n",
      "fc layer 1 self.abs_max_out: 24146.0\n",
      "lif layer 1 self.abs_max_v: 24146.0\n",
      "train - Value 0: 2047 occurrences\n",
      "train - Value 1: 1985 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 18 occurrences\n",
      "test - Value 1: 434 occurrences\n",
      "epoch-29  lr=['8.0000000'], tr/val_loss: 63.292427/ 90.752190, val:  53.54%, val_best:  84.96%, tr:  96.01%, tr_best:  96.01%, epoch time: 131.40 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483840 real_backward_count 100165  20.702%\n",
      "layer   1  Sparsity: 88.4766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5116.0\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 251 occurrences\n",
      "test - Value 1: 201 occurrences\n",
      "epoch-30  lr=['8.0000000'], tr/val_loss: 65.392319/ 51.220299, val:  81.19%, val_best:  84.96%, tr:  95.63%, tr_best:  96.01%, epoch time: 131.82 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499968 real_backward_count 102896  20.581%\n",
      "layer   1  Sparsity: 67.7734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24235.0\n",
      "lif layer 1 self.abs_max_v: 24235.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 50 occurrences\n",
      "test - Value 1: 402 occurrences\n",
      "epoch-31  lr=['8.0000000'], tr/val_loss: 63.983532/ 96.448647, val:  60.18%, val_best:  84.96%, tr:  95.49%, tr_best:  96.01%, epoch time: 130.56 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 516096 real_backward_count 105661  20.473%\n",
      "layer   1  Sparsity: 71.8750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3615.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 105 occurrences\n",
      "test - Value 1: 347 occurrences\n",
      "epoch-32  lr=['8.0000000'], tr/val_loss: 63.110619/ 67.947723, val:  71.46%, val_best:  84.96%, tr:  95.88%, tr_best:  96.01%, epoch time: 130.69 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 532224 real_backward_count 108370  20.362%\n",
      "layer   1  Sparsity: 88.1836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2048 occurrences\n",
      "train - Value 1: 1984 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-33  lr=['8.0000000'], tr/val_loss: 62.710251/ 54.356396, val:  73.45%, val_best:  84.96%, tr:  96.58%, tr_best:  96.58%, epoch time: 131.52 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548352 real_backward_count 111070  20.255%\n",
      "layer   1  Sparsity: 72.9492%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24440.0\n",
      "lif layer 1 self.abs_max_v: 24440.0\n",
      "fc layer 2 self.abs_max_out: 3774.0\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 242 occurrences\n",
      "test - Value 1: 210 occurrences\n",
      "epoch-34  lr=['8.0000000'], tr/val_loss: 56.328716/ 39.378860, val:  80.09%, val_best:  84.96%, tr:  95.31%, tr_best:  96.58%, epoch time: 131.48 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9191%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 564480 real_backward_count 113898  20.178%\n",
      "layer   1  Sparsity: 86.8164%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5299.5\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 195 occurrences\n",
      "test - Value 1: 257 occurrences\n",
      "epoch-35  lr=['8.0000000'], tr/val_loss: 58.935104/ 75.236778, val:  82.08%, val_best:  84.96%, tr:  95.61%, tr_best:  96.58%, epoch time: 130.65 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580608 real_backward_count 116605  20.083%\n",
      "layer   1  Sparsity: 93.5547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5402.5\n",
      "fc layer 2 self.abs_max_out: 3795.0\n",
      "lif layer 2 self.abs_max_v: 5427.5\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 244 occurrences\n",
      "test - Value 1: 208 occurrences\n",
      "epoch-36  lr=['8.0000000'], tr/val_loss: 62.469849/ 31.423138, val:  84.07%, val_best:  84.96%, tr:  96.65%, tr_best:  96.65%, epoch time: 130.70 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 596736 real_backward_count 119264  19.986%\n",
      "layer   1  Sparsity: 79.5898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5459.5\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 200 occurrences\n",
      "test - Value 1: 252 occurrences\n",
      "epoch-37  lr=['8.0000000'], tr/val_loss: 60.097893/ 49.680107, val:  80.97%, val_best:  84.96%, tr:  96.08%, tr_best:  96.65%, epoch time: 131.29 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612864 real_backward_count 122010  19.908%\n",
      "layer   1  Sparsity: 68.7988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 77 occurrences\n",
      "test - Value 1: 375 occurrences\n",
      "epoch-38  lr=['8.0000000'], tr/val_loss: 67.183380/ 95.012993, val:  65.27%, val_best:  84.96%, tr:  96.78%, tr_best:  96.78%, epoch time: 131.01 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8086%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 628992 real_backward_count 124670  19.821%\n",
      "layer   1  Sparsity: 81.1523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24531.0\n",
      "lif layer 1 self.abs_max_v: 24531.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-39  lr=['8.0000000'], tr/val_loss: 68.330200/ 55.478260, val:  74.56%, val_best:  84.96%, tr:  96.28%, tr_best:  96.78%, epoch time: 131.46 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9776%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 645120 real_backward_count 127298  19.732%\n",
      "layer   1  Sparsity: 75.8789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24678.0\n",
      "lif layer 1 self.abs_max_v: 24678.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-40  lr=['8.0000000'], tr/val_loss: 59.379711/ 80.780212, val:  78.76%, val_best:  84.96%, tr:  96.43%, tr_best:  96.78%, epoch time: 131.10 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 661248 real_backward_count 130013  19.662%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24728.0\n",
      "lif layer 1 self.abs_max_v: 24728.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 129 occurrences\n",
      "test - Value 1: 323 occurrences\n",
      "epoch-41  lr=['8.0000000'], tr/val_loss: 61.815182/ 44.796116, val:  75.00%, val_best:  84.96%, tr:  96.58%, tr_best:  96.78%, epoch time: 131.14 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677376 real_backward_count 132612  19.577%\n",
      "layer   1  Sparsity: 67.2852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 207 occurrences\n",
      "test - Value 1: 245 occurrences\n",
      "epoch-42  lr=['8.0000000'], tr/val_loss: 63.673553/ 44.581707, val:  82.96%, val_best:  84.96%, tr:  96.95%, tr_best:  96.95%, epoch time: 131.43 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 693504 real_backward_count 135185  19.493%\n",
      "layer   1  Sparsity: 78.5645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 249 occurrences\n",
      "test - Value 1: 203 occurrences\n",
      "epoch-43  lr=['8.0000000'], tr/val_loss: 64.662109/ 66.463417, val:  83.85%, val_best:  84.96%, tr:  97.15%, tr_best:  97.15%, epoch time: 130.67 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709632 real_backward_count 137682  19.402%\n",
      "layer   1  Sparsity: 88.1348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24733.0\n",
      "lif layer 1 self.abs_max_v: 24733.0\n",
      "fc layer 2 self.abs_max_out: 3820.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 161 occurrences\n",
      "test - Value 1: 291 occurrences\n",
      "epoch-44  lr=['8.0000000'], tr/val_loss: 62.480946/ 58.485710, val:  76.33%, val_best:  84.96%, tr:  97.42%, tr_best:  97.42%, epoch time: 131.26 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1994%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 725760 real_backward_count 140268  19.327%\n",
      "layer   1  Sparsity: 80.2734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3841.0\n",
      "fc layer 2 self.abs_max_out: 3862.0\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 211 occurrences\n",
      "test - Value 1: 241 occurrences\n",
      "epoch-45  lr=['8.0000000'], tr/val_loss: 65.144638/ 36.446445, val:  82.52%, val_best:  84.96%, tr:  96.43%, tr_best:  97.42%, epoch time: 130.79 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.4262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741888 real_backward_count 142941  19.267%\n",
      "layer   1  Sparsity: 92.6270%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 78.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24775.0\n",
      "lif layer 1 self.abs_max_v: 24775.0\n",
      "fc layer 2 self.abs_max_out: 3892.0\n",
      "fc layer 2 self.abs_max_out: 3905.0\n",
      "fc layer 2 self.abs_max_out: 3954.0\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 194 occurrences\n",
      "test - Value 1: 258 occurrences\n",
      "epoch-46  lr=['8.0000000'], tr/val_loss: 68.828682/ 41.375481, val:  80.09%, val_best:  84.96%, tr:  97.69%, tr_best:  97.69%, epoch time: 130.66 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 758016 real_backward_count 145340  19.174%\n",
      "layer   1  Sparsity: 70.5566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24871.0\n",
      "lif layer 1 self.abs_max_v: 24871.0\n",
      "lif layer 2 self.abs_max_v: 5512.0\n",
      "lif layer 2 self.abs_max_v: 5563.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 211 occurrences\n",
      "test - Value 1: 241 occurrences\n",
      "epoch-47  lr=['8.0000000'], tr/val_loss: 62.370575/ 34.550819, val:  82.52%, val_best:  84.96%, tr:  97.79%, tr_best:  97.79%, epoch time: 131.22 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 774144 real_backward_count 147709  19.080%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5787.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-48  lr=['8.0000000'], tr/val_loss: 72.817696/ 69.228775, val:  76.77%, val_best:  84.96%, tr:  97.89%, tr_best:  97.89%, epoch time: 131.35 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1353%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 790272 real_backward_count 150056  18.988%\n",
      "layer   1  Sparsity: 88.2812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 170 occurrences\n",
      "test - Value 1: 282 occurrences\n",
      "epoch-49  lr=['8.0000000'], tr/val_loss: 76.134796/ 85.434837, val:  79.65%, val_best:  84.96%, tr:  97.59%, tr_best:  97.89%, epoch time: 129.96 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806400 real_backward_count 152528  18.915%\n",
      "layer   1  Sparsity: 80.5664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 113 occurrences\n",
      "test - Value 1: 339 occurrences\n",
      "epoch-50  lr=['8.0000000'], tr/val_loss: 72.845551/ 81.463844, val:  72.35%, val_best:  84.96%, tr:  97.25%, tr_best:  97.89%, epoch time: 130.62 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822528 real_backward_count 155060  18.852%\n",
      "layer   1  Sparsity: 73.0957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 283 occurrences\n",
      "test - Value 1: 169 occurrences\n",
      "epoch-51  lr=['8.0000000'], tr/val_loss: 75.671669/ 46.776649, val:  79.87%, val_best:  84.96%, tr:  97.22%, tr_best:  97.89%, epoch time: 131.40 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8480%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838656 real_backward_count 157616  18.794%\n",
      "layer   1  Sparsity: 86.4746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 1 self.abs_max_out: 24947.0\n",
      "lif layer 1 self.abs_max_v: 24947.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 182 occurrences\n",
      "test - Value 1: 270 occurrences\n",
      "epoch-52  lr=['8.0000000'], tr/val_loss: 77.088799/ 83.771065, val:  82.30%, val_best:  84.96%, tr:  98.04%, tr_best:  98.04%, epoch time: 131.20 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 854784 real_backward_count 159993  18.717%\n",
      "layer   1  Sparsity: 90.2344%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 24960.0\n",
      "lif layer 1 self.abs_max_v: 24960.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-53  lr=['8.0000000'], tr/val_loss: 75.947266/ 90.913704, val:  66.37%, val_best:  84.96%, tr:  97.79%, tr_best:  98.04%, epoch time: 130.43 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870912 real_backward_count 162423  18.650%\n",
      "layer   1  Sparsity: 84.5703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25049.0\n",
      "lif layer 1 self.abs_max_v: 25049.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 173 occurrences\n",
      "test - Value 1: 279 occurrences\n",
      "epoch-54  lr=['8.0000000'], tr/val_loss: 73.852570/ 75.895721, val:  80.31%, val_best:  84.96%, tr:  97.50%, tr_best:  98.04%, epoch time: 130.99 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 887040 real_backward_count 164979  18.599%\n",
      "layer   1  Sparsity: 79.2480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 292 occurrences\n",
      "test - Value 1: 160 occurrences\n",
      "epoch-55  lr=['8.0000000'], tr/val_loss: 77.371651/ 48.879627, val:  79.20%, val_best:  84.96%, tr:  97.59%, tr_best:  98.04%, epoch time: 132.41 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 903168 real_backward_count 167582  18.555%\n",
      "layer   1  Sparsity: 79.7852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-56  lr=['8.0000000'], tr/val_loss: 82.220886/ 69.567970, val:  75.66%, val_best:  84.96%, tr:  97.84%, tr_best:  98.04%, epoch time: 131.41 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 919296 real_backward_count 170012  18.494%\n",
      "layer   1  Sparsity: 88.8672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25072.0\n",
      "lif layer 1 self.abs_max_v: 25072.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 125 occurrences\n",
      "test - Value 1: 327 occurrences\n",
      "epoch-57  lr=['8.0000000'], tr/val_loss: 75.217697/ 80.529236, val:  73.67%, val_best:  84.96%, tr:  97.89%, tr_best:  98.04%, epoch time: 131.49 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4790%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 935424 real_backward_count 172383  18.428%\n",
      "layer   1  Sparsity: 90.4297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25077.0\n",
      "lif layer 1 self.abs_max_v: 25077.0\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 196 occurrences\n",
      "test - Value 1: 256 occurrences\n",
      "epoch-58  lr=['8.0000000'], tr/val_loss: 76.018097/ 88.991638, val:  80.97%, val_best:  84.96%, tr:  97.74%, tr_best:  98.04%, epoch time: 131.34 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 951552 real_backward_count 174807  18.371%\n",
      "layer   1  Sparsity: 77.2949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 286 occurrences\n",
      "test - Value 1: 166 occurrences\n",
      "epoch-59  lr=['8.0000000'], tr/val_loss: 76.150551/ 50.533211, val:  81.42%, val_best:  84.96%, tr:  97.25%, tr_best:  98.04%, epoch time: 131.55 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967680 real_backward_count 177356  18.328%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 99 occurrences\n",
      "test - Value 1: 353 occurrences\n",
      "epoch-60  lr=['8.0000000'], tr/val_loss: 67.535164/ 78.060364, val:  69.69%, val_best:  84.96%, tr:  97.42%, tr_best:  98.04%, epoch time: 131.35 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 983808 real_backward_count 179914  18.288%\n",
      "layer   1  Sparsity: 72.7539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 210 occurrences\n",
      "test - Value 1: 242 occurrences\n",
      "epoch-61  lr=['8.0000000'], tr/val_loss: 73.425079/ 51.305157, val:  84.96%, val_best:  84.96%, tr:  97.17%, tr_best:  98.04%, epoch time: 130.76 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999936 real_backward_count 182347  18.236%\n",
      "layer   1  Sparsity: 77.0020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 163 occurrences\n",
      "test - Value 1: 289 occurrences\n",
      "epoch-62  lr=['8.0000000'], tr/val_loss: 71.098457/ 54.279633, val:  75.88%, val_best:  84.96%, tr:  97.52%, tr_best:  98.04%, epoch time: 131.14 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8088%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1016064 real_backward_count 184811  18.189%\n",
      "layer   1  Sparsity: 78.7598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-63  lr=['8.0000000'], tr/val_loss: 72.222160/ 65.664543, val:  84.73%, val_best:  84.96%, tr:  97.17%, tr_best:  98.04%, epoch time: 130.98 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1032192 real_backward_count 187207  18.137%\n",
      "layer   1  Sparsity: 76.8066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 185 occurrences\n",
      "test - Value 1: 267 occurrences\n",
      "epoch-64  lr=['8.0000000'], tr/val_loss: 72.673882/ 43.086628, val:  82.52%, val_best:  84.96%, tr:  97.89%, tr_best:  98.04%, epoch time: 131.10 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1048320 real_backward_count 189577  18.084%\n",
      "layer   1  Sparsity: 82.6172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25092.0\n",
      "lif layer 1 self.abs_max_v: 25092.0\n",
      "fc layer 2 self.abs_max_out: 3963.0\n",
      "fc layer 2 self.abs_max_out: 3977.0\n",
      "fc layer 2 self.abs_max_out: 3983.0\n",
      "fc layer 2 self.abs_max_out: 4007.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-65  lr=['8.0000000'], tr/val_loss: 74.662964/ 52.407902, val:  79.20%, val_best:  84.96%, tr:  97.97%, tr_best:  98.04%, epoch time: 131.41 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1064448 real_backward_count 192044  18.042%\n",
      "layer   1  Sparsity: 88.2812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25100.0\n",
      "lif layer 1 self.abs_max_v: 25100.0\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 150 occurrences\n",
      "test - Value 1: 302 occurrences\n",
      "epoch-66  lr=['8.0000000'], tr/val_loss: 72.618118/ 67.295868, val:  79.20%, val_best:  84.96%, tr:  96.55%, tr_best:  98.04%, epoch time: 131.84 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1080576 real_backward_count 194840  18.031%\n",
      "layer   1  Sparsity: 75.4395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4126.0\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-67  lr=['8.0000000'], tr/val_loss: 78.392952/ 57.294697, val:  79.87%, val_best:  84.96%, tr:  97.30%, tr_best:  98.04%, epoch time: 130.38 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096704 real_backward_count 197507  18.009%\n",
      "layer   1  Sparsity: 81.1523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 174 occurrences\n",
      "test - Value 1: 278 occurrences\n",
      "epoch-68  lr=['8.0000000'], tr/val_loss: 76.336899/ 68.765541, val:  80.97%, val_best:  84.96%, tr:  97.69%, tr_best:  98.04%, epoch time: 130.13 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0576%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1112832 real_backward_count 199990  17.971%\n",
      "layer   1  Sparsity: 88.5254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25169.0\n",
      "lif layer 1 self.abs_max_v: 25169.0\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 226 occurrences\n",
      "test - Value 1: 226 occurrences\n",
      "epoch-69  lr=['8.0000000'], tr/val_loss: 72.611847/ 40.788635, val:  84.96%, val_best:  84.96%, tr:  97.69%, tr_best:  98.04%, epoch time: 130.86 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128960 real_backward_count 202469  17.934%\n",
      "layer   1  Sparsity: 65.8691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4216.0\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 215 occurrences\n",
      "test - Value 1: 237 occurrences\n",
      "epoch-70  lr=['8.0000000'], tr/val_loss: 71.282158/ 61.277756, val:  81.19%, val_best:  84.96%, tr:  97.77%, tr_best:  98.04%, epoch time: 131.12 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145088 real_backward_count 204967  17.900%\n",
      "layer   1  Sparsity: 70.4102%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-71  lr=['8.0000000'], tr/val_loss: 75.229652/102.927994, val:  69.03%, val_best:  84.96%, tr:  97.92%, tr_best:  98.04%, epoch time: 130.46 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1161216 real_backward_count 207375  17.858%\n",
      "layer   1  Sparsity: 78.5645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-72  lr=['8.0000000'], tr/val_loss: 78.487267/ 79.023598, val:  76.33%, val_best:  84.96%, tr:  98.41%, tr_best:  98.41%, epoch time: 130.94 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1177344 real_backward_count 209715  17.813%\n",
      "layer   1  Sparsity: 80.5176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4294.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 65 occurrences\n",
      "test - Value 1: 387 occurrences\n",
      "epoch-73  lr=['8.0000000'], tr/val_loss: 78.363747/ 99.165657, val:  63.94%, val_best:  84.96%, tr:  98.09%, tr_best:  98.41%, epoch time: 130.58 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1193472 real_backward_count 212020  17.765%\n",
      "layer   1  Sparsity: 85.9375%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25318.0\n",
      "lif layer 1 self.abs_max_v: 25318.0\n",
      "lif layer 2 self.abs_max_v: 5821.0\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 194 occurrences\n",
      "test - Value 1: 258 occurrences\n",
      "epoch-74  lr=['8.0000000'], tr/val_loss: 71.422455/ 72.338547, val:  82.74%, val_best:  84.96%, tr:  97.92%, tr_best:  98.41%, epoch time: 130.97 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1209600 real_backward_count 214303  17.717%\n",
      "layer   1  Sparsity: 84.5215%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5854.5\n",
      "fc layer 2 self.abs_max_out: 4384.0\n",
      "fc layer 1 self.abs_max_out: 25455.0\n",
      "lif layer 1 self.abs_max_v: 25455.0\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 247 occurrences\n",
      "test - Value 1: 205 occurrences\n",
      "epoch-75  lr=['8.0000000'], tr/val_loss: 78.060852/ 59.694721, val:  83.41%, val_best:  84.96%, tr:  98.29%, tr_best:  98.41%, epoch time: 130.20 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225728 real_backward_count 216586  17.670%\n",
      "layer   1  Sparsity: 85.9375%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5995.5\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-76  lr=['8.0000000'], tr/val_loss: 82.023926/ 76.003738, val:  76.77%, val_best:  84.96%, tr:  98.02%, tr_best:  98.41%, epoch time: 130.67 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1241856 real_backward_count 218876  17.625%\n",
      "layer   1  Sparsity: 75.3418%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 152 occurrences\n",
      "test - Value 1: 300 occurrences\n",
      "epoch-77  lr=['8.0000000'], tr/val_loss: 81.637161/ 79.546165, val:  79.20%, val_best:  84.96%, tr:  98.51%, tr_best:  98.51%, epoch time: 131.93 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257984 real_backward_count 221254  17.588%\n",
      "layer   1  Sparsity: 76.5137%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4389.0\n",
      "lif layer 2 self.abs_max_v: 6029.5\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "max_activation_accul updated: 1211.00 at epoch 78, iter 4031\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 32 occurrences\n",
      "test - Value 1: 420 occurrences\n",
      "epoch-78  lr=['8.0000000'], tr/val_loss: 88.715042/121.988419, val:  57.08%, val_best:  84.96%, tr:  98.29%, tr_best:  98.51%, epoch time: 132.06 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1274112 real_backward_count 223472  17.539%\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6134.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 139 occurrences\n",
      "test - Value 1: 313 occurrences\n",
      "epoch-79  lr=['8.0000000'], tr/val_loss: 79.897217/ 57.685566, val:  74.56%, val_best:  84.96%, tr:  98.26%, tr_best:  98.51%, epoch time: 131.45 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1290240 real_backward_count 225795  17.500%\n",
      "layer   1  Sparsity: 81.8848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4421.0\n",
      "fc layer 2 self.abs_max_out: 4461.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 112 occurrences\n",
      "test - Value 1: 340 occurrences\n",
      "epoch-80  lr=['8.0000000'], tr/val_loss: 77.749969/ 63.984562, val:  71.68%, val_best:  84.96%, tr:  97.84%, tr_best:  98.51%, epoch time: 131.41 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1306368 real_backward_count 228078  17.459%\n",
      "layer   1  Sparsity: 88.1348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 196 occurrences\n",
      "test - Value 1: 256 occurrences\n",
      "epoch-81  lr=['8.0000000'], tr/val_loss: 75.611481/ 44.726654, val:  84.07%, val_best:  84.96%, tr:  98.39%, tr_best:  98.51%, epoch time: 131.32 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1322496 real_backward_count 230420  17.423%\n",
      "layer   1  Sparsity: 78.6621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 72 occurrences\n",
      "test - Value 1: 380 occurrences\n",
      "epoch-82  lr=['8.0000000'], tr/val_loss: 80.168198/ 89.156334, val:  65.49%, val_best:  84.96%, tr:  98.74%, tr_best:  98.74%, epoch time: 131.36 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1338624 real_backward_count 232643  17.379%\n",
      "layer   1  Sparsity: 72.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "lif layer 2 self.abs_max_v: 6156.5\n",
      "fc layer 1 self.abs_max_out: 25457.0\n",
      "lif layer 1 self.abs_max_v: 25457.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 66 occurrences\n",
      "test - Value 1: 386 occurrences\n",
      "epoch-83  lr=['8.0000000'], tr/val_loss: 78.581604/138.936569, val:  64.60%, val_best:  84.96%, tr:  98.24%, tr_best:  98.74%, epoch time: 130.75 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354752 real_backward_count 234921  17.341%\n",
      "layer   1  Sparsity: 66.5039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4499.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 67 occurrences\n",
      "test - Value 1: 385 occurrences\n",
      "epoch-84  lr=['8.0000000'], tr/val_loss: 79.772476/ 92.837227, val:  63.94%, val_best:  84.96%, tr:  98.04%, tr_best:  98.74%, epoch time: 130.68 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370880 real_backward_count 237259  17.307%\n",
      "layer   1  Sparsity: 71.7773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25458.0\n",
      "lif layer 1 self.abs_max_v: 25458.0\n",
      "lif layer 2 self.abs_max_v: 6191.0\n",
      "lif layer 2 self.abs_max_v: 6246.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 73 occurrences\n",
      "test - Value 1: 379 occurrences\n",
      "epoch-85  lr=['8.0000000'], tr/val_loss: 79.520012/111.861946, val:  66.15%, val_best:  84.96%, tr:  98.41%, tr_best:  98.74%, epoch time: 130.84 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2586%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1387008 real_backward_count 239479  17.266%\n",
      "layer   1  Sparsity: 71.2891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25482.0\n",
      "lif layer 1 self.abs_max_v: 25482.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 94 occurrences\n",
      "test - Value 1: 358 occurrences\n",
      "epoch-86  lr=['8.0000000'], tr/val_loss: 75.495636/ 80.181549, val:  69.03%, val_best:  84.96%, tr:  98.36%, tr_best:  98.74%, epoch time: 130.60 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1403136 real_backward_count 241713  17.227%\n",
      "layer   1  Sparsity: 78.3203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 129 occurrences\n",
      "test - Value 1: 323 occurrences\n",
      "epoch-87  lr=['8.0000000'], tr/val_loss: 84.467415/ 76.450775, val:  74.56%, val_best:  84.96%, tr:  98.12%, tr_best:  98.74%, epoch time: 131.44 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419264 real_backward_count 243961  17.189%\n",
      "layer   1  Sparsity: 78.3691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25498.0\n",
      "lif layer 1 self.abs_max_v: 25498.0\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-88  lr=['8.0000000'], tr/val_loss: 78.809196/ 94.273224, val:  67.70%, val_best:  84.96%, tr:  98.31%, tr_best:  98.74%, epoch time: 130.80 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1435392 real_backward_count 246173  17.150%\n",
      "layer   1  Sparsity: 91.2109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4583.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 119 occurrences\n",
      "test - Value 1: 333 occurrences\n",
      "epoch-89  lr=['8.0000000'], tr/val_loss: 81.980080/ 85.853630, val:  71.02%, val_best:  84.96%, tr:  97.99%, tr_best:  98.74%, epoch time: 130.06 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4785%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1451520 real_backward_count 248558  17.124%\n",
      "layer   1  Sparsity: 83.2031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6265.0\n",
      "lif layer 2 self.abs_max_v: 6332.0\n",
      "lif layer 2 self.abs_max_v: 6548.5\n",
      "lif layer 2 self.abs_max_v: 6557.5\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 161 occurrences\n",
      "test - Value 1: 291 occurrences\n",
      "epoch-90  lr=['8.0000000'], tr/val_loss: 81.258430/ 88.179489, val:  80.31%, val_best:  84.96%, tr:  98.07%, tr_best:  98.74%, epoch time: 131.09 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1467648 real_backward_count 250895  17.095%\n",
      "layer   1  Sparsity: 81.2012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6604.5\n",
      "lif layer 2 self.abs_max_v: 6645.5\n",
      "fc layer 1 self.abs_max_out: 25512.0\n",
      "lif layer 1 self.abs_max_v: 25512.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 215 occurrences\n",
      "test - Value 1: 237 occurrences\n",
      "epoch-91  lr=['8.0000000'], tr/val_loss: 75.171448/ 39.767437, val:  80.31%, val_best:  84.96%, tr:  97.82%, tr_best:  98.74%, epoch time: 130.91 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483776 real_backward_count 253288  17.071%\n",
      "layer   1  Sparsity: 79.5898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 27 occurrences\n",
      "test - Value 1: 425 occurrences\n",
      "epoch-92  lr=['8.0000000'], tr/val_loss: 80.314011/128.658142, val:  55.97%, val_best:  84.96%, tr:  98.26%, tr_best:  98.74%, epoch time: 131.22 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1499904 real_backward_count 255556  17.038%\n",
      "layer   1  Sparsity: 79.7852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 104 occurrences\n",
      "test - Value 1: 348 occurrences\n",
      "epoch-93  lr=['8.0000000'], tr/val_loss: 77.798630/ 92.324585, val:  70.80%, val_best:  84.96%, tr:  98.64%, tr_best:  98.74%, epoch time: 131.04 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1516032 real_backward_count 257801  17.005%\n",
      "layer   1  Sparsity: 75.1465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25544.0\n",
      "lif layer 1 self.abs_max_v: 25544.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 202 occurrences\n",
      "test - Value 1: 250 occurrences\n",
      "epoch-94  lr=['8.0000000'], tr/val_loss: 77.422165/ 41.531559, val:  83.19%, val_best:  84.96%, tr:  98.74%, tr_best:  98.74%, epoch time: 130.82 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1532160 real_backward_count 259996  16.969%\n",
      "layer   1  Sparsity: 66.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-95  lr=['8.0000000'], tr/val_loss: 72.528915/ 90.864296, val:  69.91%, val_best:  84.96%, tr:  98.12%, tr_best:  98.74%, epoch time: 131.02 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1548288 real_backward_count 262546  16.957%\n",
      "layer   1  Sparsity: 89.1602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25683.0\n",
      "lif layer 1 self.abs_max_v: 25683.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 116 occurrences\n",
      "test - Value 1: 336 occurrences\n",
      "epoch-96  lr=['8.0000000'], tr/val_loss: 65.495293/ 67.057777, val:  72.12%, val_best:  84.96%, tr:  97.59%, tr_best:  98.74%, epoch time: 131.71 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1564416 real_backward_count 265127  16.947%\n",
      "layer   1  Sparsity: 65.7227%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25738.0\n",
      "lif layer 1 self.abs_max_v: 25738.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 145 occurrences\n",
      "test - Value 1: 307 occurrences\n",
      "epoch-97  lr=['8.0000000'], tr/val_loss: 68.520302/ 60.802948, val:  77.65%, val_best:  84.96%, tr:  98.44%, tr_best:  98.74%, epoch time: 131.24 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1580544 real_backward_count 267558  16.928%\n",
      "layer   1  Sparsity: 71.1426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-98  lr=['8.0000000'], tr/val_loss: 74.824852/ 65.189766, val:  73.89%, val_best:  84.96%, tr:  97.72%, tr_best:  98.74%, epoch time: 131.94 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1596672 real_backward_count 270122  16.918%\n",
      "layer   1  Sparsity: 77.0020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25763.0\n",
      "lif layer 1 self.abs_max_v: 25763.0\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 194 occurrences\n",
      "test - Value 1: 258 occurrences\n",
      "epoch-99  lr=['8.0000000'], tr/val_loss: 71.160065/ 39.051479, val:  79.20%, val_best:  84.96%, tr:  98.71%, tr_best:  98.74%, epoch time: 132.04 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612800 real_backward_count 272509  16.897%\n",
      "layer   1  Sparsity: 82.6172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25798.0\n",
      "lif layer 1 self.abs_max_v: 25798.0\n",
      "fc layer 1 self.abs_max_out: 25811.0\n",
      "lif layer 1 self.abs_max_v: 25811.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 253 occurrences\n",
      "test - Value 1: 199 occurrences\n",
      "epoch-100 lr=['8.0000000'], tr/val_loss: 76.739349/ 43.370438, val:  82.52%, val_best:  84.96%, tr:  98.83%, tr_best:  98.83%, epoch time: 131.32 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1628928 real_backward_count 274876  16.875%\n",
      "layer   1  Sparsity: 66.2109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25841.0\n",
      "lif layer 1 self.abs_max_v: 25841.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 31 occurrences\n",
      "test - Value 1: 421 occurrences\n",
      "epoch-101 lr=['8.0000000'], tr/val_loss: 80.750450/150.226074, val:  56.86%, val_best:  84.96%, tr:  98.31%, tr_best:  98.83%, epoch time: 132.31 seconds, 2.21 minutes\n",
      "layer   1  Sparsity: 79.4840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1645056 real_backward_count 277182  16.849%\n",
      "layer   1  Sparsity: 84.1309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25878.0\n",
      "lif layer 1 self.abs_max_v: 25878.0\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 214 occurrences\n",
      "test - Value 1: 238 occurrences\n",
      "epoch-102 lr=['8.0000000'], tr/val_loss: 74.503906/ 76.841331, val:  80.97%, val_best:  84.96%, tr:  98.56%, tr_best:  98.83%, epoch time: 131.38 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1661184 real_backward_count 279508  16.826%\n",
      "layer   1  Sparsity: 67.3828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25912.0\n",
      "lif layer 1 self.abs_max_v: 25912.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 144 occurrences\n",
      "test - Value 1: 308 occurrences\n",
      "epoch-103 lr=['8.0000000'], tr/val_loss: 69.678925/105.042267, val:  75.66%, val_best:  84.96%, tr:  98.41%, tr_best:  98.83%, epoch time: 130.45 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1677312 real_backward_count 281896  16.806%\n",
      "layer   1  Sparsity: 78.1250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6671.5\n",
      "fc layer 1 self.abs_max_out: 25914.0\n",
      "lif layer 1 self.abs_max_v: 25914.0\n",
      "fc layer 2 self.abs_max_out: 4590.0\n",
      "lif layer 2 self.abs_max_v: 6785.0\n",
      "fc layer 2 self.abs_max_out: 4616.0\n",
      "lif layer 2 self.abs_max_v: 6868.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 59 occurrences\n",
      "test - Value 1: 393 occurrences\n",
      "epoch-104 lr=['8.0000000'], tr/val_loss: 72.493515/ 93.058823, val:  62.17%, val_best:  84.96%, tr:  98.61%, tr_best:  98.83%, epoch time: 131.16 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693440 real_backward_count 284341  16.791%\n",
      "layer   1  Sparsity: 74.2188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4648.0\n",
      "fc layer 2 self.abs_max_out: 4666.0\n",
      "lif layer 2 self.abs_max_v: 7078.0\n",
      "fc layer 2 self.abs_max_out: 4723.0\n",
      "fc layer 1 self.abs_max_out: 25938.0\n",
      "lif layer 1 self.abs_max_v: 25938.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 59 occurrences\n",
      "test - Value 1: 393 occurrences\n",
      "epoch-105 lr=['8.0000000'], tr/val_loss: 75.354233/ 87.078476, val:  62.17%, val_best:  84.96%, tr:  98.69%, tr_best:  98.83%, epoch time: 131.42 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1709568 real_backward_count 286728  16.772%\n",
      "layer   1  Sparsity: 83.3496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 25955.0\n",
      "lif layer 1 self.abs_max_v: 25955.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 43 occurrences\n",
      "test - Value 1: 409 occurrences\n",
      "epoch-106 lr=['8.0000000'], tr/val_loss: 70.606285/120.433853, val:  59.07%, val_best:  84.96%, tr:  98.61%, tr_best:  98.83%, epoch time: 130.79 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4618%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1725696 real_backward_count 289122  16.754%\n",
      "layer   1  Sparsity: 89.9414%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 136 occurrences\n",
      "test - Value 1: 316 occurrences\n",
      "epoch-107 lr=['8.0000000'], tr/val_loss: 71.176262/ 68.103996, val:  74.78%, val_best:  84.96%, tr:  98.59%, tr_best:  98.83%, epoch time: 130.65 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6486%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1741824 real_backward_count 291476  16.734%\n",
      "layer   1  Sparsity: 84.5215%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26019.0\n",
      "lif layer 1 self.abs_max_v: 26019.0\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-108 lr=['8.0000000'], tr/val_loss: 75.312889/ 66.540886, val:  79.20%, val_best:  84.96%, tr:  98.51%, tr_best:  98.83%, epoch time: 130.65 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4618%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1757952 real_backward_count 293838  16.715%\n",
      "layer   1  Sparsity: 85.2051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 116 occurrences\n",
      "test - Value 1: 336 occurrences\n",
      "epoch-109 lr=['8.0000000'], tr/val_loss: 76.643425/ 87.133461, val:  72.12%, val_best:  84.96%, tr:  98.21%, tr_best:  98.83%, epoch time: 131.81 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1774080 real_backward_count 296299  16.702%\n",
      "layer   1  Sparsity: 87.3047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26079.0\n",
      "lif layer 1 self.abs_max_v: 26079.0\n",
      "fc layer 2 self.abs_max_out: 4768.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 193 occurrences\n",
      "test - Value 1: 259 occurrences\n",
      "epoch-110 lr=['8.0000000'], tr/val_loss: 74.488922/ 47.590942, val:  82.52%, val_best:  84.96%, tr:  98.14%, tr_best:  98.83%, epoch time: 130.97 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1790208 real_backward_count 298890  16.696%\n",
      "layer   1  Sparsity: 78.5645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 88 occurrences\n",
      "test - Value 1: 364 occurrences\n",
      "epoch-111 lr=['8.0000000'], tr/val_loss: 74.128647/ 81.359055, val:  65.93%, val_best:  84.96%, tr:  98.16%, tr_best:  98.83%, epoch time: 131.70 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5901%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1806336 real_backward_count 301400  16.686%\n",
      "layer   1  Sparsity: 77.4902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26085.0\n",
      "lif layer 1 self.abs_max_v: 26085.0\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 211 occurrences\n",
      "test - Value 1: 241 occurrences\n",
      "epoch-112 lr=['8.0000000'], tr/val_loss: 74.219582/ 48.452671, val:  81.64%, val_best:  84.96%, tr:  98.76%, tr_best:  98.83%, epoch time: 131.33 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1822464 real_backward_count 303685  16.663%\n",
      "layer   1  Sparsity: 83.7891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26186.0\n",
      "lif layer 1 self.abs_max_v: 26186.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 167 occurrences\n",
      "test - Value 1: 285 occurrences\n",
      "epoch-113 lr=['8.0000000'], tr/val_loss: 73.826302/ 44.761116, val:  75.88%, val_best:  84.96%, tr:  98.29%, tr_best:  98.83%, epoch time: 131.24 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5257%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1838592 real_backward_count 306074  16.647%\n",
      "layer   1  Sparsity: 68.0664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 154 occurrences\n",
      "test - Value 1: 298 occurrences\n",
      "epoch-114 lr=['8.0000000'], tr/val_loss: 69.329521/ 69.928185, val:  78.32%, val_best:  84.96%, tr:  98.44%, tr_best:  98.83%, epoch time: 130.68 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9994%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1854720 real_backward_count 308515  16.634%\n",
      "layer   1  Sparsity: 75.1953%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 250 occurrences\n",
      "test - Value 1: 202 occurrences\n",
      "epoch-115 lr=['8.0000000'], tr/val_loss: 72.743866/ 63.288551, val:  80.09%, val_best:  84.96%, tr:  96.83%, tr_best:  98.83%, epoch time: 130.70 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1870848 real_backward_count 311265  16.638%\n",
      "layer   1  Sparsity: 85.5469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26198.0\n",
      "lif layer 1 self.abs_max_v: 26198.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 234 occurrences\n",
      "test - Value 1: 218 occurrences\n",
      "epoch-116 lr=['8.0000000'], tr/val_loss: 69.108925/ 58.889999, val:  80.09%, val_best:  84.96%, tr:  96.58%, tr_best:  98.83%, epoch time: 127.10 seconds, 2.12 minutes\n",
      "layer   1  Sparsity: 79.4797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.2104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1886976 real_backward_count 314197  16.651%\n",
      "layer   1  Sparsity: 69.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 289 occurrences\n",
      "test - Value 1: 163 occurrences\n",
      "epoch-117 lr=['8.0000000'], tr/val_loss: 68.171715/ 47.333454, val:  80.31%, val_best:  84.96%, tr:  96.70%, tr_best:  98.83%, epoch time: 127.50 seconds, 2.13 minutes\n",
      "layer   1  Sparsity: 79.4834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1903104 real_backward_count 317035  16.659%\n",
      "layer   1  Sparsity: 85.9375%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 228 occurrences\n",
      "test - Value 1: 224 occurrences\n",
      "epoch-118 lr=['8.0000000'], tr/val_loss: 72.202995/ 56.074360, val:  82.30%, val_best:  84.96%, tr:  97.42%, tr_best:  98.83%, epoch time: 130.64 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1919232 real_backward_count 319729  16.659%\n",
      "layer   1  Sparsity: 87.2070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 178 occurrences\n",
      "test - Value 1: 274 occurrences\n",
      "epoch-119 lr=['8.0000000'], tr/val_loss: 67.052696/ 60.999210, val:  80.09%, val_best:  84.96%, tr:  97.62%, tr_best:  98.83%, epoch time: 130.86 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1935360 real_backward_count 322279  16.652%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-120 lr=['8.0000000'], tr/val_loss: 73.319595/ 73.892235, val:  82.30%, val_best:  84.96%, tr:  97.64%, tr_best:  98.83%, epoch time: 130.80 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6140%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1951488 real_backward_count 324781  16.643%\n",
      "layer   1  Sparsity: 79.1992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 110 occurrences\n",
      "test - Value 1: 342 occurrences\n",
      "epoch-121 lr=['8.0000000'], tr/val_loss: 74.091522/ 81.672997, val:  70.80%, val_best:  84.96%, tr:  97.84%, tr_best:  98.83%, epoch time: 129.50 seconds, 2.16 minutes\n",
      "layer   1  Sparsity: 79.4811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1892%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1967616 real_backward_count 327267  16.633%\n",
      "layer   1  Sparsity: 75.5371%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 61 occurrences\n",
      "test - Value 1: 391 occurrences\n",
      "epoch-122 lr=['8.0000000'], tr/val_loss: 73.020554/ 65.831940, val:  62.17%, val_best:  84.96%, tr:  97.59%, tr_best:  98.83%, epoch time: 130.57 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1983744 real_backward_count 329831  16.627%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26260.0\n",
      "lif layer 1 self.abs_max_v: 26260.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 125 occurrences\n",
      "test - Value 1: 327 occurrences\n",
      "epoch-123 lr=['8.0000000'], tr/val_loss: 66.445305/ 53.170135, val:  73.67%, val_best:  84.96%, tr:  97.54%, tr_best:  98.83%, epoch time: 131.29 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0892%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1999872 real_backward_count 332385  16.620%\n",
      "layer   1  Sparsity: 80.3223%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 157 occurrences\n",
      "test - Value 1: 295 occurrences\n",
      "epoch-124 lr=['8.0000000'], tr/val_loss: 74.015358/ 76.823357, val:  77.65%, val_best:  84.96%, tr:  97.52%, tr_best:  98.83%, epoch time: 131.25 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6636%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2016000 real_backward_count 334958  16.615%\n",
      "layer   1  Sparsity: 84.1797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 231 occurrences\n",
      "test - Value 1: 221 occurrences\n",
      "epoch-125 lr=['8.0000000'], tr/val_loss: 82.004974/ 70.853737, val:  81.64%, val_best:  84.96%, tr:  97.74%, tr_best:  98.83%, epoch time: 130.12 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4069%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2032128 real_backward_count 337425  16.605%\n",
      "layer   1  Sparsity: 70.3125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 79 occurrences\n",
      "test - Value 1: 373 occurrences\n",
      "epoch-126 lr=['8.0000000'], tr/val_loss: 76.473068/ 72.150528, val:  65.71%, val_best:  84.96%, tr:  97.67%, tr_best:  98.83%, epoch time: 131.02 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2048256 real_backward_count 339923  16.596%\n",
      "layer   1  Sparsity: 74.0234%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26300.0\n",
      "lif layer 1 self.abs_max_v: 26300.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 329 occurrences\n",
      "test - Value 1: 123 occurrences\n",
      "epoch-127 lr=['8.0000000'], tr/val_loss: 83.909081/ 72.713921, val:  75.44%, val_best:  84.96%, tr:  98.09%, tr_best:  98.83%, epoch time: 130.54 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2064384 real_backward_count 342367  16.584%\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26314.0\n",
      "lif layer 1 self.abs_max_v: 26314.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 155 occurrences\n",
      "test - Value 1: 297 occurrences\n",
      "epoch-128 lr=['8.0000000'], tr/val_loss: 75.379143/ 72.999741, val:  78.10%, val_best:  84.96%, tr:  97.77%, tr_best:  98.83%, epoch time: 130.93 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2080512 real_backward_count 344967  16.581%\n",
      "layer   1  Sparsity: 84.5703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26341.0\n",
      "lif layer 1 self.abs_max_v: 26341.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 221 occurrences\n",
      "test - Value 1: 231 occurrences\n",
      "epoch-129 lr=['8.0000000'], tr/val_loss: 79.070015/ 55.966850, val:  83.85%, val_best:  84.96%, tr:  98.31%, tr_best:  98.83%, epoch time: 131.84 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2096640 real_backward_count 347385  16.569%\n",
      "layer   1  Sparsity: 88.5254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26361.0\n",
      "lif layer 1 self.abs_max_v: 26361.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-130 lr=['8.0000000'], tr/val_loss: 74.797577/ 72.712578, val:  69.91%, val_best:  84.96%, tr:  98.07%, tr_best:  98.83%, epoch time: 129.92 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2112768 real_backward_count 349905  16.561%\n",
      "layer   1  Sparsity: 87.7441%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 91 occurrences\n",
      "test - Value 1: 361 occurrences\n",
      "epoch-131 lr=['8.0000000'], tr/val_loss: 80.993690/ 88.673676, val:  67.92%, val_best:  84.96%, tr:  98.24%, tr_best:  98.83%, epoch time: 131.09 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2128896 real_backward_count 352281  16.548%\n",
      "layer   1  Sparsity: 74.8535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26396.0\n",
      "lif layer 1 self.abs_max_v: 26396.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 213 occurrences\n",
      "test - Value 1: 239 occurrences\n",
      "epoch-132 lr=['8.0000000'], tr/val_loss: 89.072105/ 58.980152, val:  82.08%, val_best:  84.96%, tr:  97.97%, tr_best:  98.83%, epoch time: 131.18 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4821%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2145024 real_backward_count 354714  16.537%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26447.0\n",
      "lif layer 1 self.abs_max_v: 26447.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 191 occurrences\n",
      "test - Value 1: 261 occurrences\n",
      "epoch-133 lr=['8.0000000'], tr/val_loss: 80.941124/ 47.070618, val:  83.85%, val_best:  84.96%, tr:  98.41%, tr_best:  98.83%, epoch time: 130.08 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2161152 real_backward_count 357145  16.526%\n",
      "layer   1  Sparsity: 73.4863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 149 occurrences\n",
      "test - Value 1: 303 occurrences\n",
      "epoch-134 lr=['8.0000000'], tr/val_loss: 73.803093/ 41.209190, val:  77.65%, val_best:  84.96%, tr:  98.14%, tr_best:  98.83%, epoch time: 130.92 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2177280 real_backward_count 359542  16.513%\n",
      "layer   1  Sparsity: 79.1992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 289 occurrences\n",
      "test - Value 1: 163 occurrences\n",
      "epoch-135 lr=['8.0000000'], tr/val_loss: 72.674217/ 25.420254, val:  80.31%, val_best:  84.96%, tr:  97.40%, tr_best:  98.83%, epoch time: 131.73 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.6452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9742%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2193408 real_backward_count 362131  16.510%\n",
      "layer   1  Sparsity: 81.1523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 52 occurrences\n",
      "test - Value 1: 400 occurrences\n",
      "epoch-136 lr=['8.0000000'], tr/val_loss: 79.225937/ 93.834106, val:  60.62%, val_best:  84.96%, tr:  97.32%, tr_best:  98.83%, epoch time: 130.49 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.4324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8678%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2209536 real_backward_count 364704  16.506%\n",
      "layer   1  Sparsity: 72.6562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26462.0\n",
      "lif layer 1 self.abs_max_v: 26462.0\n",
      "fc layer 1 self.abs_max_out: 26568.0\n",
      "lif layer 1 self.abs_max_v: 26568.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 155 occurrences\n",
      "test - Value 1: 297 occurrences\n",
      "epoch-137 lr=['8.0000000'], tr/val_loss: 81.106659/ 86.440689, val:  78.54%, val_best:  84.96%, tr:  98.26%, tr_best:  98.83%, epoch time: 131.11 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.6652%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2225664 real_backward_count 367063  16.492%\n",
      "layer   1  Sparsity: 82.7148%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 158 occurrences\n",
      "test - Value 1: 294 occurrences\n",
      "epoch-138 lr=['8.0000000'], tr/val_loss: 85.949409/ 55.808262, val:  77.88%, val_best:  84.96%, tr:  97.47%, tr_best:  98.83%, epoch time: 130.84 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.9181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2241792 real_backward_count 369669  16.490%\n",
      "layer   1  Sparsity: 82.6172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26594.0\n",
      "lif layer 1 self.abs_max_v: 26594.0\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 121 occurrences\n",
      "test - Value 1: 331 occurrences\n",
      "epoch-139 lr=['8.0000000'], tr/val_loss: 86.977821/ 67.731712, val:  74.12%, val_best:  84.96%, tr:  97.84%, tr_best:  98.83%, epoch time: 129.69 seconds, 2.16 minutes\n",
      "layer   1  Sparsity: 79.4804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2257920 real_backward_count 372065  16.478%\n",
      "layer   1  Sparsity: 93.6035%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 84.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-140 lr=['8.0000000'], tr/val_loss: 85.423782/ 59.881664, val:  77.21%, val_best:  84.96%, tr:  98.26%, tr_best:  98.83%, epoch time: 131.27 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.3507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2274048 real_backward_count 374473  16.467%\n",
      "layer   1  Sparsity: 77.4414%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26617.0\n",
      "lif layer 1 self.abs_max_v: 26617.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 224 occurrences\n",
      "test - Value 1: 228 occurrences\n",
      "epoch-141 lr=['8.0000000'], tr/val_loss: 81.750084/ 56.580311, val:  85.40%, val_best:  85.40%, tr:  98.26%, tr_best:  98.83%, epoch time: 130.93 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2290176 real_backward_count 376783  16.452%\n",
      "layer   1  Sparsity: 69.9707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 232 occurrences\n",
      "test - Value 1: 220 occurrences\n",
      "epoch-142 lr=['8.0000000'], tr/val_loss: 81.501129/ 50.422276, val:  83.63%, val_best:  85.40%, tr:  98.56%, tr_best:  98.83%, epoch time: 131.07 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2306304 real_backward_count 379150  16.440%\n",
      "layer   1  Sparsity: 83.0078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 160 occurrences\n",
      "test - Value 1: 292 occurrences\n",
      "epoch-143 lr=['8.0000000'], tr/val_loss: 76.766075/ 90.123108, val:  80.97%, val_best:  85.40%, tr:  98.56%, tr_best:  98.83%, epoch time: 129.95 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2322432 real_backward_count 381597  16.431%\n",
      "layer   1  Sparsity: 92.6270%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 83.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 192 occurrences\n",
      "test - Value 1: 260 occurrences\n",
      "epoch-144 lr=['8.0000000'], tr/val_loss: 74.805779/ 50.634403, val:  78.76%, val_best:  85.40%, tr:  98.14%, tr_best:  98.83%, epoch time: 129.67 seconds, 2.16 minutes\n",
      "layer   1  Sparsity: 79.4781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6768%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2338560 real_backward_count 384006  16.421%\n",
      "layer   1  Sparsity: 67.7734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 78 occurrences\n",
      "test - Value 1: 374 occurrences\n",
      "epoch-145 lr=['8.0000000'], tr/val_loss: 72.139297/ 87.040337, val:  65.49%, val_best:  85.40%, tr:  98.14%, tr_best:  98.83%, epoch time: 131.32 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2354688 real_backward_count 386536  16.416%\n",
      "layer   1  Sparsity: 73.2910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 64 occurrences\n",
      "test - Value 1: 388 occurrences\n",
      "epoch-146 lr=['8.0000000'], tr/val_loss: 72.524399/ 75.270088, val:  63.72%, val_best:  85.40%, tr:  97.59%, tr_best:  98.83%, epoch time: 131.12 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6348%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2370816 real_backward_count 389128  16.413%\n",
      "layer   1  Sparsity: 79.8340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 322 occurrences\n",
      "test - Value 1: 130 occurrences\n",
      "epoch-147 lr=['8.0000000'], tr/val_loss: 72.551750/ 41.543304, val:  73.89%, val_best:  85.40%, tr:  97.40%, tr_best:  98.83%, epoch time: 131.33 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2386944 real_backward_count 391720  16.411%\n",
      "layer   1  Sparsity: 83.7402%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 34 occurrences\n",
      "test - Value 1: 418 occurrences\n",
      "epoch-148 lr=['8.0000000'], tr/val_loss: 73.351395/133.987900, val:  57.52%, val_best:  85.40%, tr:  97.99%, tr_best:  98.83%, epoch time: 132.02 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2850%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2403072 real_backward_count 394154  16.402%\n",
      "layer   1  Sparsity: 84.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 257 occurrences\n",
      "test - Value 1: 195 occurrences\n",
      "epoch-149 lr=['8.0000000'], tr/val_loss: 74.420258/ 50.766605, val:  82.08%, val_best:  85.40%, tr:  97.72%, tr_best:  98.83%, epoch time: 131.54 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2419200 real_backward_count 396634  16.395%\n",
      "layer   1  Sparsity: 70.5078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 295 occurrences\n",
      "test - Value 1: 157 occurrences\n",
      "epoch-150 lr=['8.0000000'], tr/val_loss: 76.264320/ 32.929749, val:  79.42%, val_best:  85.40%, tr:  97.99%, tr_best:  98.83%, epoch time: 130.74 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.2103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2435328 real_backward_count 399210  16.392%\n",
      "layer   1  Sparsity: 62.4512%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 34 occurrences\n",
      "test - Value 1: 418 occurrences\n",
      "epoch-151 lr=['8.0000000'], tr/val_loss: 74.915428/116.930374, val:  57.52%, val_best:  85.40%, tr:  97.72%, tr_best:  98.83%, epoch time: 130.62 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4889%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2451456 real_backward_count 401780  16.389%\n",
      "layer   1  Sparsity: 83.0566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 247 occurrences\n",
      "test - Value 1: 205 occurrences\n",
      "epoch-152 lr=['8.0000000'], tr/val_loss: 72.355309/ 64.731636, val:  83.85%, val_best:  85.40%, tr:  97.77%, tr_best:  98.83%, epoch time: 131.14 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2467584 real_backward_count 404222  16.381%\n",
      "layer   1  Sparsity: 78.0273%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-153 lr=['8.0000000'], tr/val_loss: 80.824890/ 73.626381, val:  75.00%, val_best:  85.40%, tr:  98.02%, tr_best:  98.83%, epoch time: 130.39 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2483712 real_backward_count 406695  16.374%\n",
      "layer   1  Sparsity: 66.5039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 66 occurrences\n",
      "test - Value 1: 386 occurrences\n",
      "epoch-154 lr=['8.0000000'], tr/val_loss: 82.718880/101.833633, val:  64.60%, val_best:  85.40%, tr:  98.12%, tr_best:  98.83%, epoch time: 131.04 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.9143%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2499840 real_backward_count 409080  16.364%\n",
      "layer   1  Sparsity: 80.3223%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 195 occurrences\n",
      "test - Value 1: 257 occurrences\n",
      "epoch-155 lr=['8.0000000'], tr/val_loss: 83.100006/ 82.999054, val:  82.96%, val_best:  85.40%, tr:  98.21%, tr_best:  98.83%, epoch time: 130.57 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2515968 real_backward_count 411426  16.353%\n",
      "layer   1  Sparsity: 88.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 257 occurrences\n",
      "test - Value 1: 195 occurrences\n",
      "epoch-156 lr=['8.0000000'], tr/val_loss: 82.901001/ 66.034477, val:  81.19%, val_best:  85.40%, tr:  98.39%, tr_best:  98.83%, epoch time: 129.95 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.2160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3851%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2532096 real_backward_count 413790  16.342%\n",
      "layer   1  Sparsity: 74.5605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-157 lr=['8.0000000'], tr/val_loss: 83.895149/104.371216, val:  75.88%, val_best:  85.40%, tr:  98.14%, tr_best:  98.83%, epoch time: 130.35 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2548224 real_backward_count 416292  16.337%\n",
      "layer   1  Sparsity: 82.5684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 166 occurrences\n",
      "test - Value 1: 286 occurrences\n",
      "epoch-158 lr=['8.0000000'], tr/val_loss: 87.792603/ 67.262154, val:  80.09%, val_best:  85.40%, tr:  98.09%, tr_best:  98.83%, epoch time: 130.99 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.3301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2564352 real_backward_count 418706  16.328%\n",
      "layer   1  Sparsity: 84.8145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 57 occurrences\n",
      "test - Value 1: 395 occurrences\n",
      "epoch-159 lr=['8.0000000'], tr/val_loss: 83.007904/165.592422, val:  62.17%, val_best:  85.40%, tr:  97.74%, tr_best:  98.83%, epoch time: 131.49 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0631%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2580480 real_backward_count 421173  16.321%\n",
      "layer   1  Sparsity: 84.8145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 92 occurrences\n",
      "test - Value 1: 360 occurrences\n",
      "epoch-160 lr=['8.0000000'], tr/val_loss: 82.172287/105.853157, val:  67.26%, val_best:  85.40%, tr:  97.67%, tr_best:  98.83%, epoch time: 131.18 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2596608 real_backward_count 423622  16.314%\n",
      "layer   1  Sparsity: 62.3535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 70 occurrences\n",
      "test - Value 1: 382 occurrences\n",
      "epoch-161 lr=['8.0000000'], tr/val_loss: 86.946205/ 90.753265, val:  63.72%, val_best:  85.40%, tr:  97.84%, tr_best:  98.83%, epoch time: 130.35 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4901%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2612736 real_backward_count 426040  16.306%\n",
      "layer   1  Sparsity: 92.2852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7132.5\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 209 occurrences\n",
      "test - Value 1: 243 occurrences\n",
      "epoch-162 lr=['8.0000000'], tr/val_loss: 77.876968/ 84.054649, val:  81.64%, val_best:  85.40%, tr:  97.74%, tr_best:  98.83%, epoch time: 129.10 seconds, 2.15 minutes\n",
      "layer   1  Sparsity: 79.4782%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7238%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9395%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2628864 real_backward_count 428619  16.304%\n",
      "layer   1  Sparsity: 73.5840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 185 occurrences\n",
      "test - Value 1: 267 occurrences\n",
      "epoch-163 lr=['8.0000000'], tr/val_loss: 89.085434/ 85.810966, val:  80.31%, val_best:  85.40%, tr:  97.77%, tr_best:  98.83%, epoch time: 131.09 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7479%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2644992 real_backward_count 430949  16.293%\n",
      "layer   1  Sparsity: 68.0664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 242 occurrences\n",
      "test - Value 1: 210 occurrences\n",
      "epoch-164 lr=['8.0000000'], tr/val_loss: 92.189491/ 70.267853, val:  80.97%, val_best:  85.40%, tr:  97.84%, tr_best:  98.83%, epoch time: 130.55 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1524%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2661120 real_backward_count 433408  16.287%\n",
      "layer   1  Sparsity: 83.7402%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 165 occurrences\n",
      "test - Value 1: 287 occurrences\n",
      "epoch-165 lr=['8.0000000'], tr/val_loss: 83.278900/ 76.034195, val:  78.54%, val_best:  85.40%, tr:  97.50%, tr_best:  98.83%, epoch time: 131.54 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.4506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2677248 real_backward_count 435956  16.284%\n",
      "layer   1  Sparsity: 72.4609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 252 occurrences\n",
      "test - Value 1: 200 occurrences\n",
      "epoch-166 lr=['8.0000000'], tr/val_loss: 80.621071/ 44.094234, val:  81.42%, val_best:  85.40%, tr:  97.64%, tr_best:  98.83%, epoch time: 130.75 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.5374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2693376 real_backward_count 438424  16.278%\n",
      "layer   1  Sparsity: 83.0078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 105 occurrences\n",
      "test - Value 1: 347 occurrences\n",
      "epoch-167 lr=['8.0000000'], tr/val_loss: 77.023476/130.648270, val:  70.58%, val_best:  85.40%, tr:  97.12%, tr_best:  98.83%, epoch time: 131.26 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.7119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2709504 real_backward_count 440937  16.274%\n",
      "layer   1  Sparsity: 81.5430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26758.0\n",
      "lif layer 1 self.abs_max_v: 26758.0\n",
      "train - Value 0: 2042 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 29 occurrences\n",
      "test - Value 1: 423 occurrences\n",
      "epoch-168 lr=['8.0000000'], tr/val_loss: 77.030220/124.783867, val:  55.97%, val_best:  85.40%, tr:  97.97%, tr_best:  98.83%, epoch time: 130.51 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.9632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2725632 real_backward_count 443283  16.263%\n",
      "layer   1  Sparsity: 89.4531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26833.0\n",
      "lif layer 1 self.abs_max_v: 26833.0\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 179 occurrences\n",
      "test - Value 1: 273 occurrences\n",
      "epoch-169 lr=['8.0000000'], tr/val_loss: 75.106239/ 46.846844, val:  78.10%, val_best:  85.40%, tr:  97.79%, tr_best:  98.83%, epoch time: 130.91 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.7989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2741760 real_backward_count 445741  16.257%\n",
      "layer   1  Sparsity: 76.3672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26866.0\n",
      "lif layer 1 self.abs_max_v: 26866.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 229 occurrences\n",
      "test - Value 1: 223 occurrences\n",
      "epoch-170 lr=['8.0000000'], tr/val_loss: 81.377396/ 59.738594, val:  85.18%, val_best:  85.40%, tr:  97.59%, tr_best:  98.83%, epoch time: 130.22 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.5240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2757888 real_backward_count 448251  16.253%\n",
      "layer   1  Sparsity: 79.5898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 26872.0\n",
      "lif layer 1 self.abs_max_v: 26872.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 200 occurrences\n",
      "test - Value 1: 252 occurrences\n",
      "epoch-171 lr=['8.0000000'], tr/val_loss: 81.529518/ 61.994720, val:  82.30%, val_best:  85.40%, tr:  97.72%, tr_best:  98.83%, epoch time: 131.55 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2774016 real_backward_count 450684  16.247%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 245 occurrences\n",
      "test - Value 1: 207 occurrences\n",
      "epoch-172 lr=['8.0000000'], tr/val_loss: 82.977760/ 58.235336, val:  83.85%, val_best:  85.40%, tr:  97.67%, tr_best:  98.83%, epoch time: 130.96 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2790144 real_backward_count 453213  16.243%\n",
      "layer   1  Sparsity: 85.1562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7175.5\n",
      "lif layer 2 self.abs_max_v: 7207.5\n",
      "lif layer 2 self.abs_max_v: 7245.0\n",
      "fc layer 1 self.abs_max_out: 26982.0\n",
      "lif layer 1 self.abs_max_v: 26982.0\n",
      "fc layer 2 self.abs_max_out: 4774.0\n",
      "lif layer 2 self.abs_max_v: 7253.5\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "lif layer 2 self.abs_max_v: 7322.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 47 occurrences\n",
      "test - Value 1: 405 occurrences\n",
      "epoch-173 lr=['8.0000000'], tr/val_loss: 86.999084/114.260193, val:  60.40%, val_best:  85.40%, tr:  97.72%, tr_best:  98.83%, epoch time: 130.66 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2806272 real_backward_count 455648  16.237%\n",
      "layer   1  Sparsity: 93.4082%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7622.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-174 lr=['8.0000000'], tr/val_loss: 79.816383/ 59.119621, val:  72.79%, val_best:  85.40%, tr:  97.99%, tr_best:  98.83%, epoch time: 129.98 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7890%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2822400 real_backward_count 458032  16.228%\n",
      "layer   1  Sparsity: 90.6250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 27134.0\n",
      "lif layer 1 self.abs_max_v: 27134.0\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 327 occurrences\n",
      "test - Value 1: 125 occurrences\n",
      "epoch-175 lr=['8.0000000'], tr/val_loss: 77.866730/ 33.843582, val:  75.00%, val_best:  85.40%, tr:  98.04%, tr_best:  98.83%, epoch time: 129.71 seconds, 2.16 minutes\n",
      "layer   1  Sparsity: 79.4786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2838528 real_backward_count 460403  16.220%\n",
      "layer   1  Sparsity: 73.7793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4812.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 126 occurrences\n",
      "test - Value 1: 326 occurrences\n",
      "epoch-176 lr=['8.0000000'], tr/val_loss: 85.854439/ 96.011703, val:  73.45%, val_best:  85.40%, tr:  98.41%, tr_best:  98.83%, epoch time: 129.81 seconds, 2.16 minutes\n",
      "layer   1  Sparsity: 79.4824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2854656 real_backward_count 462822  16.213%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1992 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 151 occurrences\n",
      "test - Value 1: 301 occurrences\n",
      "epoch-177 lr=['8.0000000'], tr/val_loss: 94.649368/ 62.811874, val:  76.77%, val_best:  85.40%, tr:  98.31%, tr_best:  98.83%, epoch time: 130.62 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0196%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2870784 real_backward_count 465168  16.204%\n",
      "layer   1  Sparsity: 85.6445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 27161.0\n",
      "lif layer 1 self.abs_max_v: 27161.0\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 184 occurrences\n",
      "test - Value 1: 268 occurrences\n",
      "epoch-178 lr=['8.0000000'], tr/val_loss: 85.459969/ 70.381660, val:  82.74%, val_best:  85.40%, tr:  98.02%, tr_best:  98.83%, epoch time: 131.79 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2886912 real_backward_count 467585  16.197%\n",
      "layer   1  Sparsity: 80.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 106 occurrences\n",
      "test - Value 1: 346 occurrences\n",
      "epoch-179 lr=['8.0000000'], tr/val_loss: 84.759514/ 70.432274, val:  71.68%, val_best:  85.40%, tr:  98.29%, tr_best:  98.83%, epoch time: 130.03 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2903040 real_backward_count 469954  16.188%\n",
      "layer   1  Sparsity: 70.6543%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 233 occurrences\n",
      "test - Value 1: 219 occurrences\n",
      "epoch-180 lr=['8.0000000'], tr/val_loss: 83.318321/ 53.969666, val:  82.08%, val_best:  85.40%, tr:  98.21%, tr_best:  98.83%, epoch time: 129.88 seconds, 2.16 minutes\n",
      "layer   1  Sparsity: 79.4830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9106%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2919168 real_backward_count 472344  16.181%\n",
      "layer   1  Sparsity: 87.2070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-181 lr=['8.0000000'], tr/val_loss: 81.533737/ 72.068260, val:  83.19%, val_best:  85.40%, tr:  98.24%, tr_best:  98.83%, epoch time: 130.81 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2935296 real_backward_count 474661  16.171%\n",
      "layer   1  Sparsity: 57.8125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-182 lr=['8.0000000'], tr/val_loss: 90.176949/ 61.005348, val:  76.11%, val_best:  85.40%, tr:  98.09%, tr_best:  98.83%, epoch time: 130.72 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2951424 real_backward_count 476924  16.159%\n",
      "layer   1  Sparsity: 77.8809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 167 occurrences\n",
      "test - Value 1: 285 occurrences\n",
      "epoch-183 lr=['8.0000000'], tr/val_loss: 81.507729/ 62.311672, val:  81.64%, val_best:  85.40%, tr:  99.06%, tr_best:  99.06%, epoch time: 131.29 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2967552 real_backward_count 479190  16.148%\n",
      "layer   1  Sparsity: 71.2891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 47 occurrences\n",
      "test - Value 1: 405 occurrences\n",
      "epoch-184 lr=['8.0000000'], tr/val_loss: 92.529068/108.559196, val:  60.40%, val_best:  85.40%, tr:  98.49%, tr_best:  99.06%, epoch time: 130.76 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2983680 real_backward_count 481423  16.135%\n",
      "layer   1  Sparsity: 85.8887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-185 lr=['8.0000000'], tr/val_loss: 84.690475/ 64.535629, val:  69.69%, val_best:  85.40%, tr:  96.97%, tr_best:  99.06%, epoch time: 131.24 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2999808 real_backward_count 483930  16.132%\n",
      "layer   1  Sparsity: 85.6445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7637.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-186 lr=['8.0000000'], tr/val_loss: 69.606743/ 64.359291, val:  73.23%, val_best:  85.40%, tr:  97.37%, tr_best:  99.06%, epoch time: 131.06 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.4955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3015936 real_backward_count 486514  16.131%\n",
      "layer   1  Sparsity: 80.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 21 occurrences\n",
      "test - Value 1: 431 occurrences\n",
      "epoch-187 lr=['8.0000000'], tr/val_loss: 74.492088/142.411591, val:  54.65%, val_best:  85.40%, tr:  96.58%, tr_best:  99.06%, epoch time: 130.57 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0989%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3032064 real_backward_count 489145  16.132%\n",
      "layer   1  Sparsity: 68.0664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7919.0\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4822.0\n",
      "fc layer 2 self.abs_max_out: 4854.0\n",
      "fc layer 2 self.abs_max_out: 4910.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-188 lr=['8.0000000'], tr/val_loss: 70.998245/ 82.777771, val:  71.46%, val_best:  85.40%, tr:  97.57%, tr_best:  99.06%, epoch time: 130.60 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3048192 real_backward_count 491654  16.129%\n",
      "layer   1  Sparsity: 62.4512%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 156 occurrences\n",
      "test - Value 1: 296 occurrences\n",
      "epoch-189 lr=['8.0000000'], tr/val_loss: 74.402023/ 73.739220, val:  77.88%, val_best:  85.40%, tr:  97.35%, tr_best:  99.06%, epoch time: 131.96 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3064320 real_backward_count 494117  16.125%\n",
      "layer   1  Sparsity: 80.6152%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 46 occurrences\n",
      "test - Value 1: 406 occurrences\n",
      "epoch-190 lr=['8.0000000'], tr/val_loss: 74.233154/ 90.092972, val:  58.85%, val_best:  85.40%, tr:  97.07%, tr_best:  99.06%, epoch time: 131.01 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3080448 real_backward_count 496677  16.124%\n",
      "layer   1  Sparsity: 67.3828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4932.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 244 occurrences\n",
      "test - Value 1: 208 occurrences\n",
      "epoch-191 lr=['8.0000000'], tr/val_loss: 72.540009/ 28.547758, val:  80.09%, val_best:  85.40%, tr:  97.10%, tr_best:  99.06%, epoch time: 131.06 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8722%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3096576 real_backward_count 499224  16.122%\n",
      "layer   1  Sparsity: 75.5371%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 4980.0\n",
      "fc layer 2 self.abs_max_out: 5008.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 125 occurrences\n",
      "test - Value 1: 327 occurrences\n",
      "epoch-192 lr=['8.0000000'], tr/val_loss: 75.100151/ 68.761414, val:  73.67%, val_best:  85.40%, tr:  97.07%, tr_best:  99.06%, epoch time: 131.30 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9242%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6019%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3112704 real_backward_count 501784  16.121%\n",
      "layer   1  Sparsity: 57.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-193 lr=['8.0000000'], tr/val_loss: 73.906281/119.184982, val:  59.29%, val_best:  85.40%, tr:  96.92%, tr_best:  99.06%, epoch time: 130.39 seconds, 2.17 minutes\n",
      "layer   1  Sparsity: 79.4859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3128832 real_backward_count 504373  16.120%\n",
      "layer   1  Sparsity: 75.4883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5099.0\n",
      "lif layer 2 self.abs_max_v: 7955.0\n",
      "lif layer 2 self.abs_max_v: 8048.0\n",
      "lif layer 2 self.abs_max_v: 8172.0\n",
      "lif layer 2 self.abs_max_v: 8362.5\n",
      "fc layer 2 self.abs_max_out: 5138.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 164 occurrences\n",
      "test - Value 1: 288 occurrences\n",
      "epoch-194 lr=['8.0000000'], tr/val_loss: 77.997475/ 94.441360, val:  78.76%, val_best:  85.40%, tr:  96.73%, tr_best:  99.06%, epoch time: 131.71 seconds, 2.20 minutes\n",
      "layer   1  Sparsity: 79.4820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3144960 real_backward_count 507091  16.124%\n",
      "layer   1  Sparsity: 88.2324%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8404.5\n",
      "lif layer 2 self.abs_max_v: 8435.0\n",
      "lif layer 2 self.abs_max_v: 8566.0\n",
      "fc layer 2 self.abs_max_out: 5183.0\n",
      "fc layer 2 self.abs_max_out: 5221.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 288 occurrences\n",
      "test - Value 1: 164 occurrences\n",
      "epoch-195 lr=['8.0000000'], tr/val_loss: 79.726227/ 56.335720, val:  77.43%, val_best:  85.40%, tr:  96.50%, tr_best:  99.06%, epoch time: 131.09 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0145%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3161088 real_backward_count 509679  16.124%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8674.0\n",
      "fc layer 2 self.abs_max_out: 5237.0\n",
      "fc layer 2 self.abs_max_out: 5279.0\n",
      "fc layer 2 self.abs_max_out: 5351.0\n",
      "fc layer 2 self.abs_max_out: 5380.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 147 occurrences\n",
      "test - Value 1: 305 occurrences\n",
      "epoch-196 lr=['8.0000000'], tr/val_loss: 76.585320/ 70.261810, val:  75.88%, val_best:  85.40%, tr:  97.67%, tr_best:  99.06%, epoch time: 131.44 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3177216 real_backward_count 512123  16.119%\n",
      "layer   1  Sparsity: 80.2734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8696.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 38 occurrences\n",
      "test - Value 1: 414 occurrences\n",
      "epoch-197 lr=['8.0000000'], tr/val_loss: 80.426529/ 99.112549, val:  57.08%, val_best:  85.40%, tr:  97.20%, tr_best:  99.06%, epoch time: 129.78 seconds, 2.16 minutes\n",
      "layer   1  Sparsity: 79.4809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3193344 real_backward_count 514543  16.113%\n",
      "layer   1  Sparsity: 73.4863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 133 occurrences\n",
      "test - Value 1: 319 occurrences\n",
      "epoch-198 lr=['8.0000000'], tr/val_loss: 78.079880/ 74.941467, val:  72.79%, val_best:  85.40%, tr:  96.53%, tr_best:  99.06%, epoch time: 130.84 seconds, 2.18 minutes\n",
      "layer   1  Sparsity: 79.4824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9839%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2839%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3209472 real_backward_count 517207  16.115%\n",
      "layer   1  Sparsity: 69.7754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8866.5\n",
      "lif layer 2 self.abs_max_v: 8953.0\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 4.000000, min 4, max 4\n",
      "fc layer 2 self.abs_max_out: 5393.0\n",
      "test_spike_distribution.mean 4.000000, min 4, max 4\n",
      "test - Value 0: 145 occurrences\n",
      "test - Value 1: 307 occurrences\n",
      "epoch-199 lr=['8.0000000'], tr/val_loss: 72.784126/106.463737, val:  76.33%, val_best:  85.40%, tr:  97.02%, tr_best:  99.06%, epoch time: 131.24 seconds, 2.19 minutes\n",
      "layer   1  Sparsity: 79.4832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713842f5122747128dff1fbfaeb7cfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÑ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÇ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÅ‚ñÇ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÑ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÇ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÅ‚ñÇ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97024</td></tr><tr><td>tr_epoch_loss</td><td>72.78413</td></tr><tr><td>val_acc_best</td><td>0.85398</td></tr><tr><td>val_acc_now</td><td>0.76327</td></tr><tr><td>val_loss</td><td>106.46374</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-sweep-61</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/1k2ukckh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/1k2ukckh</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251220_050313-1k2ukckh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: alux2h5q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251220_122049-alux2h5q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/alux2h5q' target=\"_blank\">noble-sweep-62</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/alux2h5q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/alux2h5q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251220_122058_425', 'my_seed': 42, 'TIME': 8, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 0, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 16, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.03125, 0.03125, 0.03125], 'learning_rate': 4, 'learning_rate2': 4, 'loser_encourage_mode': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4032, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4032 BATCH: 1 train_data_count: 4032\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 4, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 16, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.03125, 0.03125])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.03125, 0.03125])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.03125, 0.03125])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 4\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 28.0\n",
      "lif layer 1 self.abs_max_v: 28.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 75.0\n",
      "lif layer 1 self.abs_max_v: 76.0\n",
      "fc layer 2 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 31.0\n",
      "fc layer 1 self.abs_max_out: 103.0\n",
      "lif layer 1 self.abs_max_v: 122.0\n",
      "fc layer 2 self.abs_max_out: 60.0\n",
      "lif layer 2 self.abs_max_v: 64.5\n",
      "fc layer 3 self.abs_max_out: 19.0\n",
      "fc layer 1 self.abs_max_out: 107.0\n",
      "lif layer 1 self.abs_max_v: 153.5\n",
      "lif layer 2 self.abs_max_v: 87.5\n",
      "fc layer 3 self.abs_max_out: 20.0\n",
      "fc layer 1 self.abs_max_out: 114.0\n",
      "lif layer 1 self.abs_max_v: 171.5\n",
      "lif layer 2 self.abs_max_v: 103.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "fc layer 2 self.abs_max_out: 102.0\n",
      "lif layer 2 self.abs_max_v: 116.0\n",
      "fc layer 1 self.abs_max_out: 218.0\n",
      "lif layer 1 self.abs_max_v: 237.0\n",
      "layer   1  Sparsity: 74.8535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 85.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 94.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 665.0\n",
      "lif layer 1 self.abs_max_v: 665.0\n",
      "fc layer 2 self.abs_max_out: 124.0\n",
      "lif layer 2 self.abs_max_v: 124.0\n",
      "fc layer 1 self.abs_max_out: 1370.0\n",
      "lif layer 1 self.abs_max_v: 1370.0\n",
      "fc layer 2 self.abs_max_out: 245.0\n",
      "lif layer 2 self.abs_max_v: 245.0\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "lif layer 2 self.abs_max_v: 264.5\n",
      "fc layer 2 self.abs_max_out: 294.0\n",
      "lif layer 2 self.abs_max_v: 294.0\n",
      "fc layer 2 self.abs_max_out: 301.0\n",
      "lif layer 2 self.abs_max_v: 301.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "fc layer 2 self.abs_max_out: 307.0\n",
      "lif layer 2 self.abs_max_v: 347.0\n",
      "lif layer 2 self.abs_max_v: 401.5\n",
      "fc layer 2 self.abs_max_out: 308.0\n",
      "lif layer 2 self.abs_max_v: 427.0\n",
      "lif layer 2 self.abs_max_v: 457.5\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 3 self.abs_max_out: 85.0\n",
      "fc layer 2 self.abs_max_out: 313.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "fc layer 2 self.abs_max_out: 367.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 377.0\n",
      "lif layer 2 self.abs_max_v: 468.0\n",
      "fc layer 3 self.abs_max_out: 97.0\n",
      "lif layer 2 self.abs_max_v: 536.0\n",
      "lif layer 2 self.abs_max_v: 562.5\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 586.0\n",
      "lif layer 1 self.abs_max_v: 1415.0\n",
      "fc layer 2 self.abs_max_out: 388.0\n",
      "fc layer 2 self.abs_max_out: 394.0\n",
      "lif layer 1 self.abs_max_v: 1601.0\n",
      "fc layer 2 self.abs_max_out: 395.0\n",
      "lif layer 2 self.abs_max_v: 605.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 2 self.abs_max_out: 423.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 2 self.abs_max_out: 426.0\n",
      "fc layer 2 self.abs_max_out: 433.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "fc layer 2 self.abs_max_out: 439.0\n",
      "fc layer 3 self.abs_max_out: 156.0\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "fc layer 2 self.abs_max_out: 463.0\n",
      "fc layer 2 self.abs_max_out: 538.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "fc layer 2 self.abs_max_out: 597.0\n",
      "lif layer 2 self.abs_max_v: 614.5\n",
      "fc layer 2 self.abs_max_out: 618.0\n",
      "lif layer 2 self.abs_max_v: 618.0\n",
      "fc layer 2 self.abs_max_out: 644.0\n",
      "lif layer 2 self.abs_max_v: 644.0\n",
      "fc layer 3 self.abs_max_out: 189.0\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 3 self.abs_max_out: 201.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 3 self.abs_max_out: 213.0\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "lif layer 1 self.abs_max_v: 1620.5\n",
      "lif layer 1 self.abs_max_v: 1628.5\n",
      "fc layer 1 self.abs_max_out: 1445.0\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "lif layer 2 self.abs_max_v: 646.0\n",
      "fc layer 1 self.abs_max_out: 1667.0\n",
      "lif layer 1 self.abs_max_v: 1667.0\n",
      "fc layer 2 self.abs_max_out: 659.0\n",
      "lif layer 2 self.abs_max_v: 659.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "lif layer 2 self.abs_max_v: 663.0\n",
      "fc layer 2 self.abs_max_out: 664.0\n",
      "lif layer 2 self.abs_max_v: 664.0\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "lif layer 2 self.abs_max_v: 670.0\n",
      "fc layer 2 self.abs_max_out: 673.0\n",
      "lif layer 2 self.abs_max_v: 673.0\n",
      "fc layer 1 self.abs_max_out: 1720.0\n",
      "lif layer 1 self.abs_max_v: 1720.0\n",
      "fc layer 2 self.abs_max_out: 691.0\n",
      "lif layer 2 self.abs_max_v: 691.0\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "lif layer 2 self.abs_max_v: 703.0\n",
      "fc layer 1 self.abs_max_out: 1762.0\n",
      "lif layer 1 self.abs_max_v: 1762.0\n",
      "lif layer 2 self.abs_max_v: 722.0\n",
      "fc layer 1 self.abs_max_out: 1830.0\n",
      "lif layer 1 self.abs_max_v: 1830.0\n",
      "fc layer 1 self.abs_max_out: 1910.0\n",
      "lif layer 1 self.abs_max_v: 1910.0\n",
      "fc layer 1 self.abs_max_out: 1959.0\n",
      "lif layer 1 self.abs_max_v: 1959.0\n",
      "lif layer 2 self.abs_max_v: 724.5\n",
      "lif layer 2 self.abs_max_v: 763.5\n",
      "lif layer 2 self.abs_max_v: 798.5\n",
      "lif layer 2 self.abs_max_v: 815.5\n",
      "lif layer 2 self.abs_max_v: 840.0\n",
      "lif layer 2 self.abs_max_v: 849.0\n",
      "lif layer 2 self.abs_max_v: 856.0\n",
      "lif layer 2 self.abs_max_v: 861.0\n",
      "lif layer 2 self.abs_max_v: 874.5\n",
      "fc layer 2 self.abs_max_out: 723.0\n",
      "lif layer 2 self.abs_max_v: 918.0\n",
      "fc layer 2 self.abs_max_out: 731.0\n",
      "fc layer 2 self.abs_max_out: 757.0\n",
      "lif layer 2 self.abs_max_v: 937.0\n",
      "lif layer 2 self.abs_max_v: 942.5\n",
      "lif layer 2 self.abs_max_v: 960.5\n",
      "lif layer 2 self.abs_max_v: 983.5\n",
      "lif layer 1 self.abs_max_v: 2142.5\n",
      "fc layer 2 self.abs_max_out: 782.0\n",
      "fc layer 2 self.abs_max_out: 789.0\n",
      "lif layer 2 self.abs_max_v: 989.0\n",
      "lif layer 2 self.abs_max_v: 1025.5\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "lif layer 1 self.abs_max_v: 2519.5\n",
      "lif layer 1 self.abs_max_v: 2532.0\n",
      "lif layer 2 self.abs_max_v: 1040.5\n",
      "lif layer 2 self.abs_max_v: 1042.0\n",
      "lif layer 2 self.abs_max_v: 1062.5\n",
      "lif layer 2 self.abs_max_v: 1072.5\n",
      "lif layer 2 self.abs_max_v: 1076.5\n",
      "lif layer 2 self.abs_max_v: 1084.0\n",
      "lif layer 2 self.abs_max_v: 1104.5\n",
      "lif layer 2 self.abs_max_v: 1122.5\n",
      "fc layer 1 self.abs_max_out: 2013.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 2 self.abs_max_out: 792.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "fc layer 2 self.abs_max_out: 797.0\n",
      "fc layer 2 self.abs_max_out: 813.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 2 self.abs_max_out: 815.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 836.0\n",
      "fc layer 1 self.abs_max_out: 2118.0\n",
      "fc layer 1 self.abs_max_out: 2156.0\n",
      "fc layer 1 self.abs_max_out: 2384.0\n",
      "fc layer 1 self.abs_max_out: 2660.0\n",
      "lif layer 1 self.abs_max_v: 2660.0\n",
      "fc layer 1 self.abs_max_out: 2709.0\n",
      "lif layer 1 self.abs_max_v: 2709.0\n",
      "fc layer 1 self.abs_max_out: 3028.0\n",
      "lif layer 1 self.abs_max_v: 3028.0\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "fc layer 2 self.abs_max_out: 904.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "lif layer 2 self.abs_max_v: 1154.5\n",
      "lif layer 2 self.abs_max_v: 1234.5\n",
      "lif layer 2 self.abs_max_v: 1299.5\n",
      "fc layer 2 self.abs_max_out: 1075.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "lif layer 2 self.abs_max_v: 1300.5\n",
      "lif layer 2 self.abs_max_v: 1301.5\n",
      "lif layer 2 self.abs_max_v: 1312.0\n",
      "fc layer 1 self.abs_max_out: 3126.0\n",
      "lif layer 1 self.abs_max_v: 3126.0\n",
      "lif layer 2 self.abs_max_v: 1321.5\n",
      "lif layer 2 self.abs_max_v: 1333.5\n",
      "lif layer 2 self.abs_max_v: 1341.5\n",
      "lif layer 2 self.abs_max_v: 1370.0\n",
      "lif layer 2 self.abs_max_v: 1399.5\n",
      "fc layer 3 self.abs_max_out: 475.0\n",
      "fc layer 2 self.abs_max_out: 1093.0\n",
      "lif layer 2 self.abs_max_v: 1414.5\n",
      "lif layer 1 self.abs_max_v: 3134.5\n",
      "fc layer 1 self.abs_max_out: 3216.0\n",
      "lif layer 1 self.abs_max_v: 3216.0\n",
      "lif layer 1 self.abs_max_v: 3284.0\n",
      "lif layer 1 self.abs_max_v: 3483.5\n",
      "lif layer 1 self.abs_max_v: 3799.0\n",
      "lif layer 1 self.abs_max_v: 3885.5\n",
      "lif layer 1 self.abs_max_v: 3898.5\n",
      "lif layer 1 self.abs_max_v: 3957.5\n",
      "lif layer 2 self.abs_max_v: 1433.0\n",
      "lif layer 1 self.abs_max_v: 4280.0\n",
      "lif layer 2 self.abs_max_v: 1459.0\n",
      "lif layer 2 self.abs_max_v: 1492.5\n",
      "lif layer 2 self.abs_max_v: 1509.5\n",
      "lif layer 2 self.abs_max_v: 1560.5\n",
      "lif layer 2 self.abs_max_v: 1567.5\n",
      "lif layer 2 self.abs_max_v: 1592.5\n",
      "lif layer 2 self.abs_max_v: 1682.5\n",
      "lif layer 2 self.abs_max_v: 1727.5\n",
      "lif layer 2 self.abs_max_v: 1835.0\n",
      "lif layer 1 self.abs_max_v: 4319.5\n",
      "lif layer 1 self.abs_max_v: 4513.0\n",
      "lif layer 2 self.abs_max_v: 1859.0\n",
      "lif layer 2 self.abs_max_v: 1872.5\n",
      "lif layer 1 self.abs_max_v: 4836.5\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "fc layer 1 self.abs_max_out: 3300.0\n",
      "fc layer 2 self.abs_max_out: 1309.0\n",
      "lif layer 1 self.abs_max_v: 5136.5\n",
      "lif layer 1 self.abs_max_v: 5299.0\n",
      "fc layer 2 self.abs_max_out: 1319.0\n",
      "fc layer 3 self.abs_max_out: 495.0\n",
      "lif layer 2 self.abs_max_v: 1952.0\n",
      "fc layer 2 self.abs_max_out: 1325.0\n",
      "fc layer 2 self.abs_max_out: 1345.0\n",
      "fc layer 2 self.abs_max_out: 1366.0\n",
      "lif layer 2 self.abs_max_v: 1959.0\n",
      "fc layer 2 self.abs_max_out: 1411.0\n",
      "lif layer 2 self.abs_max_v: 2007.5\n",
      "lif layer 2 self.abs_max_v: 2076.0\n",
      "lif layer 2 self.abs_max_v: 2100.0\n",
      "fc layer 1 self.abs_max_out: 3352.0\n",
      "fc layer 2 self.abs_max_out: 1421.0\n",
      "fc layer 2 self.abs_max_out: 1739.0\n",
      "fc layer 2 self.abs_max_out: 1790.0\n",
      "fc layer 2 self.abs_max_out: 1857.0\n",
      "fc layer 2 self.abs_max_out: 1975.0\n",
      "fc layer 2 self.abs_max_out: 1988.0\n",
      "lif layer 2 self.abs_max_v: 2221.5\n",
      "fc layer 1 self.abs_max_out: 3405.0\n",
      "fc layer 2 self.abs_max_out: 2033.0\n",
      "fc layer 2 self.abs_max_out: 2109.0\n",
      "lif layer 2 self.abs_max_v: 2316.5\n",
      "lif layer 2 self.abs_max_v: 2534.5\n",
      "lif layer 2 self.abs_max_v: 2681.5\n",
      "lif layer 2 self.abs_max_v: 3049.5\n",
      "lif layer 2 self.abs_max_v: 3203.0\n",
      "lif layer 2 self.abs_max_v: 3341.5\n",
      "lif layer 2 self.abs_max_v: 3411.0\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 1 self.abs_max_out: 3646.0\n",
      "fc layer 1 self.abs_max_out: 3893.0\n",
      "fc layer 1 self.abs_max_out: 3939.0\n",
      "fc layer 1 self.abs_max_out: 4054.0\n",
      "fc layer 1 self.abs_max_out: 4200.0\n",
      "fc layer 1 self.abs_max_out: 4294.0\n",
      "fc layer 1 self.abs_max_out: 4309.0\n",
      "fc layer 1 self.abs_max_out: 4611.0\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "fc layer 2 self.abs_max_out: 2191.0\n",
      "fc layer 1 self.abs_max_out: 4926.0\n",
      "lif layer 1 self.abs_max_v: 5495.0\n",
      "fc layer 1 self.abs_max_out: 4955.0\n",
      "lif layer 1 self.abs_max_v: 5497.0\n",
      "lif layer 1 self.abs_max_v: 5605.0\n",
      "fc layer 1 self.abs_max_out: 5055.0\n",
      "lif layer 1 self.abs_max_v: 5650.0\n",
      "fc layer 1 self.abs_max_out: 5336.0\n",
      "lif layer 1 self.abs_max_v: 5668.5\n",
      "lif layer 2 self.abs_max_v: 3465.0\n",
      "fc layer 2 self.abs_max_out: 2208.0\n",
      "fc layer 2 self.abs_max_out: 2250.0\n",
      "fc layer 2 self.abs_max_out: 2324.0\n",
      "fc layer 2 self.abs_max_out: 2377.0\n",
      "fc layer 2 self.abs_max_out: 2380.0\n",
      "fc layer 2 self.abs_max_out: 2403.0\n",
      "lif layer 2 self.abs_max_v: 3497.0\n",
      "lif layer 1 self.abs_max_v: 5795.5\n",
      "fc layer 2 self.abs_max_out: 2407.0\n",
      "fc layer 2 self.abs_max_out: 2420.0\n",
      "fc layer 2 self.abs_max_out: 2608.0\n",
      "fc layer 2 self.abs_max_out: 2610.0\n",
      "fc layer 2 self.abs_max_out: 2645.0\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2034.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 2367.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 2441.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 2638.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 2710.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 2829.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 3159.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 3206.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 3264.00 at epoch 0, iter 4031\n",
      "lif layer 1 self.abs_max_v: 5852.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-0   lr=['4.0000000'], tr/val_loss:168.921722/347.803040, val:  50.00%, val_best:  50.00%, tr:  95.71%, tr_best:  95.71%, epoch time: 253.59 seconds, 4.23 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32256 real_backward_count 6488  20.114%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 78.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 513.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 3 self.abs_max_out: 567.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "fc layer 2 self.abs_max_out: 2670.0\n",
      "fc layer 2 self.abs_max_out: 2708.0\n",
      "fc layer 1 self.abs_max_out: 5705.0\n",
      "fc layer 2 self.abs_max_out: 2755.0\n",
      "fc layer 2 self.abs_max_out: 2771.0\n",
      "lif layer 1 self.abs_max_v: 5884.5\n",
      "fc layer 2 self.abs_max_out: 2855.0\n",
      "fc layer 2 self.abs_max_out: 3010.0\n",
      "fc layer 2 self.abs_max_out: 3070.0\n",
      "fc layer 2 self.abs_max_out: 3114.0\n",
      "fc layer 2 self.abs_max_out: 3148.0\n",
      "fc layer 2 self.abs_max_out: 3158.0\n",
      "fc layer 2 self.abs_max_out: 3189.0\n",
      "lif layer 2 self.abs_max_v: 3537.5\n",
      "fc layer 1 self.abs_max_out: 5785.0\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "fc layer 2 self.abs_max_out: 3381.0\n",
      "fc layer 2 self.abs_max_out: 3454.0\n",
      "fc layer 1 self.abs_max_out: 5920.0\n",
      "lif layer 1 self.abs_max_v: 5920.0\n",
      "fc layer 1 self.abs_max_out: 6180.0\n",
      "lif layer 1 self.abs_max_v: 6180.0\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 3 self.abs_max_out: 666.0\n",
      "fc layer 2 self.abs_max_out: 3464.0\n",
      "fc layer 2 self.abs_max_out: 3491.0\n",
      "lif layer 1 self.abs_max_v: 6304.0\n",
      "lif layer 1 self.abs_max_v: 6826.5\n",
      "fc layer 2 self.abs_max_out: 3505.0\n",
      "fc layer 3 self.abs_max_out: 714.0\n",
      "fc layer 3 self.abs_max_out: 723.0\n",
      "fc layer 2 self.abs_max_out: 3550.0\n",
      "lif layer 2 self.abs_max_v: 3550.0\n",
      "lif layer 1 self.abs_max_v: 7128.5\n",
      "lif layer 1 self.abs_max_v: 7177.0\n",
      "fc layer 2 self.abs_max_out: 3564.0\n",
      "lif layer 2 self.abs_max_v: 3564.0\n",
      "fc layer 2 self.abs_max_out: 3629.0\n",
      "lif layer 2 self.abs_max_v: 3629.0\n",
      "lif layer 2 self.abs_max_v: 3664.0\n",
      "lif layer 2 self.abs_max_v: 4032.0\n",
      "lif layer 2 self.abs_max_v: 4413.0\n",
      "fc layer 1 self.abs_max_out: 6281.0\n",
      "fc layer 1 self.abs_max_out: 6516.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-1   lr=['4.0000000'], tr/val_loss:389.272644/324.020508, val:  50.00%, val_best:  50.00%, tr:  97.62%, tr_best:  97.62%, epoch time: 250.80 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.1175%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64512 real_backward_count 13007  20.162%\n",
      "layer   1  Sparsity: 86.3525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 7723.0\n",
      "fc layer 1 self.abs_max_out: 6716.0\n",
      "fc layer 1 self.abs_max_out: 6719.0\n",
      "fc layer 1 self.abs_max_out: 6749.0\n",
      "fc layer 1 self.abs_max_out: 7083.0\n",
      "lif layer 1 self.abs_max_v: 7783.0\n",
      "lif layer 1 self.abs_max_v: 8489.5\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "fc layer 2 self.abs_max_out: 3783.0\n",
      "fc layer 2 self.abs_max_out: 3911.0\n",
      "lif layer 2 self.abs_max_v: 4519.0\n",
      "lif layer 2 self.abs_max_v: 4539.0\n",
      "lif layer 1 self.abs_max_v: 9675.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 3515.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 3700.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 3767.00 at epoch 2, iter 4031\n",
      "lif layer 2 self.abs_max_v: 4687.5\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-2   lr=['4.0000000'], tr/val_loss:334.486115/332.123871, val:  50.00%, val_best:  50.00%, tr:  96.83%, tr_best:  97.62%, epoch time: 250.50 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96768 real_backward_count 19486  20.137%\n",
      "layer   1  Sparsity: 75.9277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4847.5\n",
      "fc layer 1 self.abs_max_out: 7236.0\n",
      "lif layer 1 self.abs_max_v: 10298.0\n",
      "fc layer 1 self.abs_max_out: 7400.0\n",
      "fc layer 1 self.abs_max_out: 7483.0\n",
      "fc layer 1 self.abs_max_out: 7676.0\n",
      "fc layer 1 self.abs_max_out: 7712.0\n",
      "lif layer 2 self.abs_max_v: 4916.0\n",
      "lif layer 2 self.abs_max_v: 5150.0\n",
      "fc layer 1 self.abs_max_out: 7714.0\n",
      "fc layer 3 self.abs_max_out: 797.0\n",
      "fc layer 3 self.abs_max_out: 826.0\n",
      "fc layer 3 self.abs_max_out: 834.0\n",
      "lif layer 2 self.abs_max_v: 5340.0\n",
      "lif layer 1 self.abs_max_v: 10470.5\n",
      "lif layer 1 self.abs_max_v: 10505.0\n",
      "lif layer 1 self.abs_max_v: 11138.0\n",
      "lif layer 2 self.abs_max_v: 5357.5\n",
      "lif layer 1 self.abs_max_v: 11316.5\n",
      "lif layer 2 self.abs_max_v: 5398.5\n",
      "lif layer 2 self.abs_max_v: 5474.5\n",
      "lif layer 2 self.abs_max_v: 5506.0\n",
      "lif layer 2 self.abs_max_v: 5550.0\n",
      "lif layer 2 self.abs_max_v: 5584.0\n",
      "fc layer 1 self.abs_max_out: 7808.0\n",
      "lif layer 2 self.abs_max_v: 5612.0\n",
      "lif layer 2 self.abs_max_v: 5678.0\n",
      "lif layer 2 self.abs_max_v: 5734.0\n",
      "lif layer 2 self.abs_max_v: 5754.5\n",
      "lif layer 2 self.abs_max_v: 5830.0\n",
      "lif layer 2 self.abs_max_v: 5951.0\n",
      "lif layer 2 self.abs_max_v: 5980.0\n",
      "lif layer 2 self.abs_max_v: 5987.5\n",
      "lif layer 2 self.abs_max_v: 6135.0\n",
      "lif layer 2 self.abs_max_v: 6156.5\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 2 self.abs_max_v: 6225.5\n",
      "lif layer 2 self.abs_max_v: 6250.5\n",
      "lif layer 2 self.abs_max_v: 6319.5\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-3   lr=['4.0000000'], tr/val_loss:359.368958/350.436981, val:  50.00%, val_best:  50.00%, tr:  96.75%, tr_best:  97.62%, epoch time: 251.25 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.1011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 129024 real_backward_count 25914  20.085%\n",
      "layer   1  Sparsity: 78.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 11527.0\n",
      "fc layer 1 self.abs_max_out: 7935.0\n",
      "fc layer 1 self.abs_max_out: 8006.0\n",
      "lif layer 1 self.abs_max_v: 11717.5\n",
      "lif layer 1 self.abs_max_v: 12075.5\n",
      "lif layer 1 self.abs_max_v: 13129.0\n",
      "fc layer 1 self.abs_max_out: 8136.0\n",
      "lif layer 1 self.abs_max_v: 13189.5\n",
      "lif layer 1 self.abs_max_v: 13319.0\n",
      "fc layer 1 self.abs_max_out: 8311.0\n",
      "fc layer 1 self.abs_max_out: 8579.0\n",
      "fc layer 1 self.abs_max_out: 8661.0\n",
      "lif layer 1 self.abs_max_v: 13755.0\n",
      "lif layer 1 self.abs_max_v: 14033.5\n",
      "fc layer 1 self.abs_max_out: 8946.0\n",
      "fc layer 1 self.abs_max_out: 9038.0\n",
      "train - Value 0: 1993 occurrences\n",
      "train - Value 1: 2039 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 3874.00 at epoch 4, iter 4031\n",
      "max_activation_accul updated: 4118.00 at epoch 4, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-4   lr=['4.0000000'], tr/val_loss:339.636078/434.504181, val:  50.00%, val_best:  50.00%, tr:  97.05%, tr_best:  97.62%, epoch time: 251.09 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0826%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.3855%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161280 real_backward_count 32331  20.047%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9185.0\n",
      "fc layer 1 self.abs_max_out: 9193.0\n",
      "lif layer 1 self.abs_max_v: 14526.0\n",
      "lif layer 1 self.abs_max_v: 14734.0\n",
      "lif layer 1 self.abs_max_v: 14963.0\n",
      "fc layer 1 self.abs_max_out: 9319.0\n",
      "fc layer 1 self.abs_max_out: 9382.0\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 3939.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-5   lr=['4.0000000'], tr/val_loss:502.323273/377.323395, val:  50.00%, val_best:  50.00%, tr:  96.97%, tr_best:  97.62%, epoch time: 248.28 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.2648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193536 real_backward_count 38947  20.124%\n",
      "layer   1  Sparsity: 85.8154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 3941.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-6   lr=['4.0000000'], tr/val_loss:420.337372/287.383301, val:  50.00%, val_best:  50.00%, tr:  97.32%, tr_best:  97.62%, epoch time: 250.47 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9339%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225792 real_backward_count 45447  20.128%\n",
      "layer   1  Sparsity: 86.9141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2058 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 3953.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 428 occurrences\n",
      "test - Value 1: 24 occurrences\n",
      "epoch-7   lr=['4.0000000'], tr/val_loss:284.970215/272.991455, val:  55.31%, val_best:  55.31%, tr:  95.73%, tr_best:  97.62%, epoch time: 249.72 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 258048 real_backward_count 52195  20.227%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 15103.5\n",
      "fc layer 1 self.abs_max_out: 9399.0\n",
      "fc layer 1 self.abs_max_out: 9487.0\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2058 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-8   lr=['4.0000000'], tr/val_loss:246.895264/346.183563, val:  50.00%, val_best:  55.31%, tr:  95.98%, tr_best:  97.62%, epoch time: 248.73 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290304 real_backward_count 58957  20.309%\n",
      "layer   1  Sparsity: 83.5938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9577.0\n",
      "fc layer 1 self.abs_max_out: 9650.0\n",
      "fc layer 1 self.abs_max_out: 9840.0\n",
      "fc layer 3 self.abs_max_out: 849.0\n",
      "fc layer 2 self.abs_max_out: 4006.0\n",
      "lif layer 1 self.abs_max_v: 15832.5\n",
      "fc layer 1 self.abs_max_out: 9951.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 4194.00 at epoch 9, iter 4031\n",
      "max_activation_accul updated: 4723.00 at epoch 9, iter 4031\n",
      "max_activation_accul updated: 4810.00 at epoch 9, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-9   lr=['4.0000000'], tr/val_loss:173.806992/331.931244, val:  50.00%, val_best:  55.31%, tr:  96.35%, tr_best:  97.62%, epoch time: 249.48 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.4968%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322560 real_backward_count 65820  20.406%\n",
      "layer   1  Sparsity: 80.6152%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 882.0\n",
      "fc layer 1 self.abs_max_out: 9955.0\n",
      "lif layer 2 self.abs_max_v: 6492.0\n",
      "lif layer 2 self.abs_max_v: 6608.5\n",
      "lif layer 2 self.abs_max_v: 6757.5\n",
      "lif layer 1 self.abs_max_v: 15840.0\n",
      "train - Value 0: 2044 occurrences\n",
      "train - Value 1: 1988 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-10  lr=['4.0000000'], tr/val_loss:202.962296/ 83.078911, val:  50.00%, val_best:  55.31%, tr:  97.07%, tr_best:  97.62%, epoch time: 250.15 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6035%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.5279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354816 real_backward_count 72469  20.424%\n",
      "layer   1  Sparsity: 87.9150%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 6848.5\n",
      "lif layer 2 self.abs_max_v: 6996.0\n",
      "lif layer 2 self.abs_max_v: 7043.0\n",
      "lif layer 2 self.abs_max_v: 7297.0\n",
      "fc layer 2 self.abs_max_out: 4032.0\n",
      "train - Value 0: 1981 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-11  lr=['4.0000000'], tr/val_loss:190.048248/194.754486, val:  50.00%, val_best:  55.31%, tr:  95.61%, tr_best:  97.62%, epoch time: 248.44 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.2817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 387072 real_backward_count 79423  20.519%\n",
      "layer   1  Sparsity: 84.6924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1015.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-12  lr=['4.0000000'], tr/val_loss:189.550690/218.827713, val:  64.16%, val_best:  64.16%, tr:  96.25%, tr_best:  97.62%, epoch time: 247.82 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.0689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419328 real_backward_count 86575  20.646%\n",
      "layer   1  Sparsity: 88.7939%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4039.0\n",
      "fc layer 2 self.abs_max_out: 4046.0\n",
      "fc layer 2 self.abs_max_out: 4079.0\n",
      "fc layer 2 self.abs_max_out: 4125.0\n",
      "fc layer 2 self.abs_max_out: 4159.0\n",
      "fc layer 2 self.abs_max_out: 4186.0\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 147 occurrences\n",
      "test - Value 1: 305 occurrences\n",
      "epoch-13  lr=['4.0000000'], tr/val_loss:266.228577/219.086044, val:  65.71%, val_best:  65.71%, tr:  96.43%, tr_best:  97.62%, epoch time: 251.10 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.2160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451584 real_backward_count 93619  20.731%\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 16134.0\n",
      "fc layer 2 self.abs_max_out: 4204.0\n",
      "fc layer 2 self.abs_max_out: 4242.0\n",
      "fc layer 1 self.abs_max_out: 10060.0\n",
      "fc layer 1 self.abs_max_out: 10069.0\n",
      "fc layer 2 self.abs_max_out: 4350.0\n",
      "fc layer 1 self.abs_max_out: 10206.0\n",
      "fc layer 1 self.abs_max_out: 10367.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 4825.00 at epoch 14, iter 4031\n",
      "max_activation_accul updated: 5471.00 at epoch 14, iter 4031\n",
      "max_activation_accul updated: 5652.00 at epoch 14, iter 4031\n",
      "max_activation_accul updated: 5696.00 at epoch 14, iter 4031\n",
      "max_activation_accul updated: 5708.00 at epoch 14, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 435 occurrences\n",
      "test - Value 1: 17 occurrences\n",
      "epoch-14  lr=['4.0000000'], tr/val_loss:271.499054/290.384003, val:  53.76%, val_best:  65.71%, tr:  96.83%, tr_best:  97.62%, epoch time: 250.24 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.9046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483840 real_backward_count 100719  20.817%\n",
      "layer   1  Sparsity: 94.1406%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10518.0\n",
      "fc layer 1 self.abs_max_out: 10755.0\n",
      "lif layer 1 self.abs_max_v: 16847.0\n",
      "fc layer 1 self.abs_max_out: 10881.0\n",
      "fc layer 2 self.abs_max_out: 4359.0\n",
      "fc layer 2 self.abs_max_out: 4408.0\n",
      "fc layer 1 self.abs_max_out: 10996.0\n",
      "fc layer 1 self.abs_max_out: 11292.0\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 352 occurrences\n",
      "test - Value 1: 100 occurrences\n",
      "epoch-15  lr=['4.0000000'], tr/val_loss:313.547241/288.869293, val:  63.27%, val_best:  65.71%, tr:  96.70%, tr_best:  97.62%, epoch time: 249.16 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.2217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 516096 real_backward_count 107641  20.857%\n",
      "layer   1  Sparsity: 86.2793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4428.0\n",
      "fc layer 1 self.abs_max_out: 11330.0\n",
      "fc layer 3 self.abs_max_out: 1022.0\n",
      "fc layer 1 self.abs_max_out: 11648.0\n",
      "fc layer 3 self.abs_max_out: 1023.0\n",
      "fc layer 3 self.abs_max_out: 1031.0\n",
      "fc layer 3 self.abs_max_out: 1052.0\n",
      "fc layer 3 self.abs_max_out: 1056.0\n",
      "fc layer 3 self.abs_max_out: 1092.0\n",
      "fc layer 3 self.abs_max_out: 1105.0\n",
      "fc layer 3 self.abs_max_out: 1121.0\n",
      "fc layer 1 self.abs_max_out: 11706.0\n",
      "fc layer 2 self.abs_max_out: 4606.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 442 occurrences\n",
      "test - Value 1: 10 occurrences\n",
      "epoch-16  lr=['4.0000000'], tr/val_loss:301.533844/252.009384, val:  52.21%, val_best:  65.71%, tr:  97.17%, tr_best:  97.62%, epoch time: 249.42 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8300%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548352 real_backward_count 114371  20.857%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 43.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4690.0\n",
      "fc layer 2 self.abs_max_out: 4936.0\n",
      "fc layer 2 self.abs_max_out: 5422.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 5908.00 at epoch 17, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-17  lr=['4.0000000'], tr/val_loss:351.838959/406.781586, val:  50.00%, val_best:  65.71%, tr:  97.87%, tr_best:  97.87%, epoch time: 249.03 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580608 real_backward_count 120787  20.804%\n",
      "layer   1  Sparsity: 76.7822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1148.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 6175.00 at epoch 18, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-18  lr=['4.0000000'], tr/val_loss:341.449768/427.153076, val:  50.00%, val_best:  65.71%, tr:  98.26%, tr_best:  98.26%, epoch time: 244.65 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.2624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612864 real_backward_count 127076  20.735%\n",
      "layer   1  Sparsity: 88.8184%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 443 occurrences\n",
      "test - Value 1: 9 occurrences\n",
      "epoch-19  lr=['4.0000000'], tr/val_loss:351.172089/312.461182, val:  51.99%, val_best:  65.71%, tr:  98.16%, tr_best:  98.26%, epoch time: 246.66 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7988%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 645120 real_backward_count 133367  20.673%\n",
      "layer   1  Sparsity: 83.1787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11871.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 170 occurrences\n",
      "test - Value 1: 282 occurrences\n",
      "epoch-20  lr=['4.0000000'], tr/val_loss:333.893555/201.846542, val:  75.22%, val_best:  75.22%, tr:  98.24%, tr_best:  98.26%, epoch time: 250.57 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677376 real_backward_count 139774  20.635%\n",
      "layer   1  Sparsity: 86.4746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 17539.0\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 352 occurrences\n",
      "test - Value 1: 100 occurrences\n",
      "epoch-21  lr=['4.0000000'], tr/val_loss:362.783630/341.843903, val:  67.70%, val_best:  75.22%, tr:  98.44%, tr_best:  98.44%, epoch time: 251.02 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709632 real_backward_count 146035  20.579%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1159.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 6357.00 at epoch 22, iter 4031\n",
      "max_activation_accul updated: 7146.00 at epoch 22, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-22  lr=['4.0000000'], tr/val_loss:416.739105/509.178009, val:  50.00%, val_best:  75.22%, tr:  98.56%, tr_best:  98.56%, epoch time: 251.61 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3350%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741888 real_backward_count 152344  20.535%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 437 occurrences\n",
      "test - Value 1: 15 occurrences\n",
      "epoch-23  lr=['4.0000000'], tr/val_loss:412.550873/334.890503, val:  53.32%, val_best:  75.22%, tr:  98.34%, tr_best:  98.56%, epoch time: 250.35 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 774144 real_backward_count 158590  20.486%\n",
      "layer   1  Sparsity: 76.9287%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1169.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 337 occurrences\n",
      "test - Value 1: 115 occurrences\n",
      "epoch-24  lr=['4.0000000'], tr/val_loss:410.686371/356.906128, val:  71.02%, val_best:  75.22%, tr:  98.44%, tr_best:  98.56%, epoch time: 248.74 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.7197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806400 real_backward_count 164816  20.438%\n",
      "layer   1  Sparsity: 74.2432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7352.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 12035.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-25  lr=['4.0000000'], tr/val_loss:411.183411/429.902008, val:  50.66%, val_best:  75.22%, tr:  98.46%, tr_best:  98.56%, epoch time: 250.16 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838656 real_backward_count 171075  20.399%\n",
      "layer   1  Sparsity: 77.8564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1177.0\n",
      "lif layer 2 self.abs_max_v: 7453.5\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-26  lr=['4.0000000'], tr/val_loss:411.341797/462.161957, val:  50.22%, val_best:  75.22%, tr:  98.74%, tr_best:  98.74%, epoch time: 250.01 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870912 real_backward_count 177183  20.345%\n",
      "layer   1  Sparsity: 73.9990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7644.0\n",
      "lif layer 2 self.abs_max_v: 7869.0\n",
      "lif layer 2 self.abs_max_v: 8202.5\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 12089.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-27  lr=['4.0000000'], tr/val_loss:370.644043/395.237823, val:  51.11%, val_best:  75.22%, tr:  98.46%, tr_best:  98.74%, epoch time: 250.04 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 903168 real_backward_count 183389  20.305%\n",
      "layer   1  Sparsity: 81.9824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1225.0\n",
      "lif layer 1 self.abs_max_v: 17836.0\n",
      "lif layer 1 self.abs_max_v: 18288.0\n",
      "fc layer 1 self.abs_max_out: 12386.0\n",
      "lif layer 1 self.abs_max_v: 18523.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 12502.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 445 occurrences\n",
      "test - Value 1: 7 occurrences\n",
      "epoch-28  lr=['4.0000000'], tr/val_loss:391.910004/352.281250, val:  51.55%, val_best:  75.22%, tr:  98.24%, tr_best:  98.74%, epoch time: 250.80 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 935424 real_backward_count 189650  20.274%\n",
      "layer   1  Sparsity: 82.2998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 12611.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-29  lr=['4.0000000'], tr/val_loss:389.316925/416.438263, val:  51.11%, val_best:  75.22%, tr:  98.44%, tr_best:  98.74%, epoch time: 250.15 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.6793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967680 real_backward_count 195822  20.236%\n",
      "layer   1  Sparsity: 90.6250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 377 occurrences\n",
      "test - Value 1: 75 occurrences\n",
      "epoch-30  lr=['4.0000000'], tr/val_loss:422.496552/346.821289, val:  63.50%, val_best:  75.22%, tr:  98.54%, tr_best:  98.74%, epoch time: 250.93 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999936 real_backward_count 201984  20.200%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 361 occurrences\n",
      "test - Value 1: 91 occurrences\n",
      "epoch-31  lr=['4.0000000'], tr/val_loss:405.070282/385.533813, val:  67.48%, val_best:  75.22%, tr:  98.66%, tr_best:  98.74%, epoch time: 251.13 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.4339%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1032192 real_backward_count 208203  20.171%\n",
      "layer   1  Sparsity: 76.0986%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1260.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 230 occurrences\n",
      "test - Value 1: 222 occurrences\n",
      "epoch-32  lr=['4.0000000'], tr/val_loss:422.534454/379.110901, val:  78.32%, val_best:  78.32%, tr:  98.83%, tr_best:  98.83%, epoch time: 251.13 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 82.6664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1064448 real_backward_count 214345  20.137%\n",
      "layer   1  Sparsity: 91.4795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12871.0\n",
      "lif layer 1 self.abs_max_v: 19051.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-33  lr=['4.0000000'], tr/val_loss:427.941193/389.622040, val:  76.11%, val_best:  78.32%, tr:  98.78%, tr_best:  98.83%, epoch time: 247.96 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.2398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096704 real_backward_count 220507  20.106%\n",
      "layer   1  Sparsity: 76.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12892.0\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 333 occurrences\n",
      "test - Value 1: 119 occurrences\n",
      "epoch-34  lr=['4.0000000'], tr/val_loss:446.724274/398.929199, val:  72.79%, val_best:  78.32%, tr:  98.88%, tr_best:  98.88%, epoch time: 247.21 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128960 real_backward_count 226753  20.085%\n",
      "layer   1  Sparsity: 88.6230%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5425.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 12997.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 10 occurrences\n",
      "test - Value 1: 442 occurrences\n",
      "epoch-35  lr=['4.0000000'], tr/val_loss:428.988922/451.776337, val:  52.21%, val_best:  78.32%, tr:  98.88%, tr_best:  98.88%, epoch time: 249.27 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.4109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1161216 real_backward_count 232839  20.051%\n",
      "layer   1  Sparsity: 95.0195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8257.5\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 448 occurrences\n",
      "test - Value 1: 4 occurrences\n",
      "epoch-36  lr=['4.0000000'], tr/val_loss:416.173523/347.806000, val:  50.88%, val_best:  78.32%, tr:  98.74%, tr_best:  98.88%, epoch time: 247.46 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1193472 real_backward_count 238971  20.023%\n",
      "layer   1  Sparsity: 81.8848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 19206.5\n",
      "fc layer 1 self.abs_max_out: 13310.0\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-37  lr=['4.0000000'], tr/val_loss:429.458893/523.223999, val:  50.00%, val_best:  78.32%, tr:  98.59%, tr_best:  98.88%, epoch time: 249.69 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1736%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225728 real_backward_count 245105  19.997%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 19442.5\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-38  lr=['4.0000000'], tr/val_loss:451.722839/389.538300, val:  50.44%, val_best:  78.32%, tr:  99.03%, tr_best:  99.03%, epoch time: 249.05 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257984 real_backward_count 251222  19.970%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1261.0\n",
      "fc layer 3 self.abs_max_out: 1268.0\n",
      "fc layer 1 self.abs_max_out: 13408.0\n",
      "train - Value 0: 1990 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-39  lr=['4.0000000'], tr/val_loss:439.082092/520.697327, val:  50.44%, val_best:  78.32%, tr:  98.46%, tr_best:  99.03%, epoch time: 249.59 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1290240 real_backward_count 257430  19.952%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 207 occurrences\n",
      "test - Value 1: 245 occurrences\n",
      "epoch-40  lr=['4.0000000'], tr/val_loss:456.902588/404.517914, val:  78.54%, val_best:  78.54%, tr:  98.83%, tr_best:  99.03%, epoch time: 248.79 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.4567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1322496 real_backward_count 263533  19.927%\n",
      "layer   1  Sparsity: 84.2773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 13611.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 54 occurrences\n",
      "test - Value 1: 398 occurrences\n",
      "epoch-41  lr=['4.0000000'], tr/val_loss:434.053864/406.301575, val:  61.06%, val_best:  78.54%, tr:  98.88%, tr_best:  99.03%, epoch time: 248.61 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3697%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.6803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354752 real_backward_count 269725  19.910%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 293 occurrences\n",
      "test - Value 1: 159 occurrences\n",
      "epoch-42  lr=['4.0000000'], tr/val_loss:430.436676/378.716034, val:  76.77%, val_best:  78.54%, tr:  98.98%, tr_best:  99.03%, epoch time: 248.16 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7761%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1387008 real_backward_count 275854  19.888%\n",
      "layer   1  Sparsity: 80.7861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 19528.0\n",
      "lif layer 1 self.abs_max_v: 19735.0\n",
      "fc layer 2 self.abs_max_out: 5668.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 183 occurrences\n",
      "test - Value 1: 269 occurrences\n",
      "epoch-43  lr=['4.0000000'], tr/val_loss:421.514526/374.092377, val:  77.65%, val_best:  78.54%, tr:  98.96%, tr_best:  99.03%, epoch time: 247.23 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0106%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419264 real_backward_count 282060  19.874%\n",
      "layer   1  Sparsity: 89.6240%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-44  lr=['4.0000000'], tr/val_loss:426.553406/476.758179, val:  50.88%, val_best:  78.54%, tr:  98.88%, tr_best:  99.03%, epoch time: 248.41 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5896%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1451520 real_backward_count 288189  19.854%\n",
      "layer   1  Sparsity: 83.5449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1269.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 428 occurrences\n",
      "test - Value 1: 24 occurrences\n",
      "epoch-45  lr=['4.0000000'], tr/val_loss:427.109924/389.972992, val:  55.31%, val_best:  78.54%, tr:  98.83%, tr_best:  99.03%, epoch time: 248.20 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483776 real_backward_count 294283  19.833%\n",
      "layer   1  Sparsity: 94.3604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-46  lr=['4.0000000'], tr/val_loss:430.267151/400.883087, val:  75.88%, val_best:  78.54%, tr:  99.13%, tr_best:  99.13%, epoch time: 248.57 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1516032 real_backward_count 300361  19.812%\n",
      "layer   1  Sparsity: 76.4893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 19828.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 445 occurrences\n",
      "test - Value 1: 7 occurrences\n",
      "epoch-47  lr=['4.0000000'], tr/val_loss:439.091095/386.286469, val:  51.55%, val_best:  78.54%, tr:  98.93%, tr_best:  99.13%, epoch time: 248.79 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1675%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1548288 real_backward_count 306438  19.792%\n",
      "layer   1  Sparsity: 85.2783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 20039.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-48  lr=['4.0000000'], tr/val_loss:430.099426/359.786560, val:  51.33%, val_best:  78.54%, tr:  98.74%, tr_best:  99.13%, epoch time: 248.73 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1580544 real_backward_count 312470  19.770%\n",
      "layer   1  Sparsity: 90.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1270.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-49  lr=['4.0000000'], tr/val_loss:422.159607/525.843872, val:  50.00%, val_best:  78.54%, tr:  98.91%, tr_best:  99.13%, epoch time: 247.56 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9446%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612800 real_backward_count 318624  19.756%\n",
      "layer   1  Sparsity: 82.8613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8366.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 17 occurrences\n",
      "test - Value 1: 435 occurrences\n",
      "epoch-50  lr=['4.0000000'], tr/val_loss:423.733917/464.435760, val:  53.76%, val_best:  78.54%, tr:  98.86%, tr_best:  99.13%, epoch time: 248.76 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1645056 real_backward_count 324742  19.740%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 189 occurrences\n",
      "test - Value 1: 263 occurrences\n",
      "epoch-51  lr=['4.0000000'], tr/val_loss:423.217865/415.585785, val:  78.54%, val_best:  78.54%, tr:  98.71%, tr_best:  99.13%, epoch time: 247.55 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4940%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1677312 real_backward_count 330856  19.725%\n",
      "layer   1  Sparsity: 88.3789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 20255.5\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-52  lr=['4.0000000'], tr/val_loss:435.831055/489.625732, val:  50.00%, val_best:  78.54%, tr:  98.86%, tr_best:  99.13%, epoch time: 247.54 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1709568 real_backward_count 336948  19.710%\n",
      "layer   1  Sparsity: 92.7490%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-53  lr=['4.0000000'], tr/val_loss:407.414642/360.144989, val:  51.33%, val_best:  78.54%, tr:  99.06%, tr_best:  99.13%, epoch time: 249.18 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8816%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1741824 real_backward_count 343051  19.695%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 356 occurrences\n",
      "test - Value 1: 96 occurrences\n",
      "epoch-54  lr=['4.0000000'], tr/val_loss:415.260101/354.398743, val:  68.14%, val_best:  78.54%, tr:  98.86%, tr_best:  99.13%, epoch time: 247.73 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1774080 real_backward_count 349155  19.681%\n",
      "layer   1  Sparsity: 82.2510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 370 occurrences\n",
      "test - Value 1: 82 occurrences\n",
      "epoch-55  lr=['4.0000000'], tr/val_loss:402.639954/325.962494, val:  66.81%, val_best:  78.54%, tr:  99.13%, tr_best:  99.13%, epoch time: 246.92 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6659%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1806336 real_backward_count 355335  19.672%\n",
      "layer   1  Sparsity: 82.3242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-56  lr=['4.0000000'], tr/val_loss:385.114410/473.110321, val:  50.44%, val_best:  78.54%, tr:  99.13%, tr_best:  99.13%, epoch time: 249.92 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2962%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1838592 real_backward_count 361322  19.652%\n",
      "layer   1  Sparsity: 90.3564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 7238.00 at epoch 57, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 448 occurrences\n",
      "test - Value 1: 4 occurrences\n",
      "epoch-57  lr=['4.0000000'], tr/val_loss:401.150543/376.691101, val:  50.88%, val_best:  78.54%, tr:  98.96%, tr_best:  99.13%, epoch time: 249.82 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1870848 real_backward_count 367425  19.639%\n",
      "layer   1  Sparsity: 92.8955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 1288.0\n",
      "fc layer 3 self.abs_max_out: 1348.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 407 occurrences\n",
      "test - Value 1: 45 occurrences\n",
      "epoch-58  lr=['4.0000000'], tr/val_loss:404.453217/349.829437, val:  59.07%, val_best:  78.54%, tr:  98.81%, tr_best:  99.13%, epoch time: 248.76 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7826%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1903104 real_backward_count 373578  19.630%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 252 occurrences\n",
      "test - Value 1: 200 occurrences\n",
      "epoch-59  lr=['4.0000000'], tr/val_loss:403.618256/355.463013, val:  81.86%, val_best:  81.86%, tr:  99.13%, tr_best:  99.13%, epoch time: 247.18 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1935360 real_backward_count 379613  19.615%\n",
      "layer   1  Sparsity: 84.2773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 410 occurrences\n",
      "test - Value 1: 42 occurrences\n",
      "epoch-60  lr=['4.0000000'], tr/val_loss:408.231995/371.499756, val:  58.41%, val_best:  81.86%, tr:  99.03%, tr_best:  99.13%, epoch time: 249.19 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1967616 real_backward_count 385742  19.605%\n",
      "layer   1  Sparsity: 75.9766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 30 occurrences\n",
      "test - Value 1: 422 occurrences\n",
      "epoch-61  lr=['4.0000000'], tr/val_loss:418.918060/418.967987, val:  56.19%, val_best:  81.86%, tr:  99.11%, tr_best:  99.13%, epoch time: 246.77 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1999872 real_backward_count 391776  19.590%\n",
      "layer   1  Sparsity: 79.2969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 448 occurrences\n",
      "test - Value 1: 4 occurrences\n",
      "epoch-62  lr=['4.0000000'], tr/val_loss:427.455292/357.805939, val:  50.88%, val_best:  81.86%, tr:  99.33%, tr_best:  99.33%, epoch time: 248.66 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2032128 real_backward_count 397836  19.577%\n",
      "layer   1  Sparsity: 81.5674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8459.0\n",
      "lif layer 2 self.abs_max_v: 8588.5\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-63  lr=['4.0000000'], tr/val_loss:416.566284/372.609375, val:  50.44%, val_best:  81.86%, tr:  99.21%, tr_best:  99.33%, epoch time: 247.89 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3713%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2064384 real_backward_count 403857  19.563%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8793.0\n",
      "lif layer 2 self.abs_max_v: 8955.0\n",
      "lif layer 2 self.abs_max_v: 9100.5\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 362 occurrences\n",
      "test - Value 1: 90 occurrences\n",
      "epoch-64  lr=['4.0000000'], tr/val_loss:417.060730/347.698608, val:  67.26%, val_best:  81.86%, tr:  99.43%, tr_best:  99.43%, epoch time: 247.47 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2096640 real_backward_count 409923  19.551%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9145.0\n",
      "lif layer 2 self.abs_max_v: 9204.0\n",
      "lif layer 2 self.abs_max_v: 9256.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 18 occurrences\n",
      "test - Value 1: 434 occurrences\n",
      "epoch-65  lr=['4.0000000'], tr/val_loss:418.675201/433.434387, val:  53.98%, val_best:  81.86%, tr:  99.26%, tr_best:  99.43%, epoch time: 247.71 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7962%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2128896 real_backward_count 415945  19.538%\n",
      "layer   1  Sparsity: 90.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5669.0\n",
      "fc layer 2 self.abs_max_out: 5785.0\n",
      "fc layer 1 self.abs_max_out: 13668.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 300 occurrences\n",
      "test - Value 1: 152 occurrences\n",
      "epoch-66  lr=['4.0000000'], tr/val_loss:417.947083/345.745361, val:  78.32%, val_best:  81.86%, tr:  99.28%, tr_best:  99.43%, epoch time: 249.29 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2161152 real_backward_count 421986  19.526%\n",
      "layer   1  Sparsity: 79.6875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5789.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 347 occurrences\n",
      "test - Value 1: 105 occurrences\n",
      "epoch-67  lr=['4.0000000'], tr/val_loss:415.872467/348.927856, val:  70.13%, val_best:  81.86%, tr:  99.03%, tr_best:  99.43%, epoch time: 248.52 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6547%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2193408 real_backward_count 427995  19.513%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9276.0\n",
      "fc layer 2 self.abs_max_out: 5924.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 74 occurrences\n",
      "test - Value 1: 378 occurrences\n",
      "epoch-68  lr=['4.0000000'], tr/val_loss:394.200562/388.321533, val:  64.16%, val_best:  81.86%, tr:  99.16%, tr_best:  99.43%, epoch time: 248.57 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2225664 real_backward_count 434102  19.504%\n",
      "layer   1  Sparsity: 90.7715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5966.0\n",
      "fc layer 1 self.abs_max_out: 13676.0\n",
      "fc layer 2 self.abs_max_out: 6115.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 442 occurrences\n",
      "test - Value 1: 10 occurrences\n",
      "epoch-69  lr=['4.0000000'], tr/val_loss:408.139648/380.184509, val:  52.21%, val_best:  81.86%, tr:  98.96%, tr_best:  99.43%, epoch time: 247.53 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2257920 real_backward_count 440244  19.498%\n",
      "layer   1  Sparsity: 78.4668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6184.0\n",
      "lif layer 1 self.abs_max_v: 20605.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 221 occurrences\n",
      "test - Value 1: 231 occurrences\n",
      "epoch-70  lr=['4.0000000'], tr/val_loss:413.396545/359.113800, val:  83.85%, val_best:  83.85%, tr:  99.26%, tr_best:  99.43%, epoch time: 246.67 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7784%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3302%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2290176 real_backward_count 446273  19.486%\n",
      "layer   1  Sparsity: 74.6094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6227.0\n",
      "lif layer 1 self.abs_max_v: 20639.5\n",
      "fc layer 1 self.abs_max_out: 13921.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 346 occurrences\n",
      "test - Value 1: 106 occurrences\n",
      "epoch-71  lr=['4.0000000'], tr/val_loss:419.183746/360.442047, val:  70.80%, val_best:  83.85%, tr:  99.13%, tr_best:  99.43%, epoch time: 245.88 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2322432 real_backward_count 452345  19.477%\n",
      "layer   1  Sparsity: 80.7861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9295.5\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-72  lr=['4.0000000'], tr/val_loss:421.263733/513.699585, val:  50.22%, val_best:  83.85%, tr:  99.13%, tr_best:  99.43%, epoch time: 245.06 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2354688 real_backward_count 458279  19.462%\n",
      "layer   1  Sparsity: 83.0078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 21041.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 113 occurrences\n",
      "test - Value 1: 339 occurrences\n",
      "epoch-73  lr=['4.0000000'], tr/val_loss:414.121094/370.746399, val:  72.35%, val_best:  83.85%, tr:  99.18%, tr_best:  99.43%, epoch time: 245.28 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2386944 real_backward_count 464277  19.451%\n",
      "layer   1  Sparsity: 88.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6289.0\n",
      "fc layer 1 self.abs_max_out: 14106.0\n",
      "fc layer 2 self.abs_max_out: 6306.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 7369.00 at epoch 74, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-74  lr=['4.0000000'], tr/val_loss:417.722443/545.315735, val:  50.00%, val_best:  83.85%, tr:  99.33%, tr_best:  99.43%, epoch time: 246.23 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2419200 real_backward_count 470266  19.439%\n",
      "layer   1  Sparsity: 86.9141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 14122.0\n",
      "fc layer 2 self.abs_max_out: 6384.0\n",
      "fc layer 2 self.abs_max_out: 6404.0\n",
      "fc layer 2 self.abs_max_out: 6564.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 204 occurrences\n",
      "test - Value 1: 248 occurrences\n",
      "epoch-75  lr=['4.0000000'], tr/val_loss:400.518433/346.922638, val:  81.86%, val_best:  83.85%, tr:  99.08%, tr_best:  99.43%, epoch time: 247.31 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2451456 real_backward_count 476180  19.424%\n",
      "layer   1  Sparsity: 88.3301%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6613.0\n",
      "fc layer 2 self.abs_max_out: 6774.0\n",
      "fc layer 1 self.abs_max_out: 14178.0\n",
      "lif layer 1 self.abs_max_v: 21046.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 318 occurrences\n",
      "test - Value 1: 134 occurrences\n",
      "epoch-76  lr=['4.0000000'], tr/val_loss:399.655518/346.174500, val:  74.78%, val_best:  83.85%, tr:  99.26%, tr_best:  99.43%, epoch time: 247.60 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2483712 real_backward_count 482187  19.414%\n",
      "layer   1  Sparsity: 78.9062%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 21176.5\n",
      "lif layer 2 self.abs_max_v: 9354.5\n",
      "lif layer 2 self.abs_max_v: 9470.5\n",
      "lif layer 2 self.abs_max_v: 9559.5\n",
      "fc layer 1 self.abs_max_out: 14596.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 306 occurrences\n",
      "test - Value 1: 146 occurrences\n",
      "epoch-77  lr=['4.0000000'], tr/val_loss:398.897675/350.005127, val:  75.66%, val_best:  83.85%, tr:  99.01%, tr_best:  99.43%, epoch time: 247.66 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7850%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2515968 real_backward_count 488107  19.400%\n",
      "layer   1  Sparsity: 79.7363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-78  lr=['4.0000000'], tr/val_loss:376.913940/536.717712, val:  50.00%, val_best:  83.85%, tr:  98.86%, tr_best:  99.43%, epoch time: 248.06 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2548224 real_backward_count 494182  19.393%\n",
      "layer   1  Sparsity: 87.4512%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 333 occurrences\n",
      "test - Value 1: 119 occurrences\n",
      "epoch-79  lr=['4.0000000'], tr/val_loss:356.556244/295.478180, val:  72.79%, val_best:  83.85%, tr:  98.56%, tr_best:  99.43%, epoch time: 244.23 seconds, 4.07 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2580480 real_backward_count 500298  19.388%\n",
      "layer   1  Sparsity: 86.2061%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-80  lr=['4.0000000'], tr/val_loss:363.583893/293.512115, val:  79.20%, val_best:  83.85%, tr:  98.59%, tr_best:  99.43%, epoch time: 246.28 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2612736 real_backward_count 506627  19.391%\n",
      "layer   1  Sparsity: 89.7949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6823.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-81  lr=['4.0000000'], tr/val_loss:383.233673/518.390869, val:  50.66%, val_best:  83.85%, tr:  97.17%, tr_best:  99.43%, epoch time: 246.77 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2644992 real_backward_count 513298  19.406%\n",
      "layer   1  Sparsity: 82.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 21194.5\n",
      "lif layer 1 self.abs_max_v: 21347.0\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-82  lr=['4.0000000'], tr/val_loss:604.261230/697.890076, val:  50.00%, val_best:  83.85%, tr:  98.61%, tr_best:  99.43%, epoch time: 249.39 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2677248 real_backward_count 519610  19.408%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 14626.0\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-83  lr=['4.0000000'], tr/val_loss:521.407959/604.143677, val:  50.00%, val_best:  83.85%, tr:  98.26%, tr_best:  99.43%, epoch time: 248.03 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9652%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2709504 real_backward_count 525948  19.411%\n",
      "layer   1  Sparsity: 75.4883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 146 occurrences\n",
      "test - Value 1: 306 occurrences\n",
      "epoch-84  lr=['4.0000000'], tr/val_loss:607.395203/648.490112, val:  76.11%, val_best:  83.85%, tr:  98.81%, tr_best:  99.43%, epoch time: 247.88 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2741760 real_backward_count 532073  19.406%\n",
      "layer   1  Sparsity: 74.8779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 153 occurrences\n",
      "test - Value 1: 299 occurrences\n",
      "epoch-85  lr=['4.0000000'], tr/val_loss:698.000610/608.878113, val:  78.54%, val_best:  83.85%, tr:  99.38%, tr_best:  99.43%, epoch time: 249.01 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2774016 real_backward_count 538161  19.400%\n",
      "layer   1  Sparsity: 74.8047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6889.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-86  lr=['4.0000000'], tr/val_loss:625.691650/599.426880, val:  50.88%, val_best:  83.85%, tr:  99.18%, tr_best:  99.43%, epoch time: 247.51 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2806272 real_backward_count 544263  19.395%\n",
      "layer   1  Sparsity: 82.7393%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6941.0\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-87  lr=['4.0000000'], tr/val_loss:655.409363/618.633545, val:  50.44%, val_best:  83.85%, tr:  98.76%, tr_best:  99.43%, epoch time: 248.32 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2838528 real_backward_count 550357  19.389%\n",
      "layer   1  Sparsity: 80.7373%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-88  lr=['4.0000000'], tr/val_loss:701.789490/623.972107, val:  59.29%, val_best:  83.85%, tr:  99.13%, tr_best:  99.43%, epoch time: 247.50 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2870784 real_backward_count 556390  19.381%\n",
      "layer   1  Sparsity: 92.7246%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 430 occurrences\n",
      "test - Value 1: 22 occurrences\n",
      "epoch-89  lr=['4.0000000'], tr/val_loss:697.205322/628.771667, val:  54.87%, val_best:  83.85%, tr:  98.98%, tr_best:  99.43%, epoch time: 246.72 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2903040 real_backward_count 562556  19.378%\n",
      "layer   1  Sparsity: 85.4004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 98 occurrences\n",
      "test - Value 1: 354 occurrences\n",
      "epoch-90  lr=['4.0000000'], tr/val_loss:692.686768/599.230652, val:  70.35%, val_best:  83.85%, tr:  98.88%, tr_best:  99.43%, epoch time: 246.90 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2935296 real_backward_count 568669  19.373%\n",
      "layer   1  Sparsity: 83.4229%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 50 occurrences\n",
      "test - Value 1: 402 occurrences\n",
      "epoch-91  lr=['4.0000000'], tr/val_loss:686.895691/599.715515, val:  61.06%, val_best:  83.85%, tr:  98.91%, tr_best:  99.43%, epoch time: 247.45 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2967552 real_backward_count 574784  19.369%\n",
      "layer   1  Sparsity: 82.1777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 14821.0\n",
      "fc layer 1 self.abs_max_out: 14972.0\n",
      "fc layer 1 self.abs_max_out: 15242.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 15 occurrences\n",
      "test - Value 1: 437 occurrences\n",
      "epoch-92  lr=['4.0000000'], tr/val_loss:708.330933/666.568237, val:  53.32%, val_best:  83.85%, tr:  99.16%, tr_best:  99.43%, epoch time: 246.49 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3524%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2999808 real_backward_count 580834  19.362%\n",
      "layer   1  Sparsity: 82.2754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 72 occurrences\n",
      "test - Value 1: 380 occurrences\n",
      "epoch-93  lr=['4.0000000'], tr/val_loss:704.528870/616.222900, val:  64.60%, val_best:  83.85%, tr:  99.08%, tr_best:  99.43%, epoch time: 246.49 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7754%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3032064 real_backward_count 586857  19.355%\n",
      "layer   1  Sparsity: 79.3701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 15314.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 270 occurrences\n",
      "test - Value 1: 182 occurrences\n",
      "epoch-94  lr=['4.0000000'], tr/val_loss:697.637695/571.423462, val:  82.30%, val_best:  83.85%, tr:  99.23%, tr_best:  99.43%, epoch time: 246.61 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4805%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9229%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3064320 real_backward_count 592818  19.346%\n",
      "layer   1  Sparsity: 69.2383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-95  lr=['4.0000000'], tr/val_loss:670.726807/634.858948, val:  50.00%, val_best:  83.85%, tr:  99.21%, tr_best:  99.43%, epoch time: 246.26 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6245%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3096576 real_backward_count 598833  19.339%\n",
      "layer   1  Sparsity: 90.9180%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 15584.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 11 occurrences\n",
      "test - Value 1: 441 occurrences\n",
      "epoch-96  lr=['4.0000000'], tr/val_loss:665.335327/633.750671, val:  52.43%, val_best:  83.85%, tr:  99.31%, tr_best:  99.43%, epoch time: 247.18 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3128832 real_backward_count 604915  19.334%\n",
      "layer   1  Sparsity: 76.7090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 378 occurrences\n",
      "test - Value 1: 74 occurrences\n",
      "epoch-97  lr=['4.0000000'], tr/val_loss:650.299683/424.603516, val:  65.04%, val_best:  83.85%, tr:  99.06%, tr_best:  99.43%, epoch time: 246.97 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3161088 real_backward_count 611052  19.330%\n",
      "layer   1  Sparsity: 82.8857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 15610.0\n",
      "fc layer 1 self.abs_max_out: 15946.0\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2041 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 268 occurrences\n",
      "test - Value 1: 184 occurrences\n",
      "epoch-98  lr=['4.0000000'], tr/val_loss:629.190613/542.677551, val:  80.53%, val_best:  83.85%, tr:  98.74%, tr_best:  99.43%, epoch time: 246.79 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3193344 real_backward_count 617145  19.326%\n",
      "layer   1  Sparsity: 81.2256%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 16186.0\n",
      "fc layer 1 self.abs_max_out: 16401.0\n",
      "fc layer 1 self.abs_max_out: 16435.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-99  lr=['4.0000000'], tr/val_loss:650.231812/678.403748, val:  50.66%, val_best:  83.85%, tr:  99.33%, tr_best:  99.43%, epoch time: 246.09 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2700%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3225600 real_backward_count 623144  19.319%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 410 occurrences\n",
      "test - Value 1: 42 occurrences\n",
      "epoch-100 lr=['4.0000000'], tr/val_loss:720.604065/666.960266, val:  59.29%, val_best:  83.85%, tr:  98.96%, tr_best:  99.43%, epoch time: 247.66 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3257856 real_backward_count 629130  19.311%\n",
      "layer   1  Sparsity: 69.5312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 274 occurrences\n",
      "test - Value 1: 178 occurrences\n",
      "epoch-101 lr=['4.0000000'], tr/val_loss:716.630554/601.552185, val:  81.42%, val_best:  83.85%, tr:  99.13%, tr_best:  99.43%, epoch time: 248.00 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3290112 real_backward_count 635185  19.306%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 321 occurrences\n",
      "test - Value 1: 131 occurrences\n",
      "epoch-102 lr=['4.0000000'], tr/val_loss:680.248230/571.732361, val:  75.88%, val_best:  83.85%, tr:  99.21%, tr_best:  99.43%, epoch time: 247.09 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9828%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3322368 real_backward_count 641145  19.298%\n",
      "layer   1  Sparsity: 71.6309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-103 lr=['4.0000000'], tr/val_loss:654.406860/537.517700, val:  72.79%, val_best:  83.85%, tr:  99.33%, tr_best:  99.43%, epoch time: 246.73 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6193%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3354624 real_backward_count 647177  19.292%\n",
      "layer   1  Sparsity: 80.1270%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 141 occurrences\n",
      "test - Value 1: 311 occurrences\n",
      "epoch-104 lr=['4.0000000'], tr/val_loss:639.127930/639.588257, val:  75.88%, val_best:  83.85%, tr:  99.23%, tr_best:  99.43%, epoch time: 247.87 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1185%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3386880 real_backward_count 653129  19.284%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 16469.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-105 lr=['4.0000000'], tr/val_loss:645.020508/704.419434, val:  50.00%, val_best:  83.85%, tr:  99.28%, tr_best:  99.43%, epoch time: 246.70 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3419136 real_backward_count 659214  19.280%\n",
      "layer   1  Sparsity: 85.7666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 9 occurrences\n",
      "test - Value 1: 443 occurrences\n",
      "epoch-106 lr=['4.0000000'], tr/val_loss:668.541687/416.073669, val:  51.99%, val_best:  83.85%, tr:  99.16%, tr_best:  99.43%, epoch time: 245.84 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3451392 real_backward_count 665205  19.274%\n",
      "layer   1  Sparsity: 91.3574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 174 occurrences\n",
      "test - Value 1: 278 occurrences\n",
      "epoch-107 lr=['4.0000000'], tr/val_loss:625.496155/534.205566, val:  79.65%, val_best:  83.85%, tr:  99.23%, tr_best:  99.43%, epoch time: 247.74 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3483648 real_backward_count 671198  19.267%\n",
      "layer   1  Sparsity: 87.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 307 occurrences\n",
      "test - Value 1: 145 occurrences\n",
      "epoch-108 lr=['4.0000000'], tr/val_loss:629.083923/590.972290, val:  76.77%, val_best:  83.85%, tr:  99.18%, tr_best:  99.43%, epoch time: 246.98 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3881%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3515904 real_backward_count 677151  19.260%\n",
      "layer   1  Sparsity: 86.3525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 7625.00 at epoch 109, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-109 lr=['4.0000000'], tr/val_loss:709.725525/713.166626, val:  50.00%, val_best:  83.85%, tr:  99.11%, tr_best:  99.43%, epoch time: 248.03 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2264%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3548160 real_backward_count 683143  19.253%\n",
      "layer   1  Sparsity: 89.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-110 lr=['4.0000000'], tr/val_loss:681.428528/719.786194, val:  50.00%, val_best:  83.85%, tr:  99.06%, tr_best:  99.43%, epoch time: 249.05 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3580416 real_backward_count 689118  19.247%\n",
      "layer   1  Sparsity: 81.3232%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 402 occurrences\n",
      "test - Value 1: 50 occurrences\n",
      "epoch-111 lr=['4.0000000'], tr/val_loss:704.563721/707.866272, val:  60.62%, val_best:  83.85%, tr:  99.13%, tr_best:  99.43%, epoch time: 246.82 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1866%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3612672 real_backward_count 695084  19.240%\n",
      "layer   1  Sparsity: 80.8594%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 130 occurrences\n",
      "test - Value 1: 322 occurrences\n",
      "epoch-112 lr=['4.0000000'], tr/val_loss:801.375305/701.886169, val:  74.78%, val_best:  83.85%, tr:  99.40%, tr_best:  99.43%, epoch time: 247.23 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1958%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3644928 real_backward_count 701172  19.237%\n",
      "layer   1  Sparsity: 85.9131%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 274 occurrences\n",
      "test - Value 1: 178 occurrences\n",
      "epoch-113 lr=['4.0000000'], tr/val_loss:748.974731/680.476257, val:  80.97%, val_best:  83.85%, tr:  99.11%, tr_best:  99.43%, epoch time: 248.62 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0090%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3677184 real_backward_count 707250  19.233%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 391 occurrences\n",
      "test - Value 1: 61 occurrences\n",
      "epoch-114 lr=['4.0000000'], tr/val_loss:723.570557/618.609192, val:  63.05%, val_best:  83.85%, tr:  99.28%, tr_best:  99.43%, epoch time: 247.72 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2997%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3709440 real_backward_count 713144  19.225%\n",
      "layer   1  Sparsity: 80.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 250 occurrences\n",
      "test - Value 1: 202 occurrences\n",
      "epoch-115 lr=['4.0000000'], tr/val_loss:697.944702/549.299438, val:  82.74%, val_best:  83.85%, tr:  99.31%, tr_best:  99.43%, epoch time: 247.86 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7718%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3741696 real_backward_count 719139  19.220%\n",
      "layer   1  Sparsity: 87.3779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 352 occurrences\n",
      "test - Value 1: 100 occurrences\n",
      "epoch-116 lr=['4.0000000'], tr/val_loss:705.825073/510.580750, val:  70.80%, val_best:  83.85%, tr:  99.06%, tr_best:  99.43%, epoch time: 247.05 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1718%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3773952 real_backward_count 725166  19.215%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-117 lr=['4.0000000'], tr/val_loss:583.781067/536.438477, val:  59.29%, val_best:  83.85%, tr:  99.16%, tr_best:  99.43%, epoch time: 247.83 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0592%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3806208 real_backward_count 731154  19.210%\n",
      "layer   1  Sparsity: 87.9639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 178 occurrences\n",
      "test - Value 1: 274 occurrences\n",
      "epoch-118 lr=['4.0000000'], tr/val_loss:644.767151/573.066101, val:  81.42%, val_best:  83.85%, tr:  99.38%, tr_best:  99.43%, epoch time: 246.92 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0106%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3838464 real_backward_count 737234  19.206%\n",
      "layer   1  Sparsity: 88.8672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 211 occurrences\n",
      "test - Value 1: 241 occurrences\n",
      "epoch-119 lr=['4.0000000'], tr/val_loss:617.400696/618.719604, val:  83.41%, val_best:  83.85%, tr:  99.01%, tr_best:  99.43%, epoch time: 248.00 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3870720 real_backward_count 743266  19.202%\n",
      "layer   1  Sparsity: 87.5977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 64 occurrences\n",
      "test - Value 1: 388 occurrences\n",
      "epoch-120 lr=['4.0000000'], tr/val_loss:722.284912/666.085083, val:  63.72%, val_best:  83.85%, tr:  99.33%, tr_best:  99.43%, epoch time: 248.07 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3902976 real_backward_count 749349  19.199%\n",
      "layer   1  Sparsity: 82.3730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-121 lr=['4.0000000'], tr/val_loss:712.130859/629.561768, val:  50.66%, val_best:  83.85%, tr:  99.50%, tr_best:  99.50%, epoch time: 246.52 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5440%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3935232 real_backward_count 755480  19.198%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-122 lr=['4.0000000'], tr/val_loss:691.155640/694.578491, val:  50.66%, val_best:  83.85%, tr:  99.06%, tr_best:  99.50%, epoch time: 247.68 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3967488 real_backward_count 761516  19.194%\n",
      "layer   1  Sparsity: 88.7207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-123 lr=['4.0000000'], tr/val_loss:680.787537/656.451111, val:  50.66%, val_best:  83.85%, tr:  99.40%, tr_best:  99.50%, epoch time: 248.44 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3999744 real_backward_count 767543  19.190%\n",
      "layer   1  Sparsity: 83.9111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 444 occurrences\n",
      "test - Value 1: 8 occurrences\n",
      "epoch-124 lr=['4.0000000'], tr/val_loss:712.588562/684.222412, val:  51.77%, val_best:  83.85%, tr:  99.21%, tr_best:  99.50%, epoch time: 248.59 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9726%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4032000 real_backward_count 773574  19.186%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 391 occurrences\n",
      "test - Value 1: 61 occurrences\n",
      "epoch-125 lr=['4.0000000'], tr/val_loss:705.443481/676.552368, val:  63.05%, val_best:  83.85%, tr:  99.50%, tr_best:  99.50%, epoch time: 247.61 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4064256 real_backward_count 779589  19.182%\n",
      "layer   1  Sparsity: 73.3154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-126 lr=['4.0000000'], tr/val_loss:682.353516/665.121704, val:  50.00%, val_best:  83.85%, tr:  99.08%, tr_best:  99.50%, epoch time: 246.80 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4038%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4096512 real_backward_count 785656  19.179%\n",
      "layer   1  Sparsity: 77.4170%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 444 occurrences\n",
      "test - Value 1: 8 occurrences\n",
      "epoch-127 lr=['4.0000000'], tr/val_loss:685.059021/682.582825, val:  51.77%, val_best:  83.85%, tr:  99.43%, tr_best:  99.50%, epoch time: 246.63 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2844%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4128768 real_backward_count 791755  19.177%\n",
      "layer   1  Sparsity: 91.1133%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 223 occurrences\n",
      "test - Value 1: 229 occurrences\n",
      "epoch-128 lr=['4.0000000'], tr/val_loss:697.503357/654.557495, val:  83.41%, val_best:  83.85%, tr:  99.18%, tr_best:  99.50%, epoch time: 247.71 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4161024 real_backward_count 797784  19.173%\n",
      "layer   1  Sparsity: 86.2061%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 123 occurrences\n",
      "test - Value 1: 329 occurrences\n",
      "epoch-129 lr=['4.0000000'], tr/val_loss:727.513916/613.494873, val:  72.35%, val_best:  83.85%, tr:  99.16%, tr_best:  99.50%, epoch time: 248.27 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4193280 real_backward_count 803792  19.169%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 254 occurrences\n",
      "test - Value 1: 198 occurrences\n",
      "epoch-130 lr=['4.0000000'], tr/val_loss:630.219604/574.354736, val:  80.97%, val_best:  83.85%, tr:  99.33%, tr_best:  99.50%, epoch time: 246.36 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4225536 real_backward_count 809765  19.164%\n",
      "layer   1  Sparsity: 90.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-131 lr=['4.0000000'], tr/val_loss:659.959900/631.600708, val:  50.44%, val_best:  83.85%, tr:  99.23%, tr_best:  99.50%, epoch time: 248.02 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4257792 real_backward_count 815728  19.158%\n",
      "layer   1  Sparsity: 79.1260%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 418 occurrences\n",
      "test - Value 1: 34 occurrences\n",
      "epoch-132 lr=['4.0000000'], tr/val_loss:648.080994/577.099609, val:  57.52%, val_best:  83.85%, tr:  99.28%, tr_best:  99.50%, epoch time: 247.58 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6816%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4290048 real_backward_count 821806  19.156%\n",
      "layer   1  Sparsity: 88.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-133 lr=['4.0000000'], tr/val_loss:485.932037/416.154938, val:  59.73%, val_best:  83.85%, tr:  98.69%, tr_best:  99.50%, epoch time: 248.48 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4322304 real_backward_count 828039  19.157%\n",
      "layer   1  Sparsity: 77.1729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 21503.5\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-134 lr=['4.0000000'], tr/val_loss:394.794739/459.958801, val:  50.00%, val_best:  83.85%, tr:  98.56%, tr_best:  99.50%, epoch time: 248.66 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4354560 real_backward_count 834191  19.157%\n",
      "layer   1  Sparsity: 82.8125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 21991.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 444 occurrences\n",
      "test - Value 1: 8 occurrences\n",
      "epoch-135 lr=['4.0000000'], tr/val_loss:363.484131/318.814972, val:  51.77%, val_best:  83.85%, tr:  98.81%, tr_best:  99.50%, epoch time: 248.83 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4386816 real_backward_count 840325  19.156%\n",
      "layer   1  Sparsity: 83.5938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 22149.5\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 225 occurrences\n",
      "test - Value 1: 227 occurrences\n",
      "epoch-136 lr=['4.0000000'], tr/val_loss:383.255005/334.746216, val:  82.96%, val_best:  83.85%, tr:  99.08%, tr_best:  99.50%, epoch time: 247.54 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4800%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4419072 real_backward_count 846364  19.153%\n",
      "layer   1  Sparsity: 78.1738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 424 occurrences\n",
      "test - Value 1: 28 occurrences\n",
      "epoch-137 lr=['4.0000000'], tr/val_loss:386.628784/343.592468, val:  56.19%, val_best:  83.85%, tr:  98.96%, tr_best:  99.50%, epoch time: 246.70 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4451328 real_backward_count 852512  19.152%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 16759.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 278 occurrences\n",
      "test - Value 1: 174 occurrences\n",
      "epoch-138 lr=['4.0000000'], tr/val_loss:402.500671/361.992676, val:  78.76%, val_best:  83.85%, tr:  99.28%, tr_best:  99.50%, epoch time: 245.24 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2478%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4483584 real_backward_count 858689  19.152%\n",
      "layer   1  Sparsity: 84.5459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 52 occurrences\n",
      "test - Value 1: 400 occurrences\n",
      "epoch-139 lr=['4.0000000'], tr/val_loss:404.199097/397.103638, val:  61.50%, val_best:  83.85%, tr:  99.33%, tr_best:  99.50%, epoch time: 248.08 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4515840 real_backward_count 864800  19.150%\n",
      "layer   1  Sparsity: 95.0195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 404 occurrences\n",
      "test - Value 1: 48 occurrences\n",
      "epoch-140 lr=['4.0000000'], tr/val_loss:374.068970/329.690002, val:  59.73%, val_best:  83.85%, tr:  99.21%, tr_best:  99.50%, epoch time: 246.55 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4548096 real_backward_count 870869  19.148%\n",
      "layer   1  Sparsity: 80.5420%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 16799.0\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 124 occurrences\n",
      "test - Value 1: 328 occurrences\n",
      "epoch-141 lr=['4.0000000'], tr/val_loss:373.656525/347.404297, val:  76.55%, val_best:  83.85%, tr:  98.98%, tr_best:  99.50%, epoch time: 246.22 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3754%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4580352 real_backward_count 876973  19.146%\n",
      "layer   1  Sparsity: 72.3389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 449 occurrences\n",
      "test - Value 1: 3 occurrences\n",
      "epoch-142 lr=['4.0000000'], tr/val_loss:395.716003/380.512482, val:  50.66%, val_best:  83.85%, tr:  99.33%, tr_best:  99.50%, epoch time: 243.28 seconds, 4.05 minutes\n",
      "layer   1  Sparsity: 82.6673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4612608 real_backward_count 883031  19.144%\n",
      "layer   1  Sparsity: 84.8877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 309 occurrences\n",
      "test - Value 1: 143 occurrences\n",
      "epoch-143 lr=['4.0000000'], tr/val_loss:395.296783/317.825256, val:  77.65%, val_best:  83.85%, tr:  98.96%, tr_best:  99.50%, epoch time: 244.92 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4644864 real_backward_count 889008  19.140%\n",
      "layer   1  Sparsity: 94.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 22414.5\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 444 occurrences\n",
      "test - Value 1: 8 occurrences\n",
      "epoch-144 lr=['4.0000000'], tr/val_loss:384.327911/321.235748, val:  51.77%, val_best:  83.85%, tr:  99.26%, tr_best:  99.50%, epoch time: 248.23 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5154%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4677120 real_backward_count 895037  19.136%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 22867.5\n",
      "fc layer 1 self.abs_max_out: 17036.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 165 occurrences\n",
      "test - Value 1: 287 occurrences\n",
      "epoch-145 lr=['4.0000000'], tr/val_loss:393.740112/341.706879, val:  80.75%, val_best:  83.85%, tr:  99.33%, tr_best:  99.50%, epoch time: 248.18 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4709376 real_backward_count 901108  19.134%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 22989.5\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 432 occurrences\n",
      "test - Value 1: 20 occurrences\n",
      "epoch-146 lr=['4.0000000'], tr/val_loss:384.866943/316.706360, val:  54.42%, val_best:  83.85%, tr:  99.18%, tr_best:  99.50%, epoch time: 245.78 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4305%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4741632 real_backward_count 907316  19.135%\n",
      "layer   1  Sparsity: 82.8857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 23365.5\n",
      "fc layer 1 self.abs_max_out: 17343.0\n",
      "fc layer 2 self.abs_max_out: 7038.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 412 occurrences\n",
      "test - Value 1: 40 occurrences\n",
      "epoch-147 lr=['4.0000000'], tr/val_loss:397.630463/356.546204, val:  58.85%, val_best:  83.85%, tr:  99.08%, tr_best:  99.50%, epoch time: 248.07 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1871%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4773888 real_backward_count 913333  19.132%\n",
      "layer   1  Sparsity: 86.1084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 23 occurrences\n",
      "test - Value 1: 429 occurrences\n",
      "epoch-148 lr=['4.0000000'], tr/val_loss:408.139923/410.870361, val:  55.09%, val_best:  83.85%, tr:  99.08%, tr_best:  99.50%, epoch time: 248.68 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4806144 real_backward_count 919311  19.128%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 7309.0\n",
      "fc layer 2 self.abs_max_out: 7634.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 308 occurrences\n",
      "test - Value 1: 144 occurrences\n",
      "epoch-149 lr=['4.0000000'], tr/val_loss:398.861664/359.719727, val:  78.32%, val_best:  83.85%, tr:  99.33%, tr_best:  99.50%, epoch time: 246.63 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7670%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4838400 real_backward_count 925382  19.126%\n",
      "layer   1  Sparsity: 74.2432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 7699.0\n",
      "fc layer 2 self.abs_max_out: 7937.0\n",
      "fc layer 2 self.abs_max_out: 7970.0\n",
      "fc layer 2 self.abs_max_out: 8097.0\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 252 occurrences\n",
      "test - Value 1: 200 occurrences\n",
      "epoch-150 lr=['4.0000000'], tr/val_loss:395.574280/362.000061, val:  79.20%, val_best:  83.85%, tr:  98.81%, tr_best:  99.50%, epoch time: 248.03 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2150%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4870656 real_backward_count 931549  19.126%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-151 lr=['4.0000000'], tr/val_loss:390.886780/438.880310, val:  50.66%, val_best:  83.85%, tr:  98.59%, tr_best:  99.50%, epoch time: 247.77 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4902912 real_backward_count 937965  19.131%\n",
      "layer   1  Sparsity: 85.7910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9586.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 440 occurrences\n",
      "test - Value 1: 12 occurrences\n",
      "epoch-152 lr=['4.0000000'], tr/val_loss:413.664520/389.026123, val:  52.65%, val_best:  83.85%, tr:  98.44%, tr_best:  99.50%, epoch time: 248.81 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4935168 real_backward_count 944298  19.134%\n",
      "layer   1  Sparsity: 82.1289%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 285 occurrences\n",
      "test - Value 1: 167 occurrences\n",
      "epoch-153 lr=['4.0000000'], tr/val_loss:411.538879/337.001953, val:  72.79%, val_best:  83.85%, tr:  98.29%, tr_best:  99.50%, epoch time: 247.02 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4967424 real_backward_count 950547  19.136%\n",
      "layer   1  Sparsity: 75.4883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 17448.0\n",
      "lif layer 2 self.abs_max_v: 9750.0\n",
      "lif layer 1 self.abs_max_v: 24225.5\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 60 occurrences\n",
      "test - Value 1: 392 occurrences\n",
      "epoch-154 lr=['4.0000000'], tr/val_loss:378.337830/360.355896, val:  62.83%, val_best:  83.85%, tr:  98.51%, tr_best:  99.50%, epoch time: 248.37 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4999680 real_backward_count 956925  19.140%\n",
      "layer   1  Sparsity: 83.5693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 17569.0\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 273 occurrences\n",
      "test - Value 1: 179 occurrences\n",
      "epoch-155 lr=['4.0000000'], tr/val_loss:394.935455/357.793793, val:  80.75%, val_best:  83.85%, tr:  98.51%, tr_best:  99.50%, epoch time: 246.83 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5031936 real_backward_count 963237  19.142%\n",
      "layer   1  Sparsity: 89.7705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9871.0\n",
      "lif layer 2 self.abs_max_v: 9912.5\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 214 occurrences\n",
      "test - Value 1: 238 occurrences\n",
      "epoch-156 lr=['4.0000000'], tr/val_loss:443.038239/409.397491, val:  80.97%, val_best:  83.85%, tr:  98.36%, tr_best:  99.50%, epoch time: 247.31 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0663%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5064192 real_backward_count 969633  19.147%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9991.5\n",
      "fc layer 1 self.abs_max_out: 18144.0\n",
      "lif layer 2 self.abs_max_v: 10165.0\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 408 occurrences\n",
      "test - Value 1: 44 occurrences\n",
      "epoch-157 lr=['4.0000000'], tr/val_loss:435.110901/365.370514, val:  59.73%, val_best:  83.85%, tr:  98.14%, tr_best:  99.50%, epoch time: 248.11 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5096448 real_backward_count 975976  19.150%\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 18197.0\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 287 occurrences\n",
      "test - Value 1: 165 occurrences\n",
      "epoch-158 lr=['4.0000000'], tr/val_loss:437.382843/355.623322, val:  80.31%, val_best:  83.85%, tr:  98.21%, tr_best:  99.50%, epoch time: 247.05 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1479%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5128704 real_backward_count 982384  19.155%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 24437.5\n",
      "fc layer 1 self.abs_max_out: 18411.0\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-159 lr=['4.0000000'], tr/val_loss:416.671600/673.065430, val:  50.00%, val_best:  83.85%, tr:  98.51%, tr_best:  99.50%, epoch time: 246.29 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1582%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5160960 real_backward_count 988620  19.156%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-160 lr=['4.0000000'], tr/val_loss:407.171478/388.042999, val:  50.44%, val_best:  83.85%, tr:  98.24%, tr_best:  99.50%, epoch time: 247.51 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3529%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5193216 real_backward_count 994795  19.156%\n",
      "layer   1  Sparsity: 64.9170%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 17 occurrences\n",
      "test - Value 1: 435 occurrences\n",
      "epoch-161 lr=['4.0000000'], tr/val_loss:355.363434/356.409485, val:  53.76%, val_best:  83.85%, tr:  98.34%, tr_best:  99.50%, epoch time: 248.00 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1816%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5225472 real_backward_count 1000934  19.155%\n",
      "layer   1  Sparsity: 93.9453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 445 occurrences\n",
      "test - Value 1: 7 occurrences\n",
      "epoch-162 lr=['4.0000000'], tr/val_loss:390.603607/352.805969, val:  51.55%, val_best:  83.85%, tr:  98.51%, tr_best:  99.50%, epoch time: 246.46 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.9719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.9515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5257728 real_backward_count 1006921  19.151%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 120 occurrences\n",
      "test - Value 1: 332 occurrences\n",
      "epoch-163 lr=['4.0000000'], tr/val_loss:427.198761/406.386536, val:  74.34%, val_best:  83.85%, tr:  98.44%, tr_best:  99.50%, epoch time: 248.42 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5289984 real_backward_count 1012976  19.149%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 405 occurrences\n",
      "test - Value 1: 47 occurrences\n",
      "epoch-164 lr=['4.0000000'], tr/val_loss:439.751312/393.834717, val:  59.96%, val_best:  83.85%, tr:  99.13%, tr_best:  99.50%, epoch time: 247.32 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1860%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5322240 real_backward_count 1018915  19.144%\n",
      "layer   1  Sparsity: 85.8154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-165 lr=['4.0000000'], tr/val_loss:430.794922/348.578583, val:  51.33%, val_best:  83.85%, tr:  98.64%, tr_best:  99.50%, epoch time: 246.07 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0908%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5354496 real_backward_count 1025114  19.145%\n",
      "layer   1  Sparsity: 75.9521%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-166 lr=['4.0000000'], tr/val_loss:431.339813/502.229126, val:  50.88%, val_best:  83.85%, tr:  98.51%, tr_best:  99.50%, epoch time: 248.50 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5937%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5386752 real_backward_count 1031319  19.145%\n",
      "layer   1  Sparsity: 85.2295%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-167 lr=['4.0000000'], tr/val_loss:460.056061/512.718872, val:  50.00%, val_best:  83.85%, tr:  98.98%, tr_best:  99.50%, epoch time: 247.92 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.1325%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5419008 real_backward_count 1037378  19.143%\n",
      "layer   1  Sparsity: 83.3008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-168 lr=['4.0000000'], tr/val_loss:457.026947/559.983215, val:  50.00%, val_best:  83.85%, tr:  98.74%, tr_best:  99.50%, epoch time: 246.12 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5451264 real_backward_count 1043454  19.142%\n",
      "layer   1  Sparsity: 91.7480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 262 occurrences\n",
      "test - Value 1: 190 occurrences\n",
      "epoch-169 lr=['4.0000000'], tr/val_loss:451.324158/401.239166, val:  81.42%, val_best:  83.85%, tr:  98.86%, tr_best:  99.50%, epoch time: 249.24 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5483520 real_backward_count 1049509  19.139%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-170 lr=['4.0000000'], tr/val_loss:456.455048/508.643097, val:  50.22%, val_best:  83.85%, tr:  98.93%, tr_best:  99.50%, epoch time: 247.60 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2037%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5515776 real_backward_count 1055576  19.137%\n",
      "layer   1  Sparsity: 82.9590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 310 occurrences\n",
      "test - Value 1: 142 occurrences\n",
      "epoch-171 lr=['4.0000000'], tr/val_loss:474.484222/413.645660, val:  75.66%, val_best:  83.85%, tr:  98.93%, tr_best:  99.50%, epoch time: 247.42 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6757%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5548032 real_backward_count 1061587  19.134%\n",
      "layer   1  Sparsity: 82.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 36 occurrences\n",
      "test - Value 1: 416 occurrences\n",
      "epoch-172 lr=['4.0000000'], tr/val_loss:464.135162/465.430573, val:  57.96%, val_best:  83.85%, tr:  98.96%, tr_best:  99.50%, epoch time: 248.51 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8606%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5580288 real_backward_count 1067550  19.131%\n",
      "layer   1  Sparsity: 87.0117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 138 occurrences\n",
      "test - Value 1: 314 occurrences\n",
      "epoch-173 lr=['4.0000000'], tr/val_loss:461.538910/427.787842, val:  74.78%, val_best:  83.85%, tr:  99.13%, tr_best:  99.50%, epoch time: 248.50 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5612544 real_backward_count 1073436  19.126%\n",
      "layer   1  Sparsity: 95.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 82.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-174 lr=['4.0000000'], tr/val_loss:460.322937/548.132446, val:  50.00%, val_best:  83.85%, tr:  99.16%, tr_best:  99.50%, epoch time: 247.11 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.3641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5644800 real_backward_count 1079415  19.122%\n",
      "layer   1  Sparsity: 93.3350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 130 occurrences\n",
      "test - Value 1: 322 occurrences\n",
      "epoch-175 lr=['4.0000000'], tr/val_loss:449.848053/399.514130, val:  77.43%, val_best:  83.85%, tr:  99.11%, tr_best:  99.50%, epoch time: 247.10 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5677056 real_backward_count 1085318  19.118%\n",
      "layer   1  Sparsity: 77.2217%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-176 lr=['4.0000000'], tr/val_loss:452.897949/390.139923, val:  51.33%, val_best:  83.85%, tr:  98.69%, tr_best:  99.50%, epoch time: 248.44 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5709312 real_backward_count 1091297  19.114%\n",
      "layer   1  Sparsity: 85.3027%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 321 occurrences\n",
      "test - Value 1: 131 occurrences\n",
      "epoch-177 lr=['4.0000000'], tr/val_loss:448.606873/367.936646, val:  75.00%, val_best:  83.85%, tr:  98.61%, tr_best:  99.50%, epoch time: 246.56 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.2875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5741568 real_backward_count 1097243  19.111%\n",
      "layer   1  Sparsity: 87.4756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 386 occurrences\n",
      "test - Value 1: 66 occurrences\n",
      "epoch-178 lr=['4.0000000'], tr/val_loss:467.425629/423.446594, val:  63.27%, val_best:  83.85%, tr:  98.83%, tr_best:  99.50%, epoch time: 246.38 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.9707%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5773824 real_backward_count 1103181  19.107%\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-179 lr=['4.0000000'], tr/val_loss:483.758392/447.117401, val:  52.88%, val_best:  83.85%, tr:  99.01%, tr_best:  99.50%, epoch time: 246.77 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8975%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5806080 real_backward_count 1109319  19.106%\n",
      "layer   1  Sparsity: 80.6396%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 418 occurrences\n",
      "test - Value 1: 34 occurrences\n",
      "epoch-180 lr=['4.0000000'], tr/val_loss:478.748016/437.665039, val:  57.52%, val_best:  83.85%, tr:  99.16%, tr_best:  99.50%, epoch time: 247.38 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5838336 real_backward_count 1115270  19.103%\n",
      "layer   1  Sparsity: 89.4043%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 321 occurrences\n",
      "test - Value 1: 131 occurrences\n",
      "epoch-181 lr=['4.0000000'], tr/val_loss:513.281555/455.807495, val:  73.67%, val_best:  83.85%, tr:  99.33%, tr_best:  99.50%, epoch time: 247.87 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5870592 real_backward_count 1121272  19.100%\n",
      "layer   1  Sparsity: 61.9385%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 42.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 311 occurrences\n",
      "test - Value 1: 141 occurrences\n",
      "epoch-182 lr=['4.0000000'], tr/val_loss:501.271576/451.758057, val:  77.21%, val_best:  83.85%, tr:  99.11%, tr_best:  99.50%, epoch time: 248.34 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5902848 real_backward_count 1127235  19.096%\n",
      "layer   1  Sparsity: 80.3955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-183 lr=['4.0000000'], tr/val_loss:491.428802/466.570892, val:  63.94%, val_best:  83.85%, tr:  98.14%, tr_best:  99.50%, epoch time: 248.04 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5935104 real_backward_count 1133451  19.097%\n",
      "layer   1  Sparsity: 79.7852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-184 lr=['4.0000000'], tr/val_loss:485.191986/688.860352, val:  50.00%, val_best:  83.85%, tr:  98.04%, tr_best:  99.50%, epoch time: 246.56 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5967360 real_backward_count 1139636  19.098%\n",
      "layer   1  Sparsity: 89.0381%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 389 occurrences\n",
      "test - Value 1: 63 occurrences\n",
      "epoch-185 lr=['4.0000000'], tr/val_loss:496.066040/420.341248, val:  63.05%, val_best:  83.85%, tr:  98.19%, tr_best:  99.50%, epoch time: 247.74 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0145%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.4419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5999616 real_backward_count 1145810  19.098%\n",
      "layer   1  Sparsity: 87.4756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 448 occurrences\n",
      "test - Value 1: 4 occurrences\n",
      "epoch-186 lr=['4.0000000'], tr/val_loss:479.101105/414.767578, val:  50.88%, val_best:  83.85%, tr:  98.19%, tr_best:  99.50%, epoch time: 247.40 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2163%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6031872 real_backward_count 1152065  19.100%\n",
      "layer   1  Sparsity: 83.4961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-187 lr=['4.0000000'], tr/val_loss:496.731934/595.956482, val:  50.00%, val_best:  83.85%, tr:  98.46%, tr_best:  99.50%, epoch time: 246.66 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5691%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.2549%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6064128 real_backward_count 1158241  19.100%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 157 occurrences\n",
      "test - Value 1: 295 occurrences\n",
      "epoch-188 lr=['4.0000000'], tr/val_loss:498.250366/423.554474, val:  76.33%, val_best:  83.85%, tr:  98.81%, tr_best:  99.50%, epoch time: 249.04 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6096384 real_backward_count 1164278  19.098%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-189 lr=['4.0000000'], tr/val_loss:491.068115/442.856049, val:  74.56%, val_best:  83.85%, tr:  98.19%, tr_best:  99.50%, epoch time: 247.67 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3811%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.4188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6128640 real_backward_count 1170540  19.100%\n",
      "layer   1  Sparsity: 82.5439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-190 lr=['4.0000000'], tr/val_loss:486.019897/529.596863, val:  50.44%, val_best:  83.85%, tr:  97.84%, tr_best:  99.50%, epoch time: 248.71 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3524%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1222%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6160896 real_backward_count 1176971  19.104%\n",
      "layer   1  Sparsity: 71.6309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 434 occurrences\n",
      "test - Value 1: 18 occurrences\n",
      "epoch-191 lr=['4.0000000'], tr/val_loss:470.991486/408.544342, val:  53.98%, val_best:  83.85%, tr:  97.54%, tr_best:  99.50%, epoch time: 248.49 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.0538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6193152 real_backward_count 1183322  19.107%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 104 occurrences\n",
      "test - Value 1: 348 occurrences\n",
      "epoch-192 lr=['4.0000000'], tr/val_loss:469.067566/462.979675, val:  69.91%, val_best:  83.85%, tr:  98.66%, tr_best:  99.50%, epoch time: 246.96 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.8572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6225408 real_backward_count 1189538  19.108%\n",
      "layer   1  Sparsity: 69.5312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-193 lr=['4.0000000'], tr/val_loss:493.500031/535.675720, val:  50.00%, val_best:  83.85%, tr:  97.57%, tr_best:  99.50%, epoch time: 246.83 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.9638%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6257664 real_backward_count 1195956  19.112%\n",
      "layer   1  Sparsity: 78.2227%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-194 lr=['4.0000000'], tr/val_loss:471.317352/569.767700, val:  50.00%, val_best:  83.85%, tr:  98.31%, tr_best:  99.50%, epoch time: 248.67 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.6996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6289920 real_backward_count 1202271  19.114%\n",
      "layer   1  Sparsity: 90.0391%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2044 occurrences\n",
      "train - Value 1: 1988 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 11 occurrences\n",
      "test - Value 1: 441 occurrences\n",
      "epoch-195 lr=['4.0000000'], tr/val_loss:464.563507/502.242279, val:  52.43%, val_best:  83.85%, tr:  97.87%, tr_best:  99.50%, epoch time: 246.59 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8335%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6322176 real_backward_count 1208660  19.118%\n",
      "layer   1  Sparsity: 84.6924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 437 occurrences\n",
      "test - Value 1: 15 occurrences\n",
      "epoch-196 lr=['4.0000000'], tr/val_loss:464.624756/417.832703, val:  53.32%, val_best:  83.85%, tr:  98.09%, tr_best:  99.50%, epoch time: 247.46 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6354432 real_backward_count 1215040  19.121%\n",
      "layer   1  Sparsity: 83.5449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 80 occurrences\n",
      "test - Value 1: 372 occurrences\n",
      "epoch-197 lr=['4.0000000'], tr/val_loss:458.603058/442.378754, val:  65.04%, val_best:  83.85%, tr:  97.99%, tr_best:  99.50%, epoch time: 248.78 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6386688 real_backward_count 1221346  19.123%\n",
      "layer   1  Sparsity: 77.0020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 165 occurrences\n",
      "test - Value 1: 287 occurrences\n",
      "epoch-198 lr=['4.0000000'], tr/val_loss:455.743042/420.986206, val:  76.77%, val_best:  83.85%, tr:  98.46%, tr_best:  99.50%, epoch time: 248.78 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6418944 real_backward_count 1227585  19.124%\n",
      "layer   1  Sparsity: 73.3887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-199 lr=['4.0000000'], tr/val_loss:459.852112/528.923218, val:  50.00%, val_best:  83.85%, tr:  98.36%, tr_best:  99.50%, epoch time: 248.98 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8161%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.7107%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8239c754bb41d69a18cf12a47303d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98363</td></tr><tr><td>tr_epoch_loss</td><td>459.85211</td></tr><tr><td>val_acc_best</td><td>0.8385</td></tr><tr><td>val_acc_now</td><td>0.5</td></tr><tr><td>val_loss</td><td>528.92322</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-sweep-62</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/alux2h5q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/alux2h5q</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251220_122049-alux2h5q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e1vd0qcj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251221_020823-e1vd0qcj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/e1vd0qcj' target=\"_blank\">polar-sweep-63</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/e1vd0qcj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/e1vd0qcj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251221_020831_858', 'my_seed': 42, 'TIME': 8, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 16, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 0, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 2, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.25, 0.0625, 0.03125], 'learning_rate': 1, 'learning_rate2': 2, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4032, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4032 BATCH: 1 train_data_count: 4032\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHFCAYAAAAT5Oa6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACAPUlEQVR4nO3deVxU1f8/8NfMMAyLiALJQKLivoAb5q5ICmjikp+01EjN1HIlNXNJRXNJM7XcNRNTSftV2KKhWOISrii5pmW4i6QSKCAzzJzfH3zn5ggMi+DMOK/n4zGPuXPuueeee+7l3jf3nnuvTAghQERERGTD5OauABEREZG5MSAiIiIim8eAiIiIiGweAyIiIiKyeQyIiIiIyOYxICIiIiKbx4CIiIiIbB4DIiIiIrJ5DIiIiIjI5jEgIpsWFRUFmUxW4GfixIlGeXNycrB8+XK0b98elStXhr29PZ5//nn069cP+/btM8r7wQcfICwsDM8//zxkMhkGDx5crPp88803kMlk2LZtW75xTZo0gUwmw65du/KNq1WrFpo3b178BQcwePBg1KhRo0TTGERGRkImk+HOnTtF5p03bx62b99e7LIfXQcKhQKVK1dGkyZNMGLECBw+fDhf/suXL0MmkyEqKqoESwBER0dj6dKlJZqmoHmVpC2K69y5c4iMjMTly5fzjXuS9VYWLl26BJVKhUOHDklpnTp1gp+fX7Gml8lkiIyMlH6bWtbSEkJg3bp1CAgIQMWKFeHu7o7AwEDs2LHDKN/Fixdhb2+PEydOlNm8yXoxICICsGHDBhw6dMjoM3bsWGn8nTt30K5dO4wfPx5+fn6IiorCL7/8gk8++QQKhQKdO3fG77//LuVfsmQJ7t69i549e8Le3r7Y9ejUqRNkMhn27t1rlH7v3j2cPn0azs7O+cZdv34df//9N4KCgkq0zNOnT0dMTEyJpimNkgZEAPDKK6/g0KFDOHjwILZu3Yo33ngDhw8fRps2bTBu3DijvF5eXjh06BC6d+9eonmUJiAq7bxK6ty5c5g1a1aBQcLTWm+FmThxIoKDg9GmTZtSTX/o0CG89dZb0m9Ty1paM2fOxPDhw9GyZUt8++23iIqKgkqlQlhYGL777jspX926dTFw4EC8++67ZTZvsl525q4AkSXw8/NDixYtCh3/xhtv4Pfff8euXbvw4osvGo177bXXMH78eFSuXFlKu3//PuTyvP83Nm3aVOx6eHh4wM/PD/Hx8Ubp+/btg52dHYYOHZovIDL8LmlAVKtWrRLlf5o8PT3RunVr6XdoaCgiIiIwfPhwfPbZZ6hfvz7eeecdAIBKpTLKWx50Oh1yc3OfyryKYs71dv78eWzfvh2xsbGlLuNptN8XX3yB9u3bY9WqVVJacHAw1Go1Nm7ciD59+kjpo0ePRosWLZCQkIC2bduWe93IcvEMEVEREhMT8fPPP2Po0KH5giGDF154AdWqVZN+G4Kh0ggKCsKFCxdw69YtKS0+Ph4vvPACXnrpJSQmJuL+/ftG4xQKBTp06AAg73LBypUr0bRpUzg6OqJy5cp45ZVX8PfffxvNp6BLL//++y+GDh0KNzc3VKhQAd27d8fff/+d7zKHwe3bt9G/f3+4urrC09MTb775JtLT06XxMpkMmZmZ2Lhxo3QZrFOnTqVqF4VCgeXLl8PDwwMff/yxlF7QZax//vkHw4cPh4+PD1QqFZ577jm0a9cOe/bsAZB3Jm7Hjh24cuWK0SW6R8tbuHAh5syZA19fX6hUKuzdu9fk5blr166hT58+qFixIlxdXfH666/jn3/+McpTWDvWqFFDuqwaFRWFvn37AsjbFgx1M8yzoPX28OFDTJkyBb6+vtKl3FGjRuHff//NN5+wsDDExsaiefPmcHR0RP369fHFF18U0fp5Vq1aBbVajeDg4ALHHzhwAK1bt4ajoyOef/55TJ8+HTqdrtA2KGpZS0upVMLV1dUozcHBQfo8KiAgAA0aNMDq1aufaJ5k/RgQEeG/MwCPfgx2794NAOjdu/dTqYvhTM+jZ4n27t2LwMBAtGvXDjKZDAcOHDAa17x5c+kAMGLECERERKBLly7Yvn07Vq5cibNnz6Jt27a4fft2ofPV6/Xo0aMHoqOj8f777yMmJgatWrVC165dC53mf//7H+rWrYtvv/0WkydPRnR0tNHlh0OHDsHR0REvvfSSdCly5cqVpW0aODo6okuXLkhOTsb169cLzRceHo7t27djxowZ2L17Nz7//HN06dIFd+/eBQCsXLkS7dq1g1qtNrpM+qjPPvsMv/76KxYtWoSff/4Z9evXN1m3l19+GbVr18Y333yDyMhIbN++HaGhodBqtSVaxu7du2PevHkAgBUrVkh1K+wynRACvXv3xqJFixAeHo4dO3Zg/Pjx2LhxI1588UXk5OQY5f/9998xYcIEvPvuu/j+++/RuHFjDB06FPv37y+ybjt27EDHjh0LDPhTUlLw2muvYeDAgfj+++/xyiuvYM6cOfkucZZkWfV6fb6/y4I+jwdd48aNQ2xsLNavX4+0tDTcunUL48ePR3p6utGlcINOnTrh559/hhCiyDagZ5ggsmEbNmwQAAr8aLVaIYQQb7/9tgAg/vjjj1LNw9nZWQwaNKjY+e/duyfkcrkYPny4EEKIO3fuCJlMJmJjY4UQQrRs2VJMnDhRCCHE1atXBQAxadIkIYQQhw4dEgDEJ598YlTmtWvXhKOjo5RPCCEGDRokqlevLv3esWOHACBWrVplNO38+fMFADFz5kwpbebMmQKAWLhwoVHekSNHCgcHB6HX60u9/ADEqFGjCh3//vvvCwDiyJEjQgghkpOTBQCxYcMGKU+FChVERESEyfl0797daPkNDOXVqlVLaDSaAsc9Oi9DW7z77rtGebds2SIAiM2bNxst26PtaFC9enWjNvp//+//CQBi7969+fI+vt5iY2MLXBfbtm0TAMTatWuN5uPg4CCuXLkipWVnZws3NzcxYsSIfPN61O3btwUA8dFHH+UbFxgYKACI77//3ih92LBhQi6XG83v8TYwtayGti3qU9B6XL16tVCpVFIeNzc3ERcXV+CyrVu3TgAQ58+fN9kG9GzjGSIiAF9++SWOHTtm9LGzM08XO8NdVYYzRPv27YNCoUC7du0AAIGBgVK/ocf7D/3000+QyWR4/fXXjf6DVqvVRmUWxHCnXL9+/YzS+/fvX+g0PXv2NPrduHFjPHz4EKmpqcVf4BISxfgvvmXLloiKisKcOXNw+PDhEp+lAfKWTalUFjv/wIEDjX7369cPdnZ2+fp8lbVff/0VAPLdydi3b184Ozvjl19+MUpv2rSp0eVdBwcH1K1bF1euXDE5n5s3bwIAqlSpUuB4FxeXfNvDgAEDoNfri3X2qSDDhw/P93dZ0OfHH380mm7Dhg0YN24cRo8ejT179mDnzp0ICQlBr169CrxL07BMN27cKFU96dnATtVEABo0aFBop2rDwSM5ORn16tV7KvUJCgrC4sWLcfPmTezduxcBAQGoUKECgLyA6JNPPkF6ejr27t0LOzs7tG/fHkBenx4hBDw9PQsst2bNmoXO8+7du7Czs4Obm5tRemFlAYC7u7vRb5VKBQDIzs4ueiFLyXDg9vb2LjTPtm3bMGfOHHz++eeYPn06KlSogJdffhkLFy6EWq0u1ny8vLxKVK/Hy7Wzs4O7u7t0ma68GNbbc889Z5Quk8mgVqvzzf/xdQbkrbei1plh/ON9cAwK2k4MbVLaNlCr1YUGYI8y9P8CgLS0NIwaNQpvvfUWFi1aJKV369YNnTp1wttvv43k5GSj6Q3LVJ7bLVk+niEiKkJoaCgAlPjW8SfxaD+i+Ph4BAYGSuMMwc/+/fulztaGYMnDwwMymQwHDx4s8D9pU8vg7u6O3Nxc3Lt3zyg9JSWljJeu9LKzs7Fnzx7UqlULVatWLTSfh4cHli5disuXL+PKlSuYP38+vvvuu2I/DwowPsgWx+PtlJubi7t37xoFICqVKl+fHqD0AQPw33p7vAO3EAIpKSnw8PAoddmPMpTz+PZhUFD/NEObFBSEFcfs2bOhVCqL/Dx6592FCxeQnZ2NF154IV95LVq0wOXLl/HgwQOjdMMylVVbkXViQERUhObNm6Nbt25Yv369dHniccePH8fVq1fLbJ4dO3aEQqHAN998g7NnzxrdmeXq6oqmTZti48aNuHz5stHt9mFhYRBC4MaNG2jRokW+j7+/f6HzNARdjz8UcuvWrU+0LMU5+1AcOp0Oo0ePxt27d/H+++8Xe7pq1aph9OjRCA4ONnoAX1nVy2DLli1Gv7/++mvk5uYarbsaNWrg1KlTRvl+/fXXfAfokpxp69y5MwBg8+bNRunffvstMjMzpfFPqnr16nB0dMSlS5cKHH///n388MMPRmnR0dGQy+Xo2LFjoeWaWtbSXDIznDl8/CGeQggcPnwYlStXhrOzs9G4v//+G3K5/KmdASbLxEtmRMXw5ZdfomvXrujWrRvefPNNdOvWDZUrV8atW7fw448/4quvvkJiYqJ0eW3fvn3Sf+w6nQ5XrlzBN998AyAv8Hj88sbjKlasiObNm2P79u2Qy+VS/yGDwMBA6aGCjwZE7dq1w/DhwzFkyBAcP34cHTt2hLOzM27duoWDBw/C399fen7P47p27Yp27dphwoQJyMjIQEBAAA4dOoQvv/wSQOkfJeDv74/4+Hj8+OOP8PLygouLS5EHntu3b+Pw4cMQQuD+/fs4c+YMvvzyS/z+++949913MWzYsEKnTU9PR1BQEAYMGID69evDxcUFx44dQ2xsrNHzZ/z9/fHdd99h1apVCAgIgFwuN/ksqqJ89913sLOzQ3BwMM6ePYvp06ejSZMmRn2ywsPDMX36dMyYMQOBgYE4d+4cli9fnu8WccNTn9euXQsXFxc4ODjA19e3wDMtwcHBCA0Nxfvvv4+MjAy0a9cOp06dwsyZM9GsWTOEh4eXepkeZW9vjzZt2hT4tHAg7yzQO++8g6tXr6Ju3brYuXMn1q1bh3feeceoz9LjTC2rt7e3yUujBalWrRr69OmDtWvXQqVS4aWXXkJOTg42btyI3377DR9++GG+s3+HDx9G06ZNjZ4lRjbInD26iczNcJfZsWPHisybnZ0tPvvsM9GmTRtRsWJFYWdnJ7y9vUWfPn3Ejh07jPIa7rop6FPQ3TQFmTRpkgAgWrRokW/c9u3bBQBhb28vMjMz843/4osvRKtWrYSzs7NwdHQUtWrVEm+88YY4fvy4lOfxu5WEyLvDbciQIaJSpUrCyclJBAcHi8OHDwsA4tNPP5XyGe7++eeff4ymN7RncnKylJaUlCTatWsnnJycBAARGBhocrkfbSu5XC4qVqwo/P39xfDhw8WhQ4fy5X/8zq+HDx+Kt99+WzRu3FhUrFhRODo6inr16omZM2catdW9e/fEK6+8IipVqiRkMpkw7A4N5X388cdFzuvRtkhMTBQ9evQQFSpUEC4uLqJ///7i9u3bRtPn5OSISZMmCR8fH+Ho6CgCAwNFUlJSvrvMhBBi6dKlwtfXVygUCqN5FrTesrOzxfvvvy+qV68ulEql8PLyEu+8845IS0szyle9enXRvXv3fMsVGBhY5HoRQoj169cLhUIhbt68mW/6Ro0aifj4eNGiRQuhUqmEl5eXmDp1qnS3pgEKuNOusGUtrezsbPHxxx+Lxo0bCxcXF+Hm5iZat24tNm/ebHQHpBBC3L9/Xzg5OeW7M5Nsj0wIPniBiAoXHR2NgQMH4rfffuOTfG3cw4cPUa1aNUyYMKFEly0t2fr16zFu3Dhcu3aNZ4hsHAMiIpJ89dVXuHHjBvz9/SGXy3H48GF8/PHHaNasWb4X2JJtWrVqFSIjI/H333/n64tjbXJzc9GwYUMMGjQI06ZNM3d1yMzYh4iIJC4uLti6dSvmzJmDzMxMeHl5YfDgwZgzZ465q0YWYvjw4fj333/x999/m+ykbw2uXbuG119/HRMmTDB3VcgC8AwRERER2Tzedk9EREQ2jwERERER2TwGRERERGTz2Km6mPR6PW7evAkXF5cSP9KfiIiIzEP83wNevb29TT5glgFRMd28eRM+Pj7mrgYRERGVwrVr10y+A5EBUTG5uLgAyGvQihUrlkmZWZpctJz7CwDg6LTOcLK33tWh1Wqxe/duhISEQKlUmrs6zxy2b/ljG5cvtm/5sub2Le9jYUZGBnx8fKTjeGGs9wj8lBkuk1WsWLHMAiI7TS7kKiepXGsPiJycnFCxYkWr+2O0Bmzf8sc2Ll9s3/Jlze37tI6FRXV3YadqIjLpoVaHkVsSMXJLIh5qdeauDpUC1yFR0RgQEZFJeiGw83QKdp5OgZ7PcbVKXIdERbPeazTPAIVchv81ryoNExER2RpLORYyIDIjlZ0Cn/RrYu5qEBGVKb1eD41GY5Sm1WphZ2eHhw8fQqfjZbuyZu3tO7dnPQCAyNXiYa62RNMqlUooFIonrgMDIiIiKjMajQbJycnQ6/VG6UIIqNVqXLt2jc9yKwe23r6VKlWCWq1+omVnQGRGQghk/18HR0elwiY3YiJ6dgghcOvWLSgUCvj4+Bg9BE+v1+PBgweoUKGCyYfjUelYc/sKIaD/v65tclnRd4M9Pm1WVhZSU1MBAF5eXqWuBwMiM8rW6tBwxi4AwLnZoVZ92z0RUW5uLrKysuDt7Q0nJyejcYbLaA4ODlZ3wLYG1ty+Or3A2ZvpAIBG3q4l7kfk6OgIAEhNTUWVKlVKffnMulqNiIgslqHvir29vZlrQrbGEIBrtSXrf/QoBkRERFSmePmfnray2OYYEBEREZHNY0BEREREhbp79y6qVKmCy5cvP/V5T5w4EWPHjn0q82JARERENm3w4MHo3bu30W+ZTIaPPvrIKN/27dulSzOGPKY+QF5H8w8++AC+vr5wdHREzZo1MXv27HyPJbBk8+fPR48ePVCjRg0pbdy4cQgICIBKpULTpk3zTRMfH49evXrBy8sLzs7OaNq0KbZs2WKUx9CGdgo5mvhURhOfyrBTyNGoUSMpz6RJk7BhwwYkJyeX1+JJGBARERE9xsHBAQsWLEBaWlqB4z/99FPcunVL+gDAhg0b8qUtWLAAq1evxvLly3H+/HksXLgQH3/8MZYtW/bUluVJZGdnY/369XjrrbeM0oUQePPNN/Hqq68WOF1CQgIaN26Mb7/9FqdOncKbb76JN954Az/++KOUx9CG12/cxC+Jf2D30TNwc3ND3759pTxVqlRBSEgIVq9eXT4L+AgGRGYkl8nwkr8aL/mrIWcnRLJQ3E6tH9dhyXXp0gVqtRrz588vcLyrqyvUarX0Af57OOCjaYcOHUKvXr3QvXt31KhRA6+88gpCQkJw/PjxQucdGRmJpk2b4osvvkC1atVQoUIFvPPOO9DpdFi4cCHUajWqVKmCuXPnGk23YsUKNGnSBM7OzvDx8cHIkSPx4MEDafybb76Jxo0bIycnB0DeHVkBAQEYOHBgoXX5+eefYWdnhzZt2hilf/bZZxg1ahRq1qxZ4HRTp07Fhx9+iLZt26JWrVoYO3YsunbtipiYmHxt6KVWo1b1qkj+4zTS0tIwZMgQo7J69uyJr776qtA6lhUGRGbkoFRg5cAArBwYAAflkz92nKg8cDu1fuZeh1maXGRpcpGt0UnDhs9Dra7AvAV9ipu3LCgUCsybNw/Lli3D9evXS11O+/bt8csvv+DixYsAgN9//x0HDx7ESy+9ZHK6S5cu4eeff0ZsbCy++uorfPHFF+jevTuuX7+Offv2YcGCBfjggw9w+PBhaRq5XI6lS5fizJkz2LhxI3799VdMmjRJGv/ZZ58hMzMTkydPBgBMnz4dd+7cwcqVKwutx/79+9GiRYtSL/+j0tPT4ebmli9dLpehurszfvx6C7p06YLq1asbjW/ZsiWuXbuGK1eulEk9CsMnARIRUbkyPIC2IEH1nsOGIS2l3wEf7pGe4P+4Vr5u2DbivzMV7Rfsxb1MTb58lz/q/gS1/c/LL7+Mpk2bYubMmVi/fn2pynj//feRnp6O+vXrQ6FQQKfTYe7cuejfv7/J6fR6Pb744gu4uLigYcOGCAoKwoULF7Bz507I5XLUq1cPCxYsQHx8PFq3bg0AeOedd1CxYkXI5XL4+vriww8/xDvvvCMFPBUqVMDmzZsRGBgIFxcXfPLJJ/jll1/g6upaaD0uX74Mb2/vUi37o7755hscO3YMa9asKXD8rVu38PPPPyM6OjrfuOeff16qy+PBUlliQERERFSIBQsW4MUXX8SECRNKNf22bduwefNmREdHo1GjRkhKSkJERAS8vb0xaNCgQqerUaMGXFxcpN+enp5QKBRGT6H29PSUXlkBAAcOHMCnn36K8+fPIyMjA7m5uXj48CEyMzPh7OwMAGjTpg0mTpyIDz/8EO+//z46duxosv7Z2dlwcHAo1bIbxMfHY/DgwVi3bp1Rh+lHRUVFoVKlSkad2w0MT6LOysp6onoUhQGRGWVpcvnqDrJ43E6tX43JO6Rhc6zDc7NDodfrcT/jPlwquhgd1B/v05Q4vUuh5Tye9+D7QWVb0QJ07NgRoaGhmDp1KgYPHlzi6d977z1MnjwZr732GgDA398fV65cwfz5800GREql0ui3TCYrMM1wt9qVK1fQr18/jBgxAnPmzIGbmxsOHjyIoUOHGj29Wa/X47fffoNCocCff/5ZZP09PDwK7VheHPv27UOPHj2wePFivPHGGwXmydXpsXrt5+jWux8Udsp84+/duwcAeO6550pdj+Lgno2IiMqVk70d9Ho9cu0VcLK3M/murZIEa08rsPvoo4/QtGlT1K1bt8TTZmVl5VtehUJR5rfdHz9+HLm5uVi0aBHs7PLa5euvv86X7+OPP8b58+exb98+hIaGYsOGDfk6MT+qWbNm2Lx5c6nqFB8fj7CwMCxYsADDhw8vNN++fftw9fLf6P3a6wWOP3PmDJRKZaFnl8oKAyIiMslRqUDiB12kYbJOXIel5+/vj4EDB5bqVvkePXpg7ty5qFatGho1aoSTJ09i8eLFePPNN8u0jrVq1UJubi6WL1+Onj174rfffst3q3pSUhJmzJiBb775Bu3atcOnn36KcePGITAwsNC7xUJDQzFlyhSkpaWhcuXKUvpff/2FBw8eICUlBdnZ2UhKSgIANGzYEPb29oiPj0f37t0xbtw4/O9//0NKSgqAvPfcPd6xesMXX8C/WQvUqd+wwDocOHAAHTp0kC6dlRfeZUZEJslkMrhXUMG9gorvqLJiXIdP5sMPP4QQosTTLVu2DK+88gpGjhyJBg0aYOLEiRgxYgQ+/PDDMq1f06ZNMXfuXCxcuBB+fn7YsmWL0SMDHj58iIEDB2Lw4MHo0aMHAGDo0KHo0qULwsPDpRfzPs7f3x8tWrTId7bprbfeQrNmzbBmzRpcvHgRzZo1Q7NmzXDz5k0AeX2CsrKyMH/+fHh5eUmfPn36GJWTnp6O7777Fi8XcnYIAL766isMGzasVO1SEjJRmjVsgzIyMuDq6or09HRUrFixTMp8lvpmaLVa7Ny5Ey+99FK+69z05Ni+5e9ZbuMak3eU2Z1Xpjx8+BDJycnw9fXN1xFXr9cjIyNDuguKylZ5tu/OnTsxceJEnDlzplzWnU4vcPZmOgCgkbcrFPL/gvYdO3bgvffew6lTp6RLgQUxte0V9/jNrZKITMrJ1WH69jOYvv0McnIL/i+S8nu0I7Ml4Dqk0nrppZcwYsQI3Lhx46nPOzMzExs2bDAZDJUV6z0lQURPhU4vsOlw3gPRprxU38y1odLiOqQnMW7cOLPMt1+/fk9tXgyIzEgukyGo3nPSMBERka2RAXBxUErD5sKAyIwclAqjJ7QSkfV7Wv11iJ4VcrkMvh7O5q4G+xAREVkbU/2TLK3vEpG1YEBERKXGgy8RPSsYEJlRliYXDabHosH02DJ7QzPRs4LBFpFt0OkFztxIx5kb6dDpzfckILMGRPv370ePHj3g7e0NmUyG7du358tz/vx59OzZE66urnBxcUHr1q1x9epVaXxOTg7GjBkDDw8PODs7o2fPnrh+/bpRGWlpaQgPD4erqytcXV0RHh6Of//9t5yXrniytbpC3+xMRKYxaCJ6NuiFgN7Mj0U0a0CUmZmJJk2aYPny5QWOv3TpEtq3b4/69esjPj4ev//+O6ZPn2700KWIiAjExMRg69atOHjwIB48eICwsDCjp24OGDAASUlJiI2NRWxsLJKSkhAeHl7uy0dERETWwax3mXXr1g3dunUrdPy0adPw0ksvYeHChVLao+9bSU9Px/r167Fp0yZ06ZL3np7NmzfDx8cHe/bsQWhoKM6fP4/Y2FgcPnwYrVq1AgCsW7cObdq0wYULF1CvXr1yWjoiAnjXFdHTUrlyZXz77bf5Xo9RUr/++itGjhyJc+fOmf2p4jk5OahTpw5iYmIQEBBQrvOy2D5Eer0eO3bsQN26dREaGooqVaqgVatWRpfVEhMTodVqERISIqV5e3vDz88PCQkJAIBDhw7B1dVVCoYAoHXr1nB1dZXyFCQnJwcZGRlGHyDv8f5l+TEo63LN8XlWlsNSP+Zs38K2U5VCQKvVot60nwqd1pCnpJ/yLLs829hQJ1N1K2iZTC1nSZa7oHH2cvHU9jVCCOj1+nwfw1uiChtvzs+gQYPQq1cvo98ymQzz5883yvfdd99BJpMZ5TH10ev10Gg0mDZtGnx9feHo6IiaNWti1qxZyM3NLdNlEELgjz/+QNeuXZ+4rEmTJmHKlCnSsbigz+nTp9GnTx/UqFEDMpkMS5YsyZdn3rx5eOGFF+Di4oIqVaqgV69eOH/+vFGejIwMjBkzGsEvNELL2l7wa9QQK1askMYrlUpMmDAB77//frHaoKh9mCkW8y4zmUyGmJgY9O7dGwCQkpICLy8vODk5Yc6cOQgKCkJsbCymTp2KvXv3IjAwENHR0RgyZAhycnKMygoJCYGvry/WrFmDefPmISoqChcvXjTKU7duXQwZMkRa6Y+LjIzErFmz8qVHR0fDycmpTJY5RwdMOpp3km5hy1yo+BJqskDcTq3f01qHdnZ2UKvV8PHxgb29ffnMpByMHDkS6enp2LJli/Q7JiYGKpUKSUlJqFSpEoC892q9/vrrSEtLQ3p6Oh4+fCiVUb9+faxYsQKdO3eW0jw9PbFo0SKsWrUKK1euRIMGDXDy5EmMHj0a06ZNw9tvv/1Ul7M4jhw5gr59++LixYv53gn2qBMnTiAmJgZNmzbFtGnTMG7cOLzzzjtGeV555RX06dMHzZo1Q25uLubMmYNz587h8OHDcHbOe+7QuHHjcODAAUxb8Bm8q1bDH4d/xXvvTcSXX36Jl156CQBw7949NGjQAPv37y/0qo5Go8G1a9eQkpKC3Fzjm5SysrIwYMCAIt9lZrEPZtTr9QCAXr164d133wWQ9zbfhIQErF69GoGBgYVOK4QweqNzQW93fjzP46ZMmYLx48dLvzMyMuDj44OQkJAyfbnrpKO/AgBCQ0Os/uWucXFxCA4OfuZejGkJzNm+prZTv8hdOBMZKn0XxNQ4U0yVXZz5ltTjbVwe9X48T1FpRc2juOMazdwlDZfnvubhw4e4du0aKlSokO9gKoTA/fv34eLiYnLfaw5KpRJ2dnbSvl2pVKJz5864dOkSVqxYgQULFgAAHB0dAQAVK1Ys8DigVqtRp04do7STJ0+iV69e6Nu3LwDAz88P33//Pc6cOVPosWTWrFn4/vvvMXr0aMyePRv37t3D66+/jmXLlmHx4sXS2ZixY8di6tSpAPLa187ODt9++y169+6Ny5cvo1atWvh//+//YcWKFThy5Ajq1KmDlStXok2bNoW2xU8//YSQkBBUqVLFZJt16tQJnTp1AgB8+OGHcHBwyLc8u3fvNvr95ZdfQq1W488//0THjh0B5F3pGTRoMF5o0x4AENxyLDZv3oRz587htddeA5DX3m3btsVPP/2EF154ocD6PHz4EI6OjujYsWOBL3ctDos9Ant4eMDOzg4NGzY0Sm/QoAEOHjwIIG/j02g0SEtLQ+XKlaU8qampaNu2rZTn9u3b+cr/559/4OnpWej8VSoVVCpVvnSlUllmByQV5Gjl65Y3bG8PpdL6//Uuy/ah/MzRvqa20xydDEqlUvouiKlxpvoXmSq7OPMtrWZzf8WFuWGlLru0bVKS+ZW0bI1e9lT2NTqdDjKZDHK53KjvSZYm7/JQtkYHO63OaJxcJoPDI/Ux9QiS4uYtacBnuMRlqJdMJoOdnR3mzZuHAQMGYNy4cahatao0vrB+NY8vNwB06NABq1evxl9//YW6devi999/x2+//YalS5cWWo5MJsOlS5ewa9cuxMbG4tKlS3jllVdw+fJl1K1bF/v27UNCQgLefPNNdOnSBa1bt5ZOIjxej+nTp2PRokWoU6cOpk2bhoEDB+Kvv/4q9GWpBw4cQP/+/Uvcd+jR9ivM/fv3AeQd3w1527dvj59++hFdXn4Nnmov7IuPx8WLF/Hpp58aldeyZUscPHjQZNvLZLIC95HF/buy2IDI3t4eL7zwAi5cuGCUfvHiRVSvXh0AEBAQAKVSibi4OOkFcLdu3cKZM2ekjtht2rRBeno6jh49ipYt816TceTIEaSnp0tBk7k4KBXYNqLwSJ3IEpR2O2Vn6rL3JG1qzn1Nwxm7Ch0XVO85o1cYBXy4p9BHkbTydTNajvYL9uJepiZfvrLa7l5++WU0bdoUM2fOxPr160tVxvvvv4/09HTUr18fCoUCOp0Oc+fORf/+/U1Op9fr8cUXX8DFxQUNGzZEUFAQLly4gJ07d0Iul6NevXpYsGAB4uPj0bp160LLmThxIrp3z2uPWbNmoVGjRvjrr79Qv37BL/m9fPkyvL29S7WspgghMH78eLRv3x5+fn5S+meffYZhw4ahfZN6sLOzg1wux+eff4727dsbTf/888/j8uXLZV6vR5k1IHrw4AH++usv6XdycjKSkpLg5uaGatWq4b333sOrr76Kjh07Sn2IfvzxR8THxwMAXF1dMXToUEyYMAHu7u5wc3PDxIkT4e/vL9111qBBA3Tt2hXDhg3DmjVrAADDhw9HWFgY7zAjolJhsGc7FixYgBdffBETJkwo1fTbtm3D5s2bER0djUaNGiEpKQkRERHw9vbGoEGDCp2uRo0acHFxkX57enpCoVAYnSHx9PREamqqyfk3btxYGvby8gKQdxWlsIAoOzvb6JLT1atXja7UTJ06VbpMVxKjR4/GqVOnpCs8Bp999hkOHz6MH374AdWrV8f+/fsxcuRIeHl5ScdxIO9yZVZWVonnWxJmDYiOHz+OoKAg6behz86gQYMQFRWFl19+GatXr8b8+fMxduxY1KtXD99++61R5LhkyRLY2dmhX79+yM7ORufOnREVFQWF4r/Tqlu2bMHYsWOlu9F69uxZ6LOPiOjpY4DxbDs3OxR6vR73M+7DpaJLvktmj0qc3uXxyQvNe/D9oEJylp2OHTsiNDQUU6dOxeDBg0s8/XvvvYfJkydL/WH8/f1x5coVzJ8/32RA9PhlHsPloMfTHr1UVlQ5hr5bpqbx8PBAWlqa9Nvb2xtJSUnSbzc3N5PzK8iYMWPwww8/YP/+/ahataqUnp2djalTpyImJkY6i9W4cWMkJSVh0aJFRgHRvXv38Nxzz5V43iVh1oCoU6dOKOomtzfffBNvvvlmoeMdHBywbNkyLFu2rNA8bm5u2Lx5c6nrWV6yNLlov2AvgLw/bGvuVE3PLm6nT1d5BYfNP4wDYJ516GRvB71ej1x7BZzs7Uz2NSlJ3Z7Wcnz00Udo2rQp6tatW+Jps7Ky8i2vQqEoMpAxl2bNmuHcuXPSbzs7O9SuXbtUZQkhMGbMGMTExCA+Ph6+vr5G46VHNUCGczfzOj7XU7sU2D5nzpxBs2bNSlWP4rLY5xDZinuZmgKvgRNZEmvcTi3xtR7mrJM1rkNL4e/vj4EDB5r8x7swPXr0wNy5c7Fjxw5cvnwZMTExWLx4MV5++eVyqOmTCw0NzXdZqyAajQZJSUlISkqCRqPBjRs3kJSUZNQNZtSoUdLlQhcXF6SkpCAlJQXZ2dkA8u4eCwwMxOT3J+HQb/tx+XIyNkZF4csvv8zXPgcOHDB65mB54L96RGSSg50Cu9/tKA2TdeI6fDIffvghvv766xJPt2zZMkyfPh0jR45EamoqvL29MWLECMyYMaMcavnkXn/9dbz//vtFvsnh5s2bRmdsFi1ahEWLFiEwMFDq57tq1SoAkG7PN9iwYYN0+XHr1q2YPHkKpowZjox/01CjRnXMnTvX6BlNhw4dQnp6Ol555ZWyWchCMCAiIpPkchnqeroUnZEsGtdh4aKiokz+BoDq1asbPYjxcYV1/3BxccHSpUuxdOnSYtcnMjISkZGRRdbJEHgYpKWlSc8CqlGjRr46VapUqchuKpUrV8bo0aOxePFi6UakghRU/uOK89xntVqN9V98gbM30wEAjbxdoZAb9xVbvHgx3nvvPek5UOWFl8yIiJ4BlniJkKzTtGnTUL16daOXpJtLTk4OmjRpIj2guTzxDBERmaTJ1WPF3rx+AaOCasPejv9HWaMlcXmvL+I6pKK4urqW6tb68qBSqfDBBx88lXnxr4KITMrV6/HpL3/i01/+RK6F3hlTWrZ0VuVZXYdEZYVniMxILpOhcVVXaZiIiMjWyAA42iukYXNhQGRGDkoFfhjdvuiMREREzyi5XIY6Vczf6Z+XzIiIiMjmMSAiIiIim8dLZmaUrdGhy+J9AIA94wOla6hERES2Qq8XuHj7PoC852XJ5ebpScSAyIwEBG78my0NExER2RoBQKPTS8PmwktmREREZPMYEBGRzbGl5w+RbZk1axbq168PZ2dnVK5cGV26dMGRI0ek8ffu3cOYMWNQr149ODk5oVq1ahg7dizS09OLLHvlypXw9fWFg4MDAgICcODAAaPxQghERkbC29sbjo6O6NSpE86ePVvmy1heGBARERE9I+rUqYPly5fj9OnTOHjwIGrUqIGQkBD8888/APJeynrz5k0sWrQIp0+fRlRUFGJjYzF06FCT5W7btg0RERGYNm0aTp48iQ4dOqBbt264evWqlGfhwoVYvHgxli9fjmPHjkGtViM4OBj3798v12UuKwyIiIjIpnXq1AljxoxBREQEKleuDE9PT6xduxaZmZkYMmQIXFxcUKtWLfz888/SNDqdDkOHDoWvry8cHR1Rr149fPrpp9L4hw8folGjRhg+fLiUlpycDFdXV6xbt67clmXAgAHo0qULatasiUaNGmHx4sXIyMjAqVOnAAB+fn749ttv0aNHD9SqVQsvvvgi5s6dix9//BG5ubmFlrt48WIMHToUb731Fho0aIClS5fCx8dHeqO9EAJLly7FtGnT0KdPH/j5+WHjxo3IyspCdHR0uS1vWWJARERE5SpLk4ssTS6yNTppuKhPru6/V4zk6vTI0uTioVZXYLmPf0pj48aN8PDwwNGjRzFmzBi888476Nu3L9q2bYsTJ04gNDQU4eHhyMrKAgDo9XpUrVoVX3/9Nc6dO4cZM2Zg6tSp+PrrrwEADg4O2LJlCzZu3Ijt27dDp9MhPDwcQUFBGDZsWKH16NatGypUqGDyU1wajQZr166Fq6srmjRpUmi+9PR0VKxYEXZ2Bd9npdFokJiYiJCQEKP0kJAQJCQkAMgL9lJSUozyqFQqBAYGSnksHe8yMyMZZKhTpYI0TGSJuJ0+G8y5DhvO2FXiaVYMaI7ujb0AALvO3sao6BNo5euGbSPaSHnaL9iLe5mafNNe/qh7iefXpEkT6SWiU6ZMwUcffQQPDw8peJkxYwZWrVqFU6dOoXXr1lAqlZg1a5Y0va+vLxISEvD111+jX79+AICmTZtizpw5GDZsGPr3749Lly5h+/btJuvx+eefIzs7u8T1f9RPP/2E1157DVlZWfDy8kJcXBw8PDwKzHv37l18+OGHGDFiRKHl3blzBzqdDp6enkbpnp6eSElJAQDpu6A8V65cMVlfGQAHO766w6Y52isQNz7Q3NUgMonb6bOB69C0xo0bS8MKhQLu7u7w9/eX0gwH+tTUVClt9erV+Pzzz3HlyhVkZ2dDo9GgadOmRuVOmDAB33//PZYtW4aff/650MDE4Pnnn3/iZQkKCkJSUhLu3LmDdevWoV+/fjhy5AiqVKlilC8jIwPdu3dHw4YNMXPmzCLLlT32zk0hRL604uR5nFwuQ121+V/dwYCIiIjK1bnZodDr9bifcR8uFV0glxfdW8Ne8V+e0EaeODc7NN9LsA++H1RmdVQqlUa/ZTKZUZrhoK7X513K+/rrr/Huu+/ik08+QZs2beDi4oKPP/7Y6I4uIC+AunDhAhQKBf7880907drVZD26deuW7+6txz148MDkeGdnZ9SuXRu1a9dG69atUadOHaxfvx5TpkyR8ty/fx9du3ZFhQoVEBMTk2/5H+Xh4QGFQiGdBXp02QyBolqtBpB3psjLy6vAPJaOAREREZUrJ3s76PV65Nor4GRvV6yA6FF2CjnsFPmncbI33yHswIEDaNu2LUaOHCmlXbp0KV++N998E35+fhg2bBiGDh2Kzp07o2HDhoWWWxaXzB4nhEBOTo70OyMjA6GhoVCpVPjhhx/g4OBgcnp7e3sEBAQgLi4OL7/8spQeFxeHXr16Aci7ZKhWqxEXF4dmzZoByOt7tG/fPixYsKBMl6e8MCAyo2yNDj2XHwQA/DC6PV/dQRaJ2+mzIfj/XhPEdVg2ateujS+//BK7du2Cr68vNm3ahGPHjsHX11fKs2LFChw6dAinTp2Cj48Pfv75ZwwcOBBHjhyBvb19geU+ySWzzMxMLFiwAL169YKXlxfu3r2LlStX4vr16+jbty+AvDNDISEhyMrKwubNm5GRkYGMjAwAwHPPPQeFIm/b6Ny5M15++WWMHj0aADB+/HiEh4ejRYsWaNOmDdauXYurV6/i7bffBpB3Bi0iIgLz5s1DnTp1UKdOHcybNw9OTk4YMGCAyXrr9QJ/pead9apdpQJf3WGLBAT+/L+NgK/uIEvF7fTZwHVYtt5++20kJSXh1VdfhUwmQ//+/TFy5Ejp1vw//vgD7733HtavXw8fHx8AeQFSkyZNMH369HI5a6JQKPDHH3/gyy+/xJ07d+Du7o4XXngBBw4cQKNGjQAAiYmJ0mW92rVrG02fnJyMGjVqAMg723Xnzh1p3Kuvvoq7d+9i9uzZuHXrFvz8/LBz505Ur15dyjNp0iRkZ2dj5MiRSEtLQ6tWrbB79264uJjuHyQAPMzVScPmwoCIiExS2Snw1bDW0jBZJ67DwsXHx+dLu3z5cr40If47XKtUKmzYsAEbNmwwyjN//nwAQP369aVb9A0qVqyI5OTkJ69wIRwcHPDtt9+avCTZqVMno+UoTEHLP3LkSKNLhI+TyWSIjIxEZGRkcaprcRgQEZFJCrkMbWq5m7sa9IS4DolM44MZiYiIyObxDBERmaTV6fHV0bz3FfVvWQ3KAu72Icv35aHLALgOiQrDgIiITNLq9Jjxfd4bq18JqMqDqZXiOiQyjQGRGckgw/OVHKVhIiIiWyPDfw/i5Ks7bJSjvQK/TX7R3NUgIiIyG7lchvpeFc1dDXaqJiIiIjJrQLR//3706NED3t7ekMlkJt8CPGLECMhkMixdutQoPScnB2PGjIGHhwecnZ3Rs2dPXL9+3ShPWloawsPD4erqCldXV4SHh+Pff/8t+wUiIiIiq2TWgCgzMxNNmjTB8uXLTebbvn07jhw5Am9v73zjIiIiEBMTg61bt+LgwYN48OABwsLCoNPppDwDBgxAUlISYmNjERsbi6SkJISHh5f58pTUQ23eKxF6Lj+Ih1pd0RMQERE9Y/R6gT9T7+PP1PvQ6833rGqz9iHq1q0bunXrZjLPjRs3MHr0aOzatQvdu3c3Gpeeno7169dj06ZN6NKlCwBg8+bN8PHxwZ49exAaGorz588jNjYWhw8fRqtWrQAA69atQ5s2bXDhwgXUq1evfBauGPRC4NT1dGmYiIisQ3x8PIKCgpCWloZKlSqZuzpWTSDvnYmGYXOx6E7Ver0e4eHheO+996T3sDwqMTERWq0WISEhUpq3tzf8/PyQkJCA0NBQHDp0CK6urlIwBACtW7eGq6srEhISCg2IcnJy8r0dGAC0Wi20Wm2ZLJ9Wm/vIsBZamfUGRYY2Kau2IWPmbF9T26lKIaDVaqXvR5ka96TTl6Ts4s7X8K2Sl33ZZb1MJZ3eXi6g0cuk5SyvfY1Wq4UQAnq9Hnq93mic4XURhvHWrnXr1rhx4wZcXFwsYnkM7Xvnzh2Eh4fj9OnTuHv3LqpUqYKePXti7ty5qFgxr+NyfHw8li5dimPHjiEjIwN16tTBhAkTMHDgQJPzSEtLw7hx4/Djjz8CAHr06IHPPvvMKCC8evUqRo8ejb1798LR0RH9+/fHxx9/XOjLbPPqbrwcpTlLpNfrIUTetm94Qa1BcfebMlGcl5o8BTKZDDExMejdu7eUNn/+fOzduxe7du2CTCZDjRo1EBERgYiICABAdHQ0hgwZYhS4AEBISAh8fX2xZs0azJs3D1FRUbh48aJRnrp162LIkCGYMmVKgfWJjIzErFmz8qVHR0fDycnpyRb2/+TogElH82LShS1zoeIrhsgCcTu1fk9rHdrZ2UGtVsPHx8fkAZDKz7///otvv/0WzZs3h7u7O5KTk/Hee++hcePG+PzzzwEAn3zyCR4+fIguXbqgSpUq2L17N6ZOnYrNmzebvGrzyiuv4ObNm1Jf3oiICFSrVg1bt24FAOh0OnTo0AEeHh6YM2cO7t27h5EjRyIsLAwLFy4stFy9AK5n5g1XdQZK87J7jUaDa9euISUlBbm5uUbjsrKyMGDAAKSnp0tBYYGEhQAgYmJipN/Hjx8Xnp6e4saNG1Ja9erVxZIlS6TfW7ZsEfb29vnK6tKlixgxYoQQQoi5c+eKunXr5stTu3ZtMX/+/ELr8/DhQ5Geni59rl27JgCIO3fuCI1GUyaffx9kierv/ySqv/+T+PdBVpmVa45PZmam2L59u8jMzDR7XZ7Fjznb19R2Wnfqj0bfxR33pNOXpOziztfQxv4f/FDmZZu7vepM+fGp7GsyMjLE2bNnRWZmptDpdEaf3NxckZaWJnJzc/ONM/cnMDBQjBo1SowdO1ZUqlRJVKlSRaxatUpkZGSIQYMGiQoVKoiaNWuKn376SZrml19+EQDE3bt3hU6nE+vXrxeurq5i586don79+sLZ2VmEhISI69evP5VlMNW+S5cuFVWrVjU5fbdu3cTgwYMLHX/mzBkBQCQkJEhpv/32mwAgzp07J3Q6nfjpp5+EXC4X165dk/Js2bJFqFQqkZaWVmjZ2lyd+P1amvj9WprQ5pZu+TMzM8XZs2dFRkZGvu3yzp07AoBIT083GYdY7CWzAwcOIDU1FdWqVZPSdDodJkyYgKVLl+Ly5ctQq9XQaDRIS0tD5cqVpXypqalo27YtAECtVuP27dv5yv/nn3/g6elZ6PxVKhVUKlW+dKVSCaVS+SSL9l9Z4r8wOK9ci10dxVaW7UP5maN9TW2nOToZlEql9P0oU+OedPqSlF3i+erLsWwztZfhchlQvvsanU4HmUwGuVxu9Mb1LE0u9Ho9sjU62Gl1Jt/GbmCvkMPu/x7Wl6vTQ6PTQy6TwUH53+mtLE1ugdM62Zd8+b788ktMmjQJR48exbZt2zBq1Cj88MMPePnllzFt2jQsWbIEgwYNwtWrV+Hk5CQtg2FZ5XI5srKysHjxYmzatAlyuRyvv/46Jk2ahC1bthQ63woVKpisV4cOHfDzzz8XWX/DZTtD+xvcvHkTMTExCAwMNNnuGRkZaNiwYaF5jhw5AldXV7Rp00ZKa9u2LVxdXXH48GE0aNAAR44cgZ+fH6pWrSrl6datG3JycnDy5EkEBQUVWLZ45BJZXv1LfopILpdDJpMVuI8s7j7TYo/A4eHhUkdpg9DQUISHh2PIkCEAgICAACiVSsTFxaFfv34AgFu3buHMmTPS6bk2bdogPT0dR48eRcuWLQHkrdj09HQpaCIiovLTcMauEk+zYkBzdG/sBQDYdfY2RkWfQCtfN2wb8d8Buf2CvbiXqck37eWPuudLK0qTJk3wwQcfAACmTJmCjz76CB4eHhg2bBgAYMaMGVi1ahVOnTqF1q1bF1iGVqvF6tWrUatWLQDA6NGjMXv2bJPzTUpKMjne0dGxhEuSp3///vj++++RnZ2NHj16SJfLCvLNN9/g2LFjWLNmTaF5UlJSUKVKlXzpVapUQUpKipTn8RMNlStXhr29vZTHkpk1IHrw4AH++usv6XdycjKSkpLg5uaGatWqwd3d3Si/UqmEWq2WOkK7urpi6NChmDBhAtzd3eHm5oaJEyfC399fCqYaNGiArl27YtiwYdLKHj58OMLCwsx6h5mBmzOvs5Pl43Zq/bgOTWvcuLE0rFAo4O7uDn9/fynNcKBPTU0ttAwnJycpGAIALy8vk/kBoHbt2qWtMrp164YDBw4AAKpXr47ffvtNGrdkyRLMnDkTFy5cwNSpUzF+/HisXLkyXxnx8fEYPHgw1q1bV+DNS4+SyfKfuRFCGKUXJ09B7Ipx1rC8mTUgOn78uNEptPHjxwMABg0ahKioqGKVsWTJEtjZ2aFfv37Izs5G586dERUVZdTLfMuWLRg7dqx0N1rPnj2LfPbR0+Bkb4cT04PNXQ0ik7idPhvMuQ7PzQ6FXq/H/Yz7cKnoUuxLZgahjTxxbnYo5I8dVA++X/AlmNJ4/LKK4fLLo78BmLyjrKAyRBH3LT3JJbPPP/8c2dnZAJDvziq1Wg21Wo369evD3d0dHTp0wPTp0+Hl5SXl2bdvH3r06IHFixfjjTfeMFmP4nQ/UavVOHLkiNH4tLQ0aLVak11UFHIZGnqb/9UdZg2IOnXqVOTG8qjLly/nS3NwcMCyZcuwbNmyQqdzc3PD5s2bS1NFIiJ6Qk72dtDr9ci1V8DJ3q5YAdGj7B7pT/R4udbuSS6ZPf/889KwXq+XHg/zOMNx9tE7suPj4xEWFoYFCxZg+PDhRdazON1P2rRpg7lz5+LWrVtS4LV7926oVCoEBAQUOQ9zs/6tiYiIyEo9ySWzguzcuRP//PMPXnjhBVSoUAHnzp3DpEmT0K5dO9SoUQNAXjDUvXt3jBs3Dv/73/+k/j329vZwc3MDABw9ehRvvPEGfvnlFzz//PPF6n4SEhKChg0bIjw8HB9//DHu3buHiRMnYtiwYaZvd7cQ5r9oZ8MeanV4dc0hvLrmEF/dQRaL2+mzgevQNjg6OmLdunVo3749GjRogIiICISFheGnn36S8kRFRSErKwvz58+Hl5eX9OnTp4+UJysrCxcuXDB6qOGWLVvg7++PkJAQhISEoHHjxti0aZM0XqFQYMeOHXBwcEC7du3Qr18/9O7dG4sWLTJZZ71e4NI/D3Dpnwe2++oOW6cXAkeS70nDRJaI2+mzgeuwcPHx8fnSCuqi8WgXj8e7fAwePBiDBw82yt+7d+8SdQspC0FBQUhISDCZJyoqqsh+ugV1aSlO95Nq1aoZBV/FIQBk5uRKw+bCgIiITLJXyLFiQHNpmKwT1yGRaQyIiMgkO4Vceh4MWS+uQyLT+K8CERER2TyeISIik3J1euw6m/f8kdBGngXe/kyWb8epWwC4DokKw4CIiEzS6PQYFX0CQN4D9ngwtU5Pcx0+7Y7ERGWxzXHPZmaOSgUclYqiMxIRWTjD05I1mvzvFyMyRS6T5XsSeUlkZWUBKP6LXAvCM0Rm5GRvh/MfdjV3NYiIyoSdnR2cnJzwzz//QKlUGj2RWq/XQ6PR4OHDhyV+UjUVzdrbt7a7CgCg1eRAW0TeRwkhkJWVhdTUVFSqVCnfK0xKggERERGVCZlMBi8vLyQnJ+PKlStG44QQyM7OhqOjY5Ev+qSSs/X2rVSpEtRq9ROVwYCIiIjKjL29PerUqZPvsplWq8X+/fvRsWPHJ7qsQQWz5fZVKpVPdGbIgAGRGT3U6vDO5kQAwKrXA+DAvkRE9AyQy+VwcHAwSlMoFMjNzYWDg4PNHbCfBmtuX0s5FjIgMiO9ENh74R9pmIiIyNZYyrHQ+npeEREREZUxBkRERERk8xgQERERkc1jQEREREQ2jwERERER2TwGRERERGTzeNu9GTnZ2+HyR93NXQ0ik7idPhu4DslSWco+hmeIiIiIyOYxICIiIiKbx4DIjB5qdRi5JREjtyTioVZn7uoQFYjb6bOB65AslaXsYxgQmZFeCOw8nYKdp1P46g6yWNxOnw1ch2SpLGUfw07VRGSSUiHH7F6NpGGyTlyHRKYxICIik5QKOd5oU8Pc1aAnxHVIZBr/VSAiIiKbxzNERGSSTi9wNPkeAKClrxsUcpmZa0SlcejSXQBch0SFYUBERCbl5OrQf91hAMC52aFwsuduwxpxHRKZZtZLZvv370ePHj3g7e0NmUyG7du3S+O0Wi3ef/99+Pv7w9nZGd7e3njjjTdw8+ZNozJycnIwZswYeHh4wNnZGT179sT169eN8qSlpSE8PByurq5wdXVFeHg4/v3336ewhERERGQNzBoQZWZmokmTJli+fHm+cVlZWThx4gSmT5+OEydO4LvvvsPFixfRs2dPo3wRERGIiYnB1q1bcfDgQTx48ABhYWHQ6f57lsGAAQOQlJSE2NhYxMbGIikpCeHh4eW+fEVxVCpwbnYozs0OhaNSYe7qEBERPXWWciw063nTbt26oVu3bgWOc3V1RVxcnFHasmXL0LJlS1y9ehXVqlVDeno61q9fj02bNqFLly4AgM2bN8PHxwd79uxBaGgozp8/j9jYWBw+fBitWrUCAKxbtw5t2rTBhQsXUK9evfJdSBNkMhlPXRMRkU2zlGOhVd1llp6eDplMhkqVKgEAEhMTodVqERISIuXx9vaGn58fEhISAACHDh2Cq6urFAwBQOvWreHq6irlISIiIttm/pCsmB4+fIjJkydjwIABqFixIgAgJSUF9vb2qFy5slFeT09PpKSkSHmqVKmSr7wqVapIeQqSk5ODnJwc6XdGRgaAvL5NWq32iZcHAHJy9Zj+wzkAwIc9G0JlZ1XxqRFDm5RV25Axc7avVptrVA+t7L8nyaoUAlqtVvp+lKlxTzp9Scou7nwN3yp52Zdd1stU0unt5QIavUxazkfX4dPCfUT5sub2Le9jYXHbRCaEZTzHXSaTISYmBr179843TqvVom/fvrh69Sri4+OlgCg6OhpDhgwxClwAIDg4GLVq1cLq1asxb948bNy4ERcuXDDKU6dOHQwdOhSTJ08usD6RkZGYNWtWvvTo6Gg4OTmVcimN5eiASUfzYtKFLXOhYjciskDcTq0f1yFZsvLePrOysjBgwACkp6dL8UNBLP4MkVarRb9+/ZCcnIxff/3VaGHUajU0Gg3S0tKMzhKlpqaibdu2Up7bt2/nK/eff/6Bp6dnofOdMmUKxo8fL/3OyMiAj48PQkJCTDZoSWRpcjHp6K8AgNDQEIu4hlpaWq0WcXFxCA4OhlKpNHd1njnmbF9T26lf5C6ciQyVvh9latyTTl+Ssos7X0MbTz8uR+KMrmVadlkvU0mnbzRzlzRsrn0N9xHly5rbt7yPhYYrPEWx6COwIRj6888/sXfvXri7uxuNDwgIgFKpRFxcHPr16wcAuHXrFs6cOYOFCxcCANq0aYP09HQcPXoULVu2BAAcOXIE6enpUtBUEJVKBZVKlS9dqVSW2camFP89HC2vXIteHcVSlu1D+ZmjfU1tpzk6GZRKpfT9KFPjnnT6kpRd4vnqy7FsM7WX4XIZYP59DfcR5csa27e8j4XFbQ+zHoEfPHiAv/76S/qdnJyMpKQkuLm5wdvbG6+88gpOnDiBn376CTqdTurz4+bmBnt7e7i6umLo0KGYMGEC3N3d4ebmhokTJ8Lf31+666xBgwbo2rUrhg0bhjVr1gAAhg8fjrCwMLPeYUZERESWw6wB0fHjxxEUFCT9NlyiGjRoECIjI/HDDz8AAJo2bWo03d69e9GpUycAwJIlS2BnZ4d+/fohOzsbnTt3RlRUFBSK/y5CbtmyBWPHjpXuRuvZs2eBzz4iIiIi22TWgKhTp04w1ae7OP29HRwcsGzZMixbtqzQPG5ubti8eXOp6khERETPPuu9z5uIiIiojFh/L14r5qhUIPGDLtIwkSXidvps4DokS2Up+xgGRGYkk8ngXiH/nWxEloTb6bOB65AslaXsY3jJjIiIiGwezxCZUU6uDnN+Og8A+CCsAVR2PJVNlofb6bNh+vYzALgOyfJYyj6GZ4jMSKcX2HT4CjYdvgKd3iLeoEKUD7fTZwPXIVkqS9nH8AwREZlkJ5djXOc60jBZJ65DItMYEBGRSfZ2crwbXNfc1aAnxHVIZBr/VSAiIiKbxzNERGSSXi/w1z8PAAC1n6sAuVxWxBRkiS7evg+A65CoMAyIiMikh7k6hCzZDwA4NzsUTvbcbVgjrkMi03jJjIiIiGwe/00wIwc7BQ5MCpKGiYiIbI2lHAsZEJmRXC6Dj5uTuatBRERkNpZyLOQlMyIiIrJ5PENkRppcPRbtvgAAmBhSD/Z2jE+JiMi2WMqxkEdgM8rV67F2/99Yu/9v5Or15q4OERHRU2cpx0IGRERERGTzGBARERGRzWNARERERDaPARERERHZPAZEREREZPMYEBEREZHN43OIzMjBToHd73aUhoksEbfTZwPXIVkqS9nHMCAyI7lchrqeLuauBpFJ3E6fDVyHZKksZR/DS2ZERERk83iGyIw0uXqs2PsXAGBUUG2+uoMsErfTZ8OSuIsAuA7J8ljKPoYBkRnl6vX49Jc/AQAjAmvCnifsyAJxO302cB2SpbKUfQwDIiIySSGXIbx1dWmYrBPXIZFpDIiIyCSVnQIf9vYzdzXoCXEdEpnG86ZERERk88waEO3fvx89evSAt7c3ZDIZtm/fbjReCIHIyEh4e3vD0dERnTp1wtmzZ43y5OTkYMyYMfDw8ICzszN69uyJ69evG+VJS0tDeHg4XF1d4erqivDwcPz777/lvHREzwYhBO4+yMHdBzkQQpi7OlRKXIdEppk1IMrMzESTJk2wfPnyAscvXLgQixcvxvLly3Hs2DGo1WoEBwfj/v37Up6IiAjExMRg69atOHjwIB48eICwsDDodDopz4ABA5CUlITY2FjExsYiKSkJ4eHh5b58RM+CbK0OAXP2IGDOHmRrdUVPQBaJ65DINLP2IerWrRu6detW4DghBJYuXYpp06ahT58+AICNGzfC09MT0dHRGDFiBNLT07F+/Xps2rQJXbp0AQBs3rwZPj4+2LNnD0JDQ3H+/HnExsbi8OHDaNWqFQBg3bp1aNOmDS5cuIB69eo9nYUlIiIii2WxnaqTk5ORkpKCkJAQKU2lUiEwMBAJCQkYMWIEEhMTodVqjfJ4e3vDz88PCQkJCA0NxaFDh+Dq6ioFQwDQunVruLq6IiEhodCAKCcnBzk5OdLvjIwMAIBWq4VWqy2TZZQLgW9HtPq/YX2ZlWsOhrpb8zJYMnO2r1aba1QPrey/Sy4qhYBWq5W+H2Vq3JNOX5Kyiztfw7dKXvZll/UylXR6e7mARi+TlvPRdfi0cB9Rvqy5fcv7WFjc8mTCQi4oy2QyxMTEoHfv3gCAhIQEtGvXDjdu3IC3t7eUb/jw4bhy5Qp27dqF6OhoDBkyxChwAYCQkBD4+vpizZo1mDdvHqKionDx4kWjPHXr1sWQIUMwZcqUAusTGRmJWbNm5UuPjo6Gk5PTEy4tkfXI0QGTjub977SwZS5UfBWW1eE6JFuWlZWFAQMGID09HRUrViw0n8WeITKQyYyfmSGEyJf2uMfzFJS/qHKmTJmC8ePHS78zMjLg4+ODkJAQkw1qq7RaLeLi4hAcHAylUmnu6jxzzNm+WZpcTDr6KwAgNDQETvb/7Tb8InfhTGSo9P0oU+OedPqSlF3c+RraePpxORJndC3Tsst6mUo6faOZu6Thx9fh08J9RPli+xbOcIWnKBYbEKnVagBASkoKvLy8pPTU1FR4enpKeTQaDdLS0lC5cmWjPG3btpXy3L59O1/5//zzj1ROQVQqFVQqVb50pVJZZhubJlePDb8lAwCGtPN9Jh6nX5btQ/mZo32V4r9/HPLm/99uI0cng1KplL4fZWrck05fkrJLPF99OZZtpvYyXC4D8q/Dp437iPJlje1b3sfC4raHxR6BfX19oVarERcXJ6VpNBrs27dPCnYCAgKgVCqN8ty6dQtnzpyR8rRp0wbp6ek4evSolOfIkSNIT0+X8phLrl6P+T//gfk//4Fcvd6sdSEiIjIHSzkWmvUM0YMHD/DXX39Jv5OTk5GUlAQ3NzdUq1YNERERmDdvHurUqYM6depg3rx5cHJywoABAwAArq6uGDp0KCZMmAB3d3e4ublh4sSJ8Pf3l+46a9CgAbp27Yphw4ZhzZo1APL6IYWFhfEOMyIiIgJg5oDo+PHjCAoKkn4b+uwMGjQIUVFRmDRpErKzszFy5EikpaWhVatW2L17N1xcXKRplixZAjs7O/Tr1w/Z2dno3LkzoqKioFD812twy5YtGDt2rHQ3Ws+ePQt99hERERHZnlIFRDVr1sSxY8fg7u5ulP7vv/+iefPm+Pvvv4tVTqdOnUw+NVUmkyEyMhKRkZGF5nFwcMCyZcuwbNmyQvO4ublh8+bNxaoTERER2Z5S9SG6fPmy0ZOgDXJycnDjxo0nrhQRERHR01SiM0Q//PCDNLxr1y64urpKv3U6HX755RfUqFGjzCpHRERE9DSUKCAyPDRRJpNh0KBBRuOUSiVq1KiBTz75pMwqR0RERPQ0lCgg0v/f7XC+vr44duwYPDw8yqVStkJlp8BXw1pLw0SWiNvps4HrkCyVpexjStWpOjk5uazrYZMUchna1HIvOiORGXE7fTZwHZKlspR9TKlvu//ll1/wyy+/IDU1VTpzZPDFF188ccWIiIiInpZSBUSzZs3C7Nmz0aJFC3h5eRX5bjEqmFanx1dHrwIA+resBqXCYh8cTjaM2+mz4ctDlwFwHZLlsZR9TKkCotWrVyMqKgrh4eFlXR+botXpMeP7swCAVwKqcidFFonb6bOB65AslaXsY0oVEGk0GrO/B4yIng65TIaX/NXSMFknrkMi00oVEL311luIjo7G9OnTy7o+RGRhHJQKrBwYYO5q0BPiOiQyrVQB0cOHD7F27Vrs2bMHjRs3hlKpNBq/ePHiMqkcERER0dNQqoDo1KlTaNq0KQDgzJkzRuPYwZqIiIisTakCor1795Z1PYjIQmVpctFwxi4AwLnZoXCyL/XTOsiMakzeAYDrkKgwvNWAiIiIbF6p/k0ICgoyeWns119/LXWFbIm9Qo4vBreQhomIiGyNpRwLSxUQGfoPGWi1WiQlJeHMmTP5XvpKhbNTyPFifU9zV4OIiMhsLOVYWKqAaMmSJQWmR0ZG4sGDB09UISIiIqKnrUzPTb3++ut8j1kJaHV6/L/j1/D/jl+DVqcvegIiIqJnjKUcC8v0VoNDhw7BwcGhLIt8pml1erz3zSkAQPfGXnycPhER2RxLORaWKiDq06eP0W8hBG7duoXjx4/z6dVERERkdUoVELm6uhr9lsvlqFevHmbPno2QkJAyqRgRERHR01KqgGjDhg1lXQ8iIiIis3miPkSJiYk4f/48ZDIZGjZsiGbNmpVVvYiIiIiemlIFRKmpqXjttdcQHx+PSpUqQQiB9PR0BAUFYevWrXjuuefKup5ERERE5aZUXbnHjBmDjIwMnD17Fvfu3UNaWhrOnDmDjIwMjB07tqzrSERERFSuSnWGKDY2Fnv27EGDBg2ktIYNG2LFihXsVF0C9go5VgxoLg0TWSJup88GrkOyVJayjylVQKTX66FUKvOlK5VK6PV8wGBx2Snk6N7Yy9zVIDKJ2+mzgeuQLJWl7GNKFYq9+OKLGDduHG7evCml3bhxA++++y46d+5cZpUjIiIiehpKFRAtX74c9+/fR40aNVCrVi3Url0bvr6+uH//PpYtW1bWdXxm5er02HHqFnacuoVcvrqDLBS302cD1yFZKkvZx5TqkpmPjw9OnDiBuLg4/PHHHxBCoGHDhujSpUtZ1++ZptHpMSr6BADg3OxQ2PHaPlkgbqfPBq5DslSWso8pUUD066+/YvTo0Th8+DAqVqyI4OBgBAcHAwDS09PRqFEjrF69Gh06dCiXyhLR0yeXydDK100aJuvEdUhkWonCsKVLl2LYsGGoWLFivnGurq4YMWIEFi9eXGaVy83NxQcffABfX184OjqiZs2amD17tlHHbSEEIiMj4e3tDUdHR3Tq1Alnz541KicnJwdjxoyBh4cHnJ2d0bNnT1y/fr3M6kn0LHNQKrBtRBtsG9EGDkqFuatDpcR1SGRaiQKi33//HV27di10fEhICBITE5+4UgYLFizA6tWrsXz5cpw/fx4LFy7Exx9/bNRPaeHChVi8eDGWL1+OY8eOQa1WIzg4GPfv35fyREREICYmBlu3bsXBgwfx4MEDhIWFQafTlVldiYiIyHqV6JLZ7du3C7zdXirMzg7//PPPE1fK4NChQ+jVqxe6d+8OAKhRowa++uorHD9+HEDe2aGlS5di2rRp6NOnDwBg48aN8PT0RHR0NEaMGIH09HSsX78emzZtkvo4bd68GT4+PtizZw9CQ0PLrL5ERERknUoUED3//PM4ffo0ateuXeD4U6dOwcur7J4l0L59e6xevRoXL15E3bp18fvvv+PgwYNYunQpACA5ORkpKSlGD4NUqVQIDAxEQkICRowYgcTERGi1WqM83t7e8PPzQ0JCQqEBUU5ODnJycqTfGRkZAACtVgutVlsmy6fV5j4yrIVWJsqkXHMwtElZtQ0ZM2f7Zmly0emTAwCA+Akd4GT/325DpRDQarXS96NMjXvS6UtSdnHna/hWycu+7LJeppJOby8XaDZ7N4D86/Bp4T6ifFlz+5b3sbC4bSITQhR7zmPGjEF8fDyOHTsGBwcHo3HZ2dlo2bIlgoKC8Nlnn5WstoUQQmDq1KlYsGABFAoFdDod5s6diylTpgAAEhIS0K5dO9y4cQPe3t7SdMOHD8eVK1ewa9cuREdHY8iQIUbBDZB3ec/X1xdr1qwpcN6RkZGYNWtWvvTo6Gg4OTmVyfLl6IBJR/N2TAtb5kLFS/tkgbidWj+uQ7Jk5b19ZmVlYcCAAUhPTy+wD7RBif5N+OCDD/Ddd9+hbt26GD16NOrVqweZTIbz589jxYoV0Ol0mDZt2hNX3mDbtm3YvHkzoqOj0ahRIyQlJSEiIgLe3t4YNGiQlE/22F0TQoh8aY8rKs+UKVMwfvx46XdGRgZ8fHwQEhJiskFLQqvTQ//8LQBAWBMvKK34VlitVou4uDgEBwebvKxKpWPO9s3S5GLS0V8BAKGhIUZnF/wid+FMZKj0/ShT4550+pKUXdz5Gtp4+nE5Emd0LdOyy3qZSjp9o5m7pOHH1+HTwn1E+bLm9i3vY6HhCk9RSvRX4enpiYSEBLzzzjuYMmUKDCeXZDIZQkNDsXLlSnh6epa8toV47733MHnyZLz22msAAH9/f1y5cgXz58/HoEGDoFarAQApKSlGl+pSU1OleqjVamg0GqSlpaFy5cpGedq2bVvovFUqFVQqVb50pVJZZhubUgm81qpGmZRlKcqyfSg/c7SvUvz3j0Pe/P/bbeToZFAqldL3o0yNe9LpS1J2ieerL8eyzdReGn3h6/Bp4z6ifFlj+5b3sbC47VHiMKx69erYuXMn7ty5gyNHjuDw4cO4c+cOdu7ciRo1apS0OJOysrIglxtXUaFQSLfd+/r6Qq1WIy4uThqv0Wiwb98+KdgJCAiAUqk0ynPr1i2cOXPGZEBEREREtqPU/yZUrlwZL7zwQlnWJZ8ePXpg7ty5qFatGho1aoSTJ09i8eLFePPNNwHknZmKiIjAvHnzUKdOHdSpUwfz5s2Dk5MTBgwYACDv+UhDhw7FhAkT4O7uDjc3N0ycOBH+/v5mf7J2rk6P/X/m3ZXXsc5zfHosERHZHEs5FprvvGkxLFu2DNOnT8fIkSORmpoKb29vjBgxAjNmzJDyTJo0CdnZ2Rg5ciTS0tLQqlUr7N69Gy4uLlKeJUuWwM7ODv369UN2djY6d+6MqKgoKBTm7Vmo0enxZlTeIwT4OH0iIrJFlnIstOiAyMXFBUuXLpVusy+ITCZDZGQkIiMjC83j4OCAZcuW8cWzREREVCCekiAiIiKbx4CIiIiIbB4DIiIiIrJ5DIiIiIjI5jEgIiIiIptn0XeZPeuUCjlm92okDRNZIsN2OuP7s9xOrRj3NWSpLOVYyIDIjJQKOd5oU8Pc1SAyybCdMiCybtzXkKWylGMh925ERERk83iGyIx0eoGjyfcAAC193aCQy4qYgujpe3Q71ekFt1MrdejSXQDc15DlsZRjIc8QmVFOrg791x1G/3WHkZOrM3d1iApk2E4Nw2SduK8hS2Upx0IGRERkkgwy1KlSQRom61SnSgXUqVKB65CoELxkRkQmOdorEDc+EDUm74CjvXlfiEylFzc+0NxVILJoPENERERENo8BEREREdk8BkREZFK2RofgxfukYbJOwYv3IXjxPq5DokKwDxERmSQg8GfqA2mYrBPXIZFpDIjMyE4ux5Ru9aVhIiIiW2Mpx0IGRGZkbyfHiMBa5q4GERGR2VjKsZCnJYiIiMjm8QyRGen0AmdupAMA/J535eP0iYjI5ljKsZBniMwoJ1eHXit+Q68Vv/Fx+kREZJMs5VjIgIiIiIhsHgMiIiIisnkMiIiIiMjmMSAiIiIim8eAiIiIiGweAyIiIiKyeXwOkRnZyeUY17mONExkiQzb6ae//Mnt1IpxX0OWylKOhQyIzMjeTo53g+uauxpEJhm2009/+RP2djyYWivua8hSWcqxkHs3IiIisnkWHxDduHEDr7/+Otzd3eHk5ISmTZsiMTFRGi+EQGRkJLy9veHo6IhOnTrh7NmzRmXk5ORgzJgx8PDwgLOzM3r27Inr168/7UXJR68XuHj7Pi7evg+9Xpi7OkQFMmynhmGyTtzXkKWylGOhRQdEaWlpaNeuHZRKJX7++WecO3cOn3zyCSpVqiTlWbhwIRYvXozly5fj2LFjUKvVCA4Oxv3796U8ERERiImJwdatW3Hw4EE8ePAAYWFh0OnM+7qMh7k6hCzZj5Al+/GQr+4gC2XYTg3DZJ24ryFLZSnHQovuQ7RgwQL4+Phgw4YNUlqNGjWkYSEEli5dimnTpqFPnz4AgI0bN8LT0xPR0dEYMWIE0tPTsX79emzatAldunQBAGzevBk+Pj7Ys2cPQkNDn+oyEVkjN2d73MvUmLsa9ATcnO3NXQUii2bRZ4h++OEHtGjRAn379kWVKlXQrFkzrFu3ThqfnJyMlJQUhISESGkqlQqBgYFISEgAACQmJkKr1Rrl8fb2hp+fn5SHiArnZG+HE9ODpWGyTiemB+PE9GCuQ6JCWPRfxt9//41Vq1Zh/PjxmDp1Ko4ePYqxY8dCpVLhjTfeQEpKCgDA09PTaDpPT09cuXIFAJCSkgJ7e3tUrlw5Xx7D9AXJyclBTk6O9DsjIwMAoNVqodVqy2T5tNrcR4a10Mqs99q+oU3Kqm3ImCW0r0oh8s3fkFbScU86fUnKLu58Dd8qedmXXdbLVNqyzckStuFnmTW3b3kfC4vbJjIhhMUehe3t7dGiRQujMzljx47FsWPHcOjQISQkJKBdu3a4efMmvLy8pDzDhg3DtWvXEBsbi+joaAwZMsQouAGA4OBg1KpVC6tXry5w3pGRkZg1a1a+9OjoaDg5OZXJ8uXogElH82LShS1zoVKUSbFERERWo7yPhVlZWRgwYADS09NRsWLFQvNZ9BkiLy8vNGzY0CitQYMG+PbbbwEAarUaQN5ZoEcDotTUVOmskVqthkajQVpamtFZotTUVLRt27bQeU+ZMgXjx4+XfmdkZMDHxwchISEmG7QksjS5mHT0VwBAaGiIVZ/K1mq1iIuLQ3BwMJRKpbmr88wxZ/s+1Oow9MsTOHb5Hk7N6AIH5X97K7/IXTgTGSp9P8rUuCedviRlF3e+hjaeflyOxBldy7Tssl6mkk7faOYuNK3mBgBY/0Zzo3X4tHAfUb6suX3L+1houMJTFIs+Ardr1w4XLlwwSrt48SKqV68OAPD19YVarUZcXByaNWsGANBoNNi3bx8WLFgAAAgICIBSqURcXBz69esHALh16xbOnDmDhQsXFjpvlUoFlUqVL12pVJbZxqYUssfKtejVUSxl2T6UnznaVytkOHo5DYAMCjs7o+00RyeDUqmUvh9latyTTl+Ssks8X305lm2m9tLoDesQ+dbh08Z9RPmyxvYt72NhcdvDoo/A7777Ltq2bYt58+ahX79+OHr0KNauXYu1a9cCAGQyGSIiIjBv3jzUqVMHderUwbx58+Dk5IQBAwYAAFxdXTF06FBMmDAB7u7ucHNzw8SJE+Hv7y/ddWYudnI5hnesKQ0TERHZGks5Flp0QPTCCy8gJiYGU6ZMwezZs+Hr64ulS5di4MCBUp5JkyYhOzsbI0eORFpaGlq1aoXdu3fDxcVFyrNkyRLY2dmhX79+yM7ORufOnREVFQWFwrydduzt5Jj6UgOz1oGIiMicLOVYaNEBEQCEhYUhLCys0PEymQyRkZGIjIwsNI+DgwOWLVuGZcuWlUMNiYiIyNpZfED0LNPrBW78mw0AeL6SI+RyWRFTEBERPVss5VjIjitm9DBXhw4L96LDwr18nD4REdkkSzkWMiAiIiIim8eAiIiIiGweAyIiIiKyeQyIiIiIyOYxICIiIiKbx4CIiIiIbB6fQ2RGCrkM4a2rS8NElsiwnW46fIXbqRXjvoYslaUcCxkQmZHKToEPe/uZuxpEJhm2002Hr0BlZ97X3VDpcV9DlspSjoW8ZEZEREQ2j2eIzEgIgXuZGgCAm7M9ZDKeyibL8+h2KoTgdmql7j7IAcB9DVkeSzkW8gyRGWVrdQiYswcBc/YgW8tXd5BlMmynhmGyTtzXkKWylGMhAyIiIiKyebxkRkQmOdnb4fJH3VFj8g442XOXYa0uf9Td3FUgsmg8Q0REREQ2jwERERER2TwGRERk0kOtDiO3JErDZJ1GbknEyC2JXIdEhWBAREQm6YXAztMp0jBZp52nU7DzdArXIVEh2EPSjBRyGf7XvKo0TEREZGss5VjIgMiMVHYKfNKvibmrQUREZDaWcizkJTMiIiKyeTxDZEZCCOmpnI5KBR+nT0RENsdSjoU8Q2RG2VodGs7YhYYzdvFx+kREZJMs5VjIgIiIiIhsHgMiIiIisnkMiIiIiMjmMSAiIiIim8eAiIiIiGweAyIiIiKyeXwOkRnJZTK85K+WhokskWE73Xk6hdupFeO+hiyVpRwLreoM0fz58yGTyRARESGlCSEQGRkJb29vODo6olOnTjh79qzRdDk5ORgzZgw8PDzg7OyMnj174vr160+59vk5KBVYOTAAKwcGwEGpMHd1iApk2E4Nw2SduK8hS2Upx0KrCYiOHTuGtWvXonHjxkbpCxcuxOLFi7F8+XIcO3YMarUawcHBuH//vpQnIiICMTEx2Lp1Kw4ePIgHDx4gLCwMOh0fhkhERERWEhA9ePAAAwcOxLp161C5cmUpXQiBpUuXYtq0aejTpw/8/PywceNGZGVlITo6GgCQnp6O9evX45NPPkGXLl3QrFkzbN68GadPn8aePXvMtUhERERkQayiD9GoUaPQvXt3dOnSBXPmzJHSk5OTkZKSgpCQEClNpVIhMDAQCQkJGDFiBBITE6HVao3yeHt7w8/PDwkJCQgNDS1wnjk5OcjJyZF+Z2RkAAC0Wi20Wm2ZLFeWJhdNPvwVAPD79BfhZG8Vq6NAhjYpq7YhY+Zs30e30/TMbKPtVKUQ0Gq10vejTI170ulLUnZx52v4VsnLvuyyXqaSTm8vF6gxeQcA8+1ruI8oX9bcvuV9LCxum8iEEKJM51zGtm7dirlz5+LYsWNwcHBAp06d0LRpUyxduhQJCQlo164dbty4AW9vb2ma4cOH48qVK9i1axeio6MxZMgQo+AGAEJCQuDr64s1a9YUON/IyEjMmjUrX3p0dDScnJzKZNlydMCko3krfmHLXKh4aZ8sELdT68d1SJasvLfPrKwsDBgwAOnp6ahYsWKh+Sz6lMS1a9cwbtw47N69Gw4ODoXme/zNuEKIIt+WW1SeKVOmYPz48dLvjIwM+Pj4ICQkxGSDlkSWJheTjuZFxaGhIVZ/higuLg7BwcFQKpXmrs4zx5ztK4RAxxe16LhwL3qHdTP6u/GL3IUzkaHS96NMjXvS6UtSdnHna2jj6cflSJzRtUzLLutlKun0jWbuwuHJnQAAbk5Ks7xNnPuI8mXN7Vvex0LDFZ6iWPQRODExEampqQgICJDSdDod9u/fj+XLl+PChQsAgJSUFHh5eUl5UlNT4enpCQBQq9XQaDRIS0sz6n+UmpqKtm3bFjpvlUoFlUqVL12pVJbZxqYU/+2U8sq16NVRLGXZPpSfudpXbW8PjV4Ge3t7o/QcnQxKpVL6Lu64J52+JGWXeL76cizbTO2l0cugruRcYJlPG/cR5csa27e8j4XFbQ+L7lTduXNnnD59GklJSdKnRYsWGDhwIJKSklCzZk2o1WrExcVJ02g0Guzbt08KdgICAqBUKo3y3Lp1C2fOnDEZEBEREZHtsOhTEi4uLvDz8zNKc3Z2hru7u5QeERGBefPmoU6dOqhTpw7mzZsHJycnDBgwAADg6uqKoUOHYsKECXB3d4ebmxsmTpwIf39/dOnS5akvE5G1ycnVYc5P56VhlR07oFij6dvPAAA+CGvAdUhUAIsOiIpj0qRJyM7OxsiRI5GWloZWrVph9+7dcHFxkfIsWbIEdnZ26NevH7Kzs9G5c2dERUVBoeBOgagoOr3ApsNXpGGyToZ1OOWl+mauCZFlsrqAKD4+3ui3TCZDZGQkIiMjC53GwcEBy5Ytw7Jly8q3ciUkl8kQVO85aZiIiMjWWMqx0OoComeJg1KBDUNamrsaREREZmMpx0KL7lRNRERE9DQwICIiIiKbx4DIjLI0uWgwPRYNpsciS5Nr7uoQERE9dZZyLGQfIjPL1urMXQUiIiKzsoRjIc8QERERkc1jQEREREQ2jwERERER2TwGRERERGTzGBARERGRzeNdZmYkl8nQytdNGiayRIbt9EjyPW6nVoz7GrJUlnIsZEBkRg5KBbaNaGPuahCZZNhOa0zeAQclX4hsrbivIUtlKcdCXjIjIiIim8eAiIiIiGweAyIzytLkovmHcWj+YRxf3UEWy7CdGobJOnFfQ5bKUo6F7ENkZvcyNeauAlGRuJ1aP65DsmSWsH3yDBERmeRgp8DudztKw2Sddr/bEbvf7ch1SFQIniEiIpPkchnqerpIw2SdDOuQiArGM0RERERk83iGiIhM0uTqsWLvX9KwvR3/j7JGS+IuAgBGBdXmOiQqAAMiIjIpV6/Hp7/8KQ3b88SyVTKswxGBNbkOiQrAgMiM5DIZGld1lYaJiIhsjaUcCxkQmZGDUoEfRrc3dzWIiIjMxlKOhTxvSkRERDaPARERERHZPAZEZpSt0aHdR7+i3Ue/IlujM3d1iIiInjpLORayD5EZCQjc+DdbGiYiIrI1lnIs5BkiIiIisnkMiIiIiMjmMSAiIiIim8eAiIiIiGyeRQdE8+fPxwsvvAAXFxdUqVIFvXv3xoULF4zyCCEQGRkJb29vODo6olOnTjh79qxRnpycHIwZMwYeHh5wdnZGz549cf369ae5KERERGTBLDog2rdvH0aNGoXDhw8jLi4Oubm5CAkJQWZmppRn4cKFWLx4MZYvX45jx45BrVYjODgY9+/fl/JEREQgJiYGW7duxcGDB/HgwQOEhYVBpzPvre4yyFCnSgXUqVIBMvDVHWSZDNupYZisE/c1ZKks5Vho0bfdx8bGGv3esGEDqlSpgsTERHTs2BFCCCxduhTTpk1Dnz59AAAbN26Ep6cnoqOjMWLECKSnp2P9+vXYtGkTunTpAgDYvHkzfHx8sGfPHoSGhj715TJwtFcgbnyg2eZPVByG7bTG5B1wtFeYuzpUStzXkKWylGOhRQdEj0tPTwcAuLm5AQCSk5ORkpKCkJAQKY9KpUJgYCASEhIwYsQIJCYmQqvVGuXx9vaGn58fEhISCg2IcnJykJOTI/3OyMgAAGi1Wmi12jJfNmtnaBO2TfmwhPZVKUS++RvSSjruSacvSdnFna/hWyUv+7LLeplKW7Y5WcI2/Cxj+xauuG0iE0JYxRMBhRDo1asX0tLScODAAQBAQkIC2rVrhxs3bsDb21vKO3z4cFy5cgW7du1CdHQ0hgwZYhTcAEBISAh8fX2xZs2aAucXGRmJWbNm5UuPjo6Gk5NTGS4ZERERlZesrCwMGDAA6enpqFixYqH5rOYM0ejRo3Hq1CkcPHgw3ziZzPiaoxAiX9rjisozZcoUjB8/XvqdkZEBHx8fhISEmGzQksjW6NBn9WEAwHdvt7bqyxFarRZxcXEIDg6GUqk0d3WeOeZsX8N2eumfB/h9ehej7dQvchfORIZK348yNe5Jpy9J2cWdr6GNpx+XI3FG1zItu6yXqaTTN5q5C9Xc8/qBmWtfw31E+bLm9i3vY6HhCk9RrCIgGjNmDH744Qfs378fVatWldLVajUAICUlBV5eXlJ6amoqPD09pTwajQZpaWmoXLmyUZ62bdsWOk+VSgWVSpUvXalUltnGphUy/PVPXgdxO6UdlEqrWB0mlWX7UH7maN//tlNZvu00RyeDUqmUvh9latyTTl+Ssks8X305lm2m9tLoLWdfw31E+bLG9i3vY2Fx28Oi7zITQmD06NH47rvv8Ouvv8LX19dovK+vL9RqNeLi4qQ0jUaDffv2ScFOQEAAlEqlUZ5bt27hzJkzJgMiIsqjslPgq2GtpWGyTl8Na42vhrXmOiQqhEWfkhg1ahSio6Px/fffw8XFBSkpKQAAV1dXODo6QiaTISIiAvPmzUOdOnVQp04dzJs3D05OThgwYICUd+jQoZgwYQLc3d3h5uaGiRMnwt/fX7rrjIgKp5DL0KaWuzRM1smwDomoYBYdEK1atQoA0KlTJ6P0DRs2YPDgwQCASZMmITs7GyNHjkRaWhpatWqF3bt3w8XFRcq/ZMkS2NnZoV+/fsjOzkbnzp0RFRUFhYL/KREREZGFB0TFuQFOJpMhMjISkZGRheZxcHDAsmXLsGzZsjKsHZFt0Or0+OroVWlYqbDoK+1UiC8PXQYA9G9ZjeuQqAAWHRARkflpdXrM+P6sNMyDqXUyrMNXAqpyHRIVgAGRGckgw/OVHKVhIiIiW2Mpx0IGRGbkaK/Ab5NfNHc1iIiIzMZSjoU8b0pEREQ2jwERERER2TwGRGb0UKtDz+UH0XP5QTzU6sxdHSIioqfOUo6F7ENkRnohcOp6ujRMRERkayzlWMgzRERERGTzGBARERGRzWNARERERDaPARERERHZPAZEREREZPN4l5mZuTnbm7sKREVyc7bHvUyNuatBT4D7GrJklrB9MiAyIyd7O5yYHmzuahCZZNhOa0zeASd77jKsFfc1ZKks5VjIS2ZERERk8xgQERERkc1jQGRGD7U6vLrmEF5dc4iv7iCLZdhODcNknbivIUtlKcdCdggwI70QOJJ8TxomskTcTp8NXIdkqSxlH8MzRERkkr1CjhUDmkvDZJ1WDGiOFQOacx0SFYJ/GURkkp1Cju6NvaRhsk7dG3uhe2MvrkOiQvAvg4iIiGwe+xARkUm5Oj12nb0tDfMMg3XaceoWACC0kSfXIVEBGBARkUkanR6jok9IwzyYWifDOjw3O5TrkKgADIjMzFGpMHcViIiIzMoSjoUMiMzIyd4O5z/sau5qEBERmY2lHAt53pSIiIhsHgMiIiIisnkMiMzooVaHIRuOYsiGo3ycPhER2SRLORayD5EZ6YXA3gv/SMNERES2xlKOhTxDRERERDbPpgKilStXwtfXFw4ODggICMCBAwfMXSUiIiKyADYTEG3btg0RERGYNm0aTp48iQ4dOqBbt264evWquatGREREZmYzAdHixYsxdOhQvPXWW2jQoAGWLl0KHx8frFq1ytxVIyIiIjOziYBIo9EgMTERISEhRukhISFISEgwU62IiIjIUtjEXWZ37tyBTqeDp6enUbqnpydSUlIKnCYnJwc5OTnS7/T0dADAvXv3oNVqy6ReWZpc6HOyAAB3795Ftr31rg6tVousrCzcvXsXSqXS3NV55pizfU1tp3a5mbh79670/ShT4550+pKUXdz5GtrYTisv87LLeplKOr1CmwmtXgbAfPsa7iPKlzW3b3kfC+/fvw8AEEXdwSZswI0bNwQAkZCQYJQ+Z84cUa9evQKnmTlzpgDADz/88MMPP/w8A59r166ZjBWs95RECXh4eEChUOQ7G5SamprvrJHBlClTMH78eOm3Xq/HvXv34O7uDplMVq71tUYZGRnw8fHBtWvXULFiRXNX55nD9i1/bOPyxfYtX2zfwgkhcP/+fXh7e5vMZxMBkb29PQICAhAXF4eXX35ZSo+Li0OvXr0KnEalUkGlUhmlVapUqTyr+UyoWLEi/xjLEdu3/LGNyxfbt3yxfQvm6upaZB6bCIgAYPz48QgPD0eLFi3Qpk0brF27FlevXsXbb79t7qoRERGRmdlMQPTqq6/i7t27mD17Nm7dugU/Pz/s3LkT1atXN3fViIiIyMxsJiACgJEjR2LkyJHmrsYzSaVSYebMmfkuM1LZYPuWP7Zx+WL7li+275OTCcG3ihIREZFts4kHMxIRERGZwoCIiIiIbB4DIiIiIrJ5DIiIiIjI5jEgohKZO3cu2rZtCycnp0IfVHn16lX06NEDzs7O8PDwwNixY6HRaIzynD59GoGBgXB0dMTzzz+P2bNnF/2eGRtVo0YNyGQyo8/kyZON8hSnzalwK1euhK+vLxwcHBAQEIADBw6Yu0pWKTIyMt+2qlarpfFCCERGRsLb2xuOjo7o1KkTzp49a8YaW779+/ejR48e8Pb2hkwmw/bt243GF6dNc3JyMGbMGHh4eMDZ2Rk9e/bE9evXn+JSWAcGRFQiGo0Gffv2xTvvvFPgeJ1Oh+7duyMzMxMHDx7E1q1b8e2332LChAlSnoyMDAQHB8Pb2xvHjh3DsmXLsGjRIixevPhpLYbVMTw/y/D54IMPpHHFaXMq3LZt2xAREYFp06bh5MmT6NChA7p164arV6+au2pWqVGjRkbb6unTp6VxCxcuxOLFi7F8+XIcO3YMarUawcHB0ss3Kb/MzEw0adIEy5cvL3B8cdo0IiICMTEx2Lp1Kw4ePIgHDx4gLCwMOp3uaS2GdSiDd6eSDdqwYYNwdXXNl75z504hl8vFjRs3pLSvvvpKqFQqkZ6eLoQQYuXKlcLV1VU8fPhQyjN//nzh7e0t9Hp9udfd2lSvXl0sWbKk0PHFaXMqXMuWLcXbb79tlFa/fn0xefJkM9XIes2cOVM0adKkwHF6vV6o1Wrx0UcfSWkPHz4Urq6uYvXq1U+phtYNgIiJiZF+F6dN//33X6FUKsXWrVulPDdu3BByuVzExsY+tbpbA54hojJ16NAh+Pn5Gb1ELzQ0FDk5OUhMTJTyBAYGGj1ALDQ0FDdv3sTly5efdpWtwoIFC+Du7o6mTZti7ty5RpfDitPmVDCNRoPExESEhIQYpYeEhCAhIcFMtbJuf/75J7y9veHr64vXXnsNf//9NwAgOTkZKSkpRm2tUqkQGBjIti6l4rRpYmIitFqtUR5vb2/4+fmx3R9jU0+qpvKXkpICT09Po7TKlSvD3t4eKSkpUp4aNWoY5TFMk5KSAl9f36dSV2sxbtw4NG/eHJUrV8bRo0cxZcoUJCcn4/PPPwdQvDangt25cwc6nS5f+3l6erLtSqFVq1b48ssvUbduXdy+fRtz5sxB27ZtcfbsWak9C2rrK1eumKO6Vq84bZqSkgJ7e3tUrlw5Xx5u48Z4hogK7Aj5+Of48ePFLk8mk+VLE0IYpT+eR/xfh+qCpn0WlaTN3333XQQGBqJx48Z46623sHr1aqxfvx53796VyitOm1PhCtoe2XYl161bN/zvf/+Dv78/unTpgh07dgAANm7cKOVhW5e90rQp2z0/niEijB49Gq+99prJPI+f0SmMWq3GkSNHjNLS0tKg1Wql/2LUanW+/0xSU1MB5P9P51n1JG3eunVrAMBff/0Fd3f3YrU5FczDwwMKhaLA7ZFt9+ScnZ3h7++PP//8E7179waQd8bCy8tLysO2Lj3DHXym2lStVkOj0SAtLc3oLFFqairatm37dCts4XiGiODh4YH69eub/Dg4OBSrrDZt2uDMmTO4deuWlLZ7926oVCoEBARIefbv32/UD2b37t3w9vYuduBl7Z6kzU+ePAkA0g6wOG1OBbO3t0dAQADi4uKM0uPi4niwKAM5OTk4f/48vLy84OvrC7VabdTWGo0G+/btY1uXUnHaNCAgAEql0ijPrVu3cObMGbb748zYoZus0JUrV8TJkyfFrFmzRIUKFcTJkyfFyZMnxf3794UQQuTm5go/Pz/RuXNnceLECbFnzx5RtWpVMXr0aKmMf//9V3h6eor+/fuL06dPi++++05UrFhRLFq0yFyLZbESEhLE4sWLxcmTJ8Xff/8ttm3bJry9vUXPnj2lPMVpcyrc1q1bhVKpFOvXrxfnzp0TERERwtnZWVy+fNncVbM6EyZMEPHx8eLvv/8Whw8fFmFhYcLFxUVqy48++ki4urqK7777Tpw+fVr0799feHl5iYyMDDPX3HLdv39f2s8CkPYHV65cEUIUr03ffvttUbVqVbFnzx5x4sQJ8eKLL4omTZqI3Nxccy2WRWJARCUyaNAgASDfZ+/evVKeK1euiO7duwtHR0fh5uYmRo8ebXSLvRBCnDp1SnTo0EGoVCqhVqtFZGQkb7kvQGJiomjVqpVwdXUVDg4Ool69emLmzJkiMzPTKF9x2pwKt2LFClG9enVhb28vmjdvLvbt22fuKlmlV199VXh5eQmlUim8vb1Fnz59xNmzZ6Xxer1ezJw5U6jVaqFSqUTHjh3F6dOnzVhjy7d3794C97mDBg0SQhSvTbOzs8Xo0aOFm5ubcHR0FGFhYeLq1atmWBrLJhOCjwcmIiIi28Y+RERERGTzGBARERGRzWNARERERDaPARERERHZPAZEREREZPMYEBEREZHNY0BERERENo8BERFZhaioKFSqVKlE0wwePFh6h5a5Xb58GTKZDElJSeauChEVgAEREZWp1atXw8XFBbm5uVLagwcPoFQq0aFDB6O8Bw4cgEwmw8WLF4ss99VXXy1WvpKqUaMGli5dWublEpF1YUBERGUqKCgIDx48wPHjx6W0AwcOQK1W49ixY8jKypLS4+Pj4e3tjbp16xZZrqOjI6pUqVIudSYiYkBERGWqXr168Pb2Rnx8vJQWHx+PXr16oVatWkhISDBKDwoKApD3lu5Jkybh+eefh7OzM1q1amVURkGXzObMmYMqVarAxcUFb731FiZPnoymTZvmq9OiRYvg5eUFd3d3jBo1ClqtFgDQqVMnXLlyBe+++y5kMhlkMlmBy9S/f3+89tprRmlarRYeHh7YsGEDACA2Nhbt27dHpUqV4O7ujrCwMFy6dKnQdipoebZv356vDj/++CMCAgLg4OCAmjVrYtasWUZn34iobDAgIqIy16lTJ+zdu1f6vXfvXnTq1AmBgYFSukajwaFDh6SAaMiQIfjtt9+wdetWnDp1Cn379kXXrl3x559/FjiPLVu2YO7cuViwYAESExNRrVo1rFq1Kl++vXv34tKlS9i7dy82btyIqKgoREVFAQC+++47VK1aFbNnz8atW7dw69atAuc1cOBA/PDDD3jw4IGUtmvXLmRmZuJ///sfACAzMxPjx4/HsWPH8Msvv0Aul+Pll1+GXq8veQM+Mo/XX38dY8eOxblz57BmzRpERUVh7ty5pS6TiAph7rfLEtGzZ+3atcLZ2VlotVqRkZEh7OzsxO3bt8XWrVtF27ZthRBC7Nu3TwAQly5dEn/99ZeQyWTixo0bRuV07txZTJkyRQghxIYNG4Srq6s0rlWrVmLUqFFG+du1ayeaNGki/R40aJCoXr26yM3NldL69u0rXn31Vel39erVxZIlS0wuj0ajER4eHuLLL7+U0vr37y/69u1b6DSpqakCgPTm8eTkZAFAnDx5ssDlEUKImJgY8ehuuUOHDmLevHlGeTZt2iS8vLxM1peISo5niIiozAUFBSEzMxPHjh3DgQMHULduXVSpUgWBgYE4duwYMjMzER8fj2rVqqFmzZo4ceIEhBCoW7cuKlSoIH327dtX6GWnCxcuoGXLlkZpj/8GgEaNGkGhUEi/vby8kJqaWqLlUSqV6Nu3L7Zs2QIg72zQ999/j4EDB0p5Ll26hAEDBqBmzZqoWLEifH19AQBXr14t0bwelZiYiNmzZxu1ybBhw3Dr1i2jvlhE9OTszF0BInr21K5dG1WrVsXevXuRlpaGwMBAAIBarYavry9+++037N27Fy+++CIAQK/XQ6FQIDEx0Sh4AYAKFSoUOp/H+9sIIfLlUSqV+aYpzWWsgQMHIjAwEKmpqYiLi4ODgwO6desmje/Rowd8fHywbt06eHt7Q6/Xw8/PDxqNpsDy5HJ5vvoa+jYZ6PV6zJo1C3369Mk3vYODQ4mXgYgKx4CIiMpFUFAQ4uPjkZaWhvfee09KDwwMxK5du3D48GEMGTIEANCsWTPodDqkpqbmuzW/MPXq1cPRo0cRHh4upT16Z1tx2dvbQ6fTFZmvbdu28PHxwbZt2/Dzzz+jb9++sLe3BwDcvXsX58+fx5o1a6T6Hzx40GR5zz33HO7fv4/MzEw4OzsDQL5nFDVv3hwXLlxA7dq1S7xcRFQyDIiIqFwEBQVJd3QZzhABeQHRO++8g4cPH0odquvWrYuBAwfijTfewCeffIJmzZrhzp07+PXXX+Hv74+XXnopX/ljxozBsGHD0KJFC7Rt2xbbtm3DqVOnULNmzRLVs0aNGti/fz9ee+01qFQqeHh4FJhPJpNhwIABWL16NS5evGjUabxy5cpwd3fH2rVr4eXlhatXr2Ly5Mkm59uqVSs4OTlh6tSpGDNmDI4ePSp19jaYMWMGwsLC4OPjg759+0Iul+PUqVM4ffo05syZU6LlJCLT2IeIiMpFUFAQsrOzUbt2bXh6ekrpgYGBuH//PmrVqgUfHx8pfcOGDXjjjTcwYcIE1KtXDz179sSRI0eM8jxq4MCBmDJlCiZOnIjmzZsjOTkZgwcPLvGlpNmzZ+Py5cuoVasWnnvuOZN5Bw4ciHPnzuH5559Hu3btpHS5XI6tW7ciMTERfn5+ePfdd/Hxxx+bLMvNzQ2bN2/Gzp074e/vj6+++gqRkZFGeUJDQ/HTTz8hLi4OL7zwAlq3bo3FixejevXqJVpGIiqaTBR00Z2IyAoFBwdDrVZj06ZN5q4KEVkZXjIjIquUlZWF1atXIzQ0FAqFAl999RX27NmDuLg4c1eNiKwQzxARkVXKzs5Gjx49cOLECeTk5KBevXr44IMPCrwji4ioKAyIiIiIyOaxUzURERHZPAZEREREZPMYEBEREZHNY0BERERENo8BEREREdk8BkRERERk8xgQERERkc1jQEREREQ2jwERERER2bz/D7BiLy6KIP26AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 16, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 2, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.0625, 0.03125])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.0625, 0.03125])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.0625, 0.03125])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 206.0\n",
      "lif layer 1 self.abs_max_v: 206.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 121.0\n",
      "lif layer 2 self.abs_max_v: 121.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 24.0\n",
      "fc layer 1 self.abs_max_out: 626.0\n",
      "lif layer 1 self.abs_max_v: 638.5\n",
      "fc layer 2 self.abs_max_out: 166.0\n",
      "lif layer 2 self.abs_max_v: 166.0\n",
      "fc layer 1 self.abs_max_out: 812.0\n",
      "lif layer 1 self.abs_max_v: 1015.0\n",
      "lif layer 2 self.abs_max_v: 167.0\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "fc layer 1 self.abs_max_out: 858.0\n",
      "lif layer 1 self.abs_max_v: 1257.5\n",
      "lif layer 2 self.abs_max_v: 195.5\n",
      "fc layer 3 self.abs_max_out: 38.0\n",
      "lif layer 1 self.abs_max_v: 1424.0\n",
      "lif layer 2 self.abs_max_v: 244.0\n",
      "fc layer 3 self.abs_max_out: 44.0\n",
      "fc layer 2 self.abs_max_out: 233.0\n",
      "lif layer 2 self.abs_max_v: 295.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "layer   1  Sparsity: 74.8535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 880.0\n",
      "fc layer 2 self.abs_max_out: 234.0\n",
      "lif layer 2 self.abs_max_v: 328.5\n",
      "fc layer 2 self.abs_max_out: 261.0\n",
      "lif layer 2 self.abs_max_v: 409.5\n",
      "fc layer 3 self.abs_max_out: 58.0\n",
      "fc layer 1 self.abs_max_out: 885.0\n",
      "fc layer 2 self.abs_max_out: 274.0\n",
      "fc layer 2 self.abs_max_out: 288.0\n",
      "lif layer 2 self.abs_max_v: 461.0\n",
      "fc layer 1 self.abs_max_out: 1102.0\n",
      "lif layer 2 self.abs_max_v: 468.5\n",
      "lif layer 2 self.abs_max_v: 491.5\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "lif layer 1 self.abs_max_v: 1446.0\n",
      "lif layer 2 self.abs_max_v: 495.0\n",
      "fc layer 2 self.abs_max_out: 360.0\n",
      "fc layer 3 self.abs_max_out: 75.0\n",
      "fc layer 2 self.abs_max_out: 367.0\n",
      "fc layer 2 self.abs_max_out: 380.0\n",
      "lif layer 2 self.abs_max_v: 508.5\n",
      "lif layer 2 self.abs_max_v: 553.5\n",
      "lif layer 2 self.abs_max_v: 577.0\n",
      "fc layer 2 self.abs_max_out: 386.0\n",
      "fc layer 2 self.abs_max_out: 400.0\n",
      "fc layer 2 self.abs_max_out: 427.0\n",
      "lif layer 1 self.abs_max_v: 1486.5\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "fc layer 3 self.abs_max_out: 84.0\n",
      "lif layer 2 self.abs_max_v: 591.0\n",
      "fc layer 2 self.abs_max_out: 459.0\n",
      "lif layer 1 self.abs_max_v: 1534.0\n",
      "lif layer 2 self.abs_max_v: 621.5\n",
      "lif layer 2 self.abs_max_v: 635.0\n",
      "fc layer 2 self.abs_max_out: 473.0\n",
      "lif layer 2 self.abs_max_v: 661.0\n",
      "lif layer 2 self.abs_max_v: 671.0\n",
      "fc layer 1 self.abs_max_out: 1174.0\n",
      "fc layer 2 self.abs_max_out: 482.0\n",
      "fc layer 2 self.abs_max_out: 539.0\n",
      "lif layer 2 self.abs_max_v: 710.0\n",
      "lif layer 2 self.abs_max_v: 717.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "lif layer 2 self.abs_max_v: 735.0\n",
      "lif layer 2 self.abs_max_v: 739.5\n",
      "lif layer 2 self.abs_max_v: 763.0\n",
      "lif layer 2 self.abs_max_v: 788.5\n",
      "lif layer 2 self.abs_max_v: 819.5\n",
      "lif layer 2 self.abs_max_v: 828.5\n",
      "lif layer 2 self.abs_max_v: 841.0\n",
      "lif layer 1 self.abs_max_v: 1597.5\n",
      "lif layer 2 self.abs_max_v: 911.5\n",
      "lif layer 2 self.abs_max_v: 931.0\n",
      "lif layer 2 self.abs_max_v: 953.5\n",
      "lif layer 1 self.abs_max_v: 1603.5\n",
      "lif layer 2 self.abs_max_v: 963.5\n",
      "lif layer 1 self.abs_max_v: 1640.5\n",
      "lif layer 2 self.abs_max_v: 971.0\n",
      "lif layer 2 self.abs_max_v: 1023.5\n",
      "fc layer 2 self.abs_max_out: 630.0\n",
      "fc layer 3 self.abs_max_out: 87.0\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "lif layer 2 self.abs_max_v: 1026.5\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 3 self.abs_max_out: 118.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "fc layer 2 self.abs_max_out: 667.0\n",
      "lif layer 2 self.abs_max_v: 1063.0\n",
      "lif layer 2 self.abs_max_v: 1073.0\n",
      "lif layer 2 self.abs_max_v: 1095.5\n",
      "lif layer 2 self.abs_max_v: 1108.0\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "lif layer 2 self.abs_max_v: 1177.5\n",
      "lif layer 2 self.abs_max_v: 1220.0\n",
      "lif layer 2 self.abs_max_v: 1227.5\n",
      "lif layer 2 self.abs_max_v: 1272.0\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "lif layer 2 self.abs_max_v: 1284.0\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "fc layer 2 self.abs_max_out: 715.0\n",
      "fc layer 2 self.abs_max_out: 722.0\n",
      "fc layer 2 self.abs_max_out: 745.0\n",
      "lif layer 2 self.abs_max_v: 1364.0\n",
      "fc layer 2 self.abs_max_out: 757.0\n",
      "lif layer 1 self.abs_max_v: 1655.0\n",
      "fc layer 1 self.abs_max_out: 1179.0\n",
      "lif layer 1 self.abs_max_v: 1661.5\n",
      "fc layer 1 self.abs_max_out: 1199.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "lif layer 2 self.abs_max_v: 1415.5\n",
      "lif layer 1 self.abs_max_v: 1769.5\n",
      "fc layer 2 self.abs_max_out: 822.0\n",
      "lif layer 2 self.abs_max_v: 1475.0\n",
      "fc layer 2 self.abs_max_out: 878.0\n",
      "lif layer 2 self.abs_max_v: 1488.5\n",
      "lif layer 2 self.abs_max_v: 1596.5\n",
      "fc layer 2 self.abs_max_out: 893.0\n",
      "fc layer 1 self.abs_max_out: 1204.0\n",
      "fc layer 2 self.abs_max_out: 917.0\n",
      "lif layer 2 self.abs_max_v: 1612.5\n",
      "lif layer 2 self.abs_max_v: 1624.0\n",
      "fc layer 1 self.abs_max_out: 1213.0\n",
      "lif layer 1 self.abs_max_v: 1857.0\n",
      "lif layer 1 self.abs_max_v: 1895.5\n",
      "fc layer 1 self.abs_max_out: 1341.0\n",
      "lif layer 2 self.abs_max_v: 1629.0\n",
      "lif layer 1 self.abs_max_v: 1938.0\n",
      "lif layer 1 self.abs_max_v: 1970.5\n",
      "lif layer 2 self.abs_max_v: 1655.0\n",
      "fc layer 2 self.abs_max_out: 936.0\n",
      "lif layer 2 self.abs_max_v: 1675.0\n",
      "fc layer 2 self.abs_max_out: 961.0\n",
      "fc layer 2 self.abs_max_out: 1012.0\n",
      "lif layer 2 self.abs_max_v: 1684.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 1789.5\n",
      "lif layer 2 self.abs_max_v: 1898.0\n",
      "fc layer 2 self.abs_max_out: 1041.0\n",
      "fc layer 1 self.abs_max_out: 1357.0\n",
      "lif layer 1 self.abs_max_v: 2001.5\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 1 self.abs_max_out: 1379.0\n",
      "lif layer 1 self.abs_max_v: 2039.5\n",
      "fc layer 2 self.abs_max_out: 1058.0\n",
      "fc layer 2 self.abs_max_out: 1143.0\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "fc layer 2 self.abs_max_out: 1199.0\n",
      "lif layer 2 self.abs_max_v: 1908.5\n",
      "lif layer 2 self.abs_max_v: 2038.5\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "fc layer 2 self.abs_max_out: 1270.0\n",
      "fc layer 1 self.abs_max_out: 1387.0\n",
      "lif layer 2 self.abs_max_v: 2042.0\n",
      "lif layer 2 self.abs_max_v: 2048.0\n",
      "lif layer 2 self.abs_max_v: 2051.5\n",
      "lif layer 2 self.abs_max_v: 2112.0\n",
      "lif layer 2 self.abs_max_v: 2160.0\n",
      "fc layer 2 self.abs_max_out: 1316.0\n",
      "lif layer 1 self.abs_max_v: 2128.0\n",
      "fc layer 2 self.abs_max_out: 1361.0\n",
      "lif layer 2 self.abs_max_v: 2183.0\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "fc layer 3 self.abs_max_out: 140.0\n",
      "fc layer 1 self.abs_max_out: 1446.0\n",
      "fc layer 1 self.abs_max_out: 1469.0\n",
      "fc layer 2 self.abs_max_out: 1411.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 2 self.abs_max_out: 1416.0\n",
      "lif layer 2 self.abs_max_v: 2239.0\n",
      "fc layer 2 self.abs_max_out: 1457.0\n",
      "fc layer 2 self.abs_max_out: 1504.0\n",
      "fc layer 2 self.abs_max_out: 1616.0\n",
      "lif layer 2 self.abs_max_v: 2275.5\n",
      "fc layer 1 self.abs_max_out: 1559.0\n",
      "lif layer 1 self.abs_max_v: 2145.5\n",
      "fc layer 3 self.abs_max_out: 157.0\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "fc layer 2 self.abs_max_out: 1652.0\n",
      "fc layer 2 self.abs_max_out: 1655.0\n",
      "fc layer 2 self.abs_max_out: 1656.0\n",
      "fc layer 2 self.abs_max_out: 1673.0\n",
      "lif layer 2 self.abs_max_v: 2285.5\n",
      "lif layer 2 self.abs_max_v: 2289.0\n",
      "lif layer 2 self.abs_max_v: 2321.0\n",
      "lif layer 2 self.abs_max_v: 2362.0\n",
      "lif layer 2 self.abs_max_v: 2492.0\n",
      "lif layer 2 self.abs_max_v: 2496.0\n",
      "lif layer 2 self.abs_max_v: 2511.0\n",
      "lif layer 2 self.abs_max_v: 2585.5\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 3 self.abs_max_out: 159.0\n",
      "lif layer 2 self.abs_max_v: 2641.5\n",
      "lif layer 2 self.abs_max_v: 2678.0\n",
      "lif layer 2 self.abs_max_v: 2794.5\n",
      "lif layer 1 self.abs_max_v: 2280.0\n",
      "lif layer 2 self.abs_max_v: 2806.0\n",
      "lif layer 2 self.abs_max_v: 3008.0\n",
      "lif layer 2 self.abs_max_v: 3030.0\n",
      "lif layer 2 self.abs_max_v: 3052.5\n",
      "lif layer 2 self.abs_max_v: 3136.0\n",
      "fc layer 1 self.abs_max_out: 1561.0\n",
      "fc layer 2 self.abs_max_out: 1685.0\n",
      "fc layer 2 self.abs_max_out: 1688.0\n",
      "fc layer 2 self.abs_max_out: 1689.0\n",
      "fc layer 2 self.abs_max_out: 1701.0\n",
      "lif layer 2 self.abs_max_v: 3153.0\n",
      "fc layer 1 self.abs_max_out: 1567.0\n",
      "lif layer 2 self.abs_max_v: 3157.5\n",
      "lif layer 1 self.abs_max_v: 2281.0\n",
      "fc layer 2 self.abs_max_out: 1739.0\n",
      "lif layer 2 self.abs_max_v: 3192.0\n",
      "fc layer 3 self.abs_max_out: 171.0\n",
      "fc layer 3 self.abs_max_out: 174.0\n",
      "fc layer 2 self.abs_max_out: 1836.0\n",
      "fc layer 1 self.abs_max_out: 1579.0\n",
      "lif layer 1 self.abs_max_v: 2304.5\n",
      "lif layer 2 self.abs_max_v: 3232.5\n",
      "lif layer 2 self.abs_max_v: 3234.5\n",
      "fc layer 1 self.abs_max_out: 1581.0\n",
      "lif layer 1 self.abs_max_v: 2310.0\n",
      "fc layer 1 self.abs_max_out: 1616.0\n",
      "lif layer 1 self.abs_max_v: 2343.0\n",
      "lif layer 1 self.abs_max_v: 2606.0\n",
      "lif layer 2 self.abs_max_v: 3271.0\n",
      "lif layer 2 self.abs_max_v: 3275.5\n",
      "fc layer 1 self.abs_max_out: 1670.0\n",
      "fc layer 1 self.abs_max_out: 1680.0\n",
      "fc layer 2 self.abs_max_out: 1856.0\n",
      "fc layer 3 self.abs_max_out: 195.0\n",
      "fc layer 2 self.abs_max_out: 1897.0\n",
      "fc layer 2 self.abs_max_out: 1960.0\n",
      "fc layer 2 self.abs_max_out: 1963.0\n",
      "lif layer 2 self.abs_max_v: 3461.5\n",
      "fc layer 1 self.abs_max_out: 1685.0\n",
      "fc layer 2 self.abs_max_out: 1998.0\n",
      "fc layer 3 self.abs_max_out: 197.0\n",
      "lif layer 2 self.abs_max_v: 3466.0\n",
      "fc layer 1 self.abs_max_out: 1711.0\n",
      "fc layer 2 self.abs_max_out: 2010.0\n",
      "fc layer 2 self.abs_max_out: 2049.0\n",
      "lif layer 2 self.abs_max_v: 3552.5\n",
      "lif layer 1 self.abs_max_v: 2614.5\n",
      "lif layer 2 self.abs_max_v: 3745.0\n",
      "lif layer 2 self.abs_max_v: 3801.5\n",
      "lif layer 2 self.abs_max_v: 3870.5\n",
      "fc layer 2 self.abs_max_out: 2141.0\n",
      "fc layer 1 self.abs_max_out: 1762.0\n",
      "lif layer 2 self.abs_max_v: 3917.0\n",
      "lif layer 2 self.abs_max_v: 3922.0\n",
      "fc layer 1 self.abs_max_out: 1764.0\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "fc layer 2 self.abs_max_out: 2209.0\n",
      "fc layer 1 self.abs_max_out: 1796.0\n",
      "fc layer 1 self.abs_max_out: 1821.0\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 409.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 447.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 482.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 552.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 557.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 616.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 715.00 at epoch 0, iter 4031\n",
      "fc layer 2 self.abs_max_out: 2217.0\n",
      "max_activation_accul updated: 731.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 742.00 at epoch 0, iter 4031\n",
      "fc layer 1 self.abs_max_out: 1972.0\n",
      "max_activation_accul updated: 754.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 780.00 at epoch 0, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 451 occurrences\n",
      "test - Value 1: 1 occurrences\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 51.604816/ 55.103504, val:  50.22%, val_best:  50.22%, tr:  89.61%, tr_best:  89.61%, epoch time: 249.29 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32256 real_backward_count 7695  23.856%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 78.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2274.0\n",
      "fc layer 2 self.abs_max_out: 2397.0\n",
      "fc layer 2 self.abs_max_out: 2419.0\n",
      "fc layer 2 self.abs_max_out: 2489.0\n",
      "fc layer 2 self.abs_max_out: 2507.0\n",
      "fc layer 2 self.abs_max_out: 2634.0\n",
      "fc layer 2 self.abs_max_out: 2640.0\n",
      "fc layer 3 self.abs_max_out: 199.0\n",
      "lif layer 2 self.abs_max_v: 3974.5\n",
      "lif layer 2 self.abs_max_v: 3976.5\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 3 self.abs_max_out: 203.0\n",
      "fc layer 2 self.abs_max_out: 2651.0\n",
      "fc layer 2 self.abs_max_out: 2834.0\n",
      "lif layer 2 self.abs_max_v: 4132.0\n",
      "lif layer 2 self.abs_max_v: 4138.5\n",
      "fc layer 2 self.abs_max_out: 2837.0\n",
      "lif layer 2 self.abs_max_v: 4173.5\n",
      "lif layer 2 self.abs_max_v: 4186.5\n",
      "lif layer 2 self.abs_max_v: 4241.0\n",
      "lif layer 2 self.abs_max_v: 4343.5\n",
      "lif layer 2 self.abs_max_v: 4498.0\n",
      "lif layer 2 self.abs_max_v: 4645.0\n",
      "lif layer 2 self.abs_max_v: 4665.5\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "fc layer 1 self.abs_max_out: 1973.0\n",
      "lif layer 1 self.abs_max_v: 2639.5\n",
      "lif layer 1 self.abs_max_v: 2675.0\n",
      "lif layer 2 self.abs_max_v: 4757.5\n",
      "lif layer 2 self.abs_max_v: 4874.0\n",
      "lif layer 2 self.abs_max_v: 4928.0\n",
      "lif layer 1 self.abs_max_v: 2680.0\n",
      "lif layer 1 self.abs_max_v: 2756.0\n",
      "lif layer 2 self.abs_max_v: 5033.0\n",
      "lif layer 2 self.abs_max_v: 5114.5\n",
      "fc layer 2 self.abs_max_out: 2906.0\n",
      "fc layer 2 self.abs_max_out: 3018.0\n",
      "fc layer 2 self.abs_max_out: 3058.0\n",
      "fc layer 1 self.abs_max_out: 2009.0\n",
      "lif layer 1 self.abs_max_v: 2764.5\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 795.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 907.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 983.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 1051.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 1058.00 at epoch 1, iter 4031\n",
      "fc layer 1 self.abs_max_out: 2034.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 59.935951/ 84.800301, val:  51.33%, val_best:  51.33%, tr:  90.80%, tr_best:  90.80%, epoch time: 249.96 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64512 real_backward_count 15135  23.461%\n",
      "layer   1  Sparsity: 86.3525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2035.0\n",
      "lif layer 1 self.abs_max_v: 2929.5\n",
      "lif layer 1 self.abs_max_v: 2958.0\n",
      "fc layer 2 self.abs_max_out: 3112.0\n",
      "fc layer 2 self.abs_max_out: 3129.0\n",
      "fc layer 2 self.abs_max_out: 3140.0\n",
      "fc layer 2 self.abs_max_out: 3166.0\n",
      "fc layer 2 self.abs_max_out: 3171.0\n",
      "fc layer 2 self.abs_max_out: 3204.0\n",
      "fc layer 2 self.abs_max_out: 3255.0\n",
      "fc layer 1 self.abs_max_out: 2062.0\n",
      "fc layer 2 self.abs_max_out: 3320.0\n",
      "fc layer 1 self.abs_max_out: 2095.0\n",
      "fc layer 1 self.abs_max_out: 2112.0\n",
      "fc layer 1 self.abs_max_out: 2136.0\n",
      "lif layer 1 self.abs_max_v: 2973.5\n",
      "lif layer 1 self.abs_max_v: 2981.0\n",
      "lif layer 1 self.abs_max_v: 2983.0\n",
      "fc layer 1 self.abs_max_out: 2145.0\n",
      "fc layer 3 self.abs_max_out: 220.0\n",
      "lif layer 1 self.abs_max_v: 2992.5\n",
      "lif layer 1 self.abs_max_v: 3061.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 2305.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 26 occurrences\n",
      "test - Value 1: 426 occurrences\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 67.686066/ 47.679680, val:  55.75%, val_best:  55.75%, tr:  91.62%, tr_best:  91.62%, epoch time: 247.68 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.3724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96768 real_backward_count 22694  23.452%\n",
      "layer   1  Sparsity: 75.9277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 3198.0\n",
      "lif layer 1 self.abs_max_v: 3239.0\n",
      "lif layer 1 self.abs_max_v: 3266.0\n",
      "lif layer 1 self.abs_max_v: 3405.0\n",
      "fc layer 1 self.abs_max_out: 2333.0\n",
      "lif layer 1 self.abs_max_v: 3435.0\n",
      "fc layer 1 self.abs_max_out: 2349.0\n",
      "fc layer 1 self.abs_max_out: 2386.0\n",
      "fc layer 1 self.abs_max_out: 2405.0\n",
      "fc layer 1 self.abs_max_out: 2423.0\n",
      "fc layer 1 self.abs_max_out: 2430.0\n",
      "fc layer 2 self.abs_max_out: 3348.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1087.00 at epoch 3, iter 4031\n",
      "max_activation_accul updated: 1376.00 at epoch 3, iter 4031\n",
      "fc layer 1 self.abs_max_out: 2597.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 75.414322/117.338318, val:  50.00%, val_best:  55.75%, tr:  92.83%, tr_best:  92.83%, epoch time: 248.07 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1154%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 129024 real_backward_count 30134  23.355%\n",
      "layer   1  Sparsity: 78.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 3451.0\n",
      "fc layer 2 self.abs_max_out: 3367.0\n",
      "fc layer 2 self.abs_max_out: 3460.0\n",
      "lif layer 1 self.abs_max_v: 3481.5\n",
      "fc layer 2 self.abs_max_out: 3543.0\n",
      "lif layer 1 self.abs_max_v: 3486.5\n",
      "fc layer 2 self.abs_max_out: 3597.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "lif layer 1 self.abs_max_v: 3513.0\n",
      "fc layer 2 self.abs_max_out: 3654.0\n",
      "fc layer 2 self.abs_max_out: 3747.0\n",
      "fc layer 2 self.abs_max_out: 3770.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "fc layer 2 self.abs_max_out: 3987.0\n",
      "fc layer 2 self.abs_max_out: 4145.0\n",
      "fc layer 1 self.abs_max_out: 2611.0\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 2744.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 273 occurrences\n",
      "test - Value 1: 179 occurrences\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 75.692894/ 75.430481, val:  72.79%, val_best:  72.79%, tr:  93.06%, tr_best:  93.06%, epoch time: 247.09 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9081%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161280 real_backward_count 37606  23.317%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4270.0\n",
      "fc layer 2 self.abs_max_out: 4330.0\n",
      "fc layer 2 self.abs_max_out: 4413.0\n",
      "fc layer 1 self.abs_max_out: 2771.0\n",
      "fc layer 2 self.abs_max_out: 4489.0\n",
      "fc layer 1 self.abs_max_out: 2817.0\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 2875.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 11 occurrences\n",
      "test - Value 1: 441 occurrences\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 82.667206/ 75.154472, val:  51.99%, val_best:  72.79%, tr:  92.71%, tr_best:  93.06%, epoch time: 246.00 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193536 real_backward_count 45179  23.344%\n",
      "layer   1  Sparsity: 85.8154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "fc layer 1 self.abs_max_out: 2912.0\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2049 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 3001.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 84.178391/104.336418, val:  50.00%, val_best:  72.79%, tr:  93.97%, tr_best:  93.97%, epoch time: 248.00 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.9073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225792 real_backward_count 52698  23.339%\n",
      "layer   1  Sparsity: 86.9141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5261.5\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 1 self.abs_max_out: 3047.0\n",
      "fc layer 1 self.abs_max_out: 3079.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1527.00 at epoch 7, iter 4031\n",
      "max_activation_accul updated: 1564.00 at epoch 7, iter 4031\n",
      "max_activation_accul updated: 1612.00 at epoch 7, iter 4031\n",
      "max_activation_accul updated: 1615.00 at epoch 7, iter 4031\n",
      "max_activation_accul updated: 1616.00 at epoch 7, iter 4031\n",
      "max_activation_accul updated: 1673.00 at epoch 7, iter 4031\n",
      "max_activation_accul updated: 1677.00 at epoch 7, iter 4031\n",
      "max_activation_accul updated: 1702.00 at epoch 7, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 80.513725/121.771225, val:  50.00%, val_best:  72.79%, tr:  94.35%, tr_best:  94.35%, epoch time: 249.39 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 258048 real_backward_count 60182  23.322%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3129.0\n",
      "fc layer 1 self.abs_max_out: 3167.0\n",
      "lif layer 1 self.abs_max_v: 3550.5\n",
      "lif layer 1 self.abs_max_v: 3572.5\n",
      "fc layer 1 self.abs_max_out: 3181.0\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2049 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 414 occurrences\n",
      "test - Value 1: 38 occurrences\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 80.154655/ 27.477421, val:  57.96%, val_best:  72.79%, tr:  95.41%, tr_best:  95.41%, epoch time: 249.81 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.5432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290304 real_backward_count 67465  23.239%\n",
      "layer   1  Sparsity: 83.5938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 3610.5\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "lif layer 2 self.abs_max_v: 5296.5\n",
      "lif layer 1 self.abs_max_v: 3728.5\n",
      "lif layer 2 self.abs_max_v: 5509.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 82.408615/ 72.667397, val:  50.22%, val_best:  72.79%, tr:  95.04%, tr_best:  95.41%, epoch time: 249.16 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322560 real_backward_count 74858  23.207%\n",
      "layer   1  Sparsity: 80.6152%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 3804.5\n",
      "lif layer 2 self.abs_max_v: 5876.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 322 occurrences\n",
      "test - Value 1: 130 occurrences\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 86.459938/ 26.828138, val:  71.68%, val_best:  72.79%, tr:  96.08%, tr_best:  96.08%, epoch time: 249.65 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.2074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354816 real_backward_count 81977  23.104%\n",
      "layer   1  Sparsity: 87.9150%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "lif layer 1 self.abs_max_v: 3967.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 80.390579/128.041321, val:  50.88%, val_best:  72.79%, tr:  95.29%, tr_best:  96.08%, epoch time: 249.86 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5850%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1395%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 387072 real_backward_count 89130  23.027%\n",
      "layer   1  Sparsity: 84.6924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "fc layer 1 self.abs_max_out: 3188.0\n",
      "fc layer 1 self.abs_max_out: 3190.0\n",
      "lif layer 1 self.abs_max_v: 4042.5\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2053 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 83.528442/102.704056, val:  50.00%, val_best:  72.79%, tr:  95.61%, tr_best:  96.08%, epoch time: 248.07 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419328 real_backward_count 96380  22.984%\n",
      "layer   1  Sparsity: 88.7939%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4141.5\n",
      "lif layer 2 self.abs_max_v: 6119.0\n",
      "fc layer 1 self.abs_max_out: 3208.0\n",
      "fc layer 1 self.abs_max_out: 3256.0\n",
      "fc layer 2 self.abs_max_out: 4659.0\n",
      "lif layer 2 self.abs_max_v: 6222.0\n",
      "fc layer 2 self.abs_max_out: 4692.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 76.508133/ 91.171532, val:  50.00%, val_best:  72.79%, tr:  95.49%, tr_best:  96.08%, epoch time: 250.04 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1567%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.3646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451584 real_backward_count 103671  22.957%\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3277.0\n",
      "fc layer 1 self.abs_max_out: 3393.0\n",
      "fc layer 1 self.abs_max_out: 3416.0\n",
      "lif layer 2 self.abs_max_v: 6262.0\n",
      "lif layer 2 self.abs_max_v: 6385.5\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 243 occurrences\n",
      "test - Value 1: 209 occurrences\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 77.187508/ 41.855267, val:  80.75%, val_best:  80.75%, tr:  95.16%, tr_best:  96.08%, epoch time: 250.02 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483840 real_backward_count 110816  22.903%\n",
      "layer   1  Sparsity: 94.1406%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3496.0\n",
      "lif layer 2 self.abs_max_v: 6515.5\n",
      "lif layer 1 self.abs_max_v: 4187.0\n",
      "fc layer 1 self.abs_max_out: 3537.0\n",
      "fc layer 1 self.abs_max_out: 3615.0\n",
      "lif layer 2 self.abs_max_v: 6683.5\n",
      "lif layer 1 self.abs_max_v: 4214.5\n",
      "fc layer 1 self.abs_max_out: 3634.0\n",
      "fc layer 1 self.abs_max_out: 3635.0\n",
      "fc layer 1 self.abs_max_out: 3648.0\n",
      "fc layer 1 self.abs_max_out: 3649.0\n",
      "fc layer 1 self.abs_max_out: 3657.0\n",
      "fc layer 1 self.abs_max_out: 3689.0\n",
      "fc layer 1 self.abs_max_out: 3696.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 3759.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 433 occurrences\n",
      "test - Value 1: 19 occurrences\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 81.760704/113.767403, val:  54.20%, val_best:  80.75%, tr:  95.54%, tr_best:  96.08%, epoch time: 248.66 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 516096 real_backward_count 118109  22.885%\n",
      "layer   1  Sparsity: 86.2793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3768.0\n",
      "fc layer 1 self.abs_max_out: 3807.0\n",
      "fc layer 2 self.abs_max_out: 4896.0\n",
      "lif layer 1 self.abs_max_v: 4286.5\n",
      "fc layer 2 self.abs_max_out: 4979.0\n",
      "fc layer 2 self.abs_max_out: 5000.0\n",
      "lif layer 1 self.abs_max_v: 4298.5\n",
      "lif layer 1 self.abs_max_v: 4307.5\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 198 occurrences\n",
      "test - Value 1: 254 occurrences\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 84.976143/ 43.440201, val:  78.32%, val_best:  80.75%, tr:  95.14%, tr_best:  96.08%, epoch time: 250.37 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548352 real_backward_count 125460  22.879%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 25.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4368.5\n",
      "lif layer 1 self.abs_max_v: 4449.0\n",
      "fc layer 2 self.abs_max_out: 5130.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 81.959236/137.911102, val:  50.00%, val_best:  80.75%, tr:  95.81%, tr_best:  96.08%, epoch time: 248.71 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580608 real_backward_count 132739  22.862%\n",
      "layer   1  Sparsity: 76.7822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3880.0\n",
      "lif layer 1 self.abs_max_v: 4555.0\n",
      "lif layer 1 self.abs_max_v: 4583.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1802.00 at epoch 18, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 87.054039/187.952301, val:  50.00%, val_best:  80.75%, tr:  95.34%, tr_best:  96.08%, epoch time: 248.03 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.4725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612864 real_backward_count 140037  22.850%\n",
      "layer   1  Sparsity: 88.8184%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4108.0\n",
      "lif layer 1 self.abs_max_v: 4590.5\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 451 occurrences\n",
      "test - Value 1: 1 occurrences\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss: 88.375214/ 48.052784, val:  50.22%, val_best:  80.75%, tr:  94.92%, tr_best:  96.08%, epoch time: 248.56 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2385%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.5975%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 645120 real_backward_count 147220  22.821%\n",
      "layer   1  Sparsity: 83.1787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 20.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4700.5\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1812.00 at epoch 20, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 451 occurrences\n",
      "test - Value 1: 1 occurrences\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss: 90.335724/124.957481, val:  50.22%, val_best:  80.75%, tr:  95.76%, tr_best:  96.08%, epoch time: 249.75 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677376 real_backward_count 154509  22.810%\n",
      "layer   1  Sparsity: 86.4746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4701.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 374 occurrences\n",
      "test - Value 1: 78 occurrences\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 85.697037/ 59.018875, val:  65.49%, val_best:  80.75%, tr:  96.16%, tr_best:  96.16%, epoch time: 249.19 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709632 real_backward_count 161629  22.776%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4116.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 7 occurrences\n",
      "test - Value 1: 445 occurrences\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss: 87.897644/130.787567, val:  51.55%, val_best:  80.75%, tr:  95.93%, tr_best:  96.16%, epoch time: 248.63 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741888 real_backward_count 168794  22.752%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 26.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4729.0\n",
      "fc layer 1 self.abs_max_out: 4158.0\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 4199.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 420 occurrences\n",
      "test - Value 1: 32 occurrences\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss: 87.692902/ 43.260265, val:  57.08%, val_best:  80.75%, tr:  96.01%, tr_best:  96.16%, epoch time: 249.10 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.9917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 774144 real_backward_count 176015  22.737%\n",
      "layer   1  Sparsity: 76.9287%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 25.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4216.0\n",
      "fc layer 1 self.abs_max_out: 4227.0\n",
      "fc layer 1 self.abs_max_out: 4275.0\n",
      "fc layer 1 self.abs_max_out: 4288.0\n",
      "fc layer 1 self.abs_max_out: 4290.0\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 4380.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 418 occurrences\n",
      "test - Value 1: 34 occurrences\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss: 89.843338/ 70.044846, val:  57.52%, val_best:  80.75%, tr:  95.31%, tr_best:  96.16%, epoch time: 247.56 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.9617%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806400 real_backward_count 183365  22.739%\n",
      "layer   1  Sparsity: 74.2432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 19.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4503.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "lif layer 1 self.abs_max_v: 4752.0\n",
      "lif layer 1 self.abs_max_v: 4870.5\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 132 occurrences\n",
      "test - Value 1: 320 occurrences\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss: 91.962891/104.495117, val:  72.57%, val_best:  80.75%, tr:  96.50%, tr_best:  96.50%, epoch time: 249.00 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838656 real_backward_count 190371  22.700%\n",
      "layer   1  Sparsity: 77.8564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4956.0\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss: 94.959816/133.036514, val:  52.88%, val_best:  80.75%, tr:  96.40%, tr_best:  96.50%, epoch time: 250.30 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.0310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870912 real_backward_count 197403  22.666%\n",
      "layer   1  Sparsity: 73.9990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "lif layer 1 self.abs_max_v: 4956.5\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 4518.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 116 occurrences\n",
      "test - Value 1: 336 occurrences\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 97.231560/ 52.689461, val:  72.57%, val_best:  80.75%, tr:  96.70%, tr_best:  96.70%, epoch time: 249.07 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7929%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 903168 real_backward_count 204458  22.638%\n",
      "layer   1  Sparsity: 81.9824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4585.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 4626.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 349 occurrences\n",
      "test - Value 1: 103 occurrences\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss: 89.493073/ 77.700714, val:  69.25%, val_best:  80.75%, tr:  96.63%, tr_best:  96.70%, epoch time: 248.51 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 935424 real_backward_count 211463  22.606%\n",
      "layer   1  Sparsity: 82.2998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5131.5\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss: 91.951065/124.755539, val:  50.88%, val_best:  80.75%, tr:  96.30%, tr_best:  96.70%, epoch time: 249.15 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.3290%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967680 real_backward_count 218557  22.586%\n",
      "layer   1  Sparsity: 90.6250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "lif layer 1 self.abs_max_v: 5134.5\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 397 occurrences\n",
      "test - Value 1: 55 occurrences\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 91.655693/ 59.440559, val:  61.28%, val_best:  80.75%, tr:  97.25%, tr_best:  97.25%, epoch time: 247.61 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.3655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999936 real_backward_count 225471  22.549%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 19.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 322 occurrences\n",
      "test - Value 1: 130 occurrences\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss: 92.880409/ 64.203804, val:  74.78%, val_best:  80.75%, tr:  97.50%, tr_best:  97.50%, epoch time: 248.88 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.2021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1032192 real_backward_count 232331  22.509%\n",
      "layer   1  Sparsity: 76.0986%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 25.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 414 occurrences\n",
      "test - Value 1: 38 occurrences\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 89.095543/ 53.452789, val:  57.96%, val_best:  80.75%, tr:  97.02%, tr_best:  97.50%, epoch time: 249.21 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.9964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1064448 real_backward_count 239217  22.473%\n",
      "layer   1  Sparsity: 91.4795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2041 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 183 occurrences\n",
      "test - Value 1: 269 occurrences\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 94.022949/ 70.162025, val:  78.54%, val_best:  80.75%, tr:  97.25%, tr_best:  97.50%, epoch time: 248.03 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.9188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096704 real_backward_count 246156  22.445%\n",
      "layer   1  Sparsity: 76.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 24.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4667.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 277 occurrences\n",
      "test - Value 1: 175 occurrences\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 92.663391/ 30.073599, val:  75.00%, val_best:  80.75%, tr:  97.22%, tr_best:  97.50%, epoch time: 249.36 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.8295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128960 real_backward_count 253123  22.421%\n",
      "layer   1  Sparsity: 88.6230%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5135.5\n",
      "lif layer 1 self.abs_max_v: 5162.5\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 21 occurrences\n",
      "test - Value 1: 431 occurrences\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss: 92.702454/ 96.124161, val:  54.65%, val_best:  80.75%, tr:  96.70%, tr_best:  97.50%, epoch time: 249.09 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.4454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1161216 real_backward_count 260020  22.392%\n",
      "layer   1  Sparsity: 95.0195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5217.0\n",
      "fc layer 2 self.abs_max_out: 5247.0\n",
      "fc layer 2 self.abs_max_out: 5267.0\n",
      "fc layer 2 self.abs_max_out: 5305.0\n",
      "fc layer 2 self.abs_max_out: 5495.0\n",
      "lif layer 2 self.abs_max_v: 6825.0\n",
      "fc layer 2 self.abs_max_out: 5555.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 4723.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 416 occurrences\n",
      "test - Value 1: 36 occurrences\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss: 92.730125/ 84.466805, val:  57.52%, val_best:  80.75%, tr:  97.35%, tr_best:  97.50%, epoch time: 249.38 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5889%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.4513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1193472 real_backward_count 266957  22.368%\n",
      "layer   1  Sparsity: 81.8848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 4849.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss: 92.179321/139.204254, val:  51.11%, val_best:  80.75%, tr:  97.15%, tr_best:  97.50%, epoch time: 249.13 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.9265%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225728 real_backward_count 273880  22.344%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 29.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 4850.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 297 occurrences\n",
      "test - Value 1: 155 occurrences\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 92.684723/ 26.238377, val:  77.65%, val_best:  80.75%, tr:  97.62%, tr_best:  97.62%, epoch time: 249.88 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.9991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257984 real_backward_count 280764  22.319%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5632.0\n",
      "lif layer 2 self.abs_max_v: 6875.0\n",
      "fc layer 1 self.abs_max_out: 4877.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5033.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss: 90.292046/166.623428, val:  50.00%, val_best:  80.75%, tr:  97.17%, tr_best:  97.62%, epoch time: 247.97 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1290240 real_backward_count 287716  22.299%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 21.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1986 occurrences\n",
      "train - Value 1: 2046 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 415 occurrences\n",
      "test - Value 1: 37 occurrences\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss: 91.352455/ 46.145729, val:  57.74%, val_best:  80.75%, tr:  97.17%, tr_best:  97.62%, epoch time: 248.52 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.5462%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1322496 real_backward_count 294578  22.274%\n",
      "layer   1  Sparsity: 84.2773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 37 occurrences\n",
      "test - Value 1: 415 occurrences\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss: 95.555870/ 79.670158, val:  58.19%, val_best:  80.75%, tr:  97.20%, tr_best:  97.62%, epoch time: 248.27 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.3811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354752 real_backward_count 301524  22.257%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 14.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 272 occurrences\n",
      "test - Value 1: 180 occurrences\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss: 99.155365/ 65.617126, val:  80.97%, val_best:  80.97%, tr:  96.88%, tr_best:  97.62%, epoch time: 249.36 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.3798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1387008 real_backward_count 308546  22.245%\n",
      "layer   1  Sparsity: 80.7861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "lif layer 1 self.abs_max_v: 5284.5\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 434 occurrences\n",
      "test - Value 1: 18 occurrences\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:100.332062/ 16.616657, val:  53.98%, val_best:  80.97%, tr:  97.12%, tr_best:  97.62%, epoch time: 247.83 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.1464%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419264 real_backward_count 315550  22.233%\n",
      "layer   1  Sparsity: 89.6240%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 38 occurrences\n",
      "test - Value 1: 414 occurrences\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 94.725227/120.570808, val:  58.41%, val_best:  80.97%, tr:  97.40%, tr_best:  97.62%, epoch time: 248.78 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.4969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1451520 real_backward_count 322374  22.209%\n",
      "layer   1  Sparsity: 83.5449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5062.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 407 occurrences\n",
      "test - Value 1: 45 occurrences\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:101.482567/108.798752, val:  59.51%, val_best:  80.97%, tr:  97.35%, tr_best:  97.62%, epoch time: 249.22 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.3594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483776 real_backward_count 329333  22.196%\n",
      "layer   1  Sparsity: 94.3604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5293.5\n",
      "lif layer 1 self.abs_max_v: 5408.5\n",
      "fc layer 1 self.abs_max_out: 5133.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5235.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 334 occurrences\n",
      "test - Value 1: 118 occurrences\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 86.069786/103.765739, val:  74.34%, val_best:  80.97%, tr:  97.02%, tr_best:  97.62%, epoch time: 249.54 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1516032 real_backward_count 336421  22.191%\n",
      "layer   1  Sparsity: 76.4893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 18.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5486.0\n",
      "train - Value 0: 1990 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 430 occurrences\n",
      "test - Value 1: 22 occurrences\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 94.669304/ 50.707214, val:  54.42%, val_best:  80.97%, tr:  97.02%, tr_best:  97.62%, epoch time: 248.54 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.4832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1548288 real_backward_count 343456  22.183%\n",
      "layer   1  Sparsity: 85.2783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5519.5\n",
      "lif layer 2 self.abs_max_v: 6890.5\n",
      "lif layer 2 self.abs_max_v: 7042.5\n",
      "lif layer 2 self.abs_max_v: 7134.0\n",
      "lif layer 2 self.abs_max_v: 7224.5\n",
      "lif layer 2 self.abs_max_v: 7263.5\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5237.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 91 occurrences\n",
      "test - Value 1: 361 occurrences\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 92.052139/ 76.197220, val:  68.36%, val_best:  80.97%, tr:  97.74%, tr_best:  97.74%, epoch time: 250.05 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3932%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1580544 real_backward_count 350378  22.168%\n",
      "layer   1  Sparsity: 90.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5641.0\n",
      "lif layer 2 self.abs_max_v: 7331.0\n",
      "lif layer 2 self.abs_max_v: 7339.5\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5242.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 30 occurrences\n",
      "test - Value 1: 422 occurrences\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 91.189659/ 98.682755, val:  56.64%, val_best:  80.97%, tr:  97.22%, tr_best:  97.74%, epoch time: 249.21 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6889%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0396%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612800 real_backward_count 357307  22.154%\n",
      "layer   1  Sparsity: 82.8613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5529.0\n",
      "lif layer 2 self.abs_max_v: 7570.5\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 89.894516/132.412537, val:  63.50%, val_best:  80.97%, tr:  96.90%, tr_best:  97.74%, epoch time: 250.38 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.4866%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1645056 real_backward_count 364281  22.144%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5554.0\n",
      "lif layer 2 self.abs_max_v: 7812.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 162 occurrences\n",
      "test - Value 1: 290 occurrences\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 90.046478/ 62.406475, val:  78.76%, val_best:  80.97%, tr:  97.40%, tr_best:  97.74%, epoch time: 250.66 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.5464%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1677312 real_backward_count 371220  22.132%\n",
      "layer   1  Sparsity: 88.3789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 7963.5\n",
      "lif layer 2 self.abs_max_v: 8048.0\n",
      "fc layer 2 self.abs_max_out: 5672.0\n",
      "lif layer 2 self.abs_max_v: 8096.0\n",
      "fc layer 2 self.abs_max_out: 5734.0\n",
      "lif layer 2 self.abs_max_v: 8178.0\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 65 occurrences\n",
      "test - Value 1: 387 occurrences\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 94.637344/119.817963, val:  63.94%, val_best:  80.97%, tr:  97.40%, tr_best:  97.74%, epoch time: 249.29 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1709568 real_backward_count 378102  22.117%\n",
      "layer   1  Sparsity: 92.7490%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 8312.0\n",
      "lif layer 2 self.abs_max_v: 8503.5\n",
      "lif layer 1 self.abs_max_v: 5675.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 392 occurrences\n",
      "test - Value 1: 60 occurrences\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 96.291496/ 90.491524, val:  62.83%, val_best:  80.97%, tr:  97.42%, tr_best:  97.74%, epoch time: 249.48 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2917%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1741824 real_backward_count 385039  22.106%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5750.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5270.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 365 occurrences\n",
      "test - Value 1: 87 occurrences\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 95.442253/ 99.564934, val:  66.15%, val_best:  80.97%, tr:  97.37%, tr_best:  97.74%, epoch time: 249.97 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4030%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1774080 real_backward_count 392006  22.096%\n",
      "layer   1  Sparsity: 82.2510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5879.5\n",
      "lif layer 2 self.abs_max_v: 8930.5\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 325 occurrences\n",
      "test - Value 1: 127 occurrences\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 95.129829/ 60.454388, val:  71.46%, val_best:  80.97%, tr:  97.57%, tr_best:  97.74%, epoch time: 250.65 seconds, 4.18 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1806336 real_backward_count 399004  22.089%\n",
      "layer   1  Sparsity: 82.3242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5368.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 7 occurrences\n",
      "test - Value 1: 445 occurrences\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 92.909348/108.060135, val:  51.55%, val_best:  80.97%, tr:  97.45%, tr_best:  97.74%, epoch time: 250.36 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1838592 real_backward_count 405777  22.070%\n",
      "layer   1  Sparsity: 90.3564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 315 occurrences\n",
      "test - Value 1: 137 occurrences\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 94.183182/ 58.529839, val:  74.12%, val_best:  80.97%, tr:  97.42%, tr_best:  97.74%, epoch time: 248.62 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.2631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1870848 real_backward_count 412635  22.056%\n",
      "layer   1  Sparsity: 92.8955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5424.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 400 occurrences\n",
      "test - Value 1: 52 occurrences\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 94.743370/ 85.698906, val:  61.06%, val_best:  80.97%, tr:  97.82%, tr_best:  97.82%, epoch time: 248.36 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5997%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.6977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1903104 real_backward_count 419534  22.045%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "lif layer 1 self.abs_max_v: 5959.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 111 occurrences\n",
      "test - Value 1: 341 occurrences\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 92.267960/ 61.394737, val:  72.79%, val_best:  80.97%, tr:  97.47%, tr_best:  97.82%, epoch time: 247.76 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.5716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1935360 real_backward_count 426279  22.026%\n",
      "layer   1  Sparsity: 84.2773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 182 occurrences\n",
      "test - Value 1: 270 occurrences\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 97.980537/ 47.777065, val:  78.76%, val_best:  80.97%, tr:  97.50%, tr_best:  97.82%, epoch time: 246.92 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1967616 real_backward_count 433077  22.010%\n",
      "layer   1  Sparsity: 75.9766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6168.5\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 5505.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 39 occurrences\n",
      "test - Value 1: 413 occurrences\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 90.382507/ 78.121460, val:  58.63%, val_best:  80.97%, tr:  97.02%, tr_best:  97.82%, epoch time: 247.57 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1999872 real_backward_count 440018  22.002%\n",
      "layer   1  Sparsity: 79.2969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 401 occurrences\n",
      "test - Value 1: 51 occurrences\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss: 90.995995/105.812607, val:  61.28%, val_best:  80.97%, tr:  97.52%, tr_best:  97.82%, epoch time: 246.37 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.9381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2032128 real_backward_count 446994  21.996%\n",
      "layer   1  Sparsity: 81.5674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 356 occurrences\n",
      "test - Value 1: 96 occurrences\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss: 91.365410/ 85.114433, val:  67.70%, val_best:  80.97%, tr:  97.37%, tr_best:  97.82%, epoch time: 246.81 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.4767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2064384 real_backward_count 453981  21.991%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 132 occurrences\n",
      "test - Value 1: 320 occurrences\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 90.764862/ 72.281776, val:  73.45%, val_best:  80.97%, tr:  97.42%, tr_best:  97.82%, epoch time: 247.99 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2096640 real_backward_count 460902  21.983%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss: 92.008690/101.389191, val:  50.22%, val_best:  80.97%, tr:  97.62%, tr_best:  97.82%, epoch time: 244.84 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9694%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2128896 real_backward_count 467882  21.978%\n",
      "layer   1  Sparsity: 90.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5896.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 346 occurrences\n",
      "test - Value 1: 106 occurrences\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 85.844116/ 69.588821, val:  70.80%, val_best:  80.97%, tr:  97.42%, tr_best:  97.82%, epoch time: 247.07 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2161152 real_backward_count 474980  21.978%\n",
      "layer   1  Sparsity: 79.6875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 29.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 5956.0\n",
      "fc layer 2 self.abs_max_out: 5959.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 207 occurrences\n",
      "test - Value 1: 245 occurrences\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 87.841484/ 25.791073, val:  78.54%, val_best:  80.97%, tr:  97.15%, tr_best:  97.82%, epoch time: 244.66 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.4579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2193408 real_backward_count 482011  21.975%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 30 occurrences\n",
      "test - Value 1: 422 occurrences\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 89.618172/ 83.745750, val:  56.64%, val_best:  80.97%, tr:  97.47%, tr_best:  97.82%, epoch time: 245.98 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2225664 real_backward_count 489105  21.976%\n",
      "layer   1  Sparsity: 90.7715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5509.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 402 occurrences\n",
      "test - Value 1: 50 occurrences\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss: 88.037384/ 17.284985, val:  59.73%, val_best:  80.97%, tr:  97.37%, tr_best:  97.82%, epoch time: 247.24 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2257920 real_backward_count 496195  21.976%\n",
      "layer   1  Sparsity: 78.4668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 16.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5547.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 356 occurrences\n",
      "test - Value 1: 96 occurrences\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 89.241257/ 71.590927, val:  69.47%, val_best:  80.97%, tr:  97.67%, tr_best:  97.82%, epoch time: 246.67 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2290176 real_backward_count 503122  21.969%\n",
      "layer   1  Sparsity: 74.6094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5555.0\n",
      "fc layer 1 self.abs_max_out: 5563.0\n",
      "fc layer 1 self.abs_max_out: 5608.0\n",
      "fc layer 1 self.abs_max_out: 5630.0\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 242 occurrences\n",
      "test - Value 1: 210 occurrences\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 85.884972/ 41.919239, val:  81.42%, val_best:  81.42%, tr:  97.67%, tr_best:  97.82%, epoch time: 246.09 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2322432 real_backward_count 510073  21.963%\n",
      "layer   1  Sparsity: 80.7861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5652.0\n",
      "fc layer 1 self.abs_max_out: 5666.0\n",
      "lif layer 1 self.abs_max_v: 6192.5\n",
      "lif layer 1 self.abs_max_v: 6205.5\n",
      "lif layer 1 self.abs_max_v: 6302.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 57 occurrences\n",
      "test - Value 1: 395 occurrences\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 87.270813/112.793243, val:  61.73%, val_best:  81.42%, tr:  97.42%, tr_best:  97.82%, epoch time: 247.44 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5573%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2354688 real_backward_count 516912  21.952%\n",
      "layer   1  Sparsity: 83.0078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6048.0\n",
      "fc layer 1 self.abs_max_out: 5687.0\n",
      "lif layer 1 self.abs_max_v: 6370.0\n",
      "fc layer 1 self.abs_max_out: 5691.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 284 occurrences\n",
      "test - Value 1: 168 occurrences\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 87.454155/ 79.739883, val:  77.88%, val_best:  81.42%, tr:  97.69%, tr_best:  97.82%, epoch time: 246.67 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.5809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2386944 real_backward_count 523764  21.943%\n",
      "layer   1  Sparsity: 88.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5759.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1899.00 at epoch 74, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 92.579514/181.474930, val:  50.00%, val_best:  81.42%, tr:  97.84%, tr_best:  97.84%, epoch time: 246.78 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2419200 real_backward_count 530484  21.928%\n",
      "layer   1  Sparsity: 86.9141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 334 occurrences\n",
      "test - Value 1: 118 occurrences\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 93.169800/ 91.122826, val:  72.57%, val_best:  81.42%, tr:  98.41%, tr_best:  98.41%, epoch time: 247.46 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.2909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2451456 real_backward_count 537116  21.910%\n",
      "layer   1  Sparsity: 88.3301%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 165 occurrences\n",
      "test - Value 1: 287 occurrences\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 91.744339/ 50.244755, val:  78.98%, val_best:  81.42%, tr:  98.29%, tr_best:  98.41%, epoch time: 247.52 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.3429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2483712 real_backward_count 543838  21.896%\n",
      "layer   1  Sparsity: 78.9062%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 207 occurrences\n",
      "test - Value 1: 245 occurrences\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 89.428322/ 37.743587, val:  79.42%, val_best:  81.42%, tr:  98.24%, tr_best:  98.41%, epoch time: 245.05 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.2219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2515968 real_backward_count 550616  21.885%\n",
      "layer   1  Sparsity: 79.7363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5767.0\n",
      "fc layer 1 self.abs_max_out: 5787.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 35 occurrences\n",
      "test - Value 1: 417 occurrences\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 90.941353/104.089638, val:  57.74%, val_best:  81.42%, tr:  97.82%, tr_best:  98.41%, epoch time: 247.42 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.1496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2548224 real_backward_count 557366  21.873%\n",
      "layer   1  Sparsity: 87.4512%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5792.0\n",
      "fc layer 1 self.abs_max_out: 5799.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 6079.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 308 occurrences\n",
      "test - Value 1: 144 occurrences\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 93.731255/104.022346, val:  73.01%, val_best:  81.42%, tr:  97.87%, tr_best:  98.41%, epoch time: 247.10 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2580480 real_backward_count 564188  21.864%\n",
      "layer   1  Sparsity: 86.2061%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5812.0\n",
      "fc layer 2 self.abs_max_out: 6275.0\n",
      "fc layer 2 self.abs_max_out: 6353.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 367 occurrences\n",
      "test - Value 1: 85 occurrences\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss: 95.664627/ 68.425438, val:  66.59%, val_best:  81.42%, tr:  97.35%, tr_best:  98.41%, epoch time: 245.65 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2612736 real_backward_count 571021  21.855%\n",
      "layer   1  Sparsity: 89.7949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5835.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 18 occurrences\n",
      "test - Value 1: 434 occurrences\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 90.245392/ 96.487007, val:  53.98%, val_best:  81.42%, tr:  97.89%, tr_best:  98.41%, epoch time: 244.71 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.9983%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2644992 real_backward_count 577817  21.846%\n",
      "layer   1  Sparsity: 82.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5943.0\n",
      "fc layer 1 self.abs_max_out: 5997.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 25 occurrences\n",
      "test - Value 1: 427 occurrences\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 93.197227/ 99.972870, val:  55.53%, val_best:  81.42%, tr:  98.49%, tr_best:  98.49%, epoch time: 245.09 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1282%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2677248 real_backward_count 584592  21.836%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 14.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6004.0\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 93.527802/225.876007, val:  50.00%, val_best:  81.42%, tr:  98.29%, tr_best:  98.49%, epoch time: 246.19 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.9148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2709504 real_backward_count 591308  21.823%\n",
      "layer   1  Sparsity: 75.4883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 21.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6386.5\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 93.514206/ 70.652489, val:  59.29%, val_best:  81.42%, tr:  98.49%, tr_best:  98.49%, epoch time: 247.82 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.7508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2741760 real_backward_count 597949  21.809%\n",
      "layer   1  Sparsity: 74.8779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "fc layer 1 self.abs_max_out: 6091.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 32 occurrences\n",
      "test - Value 1: 420 occurrences\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 93.158974/104.609764, val:  57.08%, val_best:  81.42%, tr:  98.07%, tr_best:  98.49%, epoch time: 247.48 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2774016 real_backward_count 604590  21.795%\n",
      "layer   1  Sparsity: 74.8047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 96.406624/145.813644, val:  50.66%, val_best:  81.42%, tr:  97.99%, tr_best:  98.49%, epoch time: 246.08 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.1135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2806272 real_backward_count 611379  21.786%\n",
      "layer   1  Sparsity: 82.7393%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6466.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 94.859779/ 52.022793, val:  71.90%, val_best:  81.42%, tr:  98.24%, tr_best:  98.49%, epoch time: 246.81 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.4967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2838528 real_backward_count 618135  21.777%\n",
      "layer   1  Sparsity: 80.7373%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 97.912163/184.566010, val:  50.44%, val_best:  81.42%, tr:  98.61%, tr_best:  98.61%, epoch time: 247.89 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.6640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2870784 real_backward_count 624667  21.759%\n",
      "layer   1  Sparsity: 92.7246%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 403 occurrences\n",
      "test - Value 1: 49 occurrences\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 94.183479/ 56.351570, val:  59.96%, val_best:  81.42%, tr:  98.02%, tr_best:  98.61%, epoch time: 247.29 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2903040 real_backward_count 631400  21.750%\n",
      "layer   1  Sparsity: 85.4004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 247 occurrences\n",
      "test - Value 1: 205 occurrences\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 91.537682/ 82.798805, val:  81.19%, val_best:  81.42%, tr:  98.41%, tr_best:  98.61%, epoch time: 247.34 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.8974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2935296 real_backward_count 638108  21.739%\n",
      "layer   1  Sparsity: 83.4229%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 303 occurrences\n",
      "test - Value 1: 149 occurrences\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss: 91.667625/ 59.238682, val:  78.10%, val_best:  81.42%, tr:  98.61%, tr_best:  98.61%, epoch time: 247.78 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2456%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2967552 real_backward_count 644630  21.723%\n",
      "layer   1  Sparsity: 82.1777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 154 occurrences\n",
      "test - Value 1: 298 occurrences\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 97.432838/ 53.720200, val:  77.43%, val_best:  81.42%, tr:  98.46%, tr_best:  98.61%, epoch time: 247.83 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.2752%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2999808 real_backward_count 651300  21.711%\n",
      "layer   1  Sparsity: 82.2754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss: 99.043983/162.506668, val:  50.00%, val_best:  81.42%, tr:  98.46%, tr_best:  98.61%, epoch time: 246.52 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4463%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.0268%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3032064 real_backward_count 657991  21.701%\n",
      "layer   1  Sparsity: 79.3701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 26.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 2 self.abs_max_out: 6404.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 272 occurrences\n",
      "test - Value 1: 180 occurrences\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss: 93.884369/ 56.652679, val:  78.32%, val_best:  81.42%, tr:  97.97%, tr_best:  98.61%, epoch time: 246.37 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3064320 real_backward_count 664631  21.689%\n",
      "layer   1  Sparsity: 69.2383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 24.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6427.0\n",
      "fc layer 2 self.abs_max_out: 6438.0\n",
      "fc layer 2 self.abs_max_out: 6508.0\n",
      "fc layer 2 self.abs_max_out: 6564.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 168 occurrences\n",
      "test - Value 1: 284 occurrences\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 87.292389/ 51.954296, val:  76.99%, val_best:  81.42%, tr:  98.24%, tr_best:  98.61%, epoch time: 246.74 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3096576 real_backward_count 671294  21.679%\n",
      "layer   1  Sparsity: 90.9180%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6673.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 48 occurrences\n",
      "test - Value 1: 404 occurrences\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss: 91.080902/ 77.722763, val:  60.18%, val_best:  81.42%, tr:  97.84%, tr_best:  98.61%, epoch time: 245.36 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3128832 real_backward_count 678169  21.675%\n",
      "layer   1  Sparsity: 76.7090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 435 occurrences\n",
      "test - Value 1: 17 occurrences\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss: 89.706329/ 83.517067, val:  53.76%, val_best:  81.42%, tr:  98.24%, tr_best:  98.61%, epoch time: 247.81 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3161088 real_backward_count 684908  21.667%\n",
      "layer   1  Sparsity: 82.8857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 22.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 142 occurrences\n",
      "test - Value 1: 310 occurrences\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss: 89.264549/ 65.889885, val:  76.11%, val_best:  81.42%, tr:  98.34%, tr_best:  98.61%, epoch time: 248.40 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3193344 real_backward_count 691516  21.655%\n",
      "layer   1  Sparsity: 81.2256%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 37 occurrences\n",
      "test - Value 1: 415 occurrences\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 92.410408/ 83.520020, val:  58.19%, val_best:  81.42%, tr:  98.31%, tr_best:  98.61%, epoch time: 247.79 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7297%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3225600 real_backward_count 698078  21.642%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 316 occurrences\n",
      "test - Value 1: 136 occurrences\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss: 91.490746/ 70.830215, val:  76.55%, val_best:  81.42%, tr:  98.07%, tr_best:  98.61%, epoch time: 246.92 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.4173%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3257856 real_backward_count 704746  21.632%\n",
      "layer   1  Sparsity: 69.5312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 63 occurrences\n",
      "test - Value 1: 389 occurrences\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 92.390427/ 67.544434, val:  63.94%, val_best:  81.42%, tr:  98.21%, tr_best:  98.61%, epoch time: 247.04 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6661%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3290112 real_backward_count 711561  21.627%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 338 occurrences\n",
      "test - Value 1: 114 occurrences\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss: 89.857780/ 73.186333, val:  73.01%, val_best:  81.42%, tr:  98.02%, tr_best:  98.61%, epoch time: 247.20 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3322368 real_backward_count 718269  21.619%\n",
      "layer   1  Sparsity: 71.6309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6136.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 177 occurrences\n",
      "test - Value 1: 275 occurrences\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss: 93.791321/ 56.896351, val:  80.75%, val_best:  81.42%, tr:  98.51%, tr_best:  98.61%, epoch time: 247.90 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1778%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.9292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3354624 real_backward_count 725024  21.613%\n",
      "layer   1  Sparsity: 80.1270%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6146.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1990.00 at epoch 104, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 94.756058/134.305450, val:  50.44%, val_best:  81.42%, tr:  97.87%, tr_best:  98.61%, epoch time: 247.17 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0150%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3386880 real_backward_count 731851  21.608%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 25.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 59 occurrences\n",
      "test - Value 1: 393 occurrences\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 92.390648/ 82.033737, val:  63.05%, val_best:  81.42%, tr:  97.72%, tr_best:  98.61%, epoch time: 245.79 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.6902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3419136 real_backward_count 738523  21.600%\n",
      "layer   1  Sparsity: 85.7666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss: 93.483490/143.277939, val:  50.22%, val_best:  81.42%, tr:  98.12%, tr_best:  98.61%, epoch time: 246.53 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.2910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3451392 real_backward_count 745157  21.590%\n",
      "layer   1  Sparsity: 91.3574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6164.0\n",
      "fc layer 1 self.abs_max_out: 6168.0\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 17 occurrences\n",
      "test - Value 1: 435 occurrences\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss: 91.900940/109.065422, val:  53.76%, val_best:  81.42%, tr:  97.87%, tr_best:  98.61%, epoch time: 246.18 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3483648 real_backward_count 751764  21.580%\n",
      "layer   1  Sparsity: 87.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6174.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 371 occurrences\n",
      "test - Value 1: 81 occurrences\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 91.623863/ 56.601791, val:  65.71%, val_best:  81.42%, tr:  97.25%, tr_best:  98.61%, epoch time: 247.13 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1985%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3515904 real_backward_count 758710  21.579%\n",
      "layer   1  Sparsity: 86.3525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6846.5\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 49 occurrences\n",
      "test - Value 1: 403 occurrences\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss: 93.226707/ 85.893478, val:  60.84%, val_best:  81.42%, tr:  98.34%, tr_best:  98.61%, epoch time: 245.76 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3548160 real_backward_count 765489  21.574%\n",
      "layer   1  Sparsity: 89.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6853.0\n",
      "lif layer 1 self.abs_max_v: 6959.5\n",
      "lif layer 1 self.abs_max_v: 6968.5\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 23 occurrences\n",
      "test - Value 1: 429 occurrences\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss: 94.959816/120.272568, val:  55.09%, val_best:  81.42%, tr:  98.26%, tr_best:  98.61%, epoch time: 248.35 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8399%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3580416 real_backward_count 772165  21.566%\n",
      "layer   1  Sparsity: 81.3232%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 7051.5\n",
      "lif layer 1 self.abs_max_v: 7130.0\n",
      "lif layer 1 self.abs_max_v: 7320.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 425 occurrences\n",
      "test - Value 1: 27 occurrences\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss: 90.804192/104.942131, val:  55.97%, val_best:  81.42%, tr:  98.39%, tr_best:  98.61%, epoch time: 247.01 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3612672 real_backward_count 778821  21.558%\n",
      "layer   1  Sparsity: 80.8594%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6220.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 306 occurrences\n",
      "test - Value 1: 146 occurrences\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss: 89.141159/105.561462, val:  75.22%, val_best:  81.42%, tr:  97.84%, tr_best:  98.61%, epoch time: 247.24 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3644928 real_backward_count 785583  21.553%\n",
      "layer   1  Sparsity: 85.9131%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 7339.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 360 occurrences\n",
      "test - Value 1: 92 occurrences\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:100.506523/ 77.918175, val:  68.14%, val_best:  81.42%, tr:  98.54%, tr_best:  98.61%, epoch time: 247.34 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9387%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3677184 real_backward_count 792262  21.545%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 31.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 279 occurrences\n",
      "test - Value 1: 173 occurrences\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss: 93.483513/ 44.186062, val:  78.10%, val_best:  81.42%, tr:  98.26%, tr_best:  98.61%, epoch time: 246.47 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3709440 real_backward_count 798890  21.537%\n",
      "layer   1  Sparsity: 80.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 409 occurrences\n",
      "test - Value 1: 43 occurrences\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss: 95.545860/ 93.368263, val:  59.07%, val_best:  81.42%, tr:  98.31%, tr_best:  98.61%, epoch time: 246.76 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3741696 real_backward_count 805584  21.530%\n",
      "layer   1  Sparsity: 87.3779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 245 occurrences\n",
      "test - Value 1: 207 occurrences\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss: 94.689217/ 70.881203, val:  80.31%, val_best:  81.42%, tr:  98.71%, tr_best:  98.71%, epoch time: 248.16 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3773952 real_backward_count 812217  21.522%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 21 occurrences\n",
      "test - Value 1: 431 occurrences\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss: 91.654922/ 74.545082, val:  54.65%, val_best:  81.42%, tr:  98.49%, tr_best:  98.71%, epoch time: 246.85 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.2902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.9593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3806208 real_backward_count 818895  21.515%\n",
      "layer   1  Sparsity: 87.9639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6309.0\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 14 occurrences\n",
      "test - Value 1: 438 occurrences\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss: 90.060730/115.859398, val:  53.10%, val_best:  81.42%, tr:  98.12%, tr_best:  98.71%, epoch time: 247.43 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.4872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3838464 real_backward_count 825622  21.509%\n",
      "layer   1  Sparsity: 88.8672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 184 occurrences\n",
      "test - Value 1: 268 occurrences\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss: 91.879616/ 41.312885, val:  80.97%, val_best:  81.42%, tr:  98.26%, tr_best:  98.71%, epoch time: 245.15 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.4259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3870720 real_backward_count 832335  21.503%\n",
      "layer   1  Sparsity: 87.5977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 81 occurrences\n",
      "test - Value 1: 371 occurrences\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss: 91.582230/ 75.356621, val:  67.48%, val_best:  81.42%, tr:  98.14%, tr_best:  98.71%, epoch time: 246.60 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3902976 real_backward_count 839077  21.498%\n",
      "layer   1  Sparsity: 82.3730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 19 occurrences\n",
      "test - Value 1: 433 occurrences\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss: 85.069206/109.361069, val:  54.20%, val_best:  81.42%, tr:  98.12%, tr_best:  98.71%, epoch time: 247.73 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3935232 real_backward_count 845913  21.496%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss: 88.983009/141.757294, val:  50.00%, val_best:  81.42%, tr:  98.71%, tr_best:  98.71%, epoch time: 247.37 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.7557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3967488 real_backward_count 852543  21.488%\n",
      "layer   1  Sparsity: 88.7207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 7422.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 171 occurrences\n",
      "test - Value 1: 281 occurrences\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss: 97.263535/ 70.768730, val:  81.64%, val_best:  81.64%, tr:  98.41%, tr_best:  98.71%, epoch time: 247.73 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3999744 real_backward_count 859112  21.479%\n",
      "layer   1  Sparsity: 83.9111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 406 occurrences\n",
      "test - Value 1: 46 occurrences\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss: 94.963989/ 57.738338, val:  59.29%, val_best:  81.64%, tr:  98.09%, tr_best:  98.71%, epoch time: 245.43 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4032000 real_backward_count 865775  21.473%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 7572.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 317 occurrences\n",
      "test - Value 1: 135 occurrences\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss: 94.773506/ 72.081612, val:  76.33%, val_best:  81.64%, tr:  98.49%, tr_best:  98.71%, epoch time: 246.82 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0449%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4064256 real_backward_count 872430  21.466%\n",
      "layer   1  Sparsity: 73.3154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 7756.5\n",
      "lif layer 1 self.abs_max_v: 7943.5\n",
      "fc layer 1 self.abs_max_out: 6334.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss: 89.857178/135.210175, val:  50.22%, val_best:  81.64%, tr:  97.87%, tr_best:  98.71%, epoch time: 245.97 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4096512 real_backward_count 879209  21.462%\n",
      "layer   1  Sparsity: 77.4170%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6354.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 388 occurrences\n",
      "test - Value 1: 64 occurrences\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss: 93.222466/ 64.763115, val:  63.72%, val_best:  81.64%, tr:  98.49%, tr_best:  98.71%, epoch time: 244.78 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4128768 real_backward_count 885996  21.459%\n",
      "layer   1  Sparsity: 91.1133%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 390 occurrences\n",
      "test - Value 1: 62 occurrences\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss: 89.901085/ 67.528763, val:  63.72%, val_best:  81.64%, tr:  98.54%, tr_best:  98.71%, epoch time: 246.36 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4161024 real_backward_count 892648  21.453%\n",
      "layer   1  Sparsity: 86.2061%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6360.0\n",
      "lif layer 1 self.abs_max_v: 8100.0\n",
      "lif layer 1 self.abs_max_v: 8195.5\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 355 occurrences\n",
      "test - Value 1: 97 occurrences\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss: 92.256088/ 51.105061, val:  68.36%, val_best:  81.64%, tr:  98.46%, tr_best:  98.71%, epoch time: 244.89 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4193280 real_backward_count 899313  21.447%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6366.0\n",
      "lif layer 1 self.abs_max_v: 8276.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 310 occurrences\n",
      "test - Value 1: 142 occurrences\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss: 92.625580/ 55.740906, val:  70.80%, val_best:  81.64%, tr:  99.03%, tr_best:  99.03%, epoch time: 247.83 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4225536 real_backward_count 905858  21.438%\n",
      "layer   1  Sparsity: 90.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 73 occurrences\n",
      "test - Value 1: 379 occurrences\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss: 99.691185/ 98.448769, val:  66.15%, val_best:  81.64%, tr:  99.03%, tr_best:  99.03%, epoch time: 247.37 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2478%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4257792 real_backward_count 912349  21.428%\n",
      "layer   1  Sparsity: 79.1260%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 300 occurrences\n",
      "test - Value 1: 152 occurrences\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss: 95.219894/ 83.828506, val:  73.45%, val_best:  81.64%, tr:  98.66%, tr_best:  99.03%, epoch time: 247.30 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2666%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4290048 real_backward_count 918900  21.419%\n",
      "layer   1  Sparsity: 88.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 257 occurrences\n",
      "test - Value 1: 195 occurrences\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss: 94.093773/ 52.459488, val:  78.54%, val_best:  81.64%, tr:  98.61%, tr_best:  99.03%, epoch time: 245.54 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.1834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4322304 real_backward_count 925496  21.412%\n",
      "layer   1  Sparsity: 77.1729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2023 occurrences\n",
      "train - Value 1: 2009 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2263.00 at epoch 134, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss: 91.143402/190.120758, val:  50.66%, val_best:  81.64%, tr:  98.09%, tr_best:  99.03%, epoch time: 246.92 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4354560 real_backward_count 932119  21.406%\n",
      "layer   1  Sparsity: 82.8125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 252 occurrences\n",
      "test - Value 1: 200 occurrences\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss: 92.528351/ 48.615372, val:  80.53%, val_best:  81.64%, tr:  98.66%, tr_best:  99.03%, epoch time: 247.00 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1524%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4386816 real_backward_count 938670  21.398%\n",
      "layer   1  Sparsity: 83.5938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 146 occurrences\n",
      "test - Value 1: 306 occurrences\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss: 94.889046/ 59.296219, val:  75.66%, val_best:  81.64%, tr:  98.66%, tr_best:  99.03%, epoch time: 247.75 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4419072 real_backward_count 945212  21.389%\n",
      "layer   1  Sparsity: 78.1738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 23.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 249 occurrences\n",
      "test - Value 1: 203 occurrences\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss: 96.280724/ 50.377319, val:  82.08%, val_best:  82.08%, tr:  98.71%, tr_best:  99.03%, epoch time: 247.06 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1972%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4451328 real_backward_count 951756  21.381%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 300 occurrences\n",
      "test - Value 1: 152 occurrences\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss: 90.138100/ 76.659409, val:  75.66%, val_best:  82.08%, tr:  98.41%, tr_best:  99.03%, epoch time: 246.93 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.1540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4483584 real_backward_count 958372  21.375%\n",
      "layer   1  Sparsity: 84.5459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 8398.5\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 113 occurrences\n",
      "test - Value 1: 339 occurrences\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss: 92.801292/ 82.644829, val:  73.23%, val_best:  82.08%, tr:  98.19%, tr_best:  99.03%, epoch time: 247.03 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4515840 real_backward_count 965016  21.370%\n",
      "layer   1  Sparsity: 95.0195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 127 occurrences\n",
      "test - Value 1: 325 occurrences\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss: 96.805595/ 84.475578, val:  73.67%, val_best:  82.08%, tr:  98.34%, tr_best:  99.03%, epoch time: 247.48 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.7721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4548096 real_backward_count 971646  21.364%\n",
      "layer   1  Sparsity: 80.5420%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6368.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 102 occurrences\n",
      "test - Value 1: 350 occurrences\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss: 94.881012/ 64.569939, val:  69.03%, val_best:  82.08%, tr:  98.14%, tr_best:  99.03%, epoch time: 248.51 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7297%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4580352 real_backward_count 978456  21.362%\n",
      "layer   1  Sparsity: 72.3389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 14.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9209.5\n",
      "fc layer 1 self.abs_max_out: 6402.0\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 393 occurrences\n",
      "test - Value 1: 59 occurrences\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss: 92.479118/ 46.264336, val:  62.61%, val_best:  82.08%, tr:  98.12%, tr_best:  99.03%, epoch time: 246.71 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4612608 real_backward_count 985214  21.359%\n",
      "layer   1  Sparsity: 84.8877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 9741.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 231 occurrences\n",
      "test - Value 1: 221 occurrences\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss: 97.666885/ 74.904953, val:  82.08%, val_best:  82.08%, tr:  98.71%, tr_best:  99.03%, epoch time: 246.47 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4644864 real_backward_count 991699  21.350%\n",
      "layer   1  Sparsity: 94.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 81.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6403.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 381 occurrences\n",
      "test - Value 1: 71 occurrences\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss: 96.040695/ 86.614403, val:  63.05%, val_best:  82.08%, tr:  98.31%, tr_best:  99.03%, epoch time: 247.64 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4677120 real_backward_count 998360  21.346%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6407.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 51 occurrences\n",
      "test - Value 1: 401 occurrences\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss: 94.835190/103.913826, val:  60.84%, val_best:  82.08%, tr:  98.19%, tr_best:  99.03%, epoch time: 247.93 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4709376 real_backward_count 1004894  21.338%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6417.0\n",
      "fc layer 1 self.abs_max_out: 6450.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 199 occurrences\n",
      "test - Value 1: 253 occurrences\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss: 93.031723/ 64.462646, val:  77.65%, val_best:  82.08%, tr:  98.09%, tr_best:  99.03%, epoch time: 248.14 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0859%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4741632 real_backward_count 1011680  21.336%\n",
      "layer   1  Sparsity: 82.8857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 8409.5\n",
      "fc layer 1 self.abs_max_out: 6480.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 409 occurrences\n",
      "test - Value 1: 43 occurrences\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss: 95.884155/122.607117, val:  59.07%, val_best:  82.08%, tr:  98.46%, tr_best:  99.03%, epoch time: 245.50 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4773888 real_backward_count 1018283  21.330%\n",
      "layer   1  Sparsity: 86.1084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 11 occurrences\n",
      "test - Value 1: 441 occurrences\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss: 93.704247/ 89.340363, val:  52.43%, val_best:  82.08%, tr:  98.26%, tr_best:  99.03%, epoch time: 247.32 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4806144 real_backward_count 1025006  21.327%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 366 occurrences\n",
      "test - Value 1: 86 occurrences\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss: 90.129356/ 97.883118, val:  67.26%, val_best:  82.08%, tr:  98.39%, tr_best:  99.03%, epoch time: 246.21 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.4885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4838400 real_backward_count 1031676  21.323%\n",
      "layer   1  Sparsity: 74.2432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 8483.0\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 163 occurrences\n",
      "test - Value 1: 289 occurrences\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss: 89.409996/100.780914, val:  77.65%, val_best:  82.08%, tr:  98.51%, tr_best:  99.03%, epoch time: 248.44 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4870656 real_backward_count 1038428  21.320%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 13.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 185 occurrences\n",
      "test - Value 1: 267 occurrences\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss: 96.168251/ 86.345879, val:  79.42%, val_best:  82.08%, tr:  98.31%, tr_best:  99.03%, epoch time: 246.34 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4902912 real_backward_count 1045105  21.316%\n",
      "layer   1  Sparsity: 85.7910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 421 occurrences\n",
      "test - Value 1: 31 occurrences\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss: 97.397499/ 55.693634, val:  56.86%, val_best:  82.08%, tr:  98.46%, tr_best:  99.03%, epoch time: 245.52 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3641%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.9300%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4935168 real_backward_count 1051790  21.312%\n",
      "layer   1  Sparsity: 82.1289%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6503.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss: 91.157936/133.331299, val:  50.88%, val_best:  82.08%, tr:  98.59%, tr_best:  99.03%, epoch time: 246.25 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3008%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4967424 real_backward_count 1058534  21.310%\n",
      "layer   1  Sparsity: 75.4883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 30.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6524.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 108 occurrences\n",
      "test - Value 1: 344 occurrences\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss: 92.824837/ 91.143356, val:  72.12%, val_best:  82.08%, tr:  98.29%, tr_best:  99.03%, epoch time: 246.79 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4999680 real_backward_count 1065275  21.307%\n",
      "layer   1  Sparsity: 83.5693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 27.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 161 occurrences\n",
      "test - Value 1: 291 occurrences\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss: 91.255592/ 74.144363, val:  78.54%, val_best:  82.08%, tr:  98.41%, tr_best:  99.03%, epoch time: 248.04 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4752%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.1066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5031936 real_backward_count 1071923  21.302%\n",
      "layer   1  Sparsity: 89.7705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 234 occurrences\n",
      "test - Value 1: 218 occurrences\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss: 88.684288/ 59.116154, val:  75.22%, val_best:  82.08%, tr:  97.82%, tr_best:  99.03%, epoch time: 245.59 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.7711%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5064192 real_backward_count 1078759  21.302%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 28.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 87 occurrences\n",
      "test - Value 1: 365 occurrences\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss: 87.757294/103.361870, val:  67.92%, val_best:  82.08%, tr:  98.31%, tr_best:  99.03%, epoch time: 247.69 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5096448 real_backward_count 1085457  21.298%\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 440 occurrences\n",
      "test - Value 1: 12 occurrences\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss: 87.488251/102.924042, val:  52.65%, val_best:  82.08%, tr:  98.39%, tr_best:  99.03%, epoch time: 246.78 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.8216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5128704 real_backward_count 1092166  21.295%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss: 88.731865/154.900391, val:  50.44%, val_best:  82.08%, tr:  97.72%, tr_best:  99.03%, epoch time: 247.42 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5160960 real_backward_count 1098913  21.293%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6526.0\n",
      "lif layer 1 self.abs_max_v: 8576.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss: 93.299805/167.356339, val:  50.22%, val_best:  82.08%, tr:  98.51%, tr_best:  99.03%, epoch time: 247.95 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3225%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5193216 real_backward_count 1105540  21.288%\n",
      "layer   1  Sparsity: 64.9170%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 21.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6545.0\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 42 occurrences\n",
      "test - Value 1: 410 occurrences\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss: 92.722633/ 93.308174, val:  59.29%, val_best:  82.08%, tr:  98.46%, tr_best:  99.03%, epoch time: 246.86 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4803%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.8268%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5225472 real_backward_count 1112305  21.286%\n",
      "layer   1  Sparsity: 93.9453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6563.0\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 393 occurrences\n",
      "test - Value 1: 59 occurrences\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss: 95.279068/ 41.526550, val:  62.61%, val_best:  82.08%, tr:  98.26%, tr_best:  99.03%, epoch time: 246.55 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4188%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5257728 real_backward_count 1118929  21.282%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 17.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 8695.0\n",
      "fc layer 1 self.abs_max_out: 6605.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 6 occurrences\n",
      "test - Value 1: 446 occurrences\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss: 97.233170/111.941109, val:  51.33%, val_best:  82.08%, tr:  98.46%, tr_best:  99.03%, epoch time: 247.72 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5289984 real_backward_count 1125534  21.277%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 33.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 276 occurrences\n",
      "test - Value 1: 176 occurrences\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss: 91.583588/ 88.876900, val:  77.43%, val_best:  82.08%, tr:  98.71%, tr_best:  99.03%, epoch time: 247.80 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4415%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0391%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5322240 real_backward_count 1132087  21.271%\n",
      "layer   1  Sparsity: 85.8154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 47.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 8787.5\n",
      "lif layer 1 self.abs_max_v: 9141.5\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss: 95.884232/ 87.389359, val:  76.55%, val_best:  82.08%, tr:  98.74%, tr_best:  99.03%, epoch time: 246.60 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.9700%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5354496 real_backward_count 1138752  21.267%\n",
      "layer   1  Sparsity: 75.9521%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 29.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 9485.5\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 18 occurrences\n",
      "test - Value 1: 434 occurrences\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss: 85.722473/ 69.580185, val:  53.98%, val_best:  82.08%, tr:  98.59%, tr_best:  99.03%, epoch time: 246.57 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.1468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5386752 real_backward_count 1145410  21.263%\n",
      "layer   1  Sparsity: 85.2295%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 46.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 9645.5\n",
      "lif layer 1 self.abs_max_v: 10010.5\n",
      "fc layer 1 self.abs_max_out: 6794.0\n",
      "lif layer 1 self.abs_max_v: 10135.5\n",
      "lif layer 1 self.abs_max_v: 10734.0\n",
      "fc layer 1 self.abs_max_out: 7082.0\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 47 occurrences\n",
      "test - Value 1: 405 occurrences\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss: 93.008995/111.257309, val:  60.40%, val_best:  82.08%, tr:  98.41%, tr_best:  99.03%, epoch time: 247.89 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.8369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5419008 real_backward_count 1152096  21.260%\n",
      "layer   1  Sparsity: 83.3008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 37.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 10943.5\n",
      "fc layer 1 self.abs_max_out: 7216.0\n",
      "fc layer 1 self.abs_max_out: 7418.0\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 2265.00 at epoch 168, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss: 93.139999/196.518982, val:  50.00%, val_best:  82.08%, tr:  98.41%, tr_best:  99.03%, epoch time: 246.95 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.2810%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5451264 real_backward_count 1158782  21.257%\n",
      "layer   1  Sparsity: 91.7480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 11056.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 219 occurrences\n",
      "test - Value 1: 233 occurrences\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss: 90.823227/ 55.536015, val:  80.31%, val_best:  82.08%, tr:  98.88%, tr_best:  99.03%, epoch time: 247.26 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.5527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5483520 real_backward_count 1165271  21.250%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 20 occurrences\n",
      "test - Value 1: 432 occurrences\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss: 93.948433/ 95.191345, val:  54.42%, val_best:  82.08%, tr:  98.71%, tr_best:  99.03%, epoch time: 246.84 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0353%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5515776 real_backward_count 1171923  21.247%\n",
      "layer   1  Sparsity: 82.9590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 32.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2022 occurrences\n",
      "train - Value 1: 2010 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 348 occurrences\n",
      "test - Value 1: 104 occurrences\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss: 91.548355/ 89.644302, val:  68.14%, val_best:  82.08%, tr:  98.56%, tr_best:  99.03%, epoch time: 247.29 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9839%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5548032 real_backward_count 1178628  21.244%\n",
      "layer   1  Sparsity: 82.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 210 occurrences\n",
      "test - Value 1: 242 occurrences\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss: 96.706810/ 79.378036, val:  80.97%, val_best:  82.08%, tr:  99.06%, tr_best:  99.06%, epoch time: 247.76 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.9927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5580288 real_backward_count 1185103  21.237%\n",
      "layer   1  Sparsity: 87.0117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 107 occurrences\n",
      "test - Value 1: 345 occurrences\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss: 97.622528/160.746597, val:  71.46%, val_best:  82.08%, tr:  98.83%, tr_best:  99.06%, epoch time: 247.14 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.0733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5612544 real_backward_count 1191590  21.231%\n",
      "layer   1  Sparsity: 95.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 11 occurrences\n",
      "test - Value 1: 441 occurrences\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss: 90.984840/130.384415, val:  52.43%, val_best:  82.08%, tr:  98.29%, tr_best:  99.06%, epoch time: 247.55 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1118%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.0049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5644800 real_backward_count 1198116  21.225%\n",
      "layer   1  Sparsity: 93.3350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 137 occurrences\n",
      "test - Value 1: 315 occurrences\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss: 90.498497/ 80.683540, val:  75.00%, val_best:  82.08%, tr:  98.39%, tr_best:  99.06%, epoch time: 246.06 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5677056 real_backward_count 1204679  21.220%\n",
      "layer   1  Sparsity: 77.2217%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 40.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 362 occurrences\n",
      "test - Value 1: 90 occurrences\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss: 96.585022/114.263275, val:  66.81%, val_best:  82.08%, tr:  98.59%, tr_best:  99.06%, epoch time: 246.72 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.3778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5709312 real_backward_count 1211354  21.217%\n",
      "layer   1  Sparsity: 85.3027%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 44.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 305 occurrences\n",
      "test - Value 1: 147 occurrences\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss: 95.288307/ 64.297768, val:  73.67%, val_best:  82.08%, tr:  98.86%, tr_best:  99.06%, epoch time: 247.01 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5741568 real_backward_count 1217850  21.211%\n",
      "layer   1  Sparsity: 87.4756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 6878.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 350 occurrences\n",
      "test - Value 1: 102 occurrences\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss: 95.294518/113.229591, val:  70.35%, val_best:  82.08%, tr:  98.86%, tr_best:  99.06%, epoch time: 248.27 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5773824 real_backward_count 1224383  21.206%\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 33 occurrences\n",
      "test - Value 1: 419 occurrences\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss: 92.715530/ 91.607574, val:  57.30%, val_best:  82.08%, tr:  98.29%, tr_best:  99.06%, epoch time: 247.54 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.5948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5806080 real_backward_count 1231122  21.204%\n",
      "layer   1  Sparsity: 80.6396%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 19.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 265 occurrences\n",
      "test - Value 1: 187 occurrences\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss: 96.315010/ 79.991089, val:  81.19%, val_best:  82.08%, tr:  98.51%, tr_best:  99.06%, epoch time: 245.72 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.8397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5838336 real_backward_count 1237655  21.199%\n",
      "layer   1  Sparsity: 89.4043%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 179 occurrences\n",
      "test - Value 1: 273 occurrences\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss: 96.123863/ 56.301273, val:  80.75%, val_best:  82.08%, tr:  98.81%, tr_best:  99.06%, epoch time: 246.86 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5870592 real_backward_count 1244119  21.192%\n",
      "layer   1  Sparsity: 61.9385%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 20.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 99 occurrences\n",
      "test - Value 1: 353 occurrences\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:100.338295/ 75.913460, val:  71.46%, val_best:  82.08%, tr:  98.44%, tr_best:  99.06%, epoch time: 247.47 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.7878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5902848 real_backward_count 1250663  21.187%\n",
      "layer   1  Sparsity: 80.3955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 204 occurrences\n",
      "test - Value 1: 248 occurrences\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:100.024529/135.219620, val:  77.88%, val_best:  82.08%, tr:  99.03%, tr_best:  99.06%, epoch time: 249.22 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4010%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5935104 real_backward_count 1257104  21.181%\n",
      "layer   1  Sparsity: 79.7852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 22.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss: 98.062279/188.113785, val:  50.00%, val_best:  82.08%, tr:  98.93%, tr_best:  99.06%, epoch time: 246.44 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0666%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5967360 real_backward_count 1263575  21.175%\n",
      "layer   1  Sparsity: 89.0381%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 206 occurrences\n",
      "test - Value 1: 246 occurrences\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss: 97.021866/ 63.161491, val:  78.76%, val_best:  82.08%, tr:  98.59%, tr_best:  99.06%, epoch time: 245.36 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.5648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5999616 real_backward_count 1270278  21.173%\n",
      "layer   1  Sparsity: 87.4756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 135 occurrences\n",
      "test - Value 1: 317 occurrences\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss: 95.574013/ 61.737068, val:  75.88%, val_best:  82.08%, tr:  98.76%, tr_best:  99.06%, epoch time: 247.84 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.3100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.3559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6031872 real_backward_count 1276900  21.169%\n",
      "layer   1  Sparsity: 83.4961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 33 occurrences\n",
      "test - Value 1: 419 occurrences\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:101.378059/107.396294, val:  57.30%, val_best:  82.08%, tr:  99.33%, tr_best:  99.33%, epoch time: 248.71 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.3140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4328%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6064128 real_backward_count 1283306  21.162%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 35.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 29 occurrences\n",
      "test - Value 1: 423 occurrences\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss: 96.756561/103.309242, val:  56.42%, val_best:  82.08%, tr:  98.81%, tr_best:  99.33%, epoch time: 246.00 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.1047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6096384 real_backward_count 1289829  21.157%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 17.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss: 92.761902/211.860870, val:  50.00%, val_best:  82.08%, tr:  98.96%, tr_best:  99.33%, epoch time: 244.81 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 43.9877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6128640 real_backward_count 1296426  21.154%\n",
      "layer   1  Sparsity: 82.5439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss: 94.633682/120.700317, val:  51.11%, val_best:  82.08%, tr:  98.74%, tr_best:  99.33%, epoch time: 246.59 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0722%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.9361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6160896 real_backward_count 1302979  21.149%\n",
      "layer   1  Sparsity: 71.6309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 34.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 356 occurrences\n",
      "test - Value 1: 96 occurrences\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss: 97.065514/ 25.813969, val:  67.70%, val_best:  82.08%, tr:  98.88%, tr_best:  99.33%, epoch time: 244.20 seconds, 4.07 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.4712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6193152 real_backward_count 1309518  21.145%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 183 occurrences\n",
      "test - Value 1: 269 occurrences\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss: 96.804779/ 60.720451, val:  79.87%, val_best:  82.08%, tr:  98.69%, tr_best:  99.33%, epoch time: 248.82 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.8139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6225408 real_backward_count 1316068  21.140%\n",
      "layer   1  Sparsity: 69.5312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 22.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss: 89.091576/120.605515, val:  50.44%, val_best:  82.08%, tr:  98.39%, tr_best:  99.33%, epoch time: 246.79 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.6075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6257664 real_backward_count 1322683  21.137%\n",
      "layer   1  Sparsity: 78.2227%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss: 91.131607/123.833939, val:  50.22%, val_best:  82.08%, tr:  98.51%, tr_best:  99.33%, epoch time: 245.89 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0469%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 42.2709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6289920 real_backward_count 1329322  21.134%\n",
      "layer   1  Sparsity: 90.0391%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 45.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss: 95.807373/132.287125, val:  50.44%, val_best:  82.08%, tr:  98.93%, tr_best:  99.33%, epoch time: 246.94 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.6343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6322176 real_backward_count 1335841  21.129%\n",
      "layer   1  Sparsity: 84.6924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 36.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 330 occurrences\n",
      "test - Value 1: 122 occurrences\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss: 98.085686/ 50.283951, val:  70.80%, val_best:  82.08%, tr:  98.56%, tr_best:  99.33%, epoch time: 248.08 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.3721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.3132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6354432 real_backward_count 1342465  21.126%\n",
      "layer   1  Sparsity: 83.5449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss: 93.481773/128.275253, val:  50.66%, val_best:  82.08%, tr:  98.69%, tr_best:  99.33%, epoch time: 247.58 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.4283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.0437%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6386688 real_backward_count 1349135  21.124%\n",
      "layer   1  Sparsity: 77.0020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 38.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 84 occurrences\n",
      "test - Value 1: 368 occurrences\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss: 90.673477/104.120163, val:  68.58%, val_best:  82.08%, tr:  98.56%, tr_best:  99.33%, epoch time: 247.06 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.3247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.9484%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6418944 real_backward_count 1355724  21.121%\n",
      "layer   1  Sparsity: 73.3887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 39.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 7 occurrences\n",
      "test - Value 1: 445 occurrences\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss: 94.847336/181.666412, val:  51.55%, val_best:  82.08%, tr:  98.76%, tr_best:  99.33%, epoch time: 244.92 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.2755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 41.5912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b004287c4e64228abf9fff70fbfe980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñá‚ñà‚ñá‚ñÇ‚ñÜ‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñá‚ñà‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñá‚ñà‚ñá‚ñÇ‚ñÜ‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñá‚ñà‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.9876</td></tr><tr><td>tr_epoch_loss</td><td>94.84734</td></tr><tr><td>val_acc_best</td><td>0.8208</td></tr><tr><td>val_acc_now</td><td>0.51549</td></tr><tr><td>val_loss</td><td>181.66641</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-63</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/e1vd0qcj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/e1vd0qcj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251221_020823-e1vd0qcj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cvizmlyi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251221_155423-cvizmlyi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/cvizmlyi' target=\"_blank\">gentle-sweep-64</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/cvizmlyi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/cvizmlyi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251221_155432_159', 'my_seed': 42, 'TIME': 8, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 0, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 16, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.125, 0.0625, 0.0625], 'learning_rate': 2, 'learning_rate2': 2, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4032, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4032 BATCH: 1 train_data_count: 4032\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 16, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.125, 0.0625, 0.0625])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.125, 0.0625, 0.0625])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.125, 0.0625, 0.0625])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 2\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 103.0\n",
      "lif layer 1 self.abs_max_v: 103.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 104.0\n",
      "lif layer 2 self.abs_max_v: 104.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 1 self.abs_max_out: 312.0\n",
      "lif layer 1 self.abs_max_v: 318.0\n",
      "fc layer 2 self.abs_max_out: 139.0\n",
      "lif layer 2 self.abs_max_v: 139.0\n",
      "fc layer 3 self.abs_max_out: 58.0\n",
      "fc layer 1 self.abs_max_out: 400.0\n",
      "lif layer 1 self.abs_max_v: 511.0\n",
      "lif layer 2 self.abs_max_v: 171.5\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "fc layer 1 self.abs_max_out: 418.0\n",
      "lif layer 1 self.abs_max_v: 630.5\n",
      "lif layer 2 self.abs_max_v: 198.0\n",
      "lif layer 1 self.abs_max_v: 720.5\n",
      "fc layer 2 self.abs_max_out: 149.0\n",
      "lif layer 2 self.abs_max_v: 244.0\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 2 self.abs_max_out: 174.0\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "layer   1  Sparsity: 74.8535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 79.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 563.0\n",
      "fc layer 2 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 275.5\n",
      "lif layer 1 self.abs_max_v: 851.5\n",
      "lif layer 2 self.abs_max_v: 291.0\n",
      "fc layer 2 self.abs_max_out: 245.0\n",
      "fc layer 1 self.abs_max_out: 644.0\n",
      "fc layer 2 self.abs_max_out: 264.0\n",
      "lif layer 2 self.abs_max_v: 310.0\n",
      "fc layer 1 self.abs_max_out: 649.0\n",
      "lif layer 2 self.abs_max_v: 353.5\n",
      "lif layer 2 self.abs_max_v: 409.0\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "fc layer 2 self.abs_max_out: 302.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "lif layer 2 self.abs_max_v: 415.5\n",
      "lif layer 1 self.abs_max_v: 860.0\n",
      "lif layer 2 self.abs_max_v: 444.0\n",
      "lif layer 1 self.abs_max_v: 955.5\n",
      "lif layer 2 self.abs_max_v: 467.0\n",
      "fc layer 1 self.abs_max_out: 655.0\n",
      "lif layer 1 self.abs_max_v: 996.0\n",
      "fc layer 2 self.abs_max_out: 310.0\n",
      "lif layer 2 self.abs_max_v: 487.5\n",
      "fc layer 1 self.abs_max_out: 666.0\n",
      "fc layer 2 self.abs_max_out: 324.0\n",
      "fc layer 1 self.abs_max_out: 695.0\n",
      "fc layer 1 self.abs_max_out: 724.0\n",
      "lif layer 1 self.abs_max_v: 1172.5\n",
      "fc layer 2 self.abs_max_out: 325.0\n",
      "fc layer 1 self.abs_max_out: 734.0\n",
      "fc layer 2 self.abs_max_out: 335.0\n",
      "fc layer 1 self.abs_max_out: 740.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "fc layer 1 self.abs_max_out: 869.0\n",
      "fc layer 2 self.abs_max_out: 342.0\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 504.5\n",
      "lif layer 2 self.abs_max_v: 519.5\n",
      "fc layer 2 self.abs_max_out: 348.0\n",
      "fc layer 2 self.abs_max_out: 349.0\n",
      "fc layer 2 self.abs_max_out: 350.0\n",
      "fc layer 2 self.abs_max_out: 355.0\n",
      "lif layer 2 self.abs_max_v: 553.5\n",
      "fc layer 2 self.abs_max_out: 364.0\n",
      "lif layer 2 self.abs_max_v: 567.0\n",
      "fc layer 2 self.abs_max_out: 392.0\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "fc layer 3 self.abs_max_out: 99.0\n",
      "fc layer 1 self.abs_max_out: 872.0\n",
      "fc layer 1 self.abs_max_out: 879.0\n",
      "fc layer 1 self.abs_max_out: 968.0\n",
      "lif layer 1 self.abs_max_v: 1337.5\n",
      "lif layer 2 self.abs_max_v: 577.0\n",
      "lif layer 1 self.abs_max_v: 1359.5\n",
      "lif layer 1 self.abs_max_v: 1378.5\n",
      "lif layer 1 self.abs_max_v: 1386.5\n",
      "lif layer 2 self.abs_max_v: 596.0\n",
      "lif layer 2 self.abs_max_v: 611.0\n",
      "lif layer 1 self.abs_max_v: 1439.5\n",
      "lif layer 1 self.abs_max_v: 1440.0\n",
      "fc layer 1 self.abs_max_out: 987.0\n",
      "fc layer 2 self.abs_max_out: 394.0\n",
      "fc layer 2 self.abs_max_out: 419.0\n",
      "lif layer 2 self.abs_max_v: 611.5\n",
      "lif layer 1 self.abs_max_v: 1452.5\n",
      "fc layer 1 self.abs_max_out: 992.0\n",
      "fc layer 2 self.abs_max_out: 424.0\n",
      "fc layer 1 self.abs_max_out: 1005.0\n",
      "fc layer 2 self.abs_max_out: 452.0\n",
      "lif layer 1 self.abs_max_v: 1469.5\n",
      "fc layer 2 self.abs_max_out: 456.0\n",
      "lif layer 1 self.abs_max_v: 1585.0\n",
      "lif layer 2 self.abs_max_v: 613.5\n",
      "fc layer 2 self.abs_max_out: 466.0\n",
      "lif layer 2 self.abs_max_v: 628.0\n",
      "lif layer 2 self.abs_max_v: 633.0\n",
      "fc layer 1 self.abs_max_out: 1039.0\n",
      "lif layer 2 self.abs_max_v: 635.0\n",
      "fc layer 1 self.abs_max_out: 1093.0\n",
      "fc layer 2 self.abs_max_out: 473.0\n",
      "fc layer 2 self.abs_max_out: 476.0\n",
      "fc layer 2 self.abs_max_out: 484.0\n",
      "fc layer 2 self.abs_max_out: 485.0\n",
      "fc layer 2 self.abs_max_out: 487.0\n",
      "fc layer 2 self.abs_max_out: 494.0\n",
      "fc layer 1 self.abs_max_out: 1120.0\n",
      "fc layer 2 self.abs_max_out: 496.0\n",
      "fc layer 2 self.abs_max_out: 506.0\n",
      "fc layer 1 self.abs_max_out: 1209.0\n",
      "fc layer 1 self.abs_max_out: 1248.0\n",
      "fc layer 2 self.abs_max_out: 509.0\n",
      "lif layer 2 self.abs_max_v: 637.5\n",
      "fc layer 2 self.abs_max_out: 511.0\n",
      "lif layer 1 self.abs_max_v: 1654.5\n",
      "fc layer 2 self.abs_max_out: 515.0\n",
      "fc layer 2 self.abs_max_out: 530.0\n",
      "lif layer 1 self.abs_max_v: 1679.5\n",
      "lif layer 2 self.abs_max_v: 644.5\n",
      "lif layer 2 self.abs_max_v: 681.0\n",
      "lif layer 1 self.abs_max_v: 1823.0\n",
      "lif layer 1 self.abs_max_v: 1958.5\n",
      "lif layer 2 self.abs_max_v: 687.5\n",
      "lif layer 2 self.abs_max_v: 731.5\n",
      "lif layer 2 self.abs_max_v: 759.5\n",
      "lif layer 2 self.abs_max_v: 760.5\n",
      "lif layer 2 self.abs_max_v: 770.5\n",
      "lif layer 2 self.abs_max_v: 773.5\n",
      "fc layer 2 self.abs_max_out: 534.0\n",
      "fc layer 2 self.abs_max_out: 536.0\n",
      "fc layer 2 self.abs_max_out: 539.0\n",
      "lif layer 2 self.abs_max_v: 777.0\n",
      "lif layer 2 self.abs_max_v: 797.5\n",
      "lif layer 2 self.abs_max_v: 848.0\n",
      "lif layer 2 self.abs_max_v: 861.0\n",
      "fc layer 2 self.abs_max_out: 540.0\n",
      "fc layer 2 self.abs_max_out: 542.0\n",
      "fc layer 2 self.abs_max_out: 545.0\n",
      "fc layer 2 self.abs_max_out: 550.0\n",
      "fc layer 2 self.abs_max_out: 554.0\n",
      "fc layer 2 self.abs_max_out: 555.0\n",
      "fc layer 2 self.abs_max_out: 562.0\n",
      "fc layer 2 self.abs_max_out: 575.0\n",
      "fc layer 2 self.abs_max_out: 584.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "fc layer 2 self.abs_max_out: 590.0\n",
      "lif layer 2 self.abs_max_v: 865.5\n",
      "fc layer 1 self.abs_max_out: 1253.0\n",
      "fc layer 1 self.abs_max_out: 1271.0\n",
      "lif layer 2 self.abs_max_v: 884.0\n",
      "fc layer 2 self.abs_max_out: 616.0\n",
      "fc layer 1 self.abs_max_out: 1331.0\n",
      "fc layer 2 self.abs_max_out: 623.0\n",
      "fc layer 2 self.abs_max_out: 624.0\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "fc layer 2 self.abs_max_out: 682.0\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "fc layer 1 self.abs_max_out: 1373.0\n",
      "lif layer 2 self.abs_max_v: 888.0\n",
      "fc layer 2 self.abs_max_out: 705.0\n",
      "lif layer 2 self.abs_max_v: 912.0\n",
      "fc layer 2 self.abs_max_out: 708.0\n",
      "fc layer 3 self.abs_max_out: 102.0\n",
      "fc layer 2 self.abs_max_out: 717.0\n",
      "fc layer 2 self.abs_max_out: 727.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "lif layer 2 self.abs_max_v: 921.0\n",
      "lif layer 2 self.abs_max_v: 923.5\n",
      "fc layer 1 self.abs_max_out: 1378.0\n",
      "lif layer 1 self.abs_max_v: 1998.0\n",
      "lif layer 2 self.abs_max_v: 925.5\n",
      "lif layer 2 self.abs_max_v: 926.5\n",
      "lif layer 2 self.abs_max_v: 959.5\n",
      "lif layer 2 self.abs_max_v: 960.0\n",
      "lif layer 2 self.abs_max_v: 975.5\n",
      "lif layer 2 self.abs_max_v: 1014.0\n",
      "lif layer 2 self.abs_max_v: 1014.5\n",
      "lif layer 2 self.abs_max_v: 1032.5\n",
      "fc layer 1 self.abs_max_out: 1518.0\n",
      "lif layer 2 self.abs_max_v: 1045.5\n",
      "lif layer 2 self.abs_max_v: 1047.5\n",
      "lif layer 2 self.abs_max_v: 1048.0\n",
      "fc layer 2 self.abs_max_out: 738.0\n",
      "fc layer 1 self.abs_max_out: 1530.0\n",
      "fc layer 2 self.abs_max_out: 760.0\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "fc layer 1 self.abs_max_out: 1564.0\n",
      "lif layer 2 self.abs_max_v: 1051.0\n",
      "lif layer 2 self.abs_max_v: 1080.5\n",
      "lif layer 2 self.abs_max_v: 1101.5\n",
      "lif layer 1 self.abs_max_v: 2132.5\n",
      "lif layer 1 self.abs_max_v: 2251.5\n",
      "fc layer 1 self.abs_max_out: 1587.0\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "lif layer 1 self.abs_max_v: 2269.0\n",
      "fc layer 2 self.abs_max_out: 832.0\n",
      "fc layer 1 self.abs_max_out: 1622.0\n",
      "fc layer 1 self.abs_max_out: 1701.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 1 self.abs_max_out: 1708.0\n",
      "lif layer 1 self.abs_max_v: 2393.5\n",
      "lif layer 2 self.abs_max_v: 1123.0\n",
      "lif layer 2 self.abs_max_v: 1151.5\n",
      "lif layer 2 self.abs_max_v: 1172.0\n",
      "lif layer 2 self.abs_max_v: 1180.0\n",
      "lif layer 2 self.abs_max_v: 1192.0\n",
      "lif layer 2 self.abs_max_v: 1204.5\n",
      "lif layer 2 self.abs_max_v: 1223.0\n",
      "lif layer 2 self.abs_max_v: 1238.5\n",
      "fc layer 2 self.abs_max_out: 857.0\n",
      "fc layer 1 self.abs_max_out: 1765.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "lif layer 2 self.abs_max_v: 1242.0\n",
      "fc layer 2 self.abs_max_out: 914.0\n",
      "fc layer 2 self.abs_max_out: 950.0\n",
      "fc layer 2 self.abs_max_out: 982.0\n",
      "fc layer 1 self.abs_max_out: 1768.0\n",
      "fc layer 1 self.abs_max_out: 1775.0\n",
      "fc layer 1 self.abs_max_out: 1843.0\n",
      "fc layer 2 self.abs_max_out: 1007.0\n",
      "fc layer 2 self.abs_max_out: 1008.0\n",
      "fc layer 1 self.abs_max_out: 2041.0\n",
      "fc layer 2 self.abs_max_out: 1054.0\n",
      "fc layer 1 self.abs_max_out: 2107.0\n",
      "lif layer 2 self.abs_max_v: 1318.5\n",
      "lif layer 2 self.abs_max_v: 1359.5\n",
      "lif layer 2 self.abs_max_v: 1398.5\n",
      "fc layer 1 self.abs_max_out: 2416.0\n",
      "lif layer 1 self.abs_max_v: 2416.0\n",
      "lif layer 2 self.abs_max_v: 1435.5\n",
      "lif layer 2 self.abs_max_v: 1460.5\n",
      "lif layer 2 self.abs_max_v: 1490.5\n",
      "lif layer 2 self.abs_max_v: 1494.5\n",
      "lif layer 2 self.abs_max_v: 1520.5\n",
      "lif layer 1 self.abs_max_v: 2426.5\n",
      "lif layer 1 self.abs_max_v: 2449.5\n",
      "fc layer 2 self.abs_max_out: 1065.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "lif layer 1 self.abs_max_v: 2489.0\n",
      "lif layer 1 self.abs_max_v: 2566.5\n",
      "lif layer 2 self.abs_max_v: 1539.0\n",
      "lif layer 1 self.abs_max_v: 2577.5\n",
      "lif layer 1 self.abs_max_v: 2629.5\n",
      "lif layer 1 self.abs_max_v: 2686.5\n",
      "fc layer 1 self.abs_max_out: 2591.0\n",
      "lif layer 1 self.abs_max_v: 2772.0\n",
      "fc layer 1 self.abs_max_out: 2768.0\n",
      "fc layer 2 self.abs_max_out: 1068.0\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "fc layer 1 self.abs_max_out: 2972.0\n",
      "lif layer 1 self.abs_max_v: 2972.0\n",
      "fc layer 2 self.abs_max_out: 1207.0\n",
      "lif layer 1 self.abs_max_v: 2974.0\n",
      "lif layer 2 self.abs_max_v: 1560.0\n",
      "lif layer 2 self.abs_max_v: 1565.0\n",
      "lif layer 2 self.abs_max_v: 1615.5\n",
      "lif layer 2 self.abs_max_v: 1682.0\n",
      "lif layer 2 self.abs_max_v: 1698.5\n",
      "lif layer 2 self.abs_max_v: 1797.5\n",
      "lif layer 2 self.abs_max_v: 1824.0\n",
      "lif layer 2 self.abs_max_v: 1866.5\n",
      "lif layer 2 self.abs_max_v: 1879.5\n",
      "lif layer 1 self.abs_max_v: 2997.0\n",
      "lif layer 2 self.abs_max_v: 1913.5\n",
      "lif layer 2 self.abs_max_v: 1951.5\n",
      "lif layer 2 self.abs_max_v: 1959.0\n",
      "lif layer 2 self.abs_max_v: 1968.5\n",
      "lif layer 2 self.abs_max_v: 2012.5\n",
      "lif layer 2 self.abs_max_v: 2016.5\n",
      "lif layer 2 self.abs_max_v: 2037.0\n",
      "lif layer 1 self.abs_max_v: 3091.5\n",
      "fc layer 2 self.abs_max_out: 1225.0\n",
      "lif layer 2 self.abs_max_v: 2048.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "lif layer 1 self.abs_max_v: 3154.5\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "fc layer 2 self.abs_max_out: 1287.0\n",
      "lif layer 2 self.abs_max_v: 2059.5\n",
      "lif layer 2 self.abs_max_v: 2070.5\n",
      "lif layer 2 self.abs_max_v: 2072.0\n",
      "lif layer 2 self.abs_max_v: 2136.0\n",
      "lif layer 1 self.abs_max_v: 3530.0\n",
      "lif layer 1 self.abs_max_v: 3626.0\n",
      "lif layer 2 self.abs_max_v: 2150.0\n",
      "train - Value 0: 1987 occurrences\n",
      "train - Value 1: 2045 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 303.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 344.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 375.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 413.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 428.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 491.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 503.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 548.00 at epoch 0, iter 4031\n",
      "fc layer 2 self.abs_max_out: 1301.0\n",
      "max_activation_accul updated: 568.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 597.00 at epoch 0, iter 4031\n",
      "lif layer 2 self.abs_max_v: 2182.0\n",
      "lif layer 2 self.abs_max_v: 2195.5\n",
      "max_activation_accul updated: 614.00 at epoch 0, iter 4031\n",
      "lif layer 2 self.abs_max_v: 2201.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-0   lr=['2.0000000'], tr/val_loss: 37.752769/ 36.259529, val:  50.00%, val_best:  50.00%, tr:  89.06%, tr_best:  89.06%, epoch time: 251.65 seconds, 4.19 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 32256 real_backward_count 7540  23.375%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 78.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1335.0\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "lif layer 2 self.abs_max_v: 2203.5\n",
      "lif layer 2 self.abs_max_v: 2297.0\n",
      "lif layer 2 self.abs_max_v: 2312.5\n",
      "lif layer 2 self.abs_max_v: 2354.5\n",
      "lif layer 2 self.abs_max_v: 2379.5\n",
      "lif layer 2 self.abs_max_v: 2395.5\n",
      "lif layer 2 self.abs_max_v: 2411.0\n",
      "lif layer 2 self.abs_max_v: 2411.5\n",
      "lif layer 2 self.abs_max_v: 2514.5\n",
      "lif layer 1 self.abs_max_v: 3736.5\n",
      "lif layer 1 self.abs_max_v: 3793.0\n",
      "lif layer 1 self.abs_max_v: 3970.0\n",
      "fc layer 2 self.abs_max_out: 1394.0\n",
      "fc layer 2 self.abs_max_out: 1409.0\n",
      "lif layer 1 self.abs_max_v: 4018.5\n",
      "lif layer 1 self.abs_max_v: 4036.5\n",
      "lif layer 1 self.abs_max_v: 4087.0\n",
      "fc layer 2 self.abs_max_out: 1414.0\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "fc layer 1 self.abs_max_out: 2990.0\n",
      "fc layer 1 self.abs_max_out: 3139.0\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "fc layer 2 self.abs_max_out: 1553.0\n",
      "fc layer 2 self.abs_max_out: 1564.0\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "lif layer 1 self.abs_max_v: 4513.5\n",
      "lif layer 1 self.abs_max_v: 4832.0\n",
      "lif layer 1 self.abs_max_v: 4898.0\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "fc layer 2 self.abs_max_out: 1631.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "fc layer 2 self.abs_max_out: 1670.0\n",
      "fc layer 2 self.abs_max_out: 1679.0\n",
      "fc layer 2 self.abs_max_out: 1683.0\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "fc layer 1 self.abs_max_out: 3156.0\n",
      "lif layer 1 self.abs_max_v: 4945.5\n",
      "train - Value 0: 1970 occurrences\n",
      "train - Value 1: 2062 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 2 self.abs_max_v: 2575.0\n",
      "lif layer 2 self.abs_max_v: 2596.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 447 occurrences\n",
      "test - Value 1: 5 occurrences\n",
      "epoch-1   lr=['2.0000000'], tr/val_loss: 44.485271/ 22.581707, val:  51.11%, val_best:  51.11%, tr:  93.50%, tr_best:  93.50%, epoch time: 250.49 seconds, 4.17 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 64512 real_backward_count 14351  22.245%\n",
      "layer   1  Sparsity: 86.3525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 2601.5\n",
      "lif layer 2 self.abs_max_v: 2615.0\n",
      "lif layer 2 self.abs_max_v: 2647.5\n",
      "lif layer 2 self.abs_max_v: 2693.0\n",
      "lif layer 2 self.abs_max_v: 2722.5\n",
      "lif layer 2 self.abs_max_v: 2736.0\n",
      "lif layer 2 self.abs_max_v: 2824.0\n",
      "lif layer 2 self.abs_max_v: 2841.0\n",
      "lif layer 2 self.abs_max_v: 2937.0\n",
      "fc layer 1 self.abs_max_out: 3243.0\n",
      "fc layer 1 self.abs_max_out: 3293.0\n",
      "fc layer 2 self.abs_max_out: 1735.0\n",
      "fc layer 1 self.abs_max_out: 3375.0\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "fc layer 2 self.abs_max_out: 1737.0\n",
      "fc layer 2 self.abs_max_out: 1747.0\n",
      "fc layer 2 self.abs_max_out: 1751.0\n",
      "fc layer 2 self.abs_max_out: 1753.0\n",
      "fc layer 1 self.abs_max_out: 3438.0\n",
      "fc layer 2 self.abs_max_out: 1797.0\n",
      "fc layer 2 self.abs_max_out: 1802.0\n",
      "fc layer 1 self.abs_max_out: 3494.0\n",
      "fc layer 1 self.abs_max_out: 3529.0\n",
      "fc layer 1 self.abs_max_out: 3587.0\n",
      "fc layer 1 self.abs_max_out: 3682.0\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 617.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 671.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 742.00 at epoch 2, iter 4031\n",
      "max_activation_accul updated: 897.00 at epoch 2, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-2   lr=['2.0000000'], tr/val_loss: 49.232445/ 67.599869, val:  50.00%, val_best:  51.11%, tr:  91.99%, tr_best:  93.50%, epoch time: 248.97 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.4819%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96768 real_backward_count 21194  21.902%\n",
      "layer   1  Sparsity: 75.9277%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1848.0\n",
      "fc layer 2 self.abs_max_out: 1896.0\n",
      "fc layer 2 self.abs_max_out: 1920.0\n",
      "fc layer 2 self.abs_max_out: 2016.0\n",
      "fc layer 3 self.abs_max_out: 169.0\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 1 self.abs_max_out: 3825.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-3   lr=['2.0000000'], tr/val_loss: 44.367352/ 81.892570, val:  50.00%, val_best:  51.11%, tr:  93.35%, tr_best:  93.50%, epoch time: 248.49 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0908%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 129024 real_backward_count 27924  21.642%\n",
      "layer   1  Sparsity: 78.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "lif layer 1 self.abs_max_v: 5045.5\n",
      "fc layer 2 self.abs_max_out: 2098.0\n",
      "lif layer 1 self.abs_max_v: 5243.0\n",
      "fc layer 2 self.abs_max_out: 2102.0\n",
      "fc layer 1 self.abs_max_out: 3994.0\n",
      "lif layer 1 self.abs_max_v: 5826.0\n",
      "fc layer 2 self.abs_max_out: 2114.0\n",
      "fc layer 3 self.abs_max_out: 180.0\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "lif layer 1 self.abs_max_v: 5867.5\n",
      "fc layer 1 self.abs_max_out: 4068.0\n",
      "fc layer 1 self.abs_max_out: 4229.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 449 occurrences\n",
      "test - Value 1: 3 occurrences\n",
      "epoch-4   lr=['2.0000000'], tr/val_loss: 52.166653/ 56.546482, val:  50.66%, val_best:  51.11%, tr:  90.77%, tr_best:  93.50%, epoch time: 248.98 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7923%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 161280 real_backward_count 34956  21.674%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2138.0\n",
      "fc layer 2 self.abs_max_out: 2175.0\n",
      "fc layer 2 self.abs_max_out: 2207.0\n",
      "fc layer 2 self.abs_max_out: 2244.0\n",
      "fc layer 2 self.abs_max_out: 2251.0\n",
      "fc layer 2 self.abs_max_out: 2286.0\n",
      "fc layer 2 self.abs_max_out: 2337.0\n",
      "fc layer 2 self.abs_max_out: 2346.0\n",
      "lif layer 2 self.abs_max_v: 2983.5\n",
      "fc layer 2 self.abs_max_out: 2347.0\n",
      "lif layer 2 self.abs_max_v: 3003.0\n",
      "lif layer 2 self.abs_max_v: 3149.5\n",
      "fc layer 2 self.abs_max_out: 2358.0\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "lif layer 1 self.abs_max_v: 5895.5\n",
      "fc layer 2 self.abs_max_out: 2363.0\n",
      "fc layer 2 self.abs_max_out: 2377.0\n",
      "lif layer 1 self.abs_max_v: 6039.5\n",
      "fc layer 1 self.abs_max_out: 4233.0\n",
      "fc layer 2 self.abs_max_out: 2386.0\n",
      "lif layer 1 self.abs_max_v: 6065.0\n",
      "fc layer 1 self.abs_max_out: 4342.0\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "fc layer 2 self.abs_max_out: 2541.0\n",
      "lif layer 1 self.abs_max_v: 6093.0\n",
      "lif layer 1 self.abs_max_v: 6105.5\n",
      "fc layer 3 self.abs_max_out: 184.0\n",
      "lif layer 1 self.abs_max_v: 6200.5\n",
      "lif layer 1 self.abs_max_v: 6427.5\n",
      "lif layer 1 self.abs_max_v: 6757.0\n",
      "fc layer 1 self.abs_max_out: 4355.0\n",
      "train - Value 0: 2067 occurrences\n",
      "train - Value 1: 1965 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 4399.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-5   lr=['2.0000000'], tr/val_loss: 48.741695/ 25.362232, val:  50.00%, val_best:  51.11%, tr:  92.68%, tr_best:  93.50%, epoch time: 249.19 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193536 real_backward_count 41921  21.661%\n",
      "layer   1  Sparsity: 85.8154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4555.0\n",
      "fc layer 1 self.abs_max_out: 4577.0\n",
      "fc layer 1 self.abs_max_out: 4711.0\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 2544.0\n",
      "fc layer 1 self.abs_max_out: 4783.0\n",
      "fc layer 2 self.abs_max_out: 2574.0\n",
      "fc layer 2 self.abs_max_out: 2615.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1001.00 at epoch 6, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-6   lr=['2.0000000'], tr/val_loss: 51.734875/ 96.674156, val:  50.00%, val_best:  51.11%, tr:  95.49%, tr_best:  95.49%, epoch time: 248.81 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4768%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225792 real_backward_count 48472  21.468%\n",
      "layer   1  Sparsity: 86.9141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2630.0\n",
      "fc layer 2 self.abs_max_out: 2687.0\n",
      "fc layer 2 self.abs_max_out: 2734.0\n",
      "fc layer 2 self.abs_max_out: 2747.0\n",
      "fc layer 2 self.abs_max_out: 2758.0\n",
      "fc layer 2 self.abs_max_out: 2793.0\n",
      "lif layer 2 self.abs_max_v: 3179.0\n",
      "lif layer 2 self.abs_max_v: 3205.0\n",
      "lif layer 2 self.abs_max_v: 3207.5\n",
      "lif layer 2 self.abs_max_v: 3215.0\n",
      "lif layer 2 self.abs_max_v: 3273.5\n",
      "lif layer 2 self.abs_max_v: 3337.0\n",
      "lif layer 2 self.abs_max_v: 3431.0\n",
      "lif layer 2 self.abs_max_v: 3447.0\n",
      "lif layer 2 self.abs_max_v: 3512.5\n",
      "lif layer 2 self.abs_max_v: 3564.0\n",
      "lif layer 2 self.abs_max_v: 3579.0\n",
      "lif layer 2 self.abs_max_v: 3788.5\n",
      "lif layer 2 self.abs_max_v: 3807.5\n",
      "lif layer 2 self.abs_max_v: 3882.5\n",
      "lif layer 2 self.abs_max_v: 4058.5\n",
      "lif layer 2 self.abs_max_v: 4076.5\n",
      "lif layer 2 self.abs_max_v: 4116.5\n",
      "lif layer 2 self.abs_max_v: 4276.5\n",
      "lif layer 2 self.abs_max_v: 4285.5\n",
      "fc layer 2 self.abs_max_out: 2819.0\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2058 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 438 occurrences\n",
      "test - Value 1: 14 occurrences\n",
      "epoch-7   lr=['2.0000000'], tr/val_loss: 50.614983/ 35.154228, val:  52.21%, val_best:  52.21%, tr:  94.30%, tr_best:  95.49%, epoch time: 249.47 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 258048 real_backward_count 55196  21.390%\n",
      "layer   1  Sparsity: 83.2520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4287.5\n",
      "lif layer 2 self.abs_max_v: 4327.0\n",
      "lif layer 2 self.abs_max_v: 4395.5\n",
      "lif layer 2 self.abs_max_v: 4479.0\n",
      "fc layer 1 self.abs_max_out: 4865.0\n",
      "train - Value 0: 1977 occurrences\n",
      "train - Value 1: 2055 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 423 occurrences\n",
      "test - Value 1: 29 occurrences\n",
      "epoch-8   lr=['2.0000000'], tr/val_loss: 58.755669/ 30.402756, val:  55.97%, val_best:  55.97%, tr:  92.49%, tr_best:  95.49%, epoch time: 247.41 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7679%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290304 real_backward_count 61819  21.295%\n",
      "layer   1  Sparsity: 83.5938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 6853.0\n",
      "lif layer 2 self.abs_max_v: 4492.0\n",
      "fc layer 1 self.abs_max_out: 4903.0\n",
      "fc layer 1 self.abs_max_out: 4904.0\n",
      "fc layer 1 self.abs_max_out: 5031.0\n",
      "lif layer 1 self.abs_max_v: 6989.5\n",
      "lif layer 1 self.abs_max_v: 7209.0\n",
      "lif layer 2 self.abs_max_v: 4494.5\n",
      "lif layer 2 self.abs_max_v: 4543.5\n",
      "lif layer 2 self.abs_max_v: 4547.0\n",
      "train - Value 0: 1986 occurrences\n",
      "train - Value 1: 2046 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-9   lr=['2.0000000'], tr/val_loss: 57.071106/ 62.730358, val:  50.00%, val_best:  55.97%, tr:  94.00%, tr_best:  95.49%, epoch time: 248.35 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8834%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 322560 real_backward_count 68385  21.201%\n",
      "layer   1  Sparsity: 80.6152%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 7420.5\n",
      "fc layer 3 self.abs_max_out: 203.0\n",
      "train - Value 0: 1973 occurrences\n",
      "train - Value 1: 2059 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1024.00 at epoch 10, iter 4031\n",
      "max_activation_accul updated: 1062.00 at epoch 10, iter 4031\n",
      "max_activation_accul updated: 1112.00 at epoch 10, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-10  lr=['2.0000000'], tr/val_loss: 55.551994/ 32.008377, val:  50.00%, val_best:  55.97%, tr:  93.77%, tr_best:  95.49%, epoch time: 248.69 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 354816 real_backward_count 74999  21.137%\n",
      "layer   1  Sparsity: 87.9150%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5049.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "lif layer 1 self.abs_max_v: 7582.0\n",
      "lif layer 1 self.abs_max_v: 7911.0\n",
      "fc layer 1 self.abs_max_out: 5115.0\n",
      "train - Value 0: 2119 occurrences\n",
      "train - Value 1: 1913 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-11  lr=['2.0000000'], tr/val_loss: 45.955093/ 67.636177, val:  50.00%, val_best:  55.97%, tr:  90.00%, tr_best:  95.49%, epoch time: 248.87 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 387072 real_backward_count 81947  21.171%\n",
      "layer   1  Sparsity: 84.6924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5132.0\n",
      "lif layer 2 self.abs_max_v: 4562.0\n",
      "lif layer 2 self.abs_max_v: 4601.0\n",
      "lif layer 2 self.abs_max_v: 4643.5\n",
      "fc layer 1 self.abs_max_out: 5282.0\n",
      "fc layer 1 self.abs_max_out: 5382.0\n",
      "lif layer 1 self.abs_max_v: 7930.0\n",
      "lif layer 1 self.abs_max_v: 7998.0\n",
      "lif layer 1 self.abs_max_v: 8152.0\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-12  lr=['2.0000000'], tr/val_loss: 51.435658/ 55.651546, val:  50.66%, val_best:  55.97%, tr:  94.39%, tr_best:  95.49%, epoch time: 249.10 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.9337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 419328 real_backward_count 88684  21.149%\n",
      "layer   1  Sparsity: 88.7939%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5416.0\n",
      "lif layer 1 self.abs_max_v: 8433.5\n",
      "fc layer 1 self.abs_max_out: 5466.0\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "fc layer 2 self.abs_max_out: 2835.0\n",
      "fc layer 2 self.abs_max_out: 2855.0\n",
      "fc layer 3 self.abs_max_out: 224.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1168.00 at epoch 13, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-13  lr=['2.0000000'], tr/val_loss: 53.124184/125.326851, val:  50.00%, val_best:  55.97%, tr:  94.00%, tr_best:  95.49%, epoch time: 246.28 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.4169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 451584 real_backward_count 95250  21.092%\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4660.5\n",
      "fc layer 1 self.abs_max_out: 5485.0\n",
      "lif layer 1 self.abs_max_v: 8593.5\n",
      "fc layer 1 self.abs_max_out: 5677.0\n",
      "fc layer 2 self.abs_max_out: 2861.0\n",
      "fc layer 2 self.abs_max_out: 2928.0\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 117 occurrences\n",
      "test - Value 1: 335 occurrences\n",
      "epoch-14  lr=['2.0000000'], tr/val_loss: 53.863667/ 52.115959, val:  69.69%, val_best:  69.69%, tr:  93.87%, tr_best:  95.49%, epoch time: 248.32 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8780%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483840 real_backward_count 101763  21.032%\n",
      "layer   1  Sparsity: 94.1406%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 8797.5\n",
      "fc layer 2 self.abs_max_out: 3026.0\n",
      "fc layer 2 self.abs_max_out: 3116.0\n",
      "fc layer 2 self.abs_max_out: 3120.0\n",
      "fc layer 2 self.abs_max_out: 3193.0\n",
      "fc layer 2 self.abs_max_out: 3231.0\n",
      "fc layer 2 self.abs_max_out: 3268.0\n",
      "train - Value 0: 2069 occurrences\n",
      "train - Value 1: 1963 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-15  lr=['2.0000000'], tr/val_loss: 55.490772/ 38.220325, val:  50.00%, val_best:  69.69%, tr:  94.67%, tr_best:  95.49%, epoch time: 246.84 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6111%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 516096 real_backward_count 108241  20.973%\n",
      "layer   1  Sparsity: 86.2793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 8827.5\n",
      "fc layer 1 self.abs_max_out: 5703.0\n",
      "lif layer 2 self.abs_max_v: 4711.0\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1992 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 408 occurrences\n",
      "test - Value 1: 44 occurrences\n",
      "epoch-16  lr=['2.0000000'], tr/val_loss: 61.229790/  6.532701, val:  58.85%, val_best:  69.69%, tr:  95.04%, tr_best:  95.49%, epoch time: 247.22 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.9874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548352 real_backward_count 114641  20.906%\n",
      "layer   1  Sparsity: 79.7119%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 45.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3286.0\n",
      "fc layer 2 self.abs_max_out: 3327.0\n",
      "lif layer 1 self.abs_max_v: 8832.0\n",
      "train - Value 0: 1894 occurrences\n",
      "train - Value 1: 2138 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-17  lr=['2.0000000'], tr/val_loss: 58.232460/ 88.926491, val:  50.00%, val_best:  69.69%, tr:  89.78%, tr_best:  95.49%, epoch time: 247.55 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.9115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7549%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580608 real_backward_count 121370  20.904%\n",
      "layer   1  Sparsity: 76.7822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 8931.5\n",
      "fc layer 1 self.abs_max_out: 5927.0\n",
      "fc layer 1 self.abs_max_out: 5948.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 1 self.abs_max_out: 5999.0\n",
      "train - Value 0: 1915 occurrences\n",
      "train - Value 1: 2117 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-18  lr=['2.0000000'], tr/val_loss: 51.521931/ 76.204628, val:  50.00%, val_best:  69.69%, tr:  89.46%, tr_best:  95.49%, epoch time: 245.98 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 612864 real_backward_count 128081  20.899%\n",
      "layer   1  Sparsity: 88.8184%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6093.0\n",
      "fc layer 1 self.abs_max_out: 6185.0\n",
      "fc layer 1 self.abs_max_out: 6224.0\n",
      "train - Value 0: 1948 occurrences\n",
      "train - Value 1: 2084 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 438 occurrences\n",
      "test - Value 1: 14 occurrences\n",
      "epoch-19  lr=['2.0000000'], tr/val_loss: 65.058388/ 24.152992, val:  53.10%, val_best:  69.69%, tr:  95.29%, tr_best:  95.49%, epoch time: 247.67 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8439%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9663%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 645120 real_backward_count 134509  20.850%\n",
      "layer   1  Sparsity: 83.1787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6255.0\n",
      "fc layer 1 self.abs_max_out: 6276.0\n",
      "fc layer 1 self.abs_max_out: 6283.0\n",
      "fc layer 1 self.abs_max_out: 6343.0\n",
      "fc layer 1 self.abs_max_out: 6562.0\n",
      "train - Value 0: 1981 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-20  lr=['2.0000000'], tr/val_loss: 60.347202/ 19.227810, val:  50.00%, val_best:  69.69%, tr:  94.32%, tr_best:  95.49%, epoch time: 247.54 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677376 real_backward_count 141016  20.818%\n",
      "layer   1  Sparsity: 86.4746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6576.0\n",
      "train - Value 0: 1994 occurrences\n",
      "train - Value 1: 2038 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 224 occurrences\n",
      "test - Value 1: 228 occurrences\n",
      "epoch-21  lr=['2.0000000'], tr/val_loss: 55.468552/ 24.757183, val:  79.20%, val_best:  79.20%, tr:  95.93%, tr_best:  95.93%, epoch time: 247.44 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3310%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 709632 real_backward_count 147369  20.767%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6716.0\n",
      "train - Value 0: 1949 occurrences\n",
      "train - Value 1: 2083 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 12 occurrences\n",
      "test - Value 1: 440 occurrences\n",
      "epoch-22  lr=['2.0000000'], tr/val_loss: 61.787666/ 44.201157, val:  52.65%, val_best:  79.20%, tr:  96.16%, tr_best:  96.16%, epoch time: 246.23 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 741888 real_backward_count 153750  20.724%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3363.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 441 occurrences\n",
      "test - Value 1: 11 occurrences\n",
      "epoch-23  lr=['2.0000000'], tr/val_loss: 50.656982/ 38.729843, val:  51.99%, val_best:  79.20%, tr:  91.91%, tr_best:  96.16%, epoch time: 248.14 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6869%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 774144 real_backward_count 160427  20.723%\n",
      "layer   1  Sparsity: 76.9287%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3428.0\n",
      "fc layer 2 self.abs_max_out: 3530.0\n",
      "train - Value 0: 2054 occurrences\n",
      "train - Value 1: 1978 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 415 occurrences\n",
      "test - Value 1: 37 occurrences\n",
      "epoch-24  lr=['2.0000000'], tr/val_loss: 65.839523/ 33.451252, val:  58.19%, val_best:  79.20%, tr:  92.66%, tr_best:  96.16%, epoch time: 247.64 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9380%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 806400 real_backward_count 167075  20.719%\n",
      "layer   1  Sparsity: 74.2432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3542.0\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-25  lr=['2.0000000'], tr/val_loss: 62.654346/105.971283, val:  50.00%, val_best:  79.20%, tr:  95.76%, tr_best:  96.16%, epoch time: 247.64 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 838656 real_backward_count 173582  20.698%\n",
      "layer   1  Sparsity: 77.8564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3576.0\n",
      "fc layer 2 self.abs_max_out: 3616.0\n",
      "train - Value 0: 1976 occurrences\n",
      "train - Value 1: 2056 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-26  lr=['2.0000000'], tr/val_loss: 51.857048/ 51.572151, val:  50.00%, val_best:  79.20%, tr:  95.88%, tr_best:  96.16%, epoch time: 248.32 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870912 real_backward_count 179991  20.667%\n",
      "layer   1  Sparsity: 73.9990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3635.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 8964.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 14 occurrences\n",
      "test - Value 1: 438 occurrences\n",
      "epoch-27  lr=['2.0000000'], tr/val_loss: 59.684269/ 69.445969, val:  53.10%, val_best:  79.20%, tr:  95.98%, tr_best:  96.16%, epoch time: 247.25 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4762%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 903168 real_backward_count 186434  20.642%\n",
      "layer   1  Sparsity: 81.9824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3674.0\n",
      "train - Value 0: 1990 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 431 occurrences\n",
      "test - Value 1: 21 occurrences\n",
      "epoch-28  lr=['2.0000000'], tr/val_loss: 66.412514/ 42.326954, val:  54.65%, val_best:  79.20%, tr:  96.33%, tr_best:  96.33%, epoch time: 248.10 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4022%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 935424 real_backward_count 192858  20.617%\n",
      "layer   1  Sparsity: 82.2998%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1971 occurrences\n",
      "train - Value 1: 2061 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 9084.0\n",
      "lif layer 1 self.abs_max_v: 9257.5\n",
      "lif layer 1 self.abs_max_v: 9779.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-29  lr=['2.0000000'], tr/val_loss: 51.259682/ 73.881996, val:  50.88%, val_best:  79.20%, tr:  94.87%, tr_best:  96.33%, epoch time: 248.02 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967680 real_backward_count 199300  20.596%\n",
      "layer   1  Sparsity: 90.6250%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 405 occurrences\n",
      "test - Value 1: 47 occurrences\n",
      "epoch-30  lr=['2.0000000'], tr/val_loss: 54.929371/ 53.664059, val:  59.51%, val_best:  79.20%, tr:  97.45%, tr_best:  97.45%, epoch time: 248.04 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 999936 real_backward_count 205578  20.559%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 49.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 2 self.abs_max_out: 3684.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 440 occurrences\n",
      "test - Value 1: 12 occurrences\n",
      "epoch-31  lr=['2.0000000'], tr/val_loss: 51.797554/ 20.055595, val:  52.65%, val_best:  79.20%, tr:  97.30%, tr_best:  97.45%, epoch time: 248.49 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1032192 real_backward_count 211961  20.535%\n",
      "layer   1  Sparsity: 76.0986%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 6721.0\n",
      "fc layer 1 self.abs_max_out: 6897.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 183 occurrences\n",
      "test - Value 1: 269 occurrences\n",
      "epoch-32  lr=['2.0000000'], tr/val_loss: 47.306858/ 69.018112, val:  81.19%, val_best:  81.19%, tr:  97.37%, tr_best:  97.45%, epoch time: 246.33 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9923%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1064448 real_backward_count 218118  20.491%\n",
      "layer   1  Sparsity: 91.4795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7069.0\n",
      "fc layer 1 self.abs_max_out: 7094.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 400 occurrences\n",
      "test - Value 1: 52 occurrences\n",
      "epoch-33  lr=['2.0000000'], tr/val_loss: 43.272400/ 21.862341, val:  60.62%, val_best:  81.19%, tr:  97.84%, tr_best:  97.84%, epoch time: 247.98 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2380%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096704 real_backward_count 224458  20.467%\n",
      "layer   1  Sparsity: 76.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7285.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1170.00 at epoch 34, iter 4031\n",
      "max_activation_accul updated: 1200.00 at epoch 34, iter 4031\n",
      "max_activation_accul updated: 1217.00 at epoch 34, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 428 occurrences\n",
      "test - Value 1: 24 occurrences\n",
      "epoch-34  lr=['2.0000000'], tr/val_loss: 45.919334/ 46.983021, val:  55.31%, val_best:  81.19%, tr:  96.80%, tr_best:  97.84%, epoch time: 248.22 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6331%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8850%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1128960 real_backward_count 230650  20.430%\n",
      "layer   1  Sparsity: 88.6230%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3747.0\n",
      "train - Value 0: 2058 occurrences\n",
      "train - Value 1: 1974 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 54 occurrences\n",
      "test - Value 1: 398 occurrences\n",
      "epoch-35  lr=['2.0000000'], tr/val_loss: 41.581013/ 53.048885, val:  61.50%, val_best:  81.19%, tr:  96.38%, tr_best:  97.84%, epoch time: 248.50 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2478%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1161216 real_backward_count 237082  20.417%\n",
      "layer   1  Sparsity: 95.0195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7384.0\n",
      "fc layer 1 self.abs_max_out: 7397.0\n",
      "fc layer 2 self.abs_max_out: 3802.0\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-36  lr=['2.0000000'], tr/val_loss: 48.698483/ 54.616241, val:  50.00%, val_best:  81.19%, tr:  94.67%, tr_best:  97.84%, epoch time: 247.48 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3567%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1193472 real_backward_count 243518  20.404%\n",
      "layer   1  Sparsity: 81.8848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3834.0\n",
      "train - Value 0: 2042 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-37  lr=['2.0000000'], tr/val_loss: 57.497608/ 69.498924, val:  50.00%, val_best:  81.19%, tr:  94.05%, tr_best:  97.84%, epoch time: 247.49 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9677%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1225728 real_backward_count 250068  20.402%\n",
      "layer   1  Sparsity: 71.6797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3858.0\n",
      "train - Value 0: 2101 occurrences\n",
      "train - Value 1: 1931 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 258 occurrences\n",
      "test - Value 1: 194 occurrences\n",
      "epoch-38  lr=['2.0000000'], tr/val_loss: 35.779850/ 41.883148, val:  77.43%, val_best:  81.19%, tr:  91.10%, tr_best:  97.84%, epoch time: 248.50 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257984 real_backward_count 256646  20.401%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7494.0\n",
      "fc layer 1 self.abs_max_out: 7526.0\n",
      "train - Value 0: 2104 occurrences\n",
      "train - Value 1: 1928 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 9853.5\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-39  lr=['2.0000000'], tr/val_loss: 45.773842/ 43.923962, val:  50.00%, val_best:  81.19%, tr:  92.91%, tr_best:  97.84%, epoch time: 248.65 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2653%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1290240 real_backward_count 263220  20.401%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7565.0\n",
      "train - Value 0: 2060 occurrences\n",
      "train - Value 1: 1972 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 7772.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 337 occurrences\n",
      "test - Value 1: 115 occurrences\n",
      "epoch-40  lr=['2.0000000'], tr/val_loss: 57.318348/ 39.182796, val:  71.46%, val_best:  81.19%, tr:  93.45%, tr_best:  97.84%, epoch time: 248.08 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1322496 real_backward_count 269581  20.384%\n",
      "layer   1  Sparsity: 84.2773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 7817.0\n",
      "fc layer 1 self.abs_max_out: 7898.0\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 8016.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 194 occurrences\n",
      "test - Value 1: 258 occurrences\n",
      "epoch-41  lr=['2.0000000'], tr/val_loss: 64.597443/ 47.766590, val:  81.86%, val_best:  81.86%, tr:  96.30%, tr_best:  97.84%, epoch time: 246.89 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354752 real_backward_count 275882  20.364%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 8156.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 431 occurrences\n",
      "test - Value 1: 21 occurrences\n",
      "epoch-42  lr=['2.0000000'], tr/val_loss: 64.472961/ 21.925467, val:  54.20%, val_best:  81.86%, tr:  96.60%, tr_best:  97.84%, epoch time: 248.23 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1387008 real_backward_count 282105  20.339%\n",
      "layer   1  Sparsity: 80.7861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8298.0\n",
      "fc layer 2 self.abs_max_out: 3891.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 8417.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 211 occurrences\n",
      "test - Value 1: 241 occurrences\n",
      "epoch-43  lr=['2.0000000'], tr/val_loss: 57.869453/  8.875776, val:  78.54%, val_best:  81.86%, tr:  93.90%, tr_best:  97.84%, epoch time: 247.59 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419264 real_backward_count 288664  20.339%\n",
      "layer   1  Sparsity: 89.6240%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 14 occurrences\n",
      "test - Value 1: 438 occurrences\n",
      "epoch-44  lr=['2.0000000'], tr/val_loss: 53.787373/ 68.676575, val:  53.10%, val_best:  81.86%, tr:  92.46%, tr_best:  97.84%, epoch time: 247.96 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1451520 real_backward_count 295049  20.327%\n",
      "layer   1  Sparsity: 83.5449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8446.0\n",
      "fc layer 1 self.abs_max_out: 8611.0\n",
      "train - Value 0: 2074 occurrences\n",
      "train - Value 1: 1958 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-45  lr=['2.0000000'], tr/val_loss: 43.703354/ 19.850084, val:  50.00%, val_best:  81.86%, tr:  96.63%, tr_best:  97.84%, epoch time: 247.91 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1483776 real_backward_count 301162  20.297%\n",
      "layer   1  Sparsity: 94.3604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 9950.5\n",
      "lif layer 1 self.abs_max_v: 9951.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 10095.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 451 occurrences\n",
      "test - Value 1: 1 occurrences\n",
      "epoch-46  lr=['2.0000000'], tr/val_loss: 52.155170/ 48.293739, val:  50.22%, val_best:  81.86%, tr:  96.80%, tr_best:  97.84%, epoch time: 246.75 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1516032 real_backward_count 307680  20.295%\n",
      "layer   1  Sparsity: 76.4893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 449 occurrences\n",
      "test - Value 1: 3 occurrences\n",
      "epoch-47  lr=['2.0000000'], tr/val_loss: 55.036873/ 20.078592, val:  50.66%, val_best:  81.86%, tr:  97.77%, tr_best:  97.84%, epoch time: 247.73 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1548288 real_backward_count 313967  20.278%\n",
      "layer   1  Sparsity: 85.2783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 4726.5\n",
      "lif layer 2 self.abs_max_v: 4805.0\n",
      "fc layer 1 self.abs_max_out: 8645.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 300 occurrences\n",
      "test - Value 1: 152 occurrences\n",
      "epoch-48  lr=['2.0000000'], tr/val_loss: 56.302505/ 26.467342, val:  76.55%, val_best:  81.86%, tr:  94.99%, tr_best:  97.84%, epoch time: 247.82 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1580544 real_backward_count 320150  20.256%\n",
      "layer   1  Sparsity: 90.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 10126.5\n",
      "lif layer 2 self.abs_max_v: 4815.0\n",
      "lif layer 2 self.abs_max_v: 4876.5\n",
      "lif layer 2 self.abs_max_v: 4938.5\n",
      "train - Value 0: 1981 occurrences\n",
      "train - Value 1: 2051 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-49  lr=['2.0000000'], tr/val_loss: 53.434029/ 79.249718, val:  50.00%, val_best:  81.86%, tr:  95.26%, tr_best:  97.84%, epoch time: 247.88 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1612800 real_backward_count 326353  20.235%\n",
      "layer   1  Sparsity: 82.8613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8676.0\n",
      "lif layer 1 self.abs_max_v: 10322.5\n",
      "lif layer 1 self.abs_max_v: 10416.0\n",
      "train - Value 0: 1960 occurrences\n",
      "train - Value 1: 2072 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 8680.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-50  lr=['2.0000000'], tr/val_loss: 62.047115/ 60.459080, val:  50.00%, val_best:  81.86%, tr:  94.20%, tr_best:  97.84%, epoch time: 245.05 seconds, 4.08 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9170%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1645056 real_backward_count 332817  20.231%\n",
      "layer   1  Sparsity: 77.3926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 10559.0\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2066 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 172 occurrences\n",
      "test - Value 1: 280 occurrences\n",
      "epoch-51  lr=['2.0000000'], tr/val_loss: 51.303997/ 49.074539, val:  78.32%, val_best:  81.86%, tr:  92.61%, tr_best:  97.84%, epoch time: 247.30 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1677312 real_backward_count 339415  20.236%\n",
      "layer   1  Sparsity: 88.3789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8786.0\n",
      "lif layer 1 self.abs_max_v: 10619.0\n",
      "fc layer 1 self.abs_max_out: 8828.0\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2041 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 9 occurrences\n",
      "test - Value 1: 443 occurrences\n",
      "epoch-52  lr=['2.0000000'], tr/val_loss: 59.539082/ 95.440071, val:  51.99%, val_best:  81.86%, tr:  96.95%, tr_best:  97.84%, epoch time: 247.19 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1709568 real_backward_count 345329  20.200%\n",
      "layer   1  Sparsity: 92.7490%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2066 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 399 occurrences\n",
      "test - Value 1: 53 occurrences\n",
      "epoch-53  lr=['2.0000000'], tr/val_loss: 55.466919/ 53.738880, val:  61.28%, val_best:  81.86%, tr:  96.78%, tr_best:  97.84%, epoch time: 245.85 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1741824 real_backward_count 351522  20.181%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2005 occurrences\n",
      "train - Value 1: 2027 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-54  lr=['2.0000000'], tr/val_loss: 64.116493/ 31.391113, val:  51.33%, val_best:  81.86%, tr:  94.97%, tr_best:  97.84%, epoch time: 247.92 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1774080 real_backward_count 357869  20.172%\n",
      "layer   1  Sparsity: 82.2510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 10638.5\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-55  lr=['2.0000000'], tr/val_loss: 62.615341/ 96.945946, val:  50.00%, val_best:  81.86%, tr:  95.61%, tr_best:  97.84%, epoch time: 246.64 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1806336 real_backward_count 364026  20.153%\n",
      "layer   1  Sparsity: 82.3242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 10858.0\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2044 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1298.00 at epoch 56, iter 4031\n",
      "fc layer 1 self.abs_max_out: 8833.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-56  lr=['2.0000000'], tr/val_loss: 68.123558/ 85.379707, val:  50.00%, val_best:  81.86%, tr:  96.43%, tr_best:  97.84%, epoch time: 247.17 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1838592 real_backward_count 370182  20.134%\n",
      "layer   1  Sparsity: 90.3564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 8981.0\n",
      "lif layer 1 self.abs_max_v: 11179.0\n",
      "lif layer 2 self.abs_max_v: 4958.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 9000.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 432 occurrences\n",
      "test - Value 1: 20 occurrences\n",
      "epoch-57  lr=['2.0000000'], tr/val_loss: 64.861046/ 53.616550, val:  54.42%, val_best:  81.86%, tr:  98.31%, tr_best:  98.31%, epoch time: 248.64 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1870848 real_backward_count 376246  20.111%\n",
      "layer   1  Sparsity: 92.8955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9185.0\n",
      "lif layer 1 self.abs_max_v: 11188.0\n",
      "lif layer 1 self.abs_max_v: 11410.5\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 9191.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 442 occurrences\n",
      "test - Value 1: 10 occurrences\n",
      "epoch-58  lr=['2.0000000'], tr/val_loss: 59.148335/ 58.085098, val:  52.21%, val_best:  81.86%, tr:  97.40%, tr_best:  98.31%, epoch time: 247.45 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8435%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1903104 real_backward_count 382330  20.090%\n",
      "layer   1  Sparsity: 79.9316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 11463.0\n",
      "lif layer 2 self.abs_max_v: 4996.0\n",
      "fc layer 1 self.abs_max_out: 9196.0\n",
      "lif layer 2 self.abs_max_v: 5015.5\n",
      "lif layer 2 self.abs_max_v: 5054.0\n",
      "fc layer 1 self.abs_max_out: 9225.0\n",
      "fc layer 1 self.abs_max_out: 9306.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "fc layer 1 self.abs_max_out: 9406.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 7 occurrences\n",
      "test - Value 1: 445 occurrences\n",
      "epoch-59  lr=['2.0000000'], tr/val_loss: 57.536144/ 54.788635, val:  51.55%, val_best:  81.86%, tr:  96.30%, tr_best:  98.31%, epoch time: 247.29 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1935360 real_backward_count 388641  20.081%\n",
      "layer   1  Sparsity: 84.2773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9475.0\n",
      "lif layer 1 self.abs_max_v: 11607.0\n",
      "lif layer 2 self.abs_max_v: 5055.0\n",
      "lif layer 2 self.abs_max_v: 5060.5\n",
      "fc layer 1 self.abs_max_out: 9538.0\n",
      "lif layer 1 self.abs_max_v: 11813.0\n",
      "lif layer 2 self.abs_max_v: 5190.0\n",
      "lif layer 2 self.abs_max_v: 5204.0\n",
      "lif layer 1 self.abs_max_v: 11899.0\n",
      "train - Value 0: 1972 occurrences\n",
      "train - Value 1: 2060 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 281 occurrences\n",
      "test - Value 1: 171 occurrences\n",
      "epoch-60  lr=['2.0000000'], tr/val_loss: 59.688950/ 35.930054, val:  79.42%, val_best:  81.86%, tr:  96.13%, tr_best:  98.31%, epoch time: 246.33 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9065%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1967616 real_backward_count 394973  20.074%\n",
      "layer   1  Sparsity: 75.9766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5207.0\n",
      "lif layer 2 self.abs_max_v: 5265.5\n",
      "lif layer 2 self.abs_max_v: 5359.0\n",
      "lif layer 2 self.abs_max_v: 5394.5\n",
      "train - Value 0: 2129 occurrences\n",
      "train - Value 1: 1903 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-61  lr=['2.0000000'], tr/val_loss: 45.528740/ 52.743942, val:  50.00%, val_best:  81.86%, tr:  90.10%, tr_best:  98.31%, epoch time: 247.82 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1999872 real_backward_count 401533  20.078%\n",
      "layer   1  Sparsity: 79.2969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5412.5\n",
      "lif layer 2 self.abs_max_v: 5418.0\n",
      "lif layer 2 self.abs_max_v: 5428.5\n",
      "lif layer 2 self.abs_max_v: 5453.0\n",
      "lif layer 2 self.abs_max_v: 5454.5\n",
      "train - Value 0: 1845 occurrences\n",
      "train - Value 1: 2187 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 435 occurrences\n",
      "test - Value 1: 17 occurrences\n",
      "epoch-62  lr=['2.0000000'], tr/val_loss: 47.875656/ 56.295952, val:  53.76%, val_best:  81.86%, tr:  88.37%, tr_best:  98.31%, epoch time: 247.15 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2032128 real_backward_count 408102  20.082%\n",
      "layer   1  Sparsity: 81.5674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 5549.5\n",
      "lif layer 1 self.abs_max_v: 11966.5\n",
      "train - Value 0: 1976 occurrences\n",
      "train - Value 1: 2056 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 431 occurrences\n",
      "test - Value 1: 21 occurrences\n",
      "epoch-63  lr=['2.0000000'], tr/val_loss: 53.985401/ 17.119356, val:  54.65%, val_best:  81.86%, tr:  89.88%, tr_best:  98.31%, epoch time: 247.27 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7965%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2064384 real_backward_count 414739  20.090%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 11975.0\n",
      "train - Value 0: 1903 occurrences\n",
      "train - Value 1: 2129 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 437 occurrences\n",
      "test - Value 1: 15 occurrences\n",
      "epoch-64  lr=['2.0000000'], tr/val_loss: 59.197845/ 44.069836, val:  53.32%, val_best:  81.86%, tr:  93.13%, tr_best:  98.31%, epoch time: 246.62 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2096640 real_backward_count 421135  20.086%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12038.5\n",
      "train - Value 0: 1908 occurrences\n",
      "train - Value 1: 2124 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-65  lr=['2.0000000'], tr/val_loss: 71.468277/ 73.826363, val:  51.11%, val_best:  81.86%, tr:  92.41%, tr_best:  98.31%, epoch time: 246.23 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2128896 real_backward_count 427670  20.089%\n",
      "layer   1  Sparsity: 90.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12039.0\n",
      "train - Value 0: 1954 occurrences\n",
      "train - Value 1: 2078 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-66  lr=['2.0000000'], tr/val_loss: 58.867992/ 12.416414, val:  50.00%, val_best:  81.86%, tr:  92.66%, tr_best:  98.31%, epoch time: 248.09 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5504%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2711%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2161152 real_backward_count 434264  20.094%\n",
      "layer   1  Sparsity: 79.6875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2049 occurrences\n",
      "train - Value 1: 1983 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-67  lr=['2.0000000'], tr/val_loss: 47.196957/ 17.811947, val:  82.30%, val_best:  82.30%, tr:  95.56%, tr_best:  98.31%, epoch time: 247.72 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2193408 real_backward_count 440452  20.081%\n",
      "layer   1  Sparsity: 84.0088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-68  lr=['2.0000000'], tr/val_loss: 62.825851/ 58.974659, val:  74.78%, val_best:  82.30%, tr:  96.88%, tr_best:  98.31%, epoch time: 247.39 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.4481%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2225664 real_backward_count 446837  20.077%\n",
      "layer   1  Sparsity: 90.7715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 352 occurrences\n",
      "test - Value 1: 100 occurrences\n",
      "epoch-69  lr=['2.0000000'], tr/val_loss: 58.947071/ 10.611009, val:  70.35%, val_best:  82.30%, tr:  94.17%, tr_best:  98.31%, epoch time: 247.36 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2257920 real_backward_count 453368  20.079%\n",
      "layer   1  Sparsity: 78.4668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 48.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1915 occurrences\n",
      "train - Value 1: 2117 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 60 occurrences\n",
      "test - Value 1: 392 occurrences\n",
      "epoch-70  lr=['2.0000000'], tr/val_loss: 53.594284/ 52.830379, val:  62.83%, val_best:  82.30%, tr:  92.34%, tr_best:  98.31%, epoch time: 245.93 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.7601%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2290176 real_backward_count 459770  20.076%\n",
      "layer   1  Sparsity: 74.6094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2107 occurrences\n",
      "train - Value 1: 1925 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-71  lr=['2.0000000'], tr/val_loss: 51.808861/  5.917407, val:  50.00%, val_best:  82.30%, tr:  92.58%, tr_best:  98.31%, epoch time: 246.97 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2322432 real_backward_count 466286  20.077%\n",
      "layer   1  Sparsity: 80.7861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2140 occurrences\n",
      "train - Value 1: 1892 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 64 occurrences\n",
      "test - Value 1: 388 occurrences\n",
      "epoch-72  lr=['2.0000000'], tr/val_loss: 35.744438/ 19.867884, val:  62.83%, val_best:  82.30%, tr:  92.31%, tr_best:  98.31%, epoch time: 247.77 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2354688 real_backward_count 472681  20.074%\n",
      "layer   1  Sparsity: 83.0078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12057.5\n",
      "fc layer 1 self.abs_max_out: 9584.0\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 203 occurrences\n",
      "test - Value 1: 249 occurrences\n",
      "epoch-73  lr=['2.0000000'], tr/val_loss: 38.709114/ 17.221943, val:  81.19%, val_best:  82.30%, tr:  90.28%, tr_best:  98.31%, epoch time: 247.65 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.2617%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2386944 real_backward_count 479181  20.075%\n",
      "layer   1  Sparsity: 88.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12078.5\n",
      "fc layer 1 self.abs_max_out: 9677.0\n",
      "train - Value 0: 2066 occurrences\n",
      "train - Value 1: 1966 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 59 occurrences\n",
      "test - Value 1: 393 occurrences\n",
      "epoch-74  lr=['2.0000000'], tr/val_loss: 40.174404/ 58.171906, val:  62.61%, val_best:  82.30%, tr:  93.70%, tr_best:  98.31%, epoch time: 246.32 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.0915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2419200 real_backward_count 485432  20.066%\n",
      "layer   1  Sparsity: 86.9141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12087.5\n",
      "train - Value 0: 2055 occurrences\n",
      "train - Value 1: 1977 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-75  lr=['2.0000000'], tr/val_loss: 43.767693/ 20.581757, val:  50.44%, val_best:  82.30%, tr:  93.28%, tr_best:  98.31%, epoch time: 248.14 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.8493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2451456 real_backward_count 491796  20.061%\n",
      "layer   1  Sparsity: 88.3301%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12152.5\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 12200.0\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-76  lr=['2.0000000'], tr/val_loss: 32.808533/ 21.749640, val:  50.44%, val_best:  82.30%, tr:  93.25%, tr_best:  98.31%, epoch time: 247.93 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2483712 real_backward_count 498082  20.054%\n",
      "layer   1  Sparsity: 78.9062%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1937 occurrences\n",
      "train - Value 1: 2095 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 369 occurrences\n",
      "test - Value 1: 83 occurrences\n",
      "epoch-77  lr=['2.0000000'], tr/val_loss: 46.294727/ 43.816860, val:  67.48%, val_best:  82.30%, tr:  94.62%, tr_best:  98.31%, epoch time: 248.12 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2515968 real_backward_count 504520  20.053%\n",
      "layer   1  Sparsity: 79.7363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1975 occurrences\n",
      "train - Value 1: 2057 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 34 occurrences\n",
      "test - Value 1: 418 occurrences\n",
      "epoch-78  lr=['2.0000000'], tr/val_loss: 50.579826/ 41.978806, val:  57.52%, val_best:  82.30%, tr:  93.68%, tr_best:  98.31%, epoch time: 248.63 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9755%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2548224 real_backward_count 510857  20.048%\n",
      "layer   1  Sparsity: 87.4512%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9767.0\n",
      "train - Value 0: 1955 occurrences\n",
      "train - Value 1: 2077 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 211 occurrences\n",
      "test - Value 1: 241 occurrences\n",
      "epoch-79  lr=['2.0000000'], tr/val_loss: 53.219147/ 44.291473, val:  82.52%, val_best:  82.52%, tr:  91.00%, tr_best:  98.31%, epoch time: 246.95 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8474%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2580480 real_backward_count 517084  20.038%\n",
      "layer   1  Sparsity: 86.2061%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1945 occurrences\n",
      "train - Value 1: 2087 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 227 occurrences\n",
      "test - Value 1: 225 occurrences\n",
      "epoch-80  lr=['2.0000000'], tr/val_loss: 62.769962/ 45.424965, val:  83.41%, val_best:  83.41%, tr:  94.97%, tr_best:  98.31%, epoch time: 248.67 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9382%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2612736 real_backward_count 523238  20.026%\n",
      "layer   1  Sparsity: 89.7949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 9848.0\n",
      "fc layer 1 self.abs_max_out: 9870.0\n",
      "fc layer 1 self.abs_max_out: 9921.0\n",
      "lif layer 1 self.abs_max_v: 12234.5\n",
      "fc layer 1 self.abs_max_out: 9950.0\n",
      "train - Value 0: 2101 occurrences\n",
      "train - Value 1: 1931 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 183 occurrences\n",
      "test - Value 1: 269 occurrences\n",
      "epoch-81  lr=['2.0000000'], tr/val_loss: 50.389061/ 21.444632, val:  79.87%, val_best:  83.41%, tr:  94.77%, tr_best:  98.31%, epoch time: 248.15 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.7967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2644992 real_backward_count 529486  20.018%\n",
      "layer   1  Sparsity: 82.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12237.0\n",
      "lif layer 1 self.abs_max_v: 12243.5\n",
      "fc layer 1 self.abs_max_out: 9968.0\n",
      "train - Value 0: 2100 occurrences\n",
      "train - Value 1: 1932 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-82  lr=['2.0000000'], tr/val_loss: 44.411572/ 60.180099, val:  50.00%, val_best:  83.41%, tr:  89.63%, tr_best:  98.31%, epoch time: 247.28 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0768%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.6172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2677248 real_backward_count 535947  20.019%\n",
      "layer   1  Sparsity: 80.0049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10056.0\n",
      "train - Value 0: 2086 occurrences\n",
      "train - Value 1: 1946 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-83  lr=['2.0000000'], tr/val_loss: 42.239128/ 47.263443, val:  50.22%, val_best:  83.41%, tr:  91.67%, tr_best:  98.31%, epoch time: 246.61 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7175%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2709504 real_backward_count 542237  20.012%\n",
      "layer   1  Sparsity: 75.4883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10076.0\n",
      "train - Value 0: 1991 occurrences\n",
      "train - Value 1: 2041 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-84  lr=['2.0000000'], tr/val_loss: 42.722717/ 75.122749, val:  50.00%, val_best:  83.41%, tr:  89.61%, tr_best:  98.31%, epoch time: 246.23 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5076%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2741760 real_backward_count 548650  20.011%\n",
      "layer   1  Sparsity: 74.8779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10246.0\n",
      "lif layer 1 self.abs_max_v: 12253.0\n",
      "fc layer 1 self.abs_max_out: 10261.0\n",
      "train - Value 0: 1871 occurrences\n",
      "train - Value 1: 2161 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 38 occurrences\n",
      "test - Value 1: 414 occurrences\n",
      "epoch-85  lr=['2.0000000'], tr/val_loss: 51.235996/ 45.909061, val:  58.41%, val_best:  83.41%, tr:  91.64%, tr_best:  98.31%, epoch time: 247.70 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2774016 real_backward_count 554900  20.003%\n",
      "layer   1  Sparsity: 74.8047%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10303.0\n",
      "fc layer 1 self.abs_max_out: 10315.0\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2076 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-86  lr=['2.0000000'], tr/val_loss: 48.632278/ 83.438736, val:  50.00%, val_best:  83.41%, tr:  92.31%, tr_best:  98.31%, epoch time: 248.21 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2806272 real_backward_count 561100  19.994%\n",
      "layer   1  Sparsity: 82.7393%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3945.0\n",
      "lif layer 1 self.abs_max_v: 12275.5\n",
      "lif layer 1 self.abs_max_v: 12341.0\n",
      "fc layer 1 self.abs_max_out: 10353.0\n",
      "train - Value 0: 1928 occurrences\n",
      "train - Value 1: 2104 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 12351.0\n",
      "lif layer 1 self.abs_max_v: 12435.5\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 196 occurrences\n",
      "test - Value 1: 256 occurrences\n",
      "epoch-87  lr=['2.0000000'], tr/val_loss: 58.164864/ 39.950607, val:  75.66%, val_best:  83.41%, tr:  94.84%, tr_best:  98.31%, epoch time: 247.16 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2838528 real_backward_count 567263  19.984%\n",
      "layer   1  Sparsity: 80.7373%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1907 occurrences\n",
      "train - Value 1: 2125 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 58 occurrences\n",
      "test - Value 1: 394 occurrences\n",
      "epoch-88  lr=['2.0000000'], tr/val_loss: 54.232567/ 77.479729, val:  61.95%, val_best:  83.41%, tr:  93.68%, tr_best:  98.31%, epoch time: 247.37 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2870784 real_backward_count 573641  19.982%\n",
      "layer   1  Sparsity: 92.7246%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2135 occurrences\n",
      "train - Value 1: 1897 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-89  lr=['2.0000000'], tr/val_loss: 51.096302/ 44.193508, val:  50.00%, val_best:  83.41%, tr:  93.13%, tr_best:  98.31%, epoch time: 248.51 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2903040 real_backward_count 579746  19.970%\n",
      "layer   1  Sparsity: 85.4004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 3975.0\n",
      "train - Value 0: 2124 occurrences\n",
      "train - Value 1: 1908 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-90  lr=['2.0000000'], tr/val_loss: 41.406551/ 75.910980, val:  50.66%, val_best:  83.41%, tr:  95.29%, tr_best:  98.31%, epoch time: 247.66 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3958%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2935296 real_backward_count 586024  19.965%\n",
      "layer   1  Sparsity: 83.4229%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12456.5\n",
      "train - Value 0: 2079 occurrences\n",
      "train - Value 1: 1953 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 59 occurrences\n",
      "test - Value 1: 393 occurrences\n",
      "epoch-91  lr=['2.0000000'], tr/val_loss: 34.817768/ 34.955196, val:  63.05%, val_best:  83.41%, tr:  97.40%, tr_best:  98.31%, epoch time: 247.37 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2967552 real_backward_count 592227  19.957%\n",
      "layer   1  Sparsity: 82.1777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12469.0\n",
      "train - Value 0: 2088 occurrences\n",
      "train - Value 1: 1944 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 39 occurrences\n",
      "test - Value 1: 413 occurrences\n",
      "epoch-92  lr=['2.0000000'], tr/val_loss: 31.190434/ 60.751659, val:  58.63%, val_best:  83.41%, tr:  93.95%, tr_best:  98.31%, epoch time: 247.31 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2999808 real_backward_count 598672  19.957%\n",
      "layer   1  Sparsity: 82.2754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1749 occurrences\n",
      "train - Value 1: 2283 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 9 occurrences\n",
      "test - Value 1: 443 occurrences\n",
      "epoch-93  lr=['2.0000000'], tr/val_loss: 55.221527/ 65.087517, val:  51.99%, val_best:  83.41%, tr:  90.85%, tr_best:  98.31%, epoch time: 247.32 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3032064 real_backward_count 605120  19.957%\n",
      "layer   1  Sparsity: 79.3701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1930 occurrences\n",
      "train - Value 1: 2102 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 12600.5\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 363 occurrences\n",
      "test - Value 1: 89 occurrences\n",
      "epoch-94  lr=['2.0000000'], tr/val_loss: 49.918995/ 29.485785, val:  67.92%, val_best:  83.41%, tr:  90.28%, tr_best:  98.31%, epoch time: 248.27 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3064320 real_backward_count 611667  19.961%\n",
      "layer   1  Sparsity: 69.2383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2049 occurrences\n",
      "train - Value 1: 1983 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 212 occurrences\n",
      "test - Value 1: 240 occurrences\n",
      "epoch-95  lr=['2.0000000'], tr/val_loss: 41.039181/ 37.367832, val:  82.74%, val_best:  83.41%, tr:  92.83%, tr_best:  98.31%, epoch time: 248.44 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3096576 real_backward_count 617954  19.956%\n",
      "layer   1  Sparsity: 90.9180%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10440.0\n",
      "train - Value 0: 1908 occurrences\n",
      "train - Value 1: 2124 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1337.00 at epoch 96, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-96  lr=['2.0000000'], tr/val_loss: 56.826244/127.630501, val:  50.00%, val_best:  83.41%, tr:  92.81%, tr_best:  98.31%, epoch time: 247.42 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8805%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3128832 real_backward_count 624265  19.952%\n",
      "layer   1  Sparsity: 76.7090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 12602.0\n",
      "train - Value 0: 2094 occurrences\n",
      "train - Value 1: 1938 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "lif layer 1 self.abs_max_v: 12812.5\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-97  lr=['2.0000000'], tr/val_loss: 65.119965/ 34.480007, val:  50.00%, val_best:  83.41%, tr:  91.07%, tr_best:  98.31%, epoch time: 248.40 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3161088 real_backward_count 630711  19.952%\n",
      "layer   1  Sparsity: 82.8857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 85 occurrences\n",
      "test - Value 1: 367 occurrences\n",
      "epoch-98  lr=['2.0000000'], tr/val_loss: 60.776077/ 80.892014, val:  66.15%, val_best:  83.41%, tr:  94.02%, tr_best:  98.31%, epoch time: 247.41 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8031%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3193344 real_backward_count 636884  19.944%\n",
      "layer   1  Sparsity: 81.2256%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10488.0\n",
      "fc layer 1 self.abs_max_out: 10593.0\n",
      "fc layer 1 self.abs_max_out: 10597.0\n",
      "lif layer 1 self.abs_max_v: 12930.5\n",
      "fc layer 1 self.abs_max_out: 10782.0\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 246 occurrences\n",
      "test - Value 1: 206 occurrences\n",
      "epoch-99  lr=['2.0000000'], tr/val_loss: 62.879944/ 75.115265, val:  77.88%, val_best:  83.41%, tr:  91.91%, tr_best:  98.31%, epoch time: 248.09 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3939%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3225600 real_backward_count 643206  19.941%\n",
      "layer   1  Sparsity: 84.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10786.0\n",
      "fc layer 1 self.abs_max_out: 10817.0\n",
      "fc layer 1 self.abs_max_out: 10821.0\n",
      "lif layer 1 self.abs_max_v: 13028.0\n",
      "train - Value 0: 1918 occurrences\n",
      "train - Value 1: 2114 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 398 occurrences\n",
      "test - Value 1: 54 occurrences\n",
      "epoch-100 lr=['2.0000000'], tr/val_loss: 62.482059/ 35.951973, val:  61.06%, val_best:  83.41%, tr:  90.18%, tr_best:  98.31%, epoch time: 247.65 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0923%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3257856 real_backward_count 649523  19.937%\n",
      "layer   1  Sparsity: 69.5312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10940.0\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 223 occurrences\n",
      "test - Value 1: 229 occurrences\n",
      "epoch-101 lr=['2.0000000'], tr/val_loss: 70.928268/ 42.724205, val:  82.96%, val_best:  83.41%, tr:  93.90%, tr_best:  98.31%, epoch time: 247.77 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3290112 real_backward_count 655883  19.935%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 185 occurrences\n",
      "test - Value 1: 267 occurrences\n",
      "epoch-102 lr=['2.0000000'], tr/val_loss: 65.011009/ 61.314575, val:  78.54%, val_best:  83.41%, tr:  96.68%, tr_best:  98.31%, epoch time: 245.49 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9417%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3322368 real_backward_count 662015  19.926%\n",
      "layer   1  Sparsity: 71.6309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 157 occurrences\n",
      "test - Value 1: 295 occurrences\n",
      "epoch-103 lr=['2.0000000'], tr/val_loss: 53.485046/ 83.988785, val:  76.33%, val_best:  83.41%, tr:  94.69%, tr_best:  98.31%, epoch time: 249.29 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9175%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3354624 real_backward_count 668240  19.920%\n",
      "layer   1  Sparsity: 80.1270%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2042 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 28 occurrences\n",
      "test - Value 1: 424 occurrences\n",
      "epoch-104 lr=['2.0000000'], tr/val_loss: 59.998638/ 53.469311, val:  56.19%, val_best:  83.41%, tr:  97.27%, tr_best:  98.31%, epoch time: 247.33 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9469%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3386880 real_backward_count 674137  19.904%\n",
      "layer   1  Sparsity: 77.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 10961.0\n",
      "fc layer 1 self.abs_max_out: 11083.0\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-105 lr=['2.0000000'], tr/val_loss: 50.757744/ 73.910316, val:  50.22%, val_best:  83.41%, tr:  94.52%, tr_best:  98.31%, epoch time: 248.05 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.1263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3419136 real_backward_count 680324  19.898%\n",
      "layer   1  Sparsity: 85.7666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2049 occurrences\n",
      "train - Value 1: 1983 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 40 occurrences\n",
      "test - Value 1: 412 occurrences\n",
      "epoch-106 lr=['2.0000000'], tr/val_loss: 52.497601/ 90.162315, val:  58.85%, val_best:  83.41%, tr:  95.46%, tr_best:  98.31%, epoch time: 248.23 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.0604%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3451392 real_backward_count 686299  19.885%\n",
      "layer   1  Sparsity: 91.3574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11097.0\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 72 occurrences\n",
      "test - Value 1: 380 occurrences\n",
      "epoch-107 lr=['2.0000000'], tr/val_loss: 62.194122/ 54.163174, val:  65.04%, val_best:  83.41%, tr:  93.58%, tr_best:  98.31%, epoch time: 246.03 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8825%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3483648 real_backward_count 692256  19.872%\n",
      "layer   1  Sparsity: 87.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2046 occurrences\n",
      "train - Value 1: 1986 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 281 occurrences\n",
      "test - Value 1: 171 occurrences\n",
      "epoch-108 lr=['2.0000000'], tr/val_loss: 56.761429/ 35.861870, val:  80.75%, val_best:  83.41%, tr:  96.92%, tr_best:  98.31%, epoch time: 246.77 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3515904 real_backward_count 697975  19.852%\n",
      "layer   1  Sparsity: 86.3525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2110 occurrences\n",
      "train - Value 1: 1922 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 41 occurrences\n",
      "test - Value 1: 411 occurrences\n",
      "epoch-109 lr=['2.0000000'], tr/val_loss: 43.963825/ 28.712023, val:  59.07%, val_best:  83.41%, tr:  96.18%, tr_best:  98.31%, epoch time: 246.46 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3548160 real_backward_count 704011  19.842%\n",
      "layer   1  Sparsity: 89.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13031.0\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-110 lr=['2.0000000'], tr/val_loss: 38.965881/ 71.830673, val:  50.44%, val_best:  83.41%, tr:  98.69%, tr_best:  98.69%, epoch time: 248.04 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3580416 real_backward_count 709660  19.821%\n",
      "layer   1  Sparsity: 81.3232%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13038.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 277 occurrences\n",
      "test - Value 1: 175 occurrences\n",
      "epoch-111 lr=['2.0000000'], tr/val_loss: 44.034889/ 41.212708, val:  76.77%, val_best:  83.41%, tr:  92.41%, tr_best:  98.69%, epoch time: 246.25 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.2341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3612672 real_backward_count 716019  19.820%\n",
      "layer   1  Sparsity: 80.8594%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13053.5\n",
      "train - Value 0: 2089 occurrences\n",
      "train - Value 1: 1943 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 56 occurrences\n",
      "test - Value 1: 396 occurrences\n",
      "epoch-112 lr=['2.0000000'], tr/val_loss: 43.441460/ 56.487328, val:  62.39%, val_best:  83.41%, tr:  94.22%, tr_best:  98.69%, epoch time: 243.80 seconds, 4.06 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6738%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3644928 real_backward_count 722275  19.816%\n",
      "layer   1  Sparsity: 85.9131%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13075.5\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 272 occurrences\n",
      "test - Value 1: 180 occurrences\n",
      "epoch-113 lr=['2.0000000'], tr/val_loss: 42.472351/ 16.005331, val:  79.20%, val_best:  83.41%, tr:  98.71%, tr_best:  98.71%, epoch time: 247.15 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.5301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3677184 real_backward_count 727864  19.794%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11127.0\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 362 occurrences\n",
      "test - Value 1: 90 occurrences\n",
      "epoch-114 lr=['2.0000000'], tr/val_loss: 59.134106/ 28.116093, val:  68.58%, val_best:  83.41%, tr:  98.59%, tr_best:  98.71%, epoch time: 247.66 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3709440 real_backward_count 733627  19.777%\n",
      "layer   1  Sparsity: 80.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13122.5\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 236 occurrences\n",
      "test - Value 1: 216 occurrences\n",
      "epoch-115 lr=['2.0000000'], tr/val_loss: 53.368195/ 19.839222, val:  80.97%, val_best:  83.41%, tr:  96.63%, tr_best:  98.71%, epoch time: 245.54 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3741696 real_backward_count 739832  19.773%\n",
      "layer   1  Sparsity: 87.3779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 423 occurrences\n",
      "test - Value 1: 29 occurrences\n",
      "epoch-116 lr=['2.0000000'], tr/val_loss: 50.490509/ 26.201576, val:  55.97%, val_best:  83.41%, tr:  97.50%, tr_best:  98.71%, epoch time: 247.30 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3773952 real_backward_count 745977  19.766%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 154 occurrences\n",
      "test - Value 1: 298 occurrences\n",
      "epoch-117 lr=['2.0000000'], tr/val_loss: 55.540466/ 50.054485, val:  77.88%, val_best:  83.41%, tr:  96.33%, tr_best:  98.71%, epoch time: 246.75 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3806208 real_backward_count 752267  19.764%\n",
      "layer   1  Sparsity: 87.9639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11166.0\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 79 occurrences\n",
      "test - Value 1: 373 occurrences\n",
      "epoch-118 lr=['2.0000000'], tr/val_loss: 69.913216/ 67.772728, val:  66.59%, val_best:  83.41%, tr:  97.54%, tr_best:  98.71%, epoch time: 247.07 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0367%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3838464 real_backward_count 758401  19.758%\n",
      "layer   1  Sparsity: 88.8672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11296.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 232 occurrences\n",
      "test - Value 1: 220 occurrences\n",
      "epoch-119 lr=['2.0000000'], tr/val_loss: 73.096680/ 72.801102, val:  83.19%, val_best:  83.41%, tr:  97.59%, tr_best:  98.71%, epoch time: 249.42 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3870720 real_backward_count 764500  19.751%\n",
      "layer   1  Sparsity: 87.5977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 187 occurrences\n",
      "test - Value 1: 265 occurrences\n",
      "epoch-120 lr=['2.0000000'], tr/val_loss: 63.243069/ 33.575157, val:  83.41%, val_best:  83.41%, tr:  96.95%, tr_best:  98.71%, epoch time: 248.13 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3902976 real_backward_count 770506  19.741%\n",
      "layer   1  Sparsity: 82.3730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11322.0\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-121 lr=['2.0000000'], tr/val_loss: 60.953915/ 88.743958, val:  50.00%, val_best:  83.41%, tr:  97.84%, tr_best:  98.71%, epoch time: 245.74 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3935232 real_backward_count 776566  19.734%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1947 occurrences\n",
      "train - Value 1: 2085 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-122 lr=['2.0000000'], tr/val_loss: 49.470856/ 63.466618, val:  50.22%, val_best:  83.41%, tr:  94.77%, tr_best:  98.71%, epoch time: 249.22 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4581%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3967488 real_backward_count 782734  19.729%\n",
      "layer   1  Sparsity: 88.7207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11369.0\n",
      "train - Value 0: 1938 occurrences\n",
      "train - Value 1: 2094 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 29 occurrences\n",
      "test - Value 1: 423 occurrences\n",
      "epoch-123 lr=['2.0000000'], tr/val_loss: 48.847282/ 55.884125, val:  56.42%, val_best:  83.41%, tr:  96.68%, tr_best:  98.71%, epoch time: 248.45 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.2504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3999744 real_backward_count 788880  19.723%\n",
      "layer   1  Sparsity: 83.9111%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11525.0\n",
      "lif layer 1 self.abs_max_v: 13200.0\n",
      "fc layer 1 self.abs_max_out: 11567.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 448 occurrences\n",
      "test - Value 1: 4 occurrences\n",
      "epoch-124 lr=['2.0000000'], tr/val_loss: 52.828831/ 51.551392, val:  50.88%, val_best:  83.41%, tr:  96.78%, tr_best:  98.71%, epoch time: 248.19 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4032000 real_backward_count 794805  19.712%\n",
      "layer   1  Sparsity: 86.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11636.0\n",
      "lif layer 1 self.abs_max_v: 13214.5\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-125 lr=['2.0000000'], tr/val_loss: 44.021805/ 12.046409, val:  51.33%, val_best:  83.41%, tr:  95.54%, tr_best:  98.71%, epoch time: 247.52 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6187%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4064256 real_backward_count 800857  19.705%\n",
      "layer   1  Sparsity: 73.3154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11645.0\n",
      "fc layer 1 self.abs_max_out: 11684.0\n",
      "lif layer 1 self.abs_max_v: 13281.0\n",
      "train - Value 0: 2063 occurrences\n",
      "train - Value 1: 1969 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 10 occurrences\n",
      "test - Value 1: 442 occurrences\n",
      "epoch-126 lr=['2.0000000'], tr/val_loss: 38.054352/ 56.826962, val:  52.21%, val_best:  83.41%, tr:  96.11%, tr_best:  98.71%, epoch time: 246.26 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1677%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4096512 real_backward_count 806916  19.698%\n",
      "layer   1  Sparsity: 77.4170%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11701.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 399 occurrences\n",
      "test - Value 1: 53 occurrences\n",
      "epoch-127 lr=['2.0000000'], tr/val_loss: 40.496956/ 13.385389, val:  61.73%, val_best:  83.41%, tr:  95.01%, tr_best:  98.71%, epoch time: 248.58 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4128768 real_backward_count 813111  19.694%\n",
      "layer   1  Sparsity: 91.1133%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2083 occurrences\n",
      "train - Value 1: 1949 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 414 occurrences\n",
      "test - Value 1: 38 occurrences\n",
      "epoch-128 lr=['2.0000000'], tr/val_loss: 44.211975/  6.094178, val:  58.41%, val_best:  83.41%, tr:  94.92%, tr_best:  98.71%, epoch time: 248.13 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2076%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4161024 real_backward_count 819187  19.687%\n",
      "layer   1  Sparsity: 86.2061%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 202 occurrences\n",
      "test - Value 1: 250 occurrences\n",
      "epoch-129 lr=['2.0000000'], tr/val_loss: 46.119926/  8.424452, val:  82.74%, val_best:  83.41%, tr:  93.80%, tr_best:  98.71%, epoch time: 246.83 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4193280 real_backward_count 825337  19.682%\n",
      "layer   1  Sparsity: 90.5029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1937 occurrences\n",
      "train - Value 1: 2095 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 368 occurrences\n",
      "test - Value 1: 84 occurrences\n",
      "epoch-130 lr=['2.0000000'], tr/val_loss: 47.358700/ 13.621041, val:  67.70%, val_best:  83.41%, tr:  95.76%, tr_best:  98.71%, epoch time: 245.94 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4225536 real_backward_count 831420  19.676%\n",
      "layer   1  Sparsity: 90.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-131 lr=['2.0000000'], tr/val_loss: 40.070045/ 68.706207, val:  50.00%, val_best:  83.41%, tr:  96.06%, tr_best:  98.71%, epoch time: 246.28 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0558%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4257792 real_backward_count 837459  19.669%\n",
      "layer   1  Sparsity: 79.1260%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1984 occurrences\n",
      "train - Value 1: 2048 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 424 occurrences\n",
      "test - Value 1: 28 occurrences\n",
      "epoch-132 lr=['2.0000000'], tr/val_loss: 63.060581/  7.816477, val:  56.19%, val_best:  83.41%, tr:  95.24%, tr_best:  98.71%, epoch time: 246.73 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4290048 real_backward_count 843598  19.664%\n",
      "layer   1  Sparsity: 88.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11731.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 25 occurrences\n",
      "test - Value 1: 427 occurrences\n",
      "epoch-133 lr=['2.0000000'], tr/val_loss: 49.091644/ 30.066362, val:  55.53%, val_best:  83.41%, tr:  95.88%, tr_best:  98.71%, epoch time: 246.89 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4322304 real_backward_count 849774  19.660%\n",
      "layer   1  Sparsity: 77.1729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2055 occurrences\n",
      "train - Value 1: 1977 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 12 occurrences\n",
      "test - Value 1: 440 occurrences\n",
      "epoch-134 lr=['2.0000000'], tr/val_loss: 47.996647/ 99.289627, val:  52.65%, val_best:  83.41%, tr:  95.31%, tr_best:  98.71%, epoch time: 247.50 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.0316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4354560 real_backward_count 855820  19.653%\n",
      "layer   1  Sparsity: 82.8125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2087 occurrences\n",
      "train - Value 1: 1945 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 256 occurrences\n",
      "test - Value 1: 196 occurrences\n",
      "epoch-135 lr=['2.0000000'], tr/val_loss: 49.124424/ 45.342335, val:  80.97%, val_best:  83.41%, tr:  96.55%, tr_best:  98.71%, epoch time: 249.03 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3994%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4386816 real_backward_count 861867  19.647%\n",
      "layer   1  Sparsity: 83.5938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 257 occurrences\n",
      "test - Value 1: 195 occurrences\n",
      "epoch-136 lr=['2.0000000'], tr/val_loss: 41.245823/  6.087810, val:  80.31%, val_best:  83.41%, tr:  94.99%, tr_best:  98.71%, epoch time: 247.25 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4419072 real_backward_count 868060  19.643%\n",
      "layer   1  Sparsity: 78.1738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 51.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 189 occurrences\n",
      "test - Value 1: 263 occurrences\n",
      "epoch-137 lr=['2.0000000'], tr/val_loss: 47.888420/ 13.356870, val:  81.64%, val_best:  83.41%, tr:  93.23%, tr_best:  98.71%, epoch time: 247.65 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8690%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4451328 real_backward_count 874288  19.641%\n",
      "layer   1  Sparsity: 85.0098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11815.0\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 323 occurrences\n",
      "test - Value 1: 129 occurrences\n",
      "epoch-138 lr=['2.0000000'], tr/val_loss: 50.706005/ 10.341043, val:  74.56%, val_best:  83.41%, tr:  93.80%, tr_best:  98.71%, epoch time: 248.19 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4152%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4483584 real_backward_count 880522  19.639%\n",
      "layer   1  Sparsity: 84.5459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1847 occurrences\n",
      "train - Value 1: 2185 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-139 lr=['2.0000000'], tr/val_loss: 51.214527/ 77.637138, val:  50.00%, val_best:  83.41%, tr:  91.20%, tr_best:  98.71%, epoch time: 247.73 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4515840 real_backward_count 886727  19.636%\n",
      "layer   1  Sparsity: 95.0195%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1952 occurrences\n",
      "train - Value 1: 2080 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 424 occurrences\n",
      "test - Value 1: 28 occurrences\n",
      "epoch-140 lr=['2.0000000'], tr/val_loss: 46.230885/  8.280255, val:  55.75%, val_best:  83.41%, tr:  93.55%, tr_best:  98.71%, epoch time: 246.84 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.5538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4548096 real_backward_count 892892  19.632%\n",
      "layer   1  Sparsity: 80.5420%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 57.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11816.0\n",
      "fc layer 1 self.abs_max_out: 11840.0\n",
      "train - Value 0: 1976 occurrences\n",
      "train - Value 1: 2056 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 179 occurrences\n",
      "test - Value 1: 273 occurrences\n",
      "epoch-141 lr=['2.0000000'], tr/val_loss: 36.771965/  8.325909, val:  78.98%, val_best:  83.41%, tr:  94.35%, tr_best:  98.71%, epoch time: 248.09 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.3251%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4580352 real_backward_count 899024  19.628%\n",
      "layer   1  Sparsity: 72.3389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 50.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 364 occurrences\n",
      "test - Value 1: 88 occurrences\n",
      "epoch-142 lr=['2.0000000'], tr/val_loss: 45.121925/ 38.235310, val:  68.14%, val_best:  83.41%, tr:  91.91%, tr_best:  98.71%, epoch time: 246.48 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.9111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4612608 real_backward_count 905250  19.626%\n",
      "layer   1  Sparsity: 84.8877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 339 occurrences\n",
      "test - Value 1: 113 occurrences\n",
      "epoch-143 lr=['2.0000000'], tr/val_loss: 55.063900/  4.575415, val:  70.13%, val_best:  83.41%, tr:  92.53%, tr_best:  98.71%, epoch time: 247.82 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3995%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4644864 real_backward_count 911352  19.621%\n",
      "layer   1  Sparsity: 94.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11879.0\n",
      "lif layer 1 self.abs_max_v: 13308.0\n",
      "train - Value 0: 2067 occurrences\n",
      "train - Value 1: 1965 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-144 lr=['2.0000000'], tr/val_loss: 28.662197/  9.243011, val:  50.00%, val_best:  83.41%, tr:  93.97%, tr_best:  98.71%, epoch time: 248.37 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4677120 real_backward_count 917336  19.613%\n",
      "layer   1  Sparsity: 70.4346%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2052 occurrences\n",
      "train - Value 1: 1980 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 220 occurrences\n",
      "test - Value 1: 232 occurrences\n",
      "epoch-145 lr=['2.0000000'], tr/val_loss: 38.563198/ 16.310823, val:  80.53%, val_best:  83.41%, tr:  96.58%, tr_best:  98.71%, epoch time: 247.44 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.4790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4709376 real_backward_count 923351  19.607%\n",
      "layer   1  Sparsity: 76.6113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 56.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11909.0\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-146 lr=['2.0000000'], tr/val_loss: 34.489944/  5.668165, val:  50.44%, val_best:  83.41%, tr:  96.38%, tr_best:  98.71%, epoch time: 249.43 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3254%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4741632 real_backward_count 929472  19.602%\n",
      "layer   1  Sparsity: 82.8857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.5625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11921.0\n",
      "lif layer 1 self.abs_max_v: 13332.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 98 occurrences\n",
      "test - Value 1: 354 occurrences\n",
      "epoch-147 lr=['2.0000000'], tr/val_loss: 42.789131/ 21.185265, val:  69.91%, val_best:  83.41%, tr:  97.59%, tr_best:  98.71%, epoch time: 248.70 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.9410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4773888 real_backward_count 935707  19.601%\n",
      "layer   1  Sparsity: 86.1084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2018 occurrences\n",
      "train - Value 1: 2014 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 24 occurrences\n",
      "test - Value 1: 428 occurrences\n",
      "epoch-148 lr=['2.0000000'], tr/val_loss: 43.073051/ 86.230148, val:  55.31%, val_best:  83.41%, tr:  99.21%, tr_best:  99.21%, epoch time: 249.28 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5303%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4806144 real_backward_count 941464  19.589%\n",
      "layer   1  Sparsity: 85.9863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.6250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 287 occurrences\n",
      "test - Value 1: 165 occurrences\n",
      "epoch-149 lr=['2.0000000'], tr/val_loss: 49.800961/ 67.521149, val:  76.33%, val_best:  83.41%, tr:  98.59%, tr_best:  99.21%, epoch time: 246.80 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4624%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4838400 real_backward_count 947218  19.577%\n",
      "layer   1  Sparsity: 74.2432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 47 occurrences\n",
      "test - Value 1: 405 occurrences\n",
      "epoch-150 lr=['2.0000000'], tr/val_loss: 54.635544/ 68.505501, val:  60.40%, val_best:  83.41%, tr:  97.64%, tr_best:  99.21%, epoch time: 247.42 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.8097%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4870656 real_backward_count 953418  19.575%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2026 occurrences\n",
      "train - Value 1: 2006 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 16 occurrences\n",
      "test - Value 1: 436 occurrences\n",
      "epoch-151 lr=['2.0000000'], tr/val_loss: 53.069386/ 73.406647, val:  53.54%, val_best:  83.41%, tr:  96.83%, tr_best:  99.21%, epoch time: 247.41 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4902912 real_backward_count 959664  19.573%\n",
      "layer   1  Sparsity: 85.7910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 299 occurrences\n",
      "test - Value 1: 153 occurrences\n",
      "epoch-152 lr=['2.0000000'], tr/val_loss: 52.221596/ 35.273762, val:  75.88%, val_best:  83.41%, tr:  90.90%, tr_best:  99.21%, epoch time: 247.67 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.4783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4935168 real_backward_count 965996  19.574%\n",
      "layer   1  Sparsity: 82.1289%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 58.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2071 occurrences\n",
      "train - Value 1: 1961 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-153 lr=['2.0000000'], tr/val_loss: 51.091072/ 76.877571, val:  50.00%, val_best:  83.41%, tr:  92.49%, tr_best:  99.21%, epoch time: 247.52 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4967424 real_backward_count 972288  19.573%\n",
      "layer   1  Sparsity: 75.4883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13343.5\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-154 lr=['2.0000000'], tr/val_loss: 45.392742/ 64.538193, val:  59.73%, val_best:  83.41%, tr:  91.99%, tr_best:  99.21%, epoch time: 247.05 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.9324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 4999680 real_backward_count 978606  19.573%\n",
      "layer   1  Sparsity: 83.5693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 216 occurrences\n",
      "test - Value 1: 236 occurrences\n",
      "epoch-155 lr=['2.0000000'], tr/val_loss: 48.554001/ 59.328312, val:  81.42%, val_best:  83.41%, tr:  94.77%, tr_best:  99.21%, epoch time: 246.19 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5031936 real_backward_count 984639  19.568%\n",
      "layer   1  Sparsity: 89.7705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 76.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1969 occurrences\n",
      "train - Value 1: 2063 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 198 occurrences\n",
      "test - Value 1: 254 occurrences\n",
      "epoch-156 lr=['2.0000000'], tr/val_loss: 49.603722/ 58.858356, val:  81.42%, val_best:  83.41%, tr:  97.35%, tr_best:  99.21%, epoch time: 247.60 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5064192 real_backward_count 990489  19.559%\n",
      "layer   1  Sparsity: 81.3965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.1250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2054 occurrences\n",
      "train - Value 1: 1978 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 99 occurrences\n",
      "test - Value 1: 353 occurrences\n",
      "epoch-157 lr=['2.0000000'], tr/val_loss: 42.813927/ 32.713379, val:  69.25%, val_best:  83.41%, tr:  97.27%, tr_best:  99.21%, epoch time: 248.26 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.4172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5096448 real_backward_count 996488  19.553%\n",
      "layer   1  Sparsity: 84.8389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1971 occurrences\n",
      "train - Value 1: 2061 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 195 occurrences\n",
      "test - Value 1: 257 occurrences\n",
      "epoch-158 lr=['2.0000000'], tr/val_loss: 32.459702/ 33.801079, val:  80.75%, val_best:  83.41%, tr:  95.06%, tr_best:  99.21%, epoch time: 247.55 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.6641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5128704 real_backward_count 1002703  19.551%\n",
      "layer   1  Sparsity: 86.3770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1912 occurrences\n",
      "train - Value 1: 2120 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 25 occurrences\n",
      "test - Value 1: 427 occurrences\n",
      "epoch-159 lr=['2.0000000'], tr/val_loss: 50.657894/ 53.275002, val:  55.53%, val_best:  83.41%, tr:  91.87%, tr_best:  99.21%, epoch time: 247.24 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5160960 real_backward_count 1008853  19.548%\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1873 occurrences\n",
      "train - Value 1: 2159 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-160 lr=['2.0000000'], tr/val_loss: 46.087807/ 77.609016, val:  50.00%, val_best:  83.41%, tr:  90.70%, tr_best:  99.21%, epoch time: 248.05 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.5511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5193216 real_backward_count 1015137  19.547%\n",
      "layer   1  Sparsity: 64.9170%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 53.3125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1850 occurrences\n",
      "train - Value 1: 2182 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 24 occurrences\n",
      "test - Value 1: 428 occurrences\n",
      "epoch-161 lr=['2.0000000'], tr/val_loss: 53.012260/ 73.011757, val:  55.31%, val_best:  83.41%, tr:  89.83%, tr_best:  99.21%, epoch time: 246.96 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5225472 real_backward_count 1021574  19.550%\n",
      "layer   1  Sparsity: 93.9453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 448 occurrences\n",
      "test - Value 1: 4 occurrences\n",
      "epoch-162 lr=['2.0000000'], tr/val_loss: 48.716976/ 35.834007, val:  50.88%, val_best:  83.41%, tr:  92.91%, tr_best:  99.21%, epoch time: 246.52 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2193%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1120%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5257728 real_backward_count 1027922  19.551%\n",
      "layer   1  Sparsity: 81.3721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 52.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2162 occurrences\n",
      "train - Value 1: 1870 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 206 occurrences\n",
      "test - Value 1: 246 occurrences\n",
      "epoch-163 lr=['2.0000000'], tr/val_loss: 42.938801/ 37.908043, val:  75.66%, val_best:  83.41%, tr:  94.00%, tr_best:  99.21%, epoch time: 247.50 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5289984 real_backward_count 1034209  19.550%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13411.5\n",
      "train - Value 0: 2049 occurrences\n",
      "train - Value 1: 1983 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 407 occurrences\n",
      "test - Value 1: 45 occurrences\n",
      "epoch-164 lr=['2.0000000'], tr/val_loss: 50.184940/ 33.540863, val:  59.51%, val_best:  83.41%, tr:  94.32%, tr_best:  99.21%, epoch time: 246.58 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.7853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5322240 real_backward_count 1040371  19.548%\n",
      "layer   1  Sparsity: 85.8154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13429.5\n",
      "train - Value 0: 2064 occurrences\n",
      "train - Value 1: 1968 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 313 occurrences\n",
      "test - Value 1: 139 occurrences\n",
      "epoch-165 lr=['2.0000000'], tr/val_loss: 49.324471/ 35.172619, val:  76.77%, val_best:  83.41%, tr:  93.85%, tr_best:  99.21%, epoch time: 247.97 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5354496 real_backward_count 1046585  19.546%\n",
      "layer   1  Sparsity: 75.9521%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13464.5\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 41 occurrences\n",
      "test - Value 1: 411 occurrences\n",
      "epoch-166 lr=['2.0000000'], tr/val_loss: 41.864655/ 45.568203, val:  59.07%, val_best:  83.41%, tr:  93.45%, tr_best:  99.21%, epoch time: 248.30 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5386752 real_backward_count 1052839  19.545%\n",
      "layer   1  Sparsity: 85.2295%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13521.0\n",
      "train - Value 0: 2054 occurrences\n",
      "train - Value 1: 1978 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 81 occurrences\n",
      "test - Value 1: 371 occurrences\n",
      "epoch-167 lr=['2.0000000'], tr/val_loss: 31.993885/ 46.105152, val:  67.48%, val_best:  83.41%, tr:  96.58%, tr_best:  99.21%, epoch time: 248.51 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5419008 real_backward_count 1059149  19.545%\n",
      "layer   1  Sparsity: 83.3008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13538.5\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-168 lr=['2.0000000'], tr/val_loss: 31.799631/ 73.320869, val:  50.00%, val_best:  83.41%, tr:  97.22%, tr_best:  99.21%, epoch time: 246.99 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5451264 real_backward_count 1065175  19.540%\n",
      "layer   1  Sparsity: 91.7480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 11940.0\n",
      "train - Value 0: 1968 occurrences\n",
      "train - Value 1: 2064 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-169 lr=['2.0000000'], tr/val_loss: 33.396435/ 12.688352, val:  50.00%, val_best:  83.41%, tr:  95.09%, tr_best:  99.21%, epoch time: 249.60 seconds, 4.16 minutes\n",
      "layer   1  Sparsity: 82.6629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.9895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5483520 real_backward_count 1071395  19.538%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12005.0\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 44 occurrences\n",
      "test - Value 1: 408 occurrences\n",
      "epoch-170 lr=['2.0000000'], tr/val_loss: 35.506496/ 70.573669, val:  59.73%, val_best:  83.41%, tr:  98.98%, tr_best:  99.21%, epoch time: 247.69 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5515776 real_backward_count 1077264  19.531%\n",
      "layer   1  Sparsity: 82.9590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12029.0\n",
      "train - Value 0: 2028 occurrences\n",
      "train - Value 1: 2004 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 276 occurrences\n",
      "test - Value 1: 176 occurrences\n",
      "epoch-171 lr=['2.0000000'], tr/val_loss: 44.744888/ 17.300306, val:  81.42%, val_best:  83.41%, tr:  99.31%, tr_best:  99.31%, epoch time: 246.71 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2850%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5548032 real_backward_count 1082913  19.519%\n",
      "layer   1  Sparsity: 82.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12098.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 34 occurrences\n",
      "test - Value 1: 418 occurrences\n",
      "epoch-172 lr=['2.0000000'], tr/val_loss: 40.187653/ 41.916489, val:  57.52%, val_best:  83.41%, tr:  99.28%, tr_best:  99.31%, epoch time: 247.86 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5580288 real_backward_count 1088496  19.506%\n",
      "layer   1  Sparsity: 87.0117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12144.0\n",
      "fc layer 1 self.abs_max_out: 12178.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 420 occurrences\n",
      "test - Value 1: 32 occurrences\n",
      "epoch-173 lr=['2.0000000'], tr/val_loss: 38.816227/ 11.480942, val:  56.64%, val_best:  83.41%, tr:  98.81%, tr_best:  99.31%, epoch time: 242.71 seconds, 4.05 minutes\n",
      "layer   1  Sparsity: 82.6640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5612544 real_backward_count 1094131  19.494%\n",
      "layer   1  Sparsity: 95.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 77.4375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12290.0\n",
      "lif layer 1 self.abs_max_v: 13568.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 132 occurrences\n",
      "test - Value 1: 320 occurrences\n",
      "epoch-174 lr=['2.0000000'], tr/val_loss: 34.905109/ 47.197025, val:  76.11%, val_best:  83.41%, tr:  98.51%, tr_best:  99.31%, epoch time: 247.01 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5644800 real_backward_count 1100059  19.488%\n",
      "layer   1  Sparsity: 93.3350%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12317.0\n",
      "fc layer 1 self.abs_max_out: 12331.0\n",
      "fc layer 1 self.abs_max_out: 12468.0\n",
      "lif layer 1 self.abs_max_v: 13588.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 134 occurrences\n",
      "test - Value 1: 318 occurrences\n",
      "epoch-175 lr=['2.0000000'], tr/val_loss: 39.116570/ 43.687687, val:  74.78%, val_best:  83.41%, tr:  97.17%, tr_best:  99.31%, epoch time: 247.33 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5677056 real_backward_count 1106134  19.484%\n",
      "layer   1  Sparsity: 77.2217%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 97 occurrences\n",
      "test - Value 1: 355 occurrences\n",
      "epoch-176 lr=['2.0000000'], tr/val_loss: 31.835104/ 37.360065, val:  69.25%, val_best:  83.41%, tr:  98.04%, tr_best:  99.31%, epoch time: 247.57 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5709312 real_backward_count 1112202  19.480%\n",
      "layer   1  Sparsity: 85.3027%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.0625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2016 occurrences\n",
      "train - Value 1: 2016 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 419 occurrences\n",
      "test - Value 1: 33 occurrences\n",
      "epoch-177 lr=['2.0000000'], tr/val_loss: 37.495518/  4.868477, val:  57.30%, val_best:  83.41%, tr:  97.67%, tr_best:  99.31%, epoch time: 244.23 seconds, 4.07 minutes\n",
      "layer   1  Sparsity: 82.6644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4738%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.6522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5741568 real_backward_count 1118259  19.477%\n",
      "layer   1  Sparsity: 87.4756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12519.0\n",
      "train - Value 0: 2031 occurrences\n",
      "train - Value 1: 2001 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 180 occurrences\n",
      "test - Value 1: 272 occurrences\n",
      "epoch-178 lr=['2.0000000'], tr/val_loss: 44.303230/ 20.049076, val:  78.32%, val_best:  83.41%, tr:  99.43%, tr_best:  99.43%, epoch time: 248.29 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3984%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5773824 real_backward_count 1123761  19.463%\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "max_activation_accul updated: 1412.00 at epoch 179, iter 4031\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 9 occurrences\n",
      "test - Value 1: 443 occurrences\n",
      "epoch-179 lr=['2.0000000'], tr/val_loss: 51.892029/ 91.844055, val:  51.99%, val_best:  83.41%, tr:  98.83%, tr_best:  99.43%, epoch time: 247.00 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5172%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5806080 real_backward_count 1129498  19.454%\n",
      "layer   1  Sparsity: 80.6396%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 49.3125%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1947 occurrences\n",
      "train - Value 1: 2085 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 222 occurrences\n",
      "test - Value 1: 230 occurrences\n",
      "epoch-180 lr=['2.0000000'], tr/val_loss: 46.179928/ 63.263725, val:  84.51%, val_best:  84.51%, tr:  95.46%, tr_best:  99.43%, epoch time: 248.54 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5838336 real_backward_count 1135394  19.447%\n",
      "layer   1  Sparsity: 89.4043%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4048.0\n",
      "train - Value 0: 1963 occurrences\n",
      "train - Value 1: 2069 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 315 occurrences\n",
      "test - Value 1: 137 occurrences\n",
      "epoch-181 lr=['2.0000000'], tr/val_loss: 35.346115/  7.669472, val:  75.00%, val_best:  84.51%, tr:  95.21%, tr_best:  99.43%, epoch time: 248.49 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5870592 real_backward_count 1141356  19.442%\n",
      "layer   1  Sparsity: 61.9385%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4117.0\n",
      "fc layer 2 self.abs_max_out: 4272.0\n",
      "fc layer 2 self.abs_max_out: 4411.0\n",
      "train - Value 0: 2036 occurrences\n",
      "train - Value 1: 1996 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 310 occurrences\n",
      "test - Value 1: 142 occurrences\n",
      "epoch-182 lr=['2.0000000'], tr/val_loss: 40.398849/ 38.175106, val:  77.43%, val_best:  84.51%, tr:  95.09%, tr_best:  99.43%, epoch time: 247.60 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5902848 real_backward_count 1147411  19.438%\n",
      "layer   1  Sparsity: 80.3955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.5625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4467.0\n",
      "fc layer 2 self.abs_max_out: 4493.0\n",
      "fc layer 2 self.abs_max_out: 4563.0\n",
      "fc layer 2 self.abs_max_out: 4595.0\n",
      "train - Value 0: 2021 occurrences\n",
      "train - Value 1: 2011 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 68 occurrences\n",
      "test - Value 1: 384 occurrences\n",
      "epoch-183 lr=['2.0000000'], tr/val_loss: 52.883579/ 47.847439, val:  65.04%, val_best:  84.51%, tr:  94.62%, tr_best:  99.43%, epoch time: 246.56 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5935104 real_backward_count 1153506  19.435%\n",
      "layer   1  Sparsity: 79.7852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 13591.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-184 lr=['2.0000000'], tr/val_loss: 35.955921/ 59.667549, val:  50.00%, val_best:  84.51%, tr:  94.99%, tr_best:  99.43%, epoch time: 247.52 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.9379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5967360 real_backward_count 1159626  19.433%\n",
      "layer   1  Sparsity: 89.0381%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4669.0\n",
      "lif layer 1 self.abs_max_v: 14008.0\n",
      "train - Value 0: 1963 occurrences\n",
      "train - Value 1: 2069 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 409 occurrences\n",
      "test - Value 1: 43 occurrences\n",
      "epoch-185 lr=['2.0000000'], tr/val_loss: 30.260870/  8.984490, val:  59.07%, val_best:  84.51%, tr:  96.40%, tr_best:  99.43%, epoch time: 246.83 seconds, 4.11 minutes\n",
      "layer   1  Sparsity: 82.6635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9291%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 5999616 real_backward_count 1165701  19.430%\n",
      "layer   1  Sparsity: 87.4756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 14103.0\n",
      "train - Value 0: 1900 occurrences\n",
      "train - Value 1: 2132 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 240 occurrences\n",
      "test - Value 1: 212 occurrences\n",
      "epoch-186 lr=['2.0000000'], tr/val_loss: 32.390686/ 52.682777, val:  80.53%, val_best:  84.51%, tr:  93.45%, tr_best:  99.43%, epoch time: 247.23 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6031872 real_backward_count 1171844  19.428%\n",
      "layer   1  Sparsity: 83.4961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 14238.5\n",
      "train - Value 0: 1830 occurrences\n",
      "train - Value 1: 2202 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-187 lr=['2.0000000'], tr/val_loss: 42.122623/ 81.315903, val:  50.00%, val_best:  84.51%, tr:  89.93%, tr_best:  99.43%, epoch time: 245.24 seconds, 4.09 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1045%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.8372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6064128 real_backward_count 1178181  19.429%\n",
      "layer   1  Sparsity: 71.8018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 55.8125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 14428.0\n",
      "train - Value 0: 1808 occurrences\n",
      "train - Value 1: 2224 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-188 lr=['2.0000000'], tr/val_loss: 44.144928/ 80.252655, val:  50.00%, val_best:  84.51%, tr:  90.28%, tr_best:  99.43%, epoch time: 246.99 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2387%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.7644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6096384 real_backward_count 1184369  19.427%\n",
      "layer   1  Sparsity: 71.8262%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 48.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1965 occurrences\n",
      "train - Value 1: 2067 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 83 occurrences\n",
      "test - Value 1: 369 occurrences\n",
      "epoch-189 lr=['2.0000000'], tr/val_loss: 63.956783/ 45.057720, val:  67.04%, val_best:  84.51%, tr:  95.96%, tr_best:  99.43%, epoch time: 247.71 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6128640 real_backward_count 1190510  19.425%\n",
      "layer   1  Sparsity: 82.5439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4672.0\n",
      "train - Value 0: 1888 occurrences\n",
      "train - Value 1: 2144 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 26 occurrences\n",
      "test - Value 1: 426 occurrences\n",
      "epoch-190 lr=['2.0000000'], tr/val_loss: 47.944206/ 39.935162, val:  55.75%, val_best:  84.51%, tr:  92.11%, tr_best:  99.43%, epoch time: 247.49 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 61.8746%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6160896 real_backward_count 1196700  19.424%\n",
      "layer   1  Sparsity: 71.6309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4748.0\n",
      "train - Value 0: 1967 occurrences\n",
      "train - Value 1: 2065 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 314 occurrences\n",
      "test - Value 1: 138 occurrences\n",
      "epoch-191 lr=['2.0000000'], tr/val_loss: 50.043827/ 45.873020, val:  77.43%, val_best:  84.51%, tr:  95.76%, tr_best:  99.43%, epoch time: 248.80 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6193152 real_backward_count 1202753  19.421%\n",
      "layer   1  Sparsity: 78.6865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2041 occurrences\n",
      "train - Value 1: 1991 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 17 occurrences\n",
      "test - Value 1: 435 occurrences\n",
      "epoch-192 lr=['2.0000000'], tr/val_loss: 52.135220/ 60.629002, val:  53.76%, val_best:  84.51%, tr:  93.43%, tr_best:  99.43%, epoch time: 246.00 seconds, 4.10 minutes\n",
      "layer   1  Sparsity: 82.6659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6225408 real_backward_count 1208839  19.418%\n",
      "layer   1  Sparsity: 69.5312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 47.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 54.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 4771.0\n",
      "train - Value 0: 1876 occurrences\n",
      "train - Value 1: 2156 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-193 lr=['2.0000000'], tr/val_loss: 58.171883/ 63.173199, val:  50.22%, val_best:  84.51%, tr:  92.51%, tr_best:  99.43%, epoch time: 247.80 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1464%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6257664 real_backward_count 1214865  19.414%\n",
      "layer   1  Sparsity: 78.2227%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2049 occurrences\n",
      "train - Value 1: 1983 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 46 occurrences\n",
      "test - Value 1: 406 occurrences\n",
      "epoch-194 lr=['2.0000000'], tr/val_loss: 48.184566/ 30.607338, val:  60.18%, val_best:  84.51%, tr:  94.37%, tr_best:  99.43%, epoch time: 248.71 seconds, 4.15 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3565%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6289920 real_backward_count 1220954  19.411%\n",
      "layer   1  Sparsity: 90.0391%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 119 occurrences\n",
      "test - Value 1: 333 occurrences\n",
      "epoch-195 lr=['2.0000000'], tr/val_loss: 48.097420/ 51.409058, val:  73.23%, val_best:  84.51%, tr:  98.04%, tr_best:  99.43%, epoch time: 248.38 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.2816%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6322176 real_backward_count 1226881  19.406%\n",
      "layer   1  Sparsity: 84.6924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 59.8750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1989 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 392 occurrences\n",
      "test - Value 1: 60 occurrences\n",
      "epoch-196 lr=['2.0000000'], tr/val_loss: 51.071854/ 36.507423, val:  62.39%, val_best:  84.51%, tr:  95.91%, tr_best:  99.43%, epoch time: 246.94 seconds, 4.12 minutes\n",
      "layer   1  Sparsity: 82.6645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.0575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6354432 real_backward_count 1232952  19.403%\n",
      "layer   1  Sparsity: 83.5449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1907 occurrences\n",
      "train - Value 1: 2125 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 10 occurrences\n",
      "test - Value 1: 442 occurrences\n",
      "epoch-197 lr=['2.0000000'], tr/val_loss: 57.064007/ 67.685928, val:  52.21%, val_best:  84.51%, tr:  93.33%, tr_best:  99.43%, epoch time: 248.27 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.1830%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6386688 real_backward_count 1239069  19.401%\n",
      "layer   1  Sparsity: 77.0020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.3750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1844 occurrences\n",
      "train - Value 1: 2188 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 174 occurrences\n",
      "test - Value 1: 278 occurrences\n",
      "epoch-198 lr=['2.0000000'], tr/val_loss: 57.381851/ 35.182892, val:  78.32%, val_best:  84.51%, tr:  93.15%, tr_best:  99.43%, epoch time: 248.31 seconds, 4.14 minutes\n",
      "layer   1  Sparsity: 82.6662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.5338%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 6418944 real_backward_count 1245303  19.400%\n",
      "layer   1  Sparsity: 73.3887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.4375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 60.1875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 12669.0\n",
      "fc layer 2 self.abs_max_out: 4803.0\n",
      "fc layer 2 self.abs_max_out: 4899.0\n",
      "train - Value 0: 1974 occurrences\n",
      "train - Value 1: 2058 occurrences\n",
      "train_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test_spike_distribution.mean 8.000000, min 8, max 8\n",
      "test - Value 0: 21 occurrences\n",
      "test - Value 1: 431 occurrences\n",
      "epoch-199 lr=['2.0000000'], tr/val_loss: 50.855267/ 45.348461, val:  54.65%, val_best:  84.51%, tr:  95.34%, tr_best:  99.43%, epoch time: 247.92 seconds, 4.13 minutes\n",
      "layer   1  Sparsity: 82.6670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 62.3044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edd493f1e614e5691b50fedc216bd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñá‚ñá‚ñÅ‚ñÉ‚ñÇ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>tr_acc</td><td>‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÅ‚ñÉ‚ñÖ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñá‚ñá‚ñÅ‚ñÉ‚ñÇ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>val_loss</td><td>‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.95337</td></tr><tr><td>tr_epoch_loss</td><td>50.85527</td></tr><tr><td>val_acc_best</td><td>0.84513</td></tr><tr><td>val_acc_now</td><td>0.54646</td></tr><tr><td>val_loss</td><td>45.34846</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-64</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/cvizmlyi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/cvizmlyi</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251221_155423-cvizmlyi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rnqik69l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloser_encourage_mode: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttimestep_sums_threshold: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: n_tidigits_tonic\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251222_054028-rnqik69l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/rnqik69l' target=\"_blank\">sweet-sweep-65</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/sweeps/9m2jgqar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/rnqik69l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20NTIDIGITS%20SWEEP%20LOSER%20ONOFF%20new251129/runs/rnqik69l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'timestep_sums_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'loser_encourage_mode' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251222_054037_456', 'my_seed': 42, 'TIME': 6, 'BATCH': 1, 'IMAGE_SIZE': 8, 'which_data': 'n_tidigits_tonic', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 0, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': False, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 8, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'timestep_sums_threshold': 0, 'lif_layer_sg_width2': 64, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.03125, 0.03125, 0.03125], 'learning_rate': 1, 'learning_rate2': 1, 'loser_encourage_mode': True} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target word: 0\n",
      "\n",
      "\n",
      "\n",
      "train_dataset length = 4032, test_dataset length = 452\n",
      "\n",
      "len(train_loader): 4032 BATCH: 1 train_data_count: 4032\n",
      "len(test_loader): 452 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHFCAYAAADv8c1wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1UElEQVR4nO3dd1xTV/8H8E8ISRhiFCgEFBWt4gAXVkWtaBXQSq31qdZRqtaqrRNHrdZRtO7dap21at39tdUOFcG6Cziw1PnYIc6CtIqgrITk/v7gya0hjDBiAnzer1de3px7zs25JzneL+eee69EEAQBRERERFQkG0tXgIiIiKgiYNBEREREZAIGTUREREQmYNBEREREZAIGTUREREQmYNBEREREZAIGTUREREQmYNBEREREZAIGTUREREQmYNBEVIytW7dCIpEU+JoyZYpB3pycHKxZswadOnVCzZo1IZfLUatWLfTv3x8nTpwwyDtz5kyEhoaiVq1akEgkGDp0qEn1+frrryGRSLB3716jdS1atIBEIsHhw4eN1jVo0ACtW7c2fccBDB06FPXq1StRGb2IiAhIJBL8888/xeZdsGAB9u/fb/K2n/4OpFIpatasiRYtWmDUqFGIi4szyn/z5k1IJBJs3bq1BHsA7Nq1C6tWrSpRmYI+qyRtYaqrV68iIiICN2/eNFpXlu+tPPz5559QKBSIjY0V07p06QJfX1+TykskEkRERIjvi9rX0hIEAZs2bYK/vz+qV68OFxcXBAYG4sCBAwb5fvvtN8jlcly4cKHcPpsqLgZNRCbasmULYmNjDV7jx48X1//zzz/o2LEjJk2aBF9fX2zduhU//fQTli9fDqlUim7duuHXX38V869cuRIPHjxA7969IZfLTa5Hly5dIJFIcOzYMYP0hw8f4tKlS3B0dDRad/fuXdy4cQNdu3Yt0T7PmjUL+/btK1GZ0ihp0AQAr7/+OmJjY3H69Gns2bMHb731FuLi4hAQEIAJEyYY5PXw8EBsbCx69epVos8oTdBU2s8qqatXr2LOnDkFBhLP6nsrzJQpUxAUFISAgIBSlY+NjcU777wjvi9qX0vro48+wsiRI9G2bVt888032Lp1KxQKBUJDQ/Htt9+K+Ro1aoTBgwdj4sSJ5fbZVHHZWroCRBWFr68v2rRpU+j6t956C7/++isOHz6Ml156yWDdgAEDMGnSJNSsWVNMe/z4MWxs8v5u2b59u8n1cHV1ha+vL44fP26QfuLECdja2mL48OFGQZP+fUmDpgYNGpQo/7Pk7u6O9u3bi+9DQkIQHh6OkSNH4tNPP0Xjxo3x3nvvAQAUCoVBXnPQarXIzc19Jp9VHEt+b9euXcP+/fsRGRlZ6m08i/b74osv0KlTJ6xbt05MCwoKgkqlwrZt29C3b18xfezYsWjTpg1iYmLQoUMHs9eNrBdHmojKQXx8PA4dOoThw4cbBUx6L7zwAurUqSO+1wdMpdG1a1dcv34dSUlJYtrx48fxwgsv4OWXX0Z8fDweP35ssE4qleLFF18EkHdqYu3atWjZsiXs7e1Rs2ZNvP7667hx44bB5xR0mufRo0cYPnw4nJ2dUa1aNfTq1Qs3btwwOqWid//+fQwcOBBKpRLu7u54++23kZaWJq6XSCTIyMjAtm3bxFNuXbp0KVW7SKVSrFmzBq6urli6dKmYXtAps7///hsjR46El5cXFAoFnnvuOXTs2BFHjhwBkDeid+DAAdy6dcvgdODT21uyZAnmzZsHb29vKBQKHDt2rMhTgXfu3EHfvn1RvXp1KJVKvPnmm/j7778N8hTWjvXq1RNP4W7duhX9+vUDkPdb0NdN/5kFfW/Z2dmYPn06vL29xdPGY8aMwaNHj4w+JzQ0FJGRkWjdujXs7e3RuHFjfPHFF8W0fp5169ZBpVIhKCiowPWnTp1C+/btYW9vj1q1amHWrFnQarWFtkFx+1paMpkMSqXSIM3Ozk58Pc3f3x9NmjTB+vXry/SZVPExaCIykX4k4emXXlRUFACgT58+z6Qu+hGjp0ebjh07hsDAQHTs2BESiQSnTp0yWNe6dWvxIDFq1CiEh4eje/fu2L9/P9auXYsrV66gQ4cOuH//fqGfq9Pp8Morr2DXrl344IMPsG/fPrRr1w49evQotMx//vMfNGrUCN988w2mTZuGXbt2GZzqiI2Nhb29PV5++WXxtOfatWtL2zSwt7dH9+7dkZiYiLt37xaaLywsDPv378fs2bMRFRWFzz//HN27d8eDBw8AAGvXrkXHjh2hUqkMTsk+7dNPP8XRo0exbNkyHDp0CI0bNy6ybq+99hqef/55fP3114iIiMD+/fsREhICjUZTon3s1asXFixYAAD47LPPxLoVdkpQEAT06dMHy5YtQ1hYGA4cOIBJkyZh27ZteOmll5CTk2OQ/9dff8XkyZMxceJEfPfdd2jevDmGDx+OkydPFlu3AwcOoHPnzgX+UZCcnIwBAwZg8ODB+O677/D6669j3rx5RqdTS7KvOp3OqF8W9MofmE2YMAGRkZHYvHkzUlNTkZSUhEmTJiEtLc3gtLtely5dcOjQIQiCUGwbUCUmEFGRtmzZIgAo8KXRaARBEIR3331XACD897//LdVnODo6CkOGDDE5/8OHDwUbGxth5MiRgiAIwj///CNIJBIhMjJSEARBaNu2rTBlyhRBEATh9u3bAgBh6tSpgiAIQmxsrABAWL58ucE279y5I9jb24v5BEEQhgwZItStW1d8f+DAAQGAsG7dOoOyCxcuFAAIH330kZj20UcfCQCEJUuWGOQdPXq0YGdnJ+h0ulLvPwBhzJgxha7/4IMPBADCmTNnBEEQhMTERAGAsGXLFjFPtWrVhPDw8CI/p1evXgb7r6ffXoMGDQS1Wl3guqc/S98WEydONMi7c+dOAYCwY8cOg317uh316tata9BG//d//ycAEI4dO2aUN//3FhkZWeB3sXfvXgGAsHHjRoPPsbOzE27duiWmZWVlCc7OzsKoUaOMPutp9+/fFwAIixYtMloXGBgoABC+++47g/QRI0YINjY2Bp+Xvw2K2ld92xb3Kuh7XL9+vaBQKMQ8zs7OQnR0dIH7tmnTJgGAcO3atSLbgCo3jjQRmejLL7/EuXPnDF62tpaZFqi/Wkw/0nTixAlIpVJ07NgRABAYGCjOY8o/n+nHH3+ERCLBm2++afCXuEqlMthmQfRXAPbv398gfeDAgYWW6d27t8H75s2bIzs7GykpKabvcAkJJowGtG3bFlu3bsW8efMQFxdX4tEeIG/fZDKZyfkHDx5s8L5///6wtbU1moNW3o4ePQoARldo9uvXD46Ojvjpp58M0lu2bGlwKtnOzg6NGjXCrVu3ivycv/76CwDg5uZW4HonJyej38OgQYOg0+lMGsUqyMiRI436ZUGvH374waDcli1bMGHCBIwdOxZHjhzBwYMHERwcjFdffbXAq0/1+3Tv3r1S1ZMqB04EJzJRkyZNCp0Irj/AJCYmwsfH55nUp2vXrlixYgX++usvHDt2DP7+/qhWrRqAvKBp+fLlSEtLw7Fjx2Bra4tOnToByJtjJAgC3N3dC9xu/fr1C/3MBw8ewNbWFs7OzgbphW0LAFxcXAzeKxQKAEBWVlbxO1lK+oO7p6dnoXn27t2LefPm4fPPP8esWbNQrVo1vPbaa1iyZAlUKpVJn+Ph4VGieuXfrq2tLVxcXMRTguai/96ee+45g3SJRAKVSmX0+fm/MyDveyvuO9Ovzz8nSK+g34m+TUrbBiqVqtAg7Wn6+WgAkJqaijFjxuCdd97BsmXLxPSePXuiS5cuePfdd5GYmGhQXr9P5vzdkvXjSBNROQgJCQGAEl82XxZPz2s6fvw4AgMDxXX6AOnkyZPiBHF9QOXq6gqJRILTp08X+Bd5Ufvg4uKC3NxcPHz40CA9OTm5nPeu9LKysnDkyBE0aNAAtWvXLjSfq6srVq1ahZs3b+LWrVtYuHAhvv32W5PvlwUYHohNkb+dcnNz8eDBA4MgRaFQGM0xAkofVAD/fm/5J50LgoDk5GS4urqWettP028n/+9Dr6D5cvo2KShQM8XcuXMhk8mKfT19ReH169eRlZWFF154wWh7bdq0wc2bN/HkyRODdP0+lVdbUcXEoImoHLRu3Ro9e/bE5s2bxVMh+Z0/fx63b98ut8/s3LkzpFIpvv76a1y5csXgijOlUomWLVti27ZtuHnzpsGtBkJDQyEIAu7du4c2bdoYvfz8/Ar9TH1glv/Gmnv27CnTvpgyimEKrVaLsWPH4sGDB/jggw9MLlenTh2MHTsWQUFBBjcxLK966e3cudPg/VdffYXc3FyD765evXq4ePGiQb6jR48aHcRLMmLXrVs3AMCOHTsM0r/55htkZGSI68uqbt26sLe3x59//lng+sePH+P77783SNu1axdsbGzQuXPnQrdb1L6W5vScfgQy/41QBUFAXFwcatasCUdHR4N1N27cgI2NzTMbSSbrxNNzROXkyy+/RI8ePdCzZ0+8/fbb6NmzJ2rWrImkpCT88MMP2L17N+Lj48VTeSdOnBD/8tdqtbh16xa+/vprAHnBSf5TKflVr14drVu3xv79+2FjYyPOZ9ILDAwUb8z4dNDUsWNHjBw5EsOGDcP58+fRuXNnODo6IikpCadPn4afn594f6P8evTogY4dO2Ly5MlIT0+Hv78/YmNj8eWXXwIo/W0U/Pz8cPz4cfzwww/w8PCAk5NTsQen+/fvIy4uDoIg4PHjx7h8+TK+/PJL/Prrr5g4cSJGjBhRaNm0tDR07doVgwYNQuPGjeHk5IRz584hMjLS4P48fn5++Pbbb7Fu3Tr4+/vDxsamyHt1Fefbb7+Fra0tgoKCcOXKFcyaNQstWrQwmCMWFhaGWbNmYfbs2QgMDMTVq1exZs0ao8vj9XfX3rhxI5ycnGBnZwdvb+8CR2yCgoIQEhKCDz74AOnp6ejYsSMuXryIjz76CK1atUJYWFip9+lpcrkcAQEBBd6VHcgbTXrvvfdw+/ZtNGrUCAcPHsSmTZvw3nvvGcyhyq+offX09CzyNGxB6tSpg759+2Ljxo1QKBR4+eWXkZOTg23btuHnn3/Gxx9/bDSKGBcXh5YtWxrca42qIEvOQieqCPRXz507d67YvFlZWcKnn34qBAQECNWrVxdsbW0FT09PoW/fvsKBAwcM8uqvJiroVdBVQgWZOnWqAEBo06aN0br9+/cLAAS5XC5kZGQYrf/iiy+Edu3aCY6OjoK9vb3QoEED4a233hLOnz8v5sl/FZYg5F25N2zYMKFGjRqCg4ODEBQUJMTFxQkAhE8++UTMp7+q6e+//zYor2/PxMREMS0hIUHo2LGj4ODgIAAQAgMDi9zvp9vKxsZGqF69uuDn5yeMHDlSiI2NNcqf/4q27Oxs4d133xWaN28uVK9eXbC3txd8fHyEjz76yKCtHj58KLz++utCjRo1BIlEIuj/y9Rvb+nSpcV+1tNtER8fL7zyyitCtWrVBCcnJ2HgwIHC/fv3Dcrn5OQIU6dOFby8vAR7e3shMDBQSEhIMLp6ThAEYdWqVYK3t7cglUoNPrOg7y0rK0v44IMPhLp16woymUzw8PAQ3nvvPSE1NdUgX926dYVevXoZ7VdgYGCx34sgCMLmzZsFqVQq/PXXX0blmzVrJhw/flxo06aNoFAoBA8PD+HDDz8Ur0LVQwFXEBa2r6WVlZUlLF26VGjevLng5OQkODs7C+3btxd27NhhcGWnIAjC48ePBQcHB6MrTqnqkQgCbzpBRGWza9cuDB48GD///DPvmFzFZWdno06dOpg8eXKJTpFas82bN2PChAm4c+cOR5qqOAZNRFQiu3fvxr179+Dn5wcbGxvExcVh6dKlaNWqldFDialqWrduHSIiInDjxg2juUEVTW5uLpo2bYohQ4ZgxowZlq4OWRjnNBFRiTg5OWHPnj2YN28eMjIy4OHhgaFDh2LevHmWrhpZiZEjR+LRo0e4ceNGkRcWVAR37tzBm2++icmTJ1u6KmQFONJEREREZALecoCIiIjIBAyaiIiIiEzAoImIiIjIBJwIXo50Oh3++usvODk5lfjxCkRERGQZwv9ukuvp6VnkTXoZNJWjv/76C15eXpauBhEREZXCnTt3inxmJYOmcuTk5AQgr9GrV69eLtvMVOei7fyfAABnZ3SDg7zifmUajQZRUVEIDg6GTCazdHUqHbav+bGNzYvta14VuX3NfSxMT0+Hl5eXeBwvTMU9Alsh/Sm56tWrl1vQZKvOhY3CQdxuRQ+aHBwcUL169QrXYSsCtq/5sY3Ni+1rXhW5fZ/VsbC4qTWcCE5EBCBbo8XonfEYvTMe2RrtMytLRBUHgyYiIgA6QcDBS8k4eCkZuhLe87csZYmo4qi453qqCKmNBP9pXVtcJiIiqmqs5VjIoMnKKWylWN6/haWrQURUrnQ6HdRqtUGaRqOBra0tsrOzodXyNGd5q+jtO7+3DwBAyNUgO1dTorIymQxSqbTMdWDQREREz5RarUZiYiJ0Op1BuiAIUKlUuHPnDu91ZwZVvX1r1KgBlUpVpn1n0GTlBEFA1v8mltrLpFXyh05ElYcgCEhKSoJUKoWXl5fBjQR1Oh2ePHmCatWqFXmDQSqdity+giBA97/pgjaS4q9yy182MzMTKSkpAAAPD49S14NBk5XL0mjRdPZhAMDVuSEV+pYDRES5ubnIzMyEp6cnHBwcDNbpT9nZ2dlVuIN6RVCR21erE3DlrzQAQDNPZYnnNdnb2wMAUlJS4ObmVupTdRWr1YiIqELTz6WRy+UWrglVNfogXaMp2XyopzFoIiKiZ45TDehZK4/fHIMmIiIiIhMwaCIiIqIyefDgAdzc3HDz5s1n/tlTpkzB+PHjn8lnMWgiIiIqxtChQ9GnTx+D9xKJBIsWLTLIt3//fvE0kD5PUS8gb3L8zJkz4e3tDXt7e9SvXx9z5841uiWDNVu4cCFeeeUV1KtXT0ybMGEC/P39oVAo0LJlS6Myx48fx6uvvgoPDw84OjqiZcuW2Llzp0EefRvaSm3QwqsmWnjVhK3UBs2aNRPzTJ06FVu2bEFiYqK5dk/EoImIiKgU7OzssHjxYqSmpha4/pNPPkFSUpL4AoAtW7YYpS1evBjr16/HmjVrcO3aNSxZsgRLly7F6tWrn9m+lEVWVhY2b96Md955xyBdEAS8/fbbeOONNwosFxMTg+bNm+Obb77BxYsX8fbbb+Ott97CDz/8IObRt+Hde3/hp/j/IursZTg7O6Nfv35iHjc3NwQHB2P9+vXm2cGnMGiycjYSCV72U+FlPxVsOHGSyGzK0tfYT6um7t27Q6VSYeHChQWuVyqVUKlU4gv49waLT6fFxsbi1VdfRa9evVCvXj28/vrrCA4Oxvnz5wv97IiICLRs2RJffPEF6tSpg2rVquG9996DVqvFkiVLoFKp4Obmhvnz5xuU++yzz9CiRQs4OjrCy8sLo0ePxpMnT8T1b7/9Npo3b46cnBwAeVea+fv7Y/DgwYXW5dChQ7C1tUVAQIBB+qeffooxY8agfv36BZb78MMP8fHHH6NDhw5o0KABxo8fjx49emDfvn1GbeihUqFB3dpI/O8lpKamYtiwYQbb6t27N3bv3l1oHcsLgyYrZyeTYu1gf6wd7A87WdlvAU9EBStLX2M/LbtMdS4y1bnIUmvFZf0rW6MtMG9BL1PzlgepVIoFCxZg9erVuHv3bqm306lTJ/z000/47bffAAC//vorTp8+jZdffrnIcn/++ScOHTqEyMhI7N69G1988QV69eqFu3fv4sSJE1i8eDFmzpyJuLg4sYyNjQ1WrVqFy5cvY9u2bTh69CimTp0qrv/000+RkZGBadOmAQBmzZqFf/75B2vXri20HidPnkSbNm1Kvf9PS0tLg7Ozs1G6jY0EdV0c8cNXO9G9e3fUrVvXYH3btm1x584d3Lp1q1zqURjeKZGIiCxOfxPfgnT1eQ5bhrUV3/t/fER8UkJ+7bydsXfUvyMenRYfw8MMtVG+m4t6laG2/3rttdfQsmVLfPTRR9i8eXOptvHBBx8gLS0NjRs3hlQqhVarxfz58zFw4MAiy+l0OnzxxRdwcnJC06ZN0bVrV1y/fh0HDx6EjY0NfHx8sHjxYhw/fhzt27cHALz33nuoXr06bGxs4O3tjY8//hjvvfeeGBRVq1YNO3bsQGBgIJycnLB8+XL89NNPUCqVhdbj5s2b8PT0LNW+P+3rr7/GuXPnsGHDhgLXJyUl4dChQ9i1a5fRulq1aol1yR9QlScGTURERGWwePFivPTSS5g8eXKpyu/duxc7duzArl270KxZMyQkJCA8PByenp4YMmRIoeXq1asHJycn8b27uzukUqnB3b7d3d3Fx4cAwKlTp/DJJ5/g2rVrSE9PR25uLrKzs5GRkQFHR0cAQEBAAKZMmYKPP/4YH3zwATp37lxk/bOysmBnZ1eqfdc7fvw4hg4dik2bNhlM8n7a1q1bUaNGDYMJ+Xr6O35nZmaWqR7FYdBk5TLVuXyMCtEzUJa+xn5adlfnhkCn0+Fx+mM4VXcyOPDnnycWP6t7odvJn/f0B13Lt6IF6Ny5M0JCQvDhhx9i6NChJS7//vvvY9q0aRgwYAAAwM/PD7du3cLChQuLDJpkMpnBe4lEUmCa/iq8W7duoX///hg1ahTmzZsHZ2dnnD59GsOHDze4S7ZOp8PPP/8MqVSK33//vdj6u7q6FjoZ3hQnTpzAK6+8ghUrVuCtt94qME+uVof1Gz9Hzz79IbWVGa1/+PAhAOC5554rdT1MwZ5NREQW5yC3hU6nQ65cCge5bZHPRitJUPqsAthFixahZcuWaNSoUYnLZmZmGu2vVCot91sOnD9/Hrm5uVi2bBlsbfPa5auvvjLKt3TpUly7dg0nTpxASEgItmzZYjTx+mmtWrXCjh07SlWn48ePIzQ0FIsXL8bIkSMLzXfixAncvnkDfQa8WeD6y5cvQyaTFTpKVV4YNBERAbCXSRE/s7u4/KzKUuXg5+eHwYMHl+o2Aa+88grmz5+POnXqoFmzZvjll1+wYsUKvP322+VaxwYNGiA3Nxdr1qxB79698fPPPxtdpp+QkIDZs2fj66+/RseOHfHJJ59gwoQJCAwMLPQquJCQEEyfPh2pqamoWbOmmP7HH3/gyZMnSE5ORlZWFhISEgAATZs2hVwux/Hjx9GrVy9MmDAB//nPf5CcnAwg77mE+SeDb/niC/i1aoOGjZsWWIdTp07hxRdfFE/TmQuvniMiQt5pDJdqCrhUU5T4GVVlKUuVx8cffwxBEEpcbvXq1Xj99dcxevRoNGnSBFOmTMGoUaPw8ccfl2v9WrZsifnz52PJkiXw9fXFzp07DW6XkJ2djcGDB2Po0KF45ZVXAADDhw9H9+7dERYWJj5sOT8/Pz+0adPGaNTqnXfeQatWrbBhwwb89ttvaNWqFVq1aoW//voLQN4cpczMTCxcuBAeHh7iq2/fvgbbSUtLw7fffoPXChllAoDdu3djxIgRpWqXkpAIpfmGqUDp6elQKpVIS0tD9erVy2WblWmuhEajwcGDB/Hyyy8bnXensmP7mh/buOyys7ORmJgIb29vo8nDOp0O6enp4tVdVL7M2b4HDx7ElClTcPnyZbN8d1qdgCt/pQEAmnkqIbX594+TAwcO4P3338fFixfF044FKeq3Z+rxu+IegYmIylFOrhbzfrwGAJgZ2gQKW9NPs5WlLFFl8PLLL+P333/HvXv34OXl9Uw/OyMjA1u2bCkyYCovDJqIiJD3l+z2uLwb401/ufEzK0tUWUyYMMEin9u/f/9n9lkMmqycjUSCrj7PictERERVjQSAk51MXLYUBk1Wzk4mNbgTLhERUVVjYyOBt6ujpavBq+eIiIiITMGgiYiIiMgEDJqsXKY6F01mRaLJrMhyezI3ERFRRaLVCbh8Lw2X76VBq7PcnZI4p6kCKOxp3kRERFWFzgpuK8mRJiIiIiITMGgiIiKqImrWrIn9+/eXeTtHjx5F48aNy/2hwqWRk5ODOnXqID4+3uyfxaCJiIioGEOHDkWfPn0M3kskEixatMgg3/79+8XnD+rzFPUCgNzcXMycORPe3t6wt7dH/fr1MXfuXLMEJP/973/Rs2fPMm9n6tSpmDFjRpGPTLly5Qr+85//oF69epBIJFi1apVRnoULF+KFF16Ak5MT3Nzc0KdPH1y/ft0gz5MnTzB+3FgEvdAMbZ/3gG+zpli3bp24XqFQYMqUKfjggw/KvF/FYdBERERUCnZ2dli8eDFSU1MLXP/JJ58gKSlJfAHAli1bjNIWL16M9evXY82aNbh27RqWLFmCpUuXYvXq1eVeZ3d3dygUijJtIyYmBr///jv69etXZL7MzEzUr18fixYtgkqlKjDPiRMnMGbMGMTFxSE6Ohq5ubkIDg5GRkaGmGfixIk4fPgwFny6AfuOncGECeEYN24cvvvuOzHP4MGDcerUKVy7dq1M+1YcBk1ERESl0L17d6hUKixcuLDA9UqlEiqVSnwBQI0aNYzSYmNj8eqrr6JXr16oV68eXn/9dQQHB+P8+fOFfnZERARatmyJL774AnXq1EG1atXw3nvvQavVYsmSJVCpVHBzc8P8+fMNyj19eu7mzZuQSCT49ttv0bVrVzg4OKBFixaIjY0tcr/37NmD4OBgo4fe5vfCCy9g6dKlGDBgQKGBWmRkJIYOHYpmzZqhRYsW2LJlC27fvm1wqi02NhZhb72FFwI6oZZXHYwYORItWrQwaB8XFxd06NABu3fvLrJOZcWgycrZSCRo5+2Mdt7OfIwKkRmVpa+xn5ZdpjoXmepcZKm14rL+lZ3vCuL860uTtzxIpVIsWLAAq1evxt27d0u9nU6dOuGnn37Cb7/9BgD49ddfcfr0abz88stFlvvzzz9x6NAhREZGYvfu3fjiiy/Qq1cv3L17FydOnMDixYsxc+ZMxMXFFbmdGTNmYMqUKUhISECjRo0wcOBA5OYW3kYnT55EmzZtSr6jJkhLSwMAODs7i2mdOnXCjz/8gMcPU+Agl+L4sWP47bffEBISYlC2bdu2OHXqlFnqpWfRWw6cPHkSS5cuRXx8PJKSkrBv3z7xnLFGo8HMmTNx8OBB3LhxA0qlEt27d8eiRYvg6ekpbiMnJwdTpkzB7t27kZWVhW7dumHt2rWoXbu2mCc1NRXjx4/H999/DwDo3bs3Vq9ejRo1aoh5bt++jTFjxuDo0aOwt7fHoEGDsGzZMsjl8mfSFoWxk0mxd1SARetAVBWUpa+xn5Zd09mHC13X1ec5g8dJ+X98pNBbsbTzdjb4LjotPoaHGWqjfDcX9SpDbf/12muvoWXLlvjoo4+wefPmUm3jgw8+QFpaGho3bgypVAqtVov58+dj4MCBRZbT6XT44osv4OTkhKZNm6Jr1664fv06Dh48CBsbG/j4+GDx4sU4fvw42rdvX+h2pkyZgl698tpjzpw5aNasGf744w80blzww6dv3rxpcBwuL4IgYNKkSejUqRN8fX3F9E8//RQjRoxApxY+sLW1hY2NDT7//HN06tTJoHytWrVw8+bNcq/X0yw60pSRkYEWLVpgzZo1RusyMzNx4cIFzJo1CxcuXMC3336L3377Db179zbIFx4ejn379mHPnj04ffo0njx5gtDQUGi1/3aoQYMGISEhAZGRkYiMjERCQgLCwsLE9VqtFr169UJGRgZOnz6NPXv24JtvvsHkyZPNt/NERFQpLF68GNu2bcPVq1dLVX7v3r3YsWMHdu3ahQsXLmDbtm1YtmwZtm3bVmS5evXqwcnJSXzv7u6Opk2bGkzOdnd3R0pKSpHbad68ubjs4eEBAEWWycrKMjg1d/v2bVSrVk18LViwoMjPK8zYsWNx8eJFo1Nsn376KeLi4vD9998jPj4ey5cvx+jRo3HkyBGDfPb29sjMzCzVZ5vKoiNNPXv2LHQWv1KpRHR0tEHa6tWr0bZtW9y+fRt16tRBWloaNm/ejO3bt6N79+4AgB07dsDLywtHjhxBSEgIrl27hsjISMTFxaFdu3YAgE2bNiEgIADXr1+Hj48PoqKicPXqVdy5c0eMnpcvX46hQ4di/vz5qF69uhlbgYiIrs4NgU6nw+P0x3Cq7mRw4M9/yjN+VvdCt5M/7+kPupZvRQvQuXNnhISE4MMPP8TQoUNLXP7999/HtGnTMGDAAACAn58fbt26hYULF2LIkCGFlpPJZAbvJRJJgWnFXYX3dBn9FX1FlXF1dTWY/O7p6YmEhATx/dOn1kw1btw4fP/99zh58qTBmaKsrCx8+OGH2Ldvnzga1rx5cyQkJGDZsmXisR8AHj58iOeee67En10SFeqO4GlpaZBIJOJptfj4eGg0GgQHB4t5PD094evri5iYGISEhCA2NhZKpVIMmACgffv2UCqViImJgY+PD2JjY+Hr62sw3BgSEoKcnBzEx8eja1fzd7rCZKpz0WnxMQB5nd9BXqG+MqIKoyx9jf207BzkttDpdMiVS+Egty3yUvaStO+z+i4WLVqEli1bolGjRiUum5mZabS/UqnUKu6BVJBWrVoZjKrZ2tri+eefL9W2BEHAuHHjsG/fPhw/fhze3t4G6zUaDTQaDQRIcPWvdACAj8qpwPa5fPkyWrVqVap6mKrC9Ozs7GxMmzYNgwYNEkd+kpOTIZfLUbNmTYO87u7uSE5OFvO4ubkZbc/Nzc0gj7u7u8H6mjVrQi6Xi3kKkpOTg5ycHPF9enreF6r/ksuDRpMrno/XaDTQSCx/G/nS0rdJebUNGWL7lo0pfa2wNq5M/dTcNBoNBEGATqczOugJ/3tMhn69NREEwaBe+d83a9YMgwYNEm8TUFj9C9rv0NBQzJ8/H7Vr10azZs3wyy+/YMWKFRg2bFih29G31dPr89fp6XSdTieWyV+P/MuF1VMvODgYX375ZbHfkVqtFoMrtVqNu3fv4sKFC6hWrZoYZI0ZMwa7d+/Gvn374OjoiL/++gtA3tkme3t7VKtWDYGBgZj2wVRM/GgRPGp5ITYyHl9++SWWLVtmUIdTp05hzpw5Rba9IAjQaDSQSqUG60z9f7NCBE0ajQYDBgyATqfD2rVri80vCII4xAjAYLksefJbuHAh5syZY5QeFRUFBweHYutpihwtoP+aDh+OgkJaZPYKIf9pVypfbN/S0QnAtBZ5y0ejo2BTxEVw+du4JGWrOltbW6hUKjx58gRqtfEEbQB4/PjxM65V8TQaDXJzcw3+OH76PZB3mu3//u//AMAg/WlZWVlG6+bNmwcHBweMHj0a//zzD1QqFYYMGYIpU6YUup2cnBxotVqD9QXVKTc3F2q12iBNX4cnT54AyJtfrF+vb/vMzMxCP7t3796YNm0a4uPj0bBhwwLzAHlznfz9/cX3y5cvx/Lly9GxY0f8+OOPAID169cDAF566SWDsp999hkGDRoEANiwYQPmzJmL6eNGIv1RKrzqeGHmzJkYNGiQWMezZ8/i0aNHCA4OLrTearUaWVlZOHnypNHVgabOhZIIghU8AQ95QcvTV8/paTQa9O/fHzdu3MDRo0fh4uIirjt69Ci6deuGhw8fGow2tWjRAn369MGcOXPwxRdfYNKkSXj06JHBdmvUqIGVK1di2LBhmD17Nr777jv8+uuv4vrU1FQ4Ozvj6NGjhZ6eK2ikycvLC//880+5zYPKVOeixcdHAQC/znqpQg/7azQaREdHIygoyOi8O5Ud29f82MZll52djTt37qBevXpG9/kRBAGPHz+Gk5NTkX+wUumUZ/vqr/jTBz3mphOAq0l5wVBTj+pGf5j0798frVq1wvTp0wvdRnZ2Nm7evAkvLy+j3156ejpcXV2RlpZW5PHbqo/A+oDp999/x7FjxwwCJgDw9/eHTCZDdHQ0+vfvDwBISkrC5cuXsWTJEgBAQEAA0tLScPbsWbRtm3fJ6pkzZ5CWloYOHTqIeebPn4+kpCTxyoGoqCgoFAqDKDk/hUJR4A27ZDJZuf2HKhP+/WXkbdeqvzKTlGf7kDG2r/mxjUtPq9VCIpHAxsbGaB6P/rSKfj2Vr/Js35kzZ+Kzzz6DIAhGp7rMQdD9O76TV/9/j405OTlo2bIlJk2aVOR+2djYiJPl8/dfU/uzRY/AT548wR9//CG+T0xMREJCApydneHp6YnXX38dFy5cwI8//gitVivOL3J2doZcLodSqcTw4cMxefJkuLi4wNnZGVOmTIGfn584o75Jkybo0aMHRowYgQ0bNgAARo4cidDQUPj4+ADIOz/btGlThIWFYenSpXj48CGmTJmCESNG8Mo5oipCnavDZ8fy/j8a0/V5yG1NP6iUpSxRRaRUKvHhhx9auhoA8gYwZs6c+Uw+y6JB0/nz5w1OfU2aNAkAMGTIEERERIg3o2zZsqVBuWPHjqFLly4AgJUrV8LW1hb9+/cXb265detWg8h3586dGD9+vHiVXe/evQ3uDSWVSnHgwAGMHj0aHTt2NLi5JRFVDbk6HT756XcAwKjA+pCX4DZ2ZSlLRBWHRYOmLl26oKgpVaZMt7Kzs8Pq1auLfLChs7MzduzYUeR26tSpI05MsyY2Egma11aKy0RERFWNBIC9XCouW0rFnyBTydnJpPh+bKfiMxIREVVSNjYSNHRzKj6jueth6QoQERERVQQMmoiIiIhMwNNzVi5LrUX3FScAAEcmBYrndImIiKoKnU7Ab/fzbrzZyN3J4JYDzxKDJisnQMC9R1niMhERUVUjAFBrdeKypfD0HBEREZEJGDQRERFVUaNGjYJEIsGqVauKzfvNN9+gadOmUCgUaNq0Kfbt22eUZ+3atfD29oadnR38/f1x6tQpM9Tachg0ERERVUH79+/HmTNn4OnpWWze2NhYvPHGGwgLC8Ovv/6KsLAw9O/fH2fOnBHz7N27F+Hh4ZgxYwZ++eUXvPjii+jZsydu375tzt14phg0ERERFaNLly4YN24cwsPDUbNmTbi7u2Pjxo3IyMjAsGHD4OTkhAYNGuDQoUNiGa1Wi+HDh8Pb2xv29vbw8fHBJ598Iq7Pzs5Gs2bNMHLkSDEtMTERSqUSmzZtMuv+3Lt3D2PHjsXOnTtNeu7aqlWrEBQUhOnTp6Nx48aYPn06unXrZjBCtWLFCgwfPhzvvPMOmjRpglWrVsHLywvr1q0z4548WwyaiIjI4jLVuchU5yJLrRWXi3vl/m9iMADkanXIVOciW6MtcLv5X6Wxbds2uLq64uzZsxg3bhzee+899OvXDx06dMCFCxcQEhKCsLAwZGZmAsh7QG7t2rXx1Vdf4erVq5g9ezY+/PBDfPXVVwDynmixc+dObNu2Dfv374dWq0VYWBi6du2KESNGFFqPnj17olq1akW+iqLT6RAWFob3338fzZo1M2nfY2NjxUeR6YWEhCAmJgYAoFarER8fb5QnODhYzFMZ8Oo5KyeBBA3dqonLRGQeZelr7Kdl13T24RKX+WxQa/Rq7gEAOHzlPsbsuoB23s7YOypAzNNp8TE8zFAblb25qFeJP69Fixbig2GnT5+ORYsWwdXVVQxwZs+ejXXr1uHixYto3749ZDIZ5syZI5b39vZGTEwMvvrqK/Tv3x9A3rNV582bhxEjRmDgwIH4888/sX///iLr8fnnnyMrK6vE9ddbsmQJbG1tMX78eJPLJCcnw93d3SDN3d0dycnJAIB//vkHWq22yDxlIQFgZ8vHqFAx7OVSRE8KtHQ1iCq9svQ19tOqoXnz5uKyVCqFi4sL/Pz8xDR9wJCSkiKmrV+/Hp9//jlu3bqFrKwsqNVqo4fQT548Gd999x1Wr16NQ4cOwdXVtch61KpVq9T7kJCQgE8//RQXLlyApITPM82fXxAEozRT8pSGjY0EjVSWf4wKgyYiIrK4q3NDoNPp8Dj9MZyqO8HGpvjZI3Lpv3lCmrnj6twQowebn/6ga7nVMf/cH4lEYpCmDw50urzThl999RUmTpyI5cuXIyAgAE5OTli6dKnB5GkgL8i6fv06pFIpfv/9d/To0aPIevTs2bPYq9KePHlSYHpsbCxSUlJQp04dMU2r1WLy5MlYtWoVbt68WWA5lUplNGKUkpIiBoqurq6QSqVF5qkMGDQREZHFOchtodPpkCuXwkFua1LQ9DRbqQ1spcZlHOSWO8ydOnUKHTp0wOjRo8W0P//80yjf22+/DV9fX4wYMQLDhw9Ht27d0LRp00K3W5bTc2+88QZ69epl0L76uVjDhg0rtFxAQACio6MxceJEMS0qKgodOnQAAMjlcvj7+yM6OhqvvfaamCc6OhqvvvpqqepqjRg0WbkstRa915wGAHw/thMfo0JkJmXpa+ynVJDnn38eX375JQ4fPgxvb29s374d586dg7e3t5jns88+Q2xsLC5evAgvLy8cOnQIgwcPxpkzZyCXywvcbllOzzk7O6NevXoGQZNMJoNKpYKPj4+Y9tZbb6FWrVpYuHAhAGDChAno3LkzFi9ejFdffRXfffcdjhw5gtOnT4tlJk2ahLCwMLRp0wYBAQHYuHEjbt++jXfffbfU9dXT6QT8kZI3eva8WzWLPUaFV89ZOQECfk95gt9TnvAxKkRmVJa+xn5KBXn33XfRt29fvPHGG2jXrh0ePHhgMOr03//+F++//z7Wrl0LLy8vAHlB1KNHjzBr1ixLVRsAcPv2bSQlJYnvO3TogD179mDLli1o3rw5tm7dir1796Jdu3ZinjfeeAOrVq3C3Llz0bJlS5w8eRIHDx5E3bp1y1wfAUB2rhbZuVqL9jCONBERAVDYSrF7RHtx+VmVpYrh+PHjRmkFzf8RhH8P6QqFAlu2bMGWLVsM8uhHbxo3bizenkCvevXqSExMLHuFS6Cg/Shof19//XW8/vrrRW5r9OjRBoFhZcOgiYgIgNRGgoAGLs+8LBFVHDw9R0RERGQCjjQREQHQaHXYfTbvGVkD29aBrIArscxRlogqDgZNRETIC3xmf3cFAPC6f+0SB02lLUtEFQeDJisngQS1atiLy0RERFWNBP/ezJSPUaFC2cul+HnaS5auBhERkcXY2EjQ2KO6pavBieBEREREpmDQRERERGQCnp6zctkaLfpviAUAfDUqAHYy3jiPiIiqFp1OwJ//5D1GpYErH6NChdAJAi7eTcPFu2nQCXw8AxFRRXH8+HFIJBI8evTI0lWp8ATkPeMxS23Zx6gwaCIiIjKDDh06ICkpCUql0tJVKdSDBw9Qu3Ztk4K7nJwcjBs3Dq6urnB0dETv3r1x9+5dgzypqakICwuDUqmEUqlEWFhYpQoaGTQRERGZgVwuh0qlgkRivbeLGT58OJo3b25S3vDwcOzbtw979uzB6dOn8eTJE4SGhkKr1Yp5Bg0ahISEBERGRiIyMhIJCQkICwszV/WfOQZNRERExejSpQvGjRuH8PBw1KxZE+7u7ti4cSMyMjIwbNgwODk5oUGDBjh06JBYJv/pua1bt6JGjRo4fPgwmjRpgmrVqqFHjx5ISkqyyD6tW7cOjx49wpQpU4rNm5aWhs2bN2P58uXo3r07WrVqhR07duDSpUs4cuQIAODatWuIjIzE559/joCAAAQEBGDTpk348ccfcf36dXPvzjPBoImIiCwuU52LTHUustRacbm4V65WJ5bP1eqQqc5FtkZb4Hbzv0pj27ZtcHV1xdmzZzFu3Di899576NevHzp06IALFy4gJCQEYWFhyMzMLHw/MzOxbNkybN++HSdPnsTt27eLDVqqVatW5Ktnz54l3perV69i7ty5+PLLL2FjU3woEB8fD41Gg+DgYDHN09MTvr6+iImJAQDExsZCqVSiXbt2Yp727dtDqVSKeSo6Xj1HREQW13T24RKX+WxQa/Rq7gEAOHzlPsbsuoB23s7YOypAzNNp8TE8zFAblb25qFeJP69FixaYOXMmAGD69OlYtGgRXF1dMWLECADA7NmzsW7dOly8eBHt27cvcBsajQbr169HgwYNAABjx47F3Llzi/zchISEItfb29uXaD9ycnIwcOBALF26FHXq1MGNGzeKLZOcnAy5XI6aNWsapLu7uyM5OVnM4+bmZlTWzc1NzFPRMWiqAJwd5ZauAlGVUJa+xn5a+T0990cqlcLFxQV+fn5imru7OwAgJSWl0G04ODiIARMAeHh4FJkfAJ5//vnSVhk9e/bEqVOnAAB169bFzz//jA8//BBNmjTBm2++Wert6gmCYDBnq6D5W/nzlJatCSNi5sagyco5yG1xYVaQpatBVOmVpa+xn5bd1bkh0Ol0eJz+GE7VnUw6ZSR/6sHIIc3ccXVuCGzyHZxPf9C13Oook8kM3kskEoM0fWCg0+lQmIK2IRRzO5lq1aoVuf7FF180mEv1tM8//xxZWVkA8gI9ADh27BguXbqEr7/+GgDEz3d1dcWMGTMwZ84co+2oVCqo1WqkpqYajDalpKSgQ4cOYp779+8blf3777/FgLK0pDYSNPW0/GNUGDQREZHFOchtodPpkCuXwkFua1LQ9DRbqQ1spcZlHOQV/zBXltNztWrVEpd1Oh3S09Pxf//3f8jJyRHTz507h7fffhunTp0yGAV7mr+/P2QyGaKjo9G/f38AQFJSEi5fvowlS5YAAAICApCWloazZ8+ibdu2AIAzZ84gLS1NDKwquor/ayIiIqrEynJ6riANGjQwCEr/+ecfAECTJk1Qo0YNAMC9e/fQrVs3fPnll2jbti2USiWGDx+OyZMnw8XFBc7OzpgyZQr8/PzQvXt3sXyPHj0wYsQIbNiwAQAwcuRIhIaGwsfHp1z3wVIYNFm5bI0WQ744CwDY9nZbPkaFyEzK0tfYT6my0Wg0uH79usGVgCtXroStrS369++PrKwsdOvWDVu3bhVP+wHAzp07MX78ePEqu969e2PNmjVlro9OJyDxQQYAwNvF0WKPUWHQZOV0goAziQ/FZSIyj7L0NfbTyu/48eNGaTdv3jRKe3p+UpcuXQzeDx06FEOHDjXI36dPn2LnNJlb/noCQL169YzS7OzssHr1aqxevbrQbTk7O2PHjh3lXkcBQEZOrrhsKQyaiIiQN6n4s0GtxeVnVZaIKg4GTUREyJtIrL/nz7MsS0QVB/8kIiIiIjIBR5qIiJD3GI7DV/LuMRPSzL3Ay9fNUZaIKg4GTUREANRaHcbsugAg70aLJQl8ylK2qrL05GeqesrjN8eeXQHYy6Sw5yXMRFQJ6C9PV6uNnwdHVBQbicToju8lob99Qv67speERUeaTp48iaVLlyI+Ph5JSUnYt28f+vTpI64XBAFz5szBxo0bkZqainbt2uGzzz5Ds2bNxDw5OTmYMmUKdu/eLd43Yu3atahdu7aYJzU1FePHj8f3338PIO++EatXrxZv4gUAt2/fxpgxY3D06FHY29tj0KBBWLZsGeRyyz5PykFui2sf97BoHYiIyoutrS0cHBzw999/QyaTGdxkUafTQa1WIzs7u8R3BKfiVfT2fd5FAQDQqHOgKUE5QRCQmZmJlJQU1KhRw+C+UiVl0aApIyMDLVq0wLBhw/Cf//zHaP2SJUuwYsUKbN26FY0aNcK8efMQFBSE69evw8nJCQAQHh6OH374AXv27IGLiwsmT56M0NBQxMfHiw0zaNAg3L17F5GRkQDy7lAaFhaGH374AQCg1WrRq1cvPPfcczh9+jQePHiAIUOGQBCEIu9HQUREJSORSODh4YHExETcunXLYJ0gCMjKyoK9vX25POCVDFX19q1RowZUKlWZtmHRoKlnz57o2bNngesEQcCqVaswY8YM9O3bFwCwbds2uLu7Y9euXRg1ahTS0tKwefNmbN++XbyN+44dO+Dl5YUjR44gJCQE165dQ2RkJOLi4tCuXTsAwKZNmxAQEIDr16/Dx8cHUVFRuHr1Ku7cuQNPT08AwPLlyzF06FDMnz8f1atb/iGBRESVhVwuR8OGDY1O0Wk0Gpw8eRKdO3cu0ykUKlhVbl+ZTFamESY9q50InpiYiOTkZPFW7ACgUCgQGBiImJgYjBo1CvHx8dBoNAZ5PD094evri5iYGISEhCA2NhZKpVIMmACgffv2UCqViImJgY+PD2JjY+Hr6ysGTAAQEhKCnJwcxMfHo2vXgp+SnZOTY/DQw/T0dAB5P0yNpiSDh4XL0WgxZs+vAIDPBrSAogLPbdK3SXm1DRli+5aNRpP71LIGGonxpNHC2tiUsmQs/0FMp9MhNzcXUqm0XA5wZKgit29Zj4U6nQ46na7Q9ab+v2m1QVNycjIAwN3d3SDd3d1dHNJNTk6GXC5HzZo1jfLoyycnJ8PNzc1o+25ubgZ58n9OzZo1IZfLxTwFWbhwIebMmWOUHhUVBQcHh+J20SQ5WuDEb3lf08HIw1BUrN95gaKjoy1dhUqN7Vs6OVpA/1/i4cNRRfa1/G1ckrJUPP6Gzasitq+5j4VPP2OvKFYbNOnlP+8qCEKx52Lz5ykof2ny5Dd9+nRMmjRJfJ+eng4vLy8EBweX2ym9THUupp49CgAICQmGg9zqv7JCaTQaREdHIygoqMoNDT8LbN+yMaWvFdbGlamfWhJ/w+ZVkdvX3H1Mf6aoOFbbs/WTtZKTk+Hh8e/jCVJSUsRRIZVKBbVajdTUVIPRppSUFHTo0EHMc//+faPt//333wbbOXPmjMH61NRUaDQaoxGopykUCigUCqN0mUxWbj9ImfBv0Ja3Xav9ykxWnu1Dxti+pVOSvpa/jStjP7Uk/obNqyK2r7n7mKntYbXXHHp7e0OlUhkMI6rVapw4cUIMiPz9/SGTyQzyJCUl4fLly2KegIAApKWl4ezZs2KeM2fOIC0tzSDP5cuXkZSUJOaJioqCQqGAv7+/WfeTiIiIKgaL/jn05MkT/PHHH+L7xMREJCQkwNnZGXXq1EF4eDgWLFiAhg0bomHDhliwYAEcHBwwaNAgAIBSqcTw4cMxefJkuLi4wNnZGVOmTIGfn594NV2TJk3Qo0cPjBgxAhs2bACQd8uB0NBQ+Pj4AACCg4PRtGlThIWFYenSpXj48CGmTJmCESNG8Mo5IiIiAmDhoOn8+fMGV6bp5wcNGTIEW7duxdSpU5GVlYXRo0eLN7eMiooS79EEACtXroStrS369+8v3txy69atBlcG7Ny5E+PHjxevsuvduzfWrFkjrpdKpThw4ABGjx6Njh07GtzckoiIiAiwcNDUpUuXIp8FI5FIEBERgYiIiELz2NnZYfXq1UXehNLZ2Rk7duwosi516tTBjz/+WGydiYiIqGribEUr5yC3xc1FvSxdDaJKryx9jf2UyLyspY9Z7URwIiIiImvCoImIiIjIBAyarFy2RovRO+Mxemc8sjVaS1eHqNIqS19jPyUyL2vpYwyarJxOEHDwUjIOXkqGrohJ80RUNmXpa+ynROZlLX2ME8GJiADIpDaY+2ozcflZlSWiioNBExER8oKdtwLqPfOyRFRx8E8iIiIiIhNwpImICIBWJ+Bs4kMAQFtvZ0htJMWUKJ+yRFRxMGgiIgKQk6vFwE1xAICrc0PgIDf9v8eylCWiioOn54iIiIhMwD+HrJy9TIqrc0PEZSIioqrGWo6FDJqsnEQi4VA/ERFVadZyLOTpOSIiIiITMGiycjm5Wkz+6ldM/upX5OTy8QxERFT1WMuxkEGTldPqBHxz4S6+uXAXWh0fz0BERFWPtRwLGTQRERERmYBBExEREZEJGDQRERERmYBBExEREZEJGDQRERERmYBBExEREZEJLH97TSqSvUyK+JndxWUiMo+y9DX2UyLzspY+xqDJykkkErhUU1i6GkSVXln6GvspkXlZSx/j6TkiIiIiE3Ckycrl5Gox78drAICZoU2gsOXQP5E5lKWvsZ8SmZe19DGONFk5rU7A9rhb2B53i49RITKjsvQ19lMi87KWPsaRJiIiALY2NpjQraG4/KzKElHFwaCJiAiA3NYGE4MaPfOyRFRx8E8iIiIiIhNwpImICIBOJ+CPv58AAJ5/rhpsbCTPpCwRVRwMmoiIAGTnahG88iQA4OrcEDjITf/vsSxliaji4Ok5IiIiIhPwzyErZ2crxampXcVlIiKiqsZajoUMmqycjY0EXs4Olq4GERGRxVjLsZCn54iIiIhMwJEmK6fO1WFZ1HUAwJRgH8htGecSEVHVYi3HQh6BrVyuToeNJ29g48kbyNXpLF0dIiKiZ85ajoUMmoiIiIhMwKCJiIiIyAQMmoiIiIhMwKCJiIiIyAQMmoiIiIhMYNVBU25uLmbOnAlvb2/Y29ujfv36mDt3LnRPzZwXBAERERHw9PSEvb09unTpgitXrhhsJycnB+PGjYOrqyscHR3Ru3dv3L171yBPamoqwsLCoFQqoVQqERYWhkePHj2L3SQiIqIKwKrv07R48WKsX78e27ZtQ7NmzXD+/HkMGzYMSqUSEyZMAAAsWbIEK1aswNatW9GoUSPMmzcPQUFBuH79OpycnAAA4eHh+OGHH7Bnzx64uLhg8uTJCA0NRXx8PKTSvNuxDxo0CHfv3kVkZCQAYOTIkQgLC8MPP/xgmZ3/HztbKaImdhaXicg8ytLX2E+JzMta+phVB02xsbF49dVX0atXLwBAvXr1sHv3bpw/fx5A3ijTqlWrMGPGDPTt2xcAsG3bNri7u2PXrl0YNWoU0tLSsHnzZmzfvh3du3cHAOzYsQNeXl44cuQIQkJCcO3aNURGRiIuLg7t2rUDAGzatAkBAQG4fv06fHx8LLD3eWxsJGjk7mSxzyeqKsrS19hPiczLWvqYVQdNnTp1wvr16/Hbb7+hUaNG+PXXX3H69GmsWrUKAJCYmIjk5GQEBweLZRQKBQIDAxETE4NRo0YhPj4eGo3GII+npyd8fX0RExODkJAQxMbGQqlUigETALRv3x5KpRIxMTGFBk05OTnIyckR36enpwMANBoNNBpNeTZFpaBvE7aNebB9zY9tbF5sX/Ni+xbO1Dax6qDpgw8+QFpaGho3bgypVAqtVov58+dj4MCBAIDk5GQAgLu7u0E5d3d33Lp1S8wjl8tRs2ZNozz68snJyXBzczP6fDc3NzFPQRYuXIg5c+YYpUdFRcHBoXweLJirA6Lv5U09C6qlQ2V4ikp0dLSlq1CpsX1LpyR9LX8bV8Z+akn8DZtXRWxfc/exzMxMk/JZddC0d+9e7NixA7t27UKzZs2QkJCA8PBweHp6YsiQIWI+iURiUE4QBKO0/PLnKSh/cduZPn06Jk2aJL5PT0+Hl5cXgoODUb169WL3zxSZ6lxM/vgoAGDxsO5wkFv1V1YkjUaD6OhoBAUFQSaTWbo6lQ7bt2xM6WuFtXFl6qeWxN+weVXk9jV3H9OfKSqOVffs999/H9OmTcOAAQMAAH5+frh16xYWLlyIIUOGQKVSAcgbKfLw8BDLpaSkiKNPKpUKarUaqampBqNNKSkp6NChg5jn/v37Rp//999/G41iPU2hUEChUBily2SycvtByoR/g7a87Vr1V2aS8mwfMsb2LR07iQ3C2tfNW1bIIStismn+Ni5JWSoef8PmVRHb19zHQlPbw6oHkTMzM2FjY1hFqVQq3nLA29sbKpXKYKhRrVbjxIkTYkDk7+8PmUxmkCcpKQmXL18W8wQEBCAtLQ1nz54V85w5cwZpaWliHiKq3BS2Unzcxxcf9/GFooRBT1nKElHFYdXDFq+88grmz5+POnXqoFmzZvjll1+wYsUKvP322wDyTqmFh4djwYIFaNiwIRo2bIgFCxbAwcEBgwYNAgAolUoMHz4ckydPhouLC5ydnTFlyhT4+fmJV9M1adIEPXr0wIgRI7BhwwYAebccCA0NteiVc0RERGQ9rDpoWr16NWbNmoXRo0cjJSUFnp6eGDVqFGbPni3mmTp1KrKysjB69GikpqaiXbt2iIqKEu/RBAArV66Era0t+vfvj6ysLHTr1g1bt24V79EEADt37sT48ePFq+x69+6NNWvWPLudJSKLEgQBDzPUAABnR3mx8yLLqywRVRxWHTQ5OTlh1apV4i0GCiKRSBAREYGIiIhC89jZ2WH16tVYvXp1oXmcnZ2xY8eOMtSWiCqyLI0W/vOOAACuzg0p0UTTspQloorDquc0EREREVkL/jlk5RS2Unw3pqO4TEREVNVYy7GQQZOVk9pI0MKrhqWrQUREZDHWcizk6TkiIiIiE3Ckycqpc3XY8nMiAGBYR2/I+XwGIiKqYqzlWMigycrl6nRYeOi/AICwgLqQc3CQiIiqGGs5FvIITERERGQCBk1EREREJihV0FS/fn08ePDAKP3Ro0eoX79+mStFREREZG1KFTTdvHkTWq3WKD0nJwf37t0rc6WIiIiIrE2JJoJ///334vLhw4ehVCrF91qtFj/99BPq1atXbpUjIiIishYlCpr69OkDIO95b0OGDDFYJ5PJUK9ePSxfvrzcKkdERERkLUoUNOl0OgCAt7c3zp07B1dXV7NUiv6lsJVi94j24jIRmUdZ+hr7KZF5WUsfK9V9mhITE8u7HlQIqY0EAQ1cLF0NokqvLH2N/ZTIvKylj5X65pY//fQTfvrpJ6SkpIgjUHpffPFFmStGREREZE1KFTTNmTMHc+fORZs2beDh4QGJRFLe9aL/0Wh12H32NgBgYNs6kEl5ay0icyhLX2M/JTIva+ljpQqa1q9fj61btyIsLKy860P5aLQ6zP7uCgDgdf/a/M+YyEzK0tfYT4nMy1r6WKmCJrVajQ4dOpR3XYiILMZGIsHLfipx+VmVJaKKo1RB0zvvvINdu3Zh1qxZ5V0fIiKLsJNJsXaw/zMvS0QVR6mCpuzsbGzcuBFHjhxB8+bNIZPJDNavWLGiXCpHREREZC1KFTRdvHgRLVu2BABcvnzZYB0nhRMREVFlVKqg6dixY+VdDyIii8pU56Lp7MMAgKtzQ+AgN/2/x7KUJaKKg5d4EBEREZmgVH8Ode3atcjTcEePHi11hciQXGqDL4a2EZeJiIiqGms5FpYqaNLPZ9LTaDRISEjA5cuXjR7kS2VjK7XBS43dLV0NIiIii7GWY2GpgqaVK1cWmB4REYEnT56UqUJERERE1qhcx7jefPNNPneunGm0Ovzf+Tv4v/N3oNHqii9ARERUyVjLsbBcL/GIjY2FnZ1deW6yytNodXj/64sAgF7NPfh4BiIiqnKs5VhYqqCpb9++Bu8FQUBSUhLOnz/Pu4QTERFRpVSqoEmpVBq8t7GxgY+PD+bOnYvg4OByqRgRERGRNSlV0LRly5byrgcRERGRVSvTnKb4+Hhcu3YNEokETZs2RatWrcqrXkRERERWpVRBU0pKCgYMGIDjx4+jRo0aEAQBaWlp6Nq1K/bs2YPnnnuuvOtJREREZFGlmn4+btw4pKen48qVK3j48CFSU1Nx+fJlpKenY/z48eVdRyIiIiKLK9VIU2RkJI4cOYImTZqIaU2bNsVnn33GieDlTC61wWeDWovLRGQeZelr7KdE5mUtfaxUQZNOp4NMJjNKl8lk0Ol4A8byZCu1Qa/mHpauBlGlV5a+xn5KZF7W0sdKFa699NJLmDBhAv766y8x7d69e5g4cSK6detWbpUjIiIishalCprWrFmDx48fo169emjQoAGef/55eHt74/Hjx1i9enV517FKy9XqcOBiEg5cTEIuH6NCZDZl6Wvsp0TmZS19rFSn57y8vHDhwgVER0fjv//9LwRBQNOmTdG9e/fyrl+Vp9bqMGbXBQDA1bkhsOV8CSKzKEtfYz8lMi9r6WMlCpqOHj2KsWPHIi4uDtWrV0dQUBCCgoIAAGlpaWjWrBnWr1+PF1980SyVJSIyFxuJBO28ncXlZ1WWiCqOEoVqq1atwogRI1C9enWjdUqlEqNGjcKKFSvKrXJERM+KnUyKvaMCcCbxIexk0lKV3TsqoMRliajiKFHQ9Ouvv6JHjx6Frg8ODkZ8fHyZK0VERERkbUoUNN2/f7/AWw3o2dra4u+//y5zpYiIiIisTYmCplq1auHSpUuFrr948SI8PMr3Pgr37t3Dm2++CRcXFzg4OKBly5YGo1mCICAiIgKenp6wt7dHly5dcOXKFYNt5OTkYNy4cXB1dYWjoyN69+6Nu3fvGuRJTU1FWFgYlEollEolwsLC8OjRo3LdFyKyXpnqXLT+OFpcLk3Z1h9Hl7gsEVUcJQqaXn75ZcyePRvZ2dlG67KysvDRRx8hNDS03CqXmpqKjh07QiaT4dChQ7h69SqWL1+OGjVqiHmWLFmCFStWYM2aNTh37hxUKhWCgoLw+PFjMU94eDj27duHPXv24PTp03jy5AlCQ0Oh1WrFPIMGDUJCQgIiIyMRGRmJhIQEhIWFldu+EJH1e5ihLlPZspQnIutXoqvnZs6ciW+//RaNGjXC2LFj4ePjA4lEgmvXruGzzz6DVqvFjBkzyq1yixcvhpeXF7Zs2SKm1atXT1wWBAGrVq3CjBkz0LdvXwDAtm3b4O7ujl27dmHUqFFIS0vD5s2bsX37dvGWCDt27ICXlxeOHDmCkJAQXLt2DZGRkYiLi0O7du0AAJs2bUJAQACuX78OHx+fctunkpJJbbD09ebiMhERUVVjLcfCEn2yu7s7YmJi4Ovri+nTp+O1115Dnz598OGHH8LX1xc///wz3N3dy61y33//Pdq0aYN+/frBzc0NrVq1wqZNm8T1iYmJSE5ONnjenUKhQGBgIGJiYgAA8fHx0Gg0Bnk8PT3h6+sr5omNjYVSqRQDJgBo3749lEqlmMdSZFIb9GvjhX5tvBg0ERFRlWQtx8IS39yybt26OHjwIFJTU/HHH39AEAQ0bNgQNWvWLPfK3bhxA+vWrcOkSZPw4Ycf4uzZsxg/fjwUCgXeeustJCcnA4BRoObu7o5bt24BAJKTkyGXy43q5+7uLpZPTk6Gm5ub0ee7ubmJeQqSk5ODnJwc8X16ejoAQKPRQKPRlGKPKzd9m7BtzIPtWzYaTe5TyxpoJEIBeQpuY1PKUvH4GzYvtm/hTG2TUt0RHABq1qyJF154obTFTaLT6dCmTRssWLAAANCqVStcuXIF69atw1tvvSXmk+S7mZwgCEZp+eXPU1D+4razcOFCzJkzxyg9KioKDg4ORX6+qbQC8N9HeXVoXEOAtBLcNy86OtrSVajU2L6lk6MF9P8lHj4cBUURt1vK38YlKUvF42/YvCpi+5r7WJiZmWlSvlIHTc+Ch4cHmjZtapDWpEkTfPPNNwAAlUoFIG+k6Omr9lJSUsTRJ5VKBbVajdTUVIPRppSUFHTo0EHMc//+faPP//vvv4s83Th9+nRMmjRJfJ+eng4vLy8EBwcXeAPQ0shU52LSx0cBAL/OegkOcqv+yoqk0WgQHR2NoKCgIm9dQaXD9i2bTHUupp7N62shIcEF9rXC2tiUslQ8/obNqyK3r7mPhfozRcWx6p7dsWNHXL9+3SDtt99+Q926dQEA3t7eUKlUiI6ORqtWrQAAarUaJ06cwOLFiwEA/v7+kMlkiI6ORv/+/QEASUlJuHz5MpYsWQIACAgIQFpaGs6ePYu2bdsCAM6cOYO0tDQxsCqIQqGAQqEwSpfJZOX2g5QJ/4bTedu16q/MJOXZPmSM7Vs6Jelr+du4MvZTS+Jv2LwqYvuau4+Z2h5W3bMnTpyIDh06YMGCBejfvz/Onj2LjRs3YuPGjQDyTqmFh4djwYIFaNiwIRo2bIgFCxbAwcEBgwYNApD3eJfhw4dj8uTJcHFxgbOzM6ZMmQI/Pz/xaromTZqgR48eGDFiBDZs2AAAGDlyJEJDQy165RwRERFZD6sOml544QXs27cP06dPx9y5c+Ht7Y1Vq1Zh8ODBYp6pU6ciKysLo0ePRmpqKtq1a4eoqCg4OTmJeVauXAlbW1v0798fWVlZ6NatG7Zu3Qqp9N+JBzt37sT48ePFq+x69+6NNWvWPLudJSIiIqtm1UETAISGhhZ5w0yJRIKIiAhEREQUmsfOzg6rV6/G6tWrC83j7OyMHTt2lKWqREREVInxxj9EREREJmDQRERERGQCqz89V9XJpDaY+2ozcZmIzEPf12Z/d6XEfY39lMi8rKWPMWiycjKpDd4KqGfpahBVevq+Vtqgif2UyHyspY/xTyIiIiIiE3CkycppdQLOJj4EALT1dobUphI8R4XICj3d17Q6oUR9jf2UyLyspY9xpMnK5eRqMXBTHAZuikNOrtbS1SGqtPR9Tb9cmrLsp0TmYS19jEETEREACSRo6FZNXC5N2YZu1UpclogqDp6eIyICYC+XInpSIOpNOwB7ubT4AgWUJaLKjSNNRERERCZg0ERERERkAgZNREQAstRaBK04IS6XpmzQihMlLktEFQfnNBERARAg4PeUJ+LysypLRBUHgyYrZ2tjg+k9G4vLREREVY21HAsZNFk5ua0NRgU2sHQ1iIiILMZajoUcuiAiIiIyAUearJxWJ+DyvTQAgG8tJR/PQEREVY61HAs50mTlcnK1ePWzn/HqZz/z8QxERFQlWcuxkEETERERkQkYNBERERGZgEETERERkQkYNBERERGZgEETERERkQkYNBERERGZgPdpsnK2NjaY0K2huExE5qHva5/89HuJ+xr7KZF5WUsfY9Bk5eS2NpgY1MjS1SCq9PR97ZOffofctmT/KbOfEpmXtfQx/klEREREZAKONFk5nU7AH38/AQA8/1w12PAxKkRm8XRf0+mEEvU19lMi87KWPsaRJiuXnatF8MqTCF55Etl8jAqR2ej7mn65NGXZT4nMw1r6GIMmIqL/cXaUl6lsWcoTkfXj6TkiIgAOcltcmBWEetMOwEFesv8a9WWJqHLjSBMRERGRCRg0EREREZmAQRMREYBsjRZvbIgVl0tT9o0NsSUuS0QVB+c0EREB0AkCziQ+FJefVVkiqjgYNFk5WxsbjOxcX1wmIiKqaqzlWMigycrJbW3w4ctNLF0NIiIii7GWYyGHLoiIiIhMwJEmK6fTCbj3KAsAUKuGPR/PQEREVY61HAs50mTlsnO1eHHJMby45Bgfz0BERFWStRwLGTQRERERmYBBExEREZEJGDQRERERmYBBExEREZEJKlTQtHDhQkgkEoSHh4tpgiAgIiICnp6esLe3R5cuXXDlyhWDcjk5ORg3bhxcXV3h6OiI3r174+7duwZ5UlNTERYWBqVSCaVSibCwMDx69OgZ7BURERFVBBUmaDp37hw2btyI5s2bG6QvWbIEK1aswJo1a3Du3DmoVCoEBQXh8ePHYp7w8HDs27cPe/bswenTp/HkyROEhoZCq/13Bv6gQYOQkJCAyMhIREZGIiEhAWFhYc9s/4iIiMi6VYj7ND158gSDBw/Gpk2bMG/ePDFdEASsWrUKM2bMQN++fQEA27Ztg7u7O3bt2oVRo0YhLS0Nmzdvxvbt29G9e3cAwI4dO+Dl5YUjR44gJCQE165dQ2RkJOLi4tCuXTsAwKZNmxAQEIDr16/Dx8fn2e/0/0htJAhrX1dcJiLz0Pe17XG3StzX2E+JzMta+liFCJrGjBmDXr16oXv37gZBU2JiIpKTkxEcHCymKRQKBAYGIiYmBqNGjUJ8fDw0Go1BHk9PT/j6+iImJgYhISGIjY2FUqkUAyYAaN++PZRKJWJiYgoNmnJycpCTkyO+T09PBwBoNBpoNJpy2XcbALN7/e/zBR00Gl25bNcS9G1SXm1Dhti+ZaPva1+duwmbQvpaYW1cmfqpJfE3bF4VuX3N3cdMbROrD5r27NmDCxcu4Ny5c0brkpOTAQDu7u4G6e7u7rh165aYRy6Xo2bNmkZ59OWTk5Ph5uZmtH03NzcxT0EWLlyIOXPmGKVHRUXBwcGhmD2ruqKjoy1dhUqN7Vs2S9oCBw8eLDIP29i82L7mxfY1lpmZaVI+qw6a7ty5gwkTJiAqKgp2dnaF5pNIDIfqBEEwSssvf56C8he3nenTp2PSpEni+/T0dHh5eSE4OBjVq1cv8vNNJQgCHmbmRcDODrJi98uaaTQaREdHIygoCDKZzNLVqXTYvmWj72udlxzD5YjgAvtaYW1cmfqpJfE3bF4VuX3N3cf0Z4qKY9VBU3x8PFJSUuDv7y+mabVanDx5EmvWrMH169cB5I0UeXh4iHlSUlLE0SeVSgW1Wo3U1FSD0aaUlBR06NBBzHP//n2jz//777+NRrGeplAooFAojNJlMlm5/SAz1blov+g4AODq3BA4yKz6KzNJebYPGWP7ls6/fU2CXNgU2dfyt3Fl7KeWxN+weVXE9jV3HzO1Paz66rlu3brh0qVLSEhIEF9t2rTB4MGDkZCQgPr160OlUhkMNarVapw4cUIMiPz9/SGTyQzyJCUl4fLly2KegIAApKWl4ezZs2KeM2fOIC0tTcxDREREVZtV/znk5OQEX19fgzRHR0e4uLiI6eHh4ViwYAEaNmyIhg0bYsGCBXBwcMCgQYMAAEqlEsOHD8fkyZPh4uICZ2dnTJkyBX5+fuLVdE2aNEGPHj0wYsQIbNiwAQAwcuRIhIaGWvTKOSJ6dhzktri5qBfqTTsAB3nJ/mvUlyWiys2qgyZTTJ06FVlZWRg9ejRSU1PRrl07REVFwcnJScyzcuVK2Nraon///sjKykK3bt2wdetWSKVSMc/OnTsxfvx48Sq73r17Y82aNc98f4iIiMg6Vbig6fjx4wbvJRIJIiIiEBERUWgZOzs7rF69GqtXry40j7OzM3bs2FFOtSQiIqLKxqrnNBERPSvZGi1G74wXl0tTdvTO+BKXJaKKg0ETEREAnSDg4KVkcbk0ZQ9eSi5xWSKqOCrc6bmqRmojwX9a1xaXiYiIqhprORYyaLJyClsplvdvYelqEBERWYy1HAt5eo6IiIjIBBxpsnKCICDrfxNL7WVSPp6BiIiqHGs5FnKkycplabRoOvswms4+LP5giIiIqhJrORYyaCIiIiIyAYMmIiIiIhMwaCIiIiIyAYMmIiIiIhMwaCIiIiIyAYMmIiIiIhPwPk1WzkYiwct+KnGZiMxD39cOXkoucV9jPyUyL2vpYwyarJydTIq1g/0tXQ2iSk/f1+pNOwA7mbRUZYnIPKylj/H0HBEREZEJGDQRERERmYBBk5XLVOei3rQDqDftADLVuZauDlGlpe9r+uXSlGU/JTIPa+ljDJqIiIiITMCgiYgIeU9Oj5/ZXVwuTdn4md1LXJaIKg5ePUdEBEAikcClmkJcLm1ZIqq8ONJEREREZAKONBERAcjJ1WLej9fEZYWt6afZni47M7RJicoSUcXBkSYiIgBanYDtcbfE5dKU3R53q8Rliaji4EiTlbORSNDV5zlxmYiIqKqxlmMhgyYrZyeTYsuwtpauBhERkcVYy7GQp+eIiIiITMCgiYiIiMgEDJqsXKY6F01mRaLJrEg+noGIiKokazkWck5TBZCl0Vq6CkRERBZlDcdCjjQRERERmYBBExEREZEJGDQRERERmYBBExEREZEJGDQRERERmYBXz1k5G4kE7bydxWUiMg99XzuT+LDEfY39lMi8rKWPMWiycnYyKfaOCrB0NYgqPX1fqzftAOxk0lKVJSLzsJY+xtNzRERERCZg0ERERERkAgZNVi5TnYvWH0ej9cfRfIwKkRnp+5p+uTRl2U+JzMNa+hjnNFUADzPUlq4CUZVQlr7GfkpkXtbQxzjSREQEwM5WiqiJncXl0pSNmti5xGWJqOLgSBMREQAbGwkauTuJy6UtS0SVl1WPNC1cuBAvvPACnJyc4Obmhj59+uD69esGeQRBQEREBDw9PWFvb48uXbrgypUrBnlycnIwbtw4uLq6wtHREb1798bdu3cN8qSmpiIsLAxKpRJKpRJhYWF49OiRuXeRiIiIKgirDppOnDiBMWPGIC4uDtHR0cjNzUVwcDAyMjLEPEuWLMGKFSuwZs0anDt3DiqVCkFBQXj8+LGYJzw8HPv27cOePXtw+vRpPHnyBKGhodBqtWKeQYMGISEhAZGRkYiMjERCQgLCwsKe6f4SkeWoc3VYGf2buFyasiujfytxWSKqOKz69FxkZKTB+y1btsDNzQ3x8fHo3LkzBEHAqlWrMGPGDPTt2xcAsG3bNri7u2PXrl0YNWoU0tLSsHnzZmzfvh3du3cHAOzYsQNeXl44cuQIQkJCcO3aNURGRiIuLg7t2rUDAGzatAkBAQG4fv06fHx8nu2OE9Ezl6vT4ZOffheX5SX4m/LpsqMC65eoLBFVHFYdNOWXlpYGAHB2zruVemJiIpKTkxEcHCzmUSgUCAwMRExMDEaNGoX4+HhoNBqDPJ6envD19UVMTAxCQkIQGxsLpVIpBkwA0L59eyiVSsTExBQaNOXk5CAnJ0d8n56eDgDQaDTQaDTlss/aXC38alX/33IuNBKhXLZrCfo2Ka+2IUNs37LRaHKfWtYU2NcKa2NTylLx+Bs2r4rcvuY+FpraJhUmaBIEAZMmTUKnTp3g6+sLAEhOTgYAuLu7G+R1d3fHrVu3xDxyuRw1a9Y0yqMvn5ycDDc3N6PPdHNzE/MUZOHChZgzZ45RelRUFBwcHEqwd0V7p07ev0ejD5fbNi0pOjra0lWo1Ni+pZOjBfT/JR4+HAVFERfB5W/jkpSl4vE3bF4VtX3NeSzMzMw0KV+FCZrGjh2Lixcv4vTp00brJPke3icIglFafvnzFJS/uO1Mnz4dkyZNEt+np6fDy8sLwcHBqF69epGfXxVpNBpER0cjKCgIMpnM0tWpdNi+ZZOpzsXUs0cBACEhwXCQG//3WFgbm1KWisffsHmxfQunP1NUnArRs8eNG4fvv/8eJ0+eRO3atcV0lUoFIG+kyMPDQ0xPSUkRR59UKhXUajVSU1MNRptSUlLQoUMHMc/9+/eNPvfvv/82GsV6mkKhgEKhMEqXyWT8QRaB7WNebN/SkQn//oGU14aF//eYv41LUpaKx9+webF9jZnaHlY9W1EQBIwdOxbffvstjh49Cm9vb4P13t7eUKlUBkONarUaJ06cEAMif39/yGQygzxJSUm4fPmymCcgIABpaWk4e/asmOfMmTNIS0sT81hKllqLjouOouOio8hSa4svQEREVMlYy7HQqv8cGjNmDHbt2oXvvvsOTk5O4vwipVIJe3t7SCQShIeHY8GCBWjYsCEaNmyIBQsWwMHBAYMGDRLzDh8+HJMnT4aLiwucnZ0xZcoU+Pn5iVfTNWnSBD169MCIESOwYcMGAMDIkSMRGhpq8SvnBAi49yhLXCYiIqpqrOVYaNVB07p16wAAXbp0MUjfsmULhg4dCgCYOnUqsrKyMHr0aKSmpqJdu3aIioqCk9O/d+dduXIlbG1t0b9/f2RlZaFbt27YunUrpNJ/Z2vu3LkT48ePF6+y6927N9asWWPeHSQiIqIKw6qDJkEoPpqUSCSIiIhAREREoXns7OywevVqrF69utA8zs7O2LFjR2mqSURERFWAVc9pIiIiIrIWDJqIiIiITMCgiYiIiMgEVj2niQAJJGjoVk1cJiLz0Pe131OelLivsZ8SmZe19DEGTVbOXi5F9KRAS1eDqNLT97V60w7AXl6y56CwnxKZl7X0MZ6eIyIiIjIBgyYiIiIiEzBosnJZai2CVpxA0IoTfIwKkRnp+5p+uTRl2U+JzMNa+hjnNFk5AQJ+T3kiLhOReZSlr7GfEpmXtfQxjjQREQFQ2Eqxe0R7cbk0ZXePaF/iskRUcXCkiYgIgNRGgoAGLuJyacsSUeXFkSYiIiIiE3CkiYgIgEarw+6zt8VlmdT0vymfLjuwbZ0SlSWiioNBExER8gKf2d9dEZdLGjTpy77uX5tBE1ElxaDJykkgQa0a9uIyERFRVWMtx0IGTVbOXi7Fz9NesnQ1iIiILMZajoUcQyYiIiIyAYMmIiIiIhMwaLJy2Roteq85jd5rTiNbw8czEBFR1WMtx0LOabJyOkHAxbtp4jIREVFVYy3HQo40EREREZmAQRMRERGRCRg0EREREZmAQRMRERGRCRg0EREREZmAV89VAM6OcktXgahKcHaU42GGutRlich8rKGPMWiycg5yW1yYFWTpahBVevq+Vm/aATjIS/ZfI/spkXlZSx/j6TkiIiIiEzBoIiIiIjIBgyYrl63R4o0NsXhjQywfo0JkRvq+pl8uTVn2UyLzsJY+xjlNVk4nCDiT+FBcJiLzKEtfYz8lMi9r6WMcaSIiAiCX2uCzQa3F5dKU/WxQ6xKXJaKKg72biAiArdQGvZp7iMulKduruUeJyxJRxcHeTURERGQCzmkiIgKQq9Xh8JX74nJJRoyeLhvSzJ2jTUSVFIMmIiIAaq0OY3ZdEJdLEvg8Xfbq3BAGTUSVFIOmCsBeJrV0FYiIiCzKGo6FDJqsnIPcFtc+7mHpahAREVmMtRwLOYZMREREZAIGTUREREQmYNBk5bI1WgzbchbDtpzl4xmIiKhKspZjIec0WTmdIODY9b/FZSIioqrGWo6FHGkiIiIiMgGDpnzWrl0Lb29v2NnZwd/fH6dOnbJ0lYiIiMgKMGh6yt69exEeHo4ZM2bgl19+wYsvvoiePXvi9u3blq4aERERWRiDpqesWLECw4cPxzvvvIMmTZpg1apV8PLywrp16yxdNSIiIrIwBk3/o1arER8fj+DgYIP04OBgxMTEWKhWREREZC149dz//PPPP9BqtXB3dzdId3d3R3JycoFlcnJykJOTI75PS0sDADx8+BAajaZc6pWpzoUuJxMA8ODBA2TJK+5XptFokJmZiQcPHkAmk1m6OpUO27dsTOlrhbVxZeqnlsTfsHlV5PY1dx97/PgxAEAo5so89ux8JBKJwXtBEIzS9BYuXIg5c+YYpXt7e5ulbnVWmWWzRJRPWfoa+ymReZmzjz1+/BhKpbLQ9Qya/sfV1RVSqdRoVCklJcVo9Elv+vTpmDRpkvhep9Ph4cOHcHFxKTTQqsrS09Ph5eWFO3fuoHr16pauTqXD9jU/trF5sX3Ni+1bOEEQ8PjxY3h6ehaZj0HT/8jlcvj7+yM6OhqvvfaamB4dHY1XX321wDIKhQIKhcIgrUaNGuasZqVQvXp1dlgzYvuaH9vYvNi+5sX2LVhRI0x6DJqeMmnSJISFhaFNmzYICAjAxo0bcfv2bbz77ruWrhoRERFZGIOmp7zxxht48OAB5s6di6SkJPj6+uLgwYOoW7eupatGREREFsagKZ/Ro0dj9OjRlq5GpaRQKPDRRx8ZndKk8sH2NT+2sXmxfc2L7Vt2EqG46+uIiIiIiDe3JCIiIjIFgyYiIiIiEzBoIiIiIjIBgyYiIiIiEzBoonI3f/58dOjQAQ4ODoXe7PP27dt45ZVX4OjoCFdXV4wfPx5qtdogz6VLlxAYGAh7e3vUqlULc+fOLfa5QFVVvXr1IJFIDF7Tpk0zyGNKm1Ph1q5dC29vb9jZ2cHf3x+nTp2ydJUqpIiICKPfqkqlEtcLgoCIiAh4enrC3t4eXbp0wZUrVyxYY+t38uRJvPLKK/D09IREIsH+/fsN1pvSpjk5ORg3bhxcXV3h6OiI3r174+7du89wLyoGBk1U7tRqNfr164f33nuvwPVarRa9evVCRkYGTp8+jT179uCbb77B5MmTxTzp6ekICgqCp6cnzp07h9WrV2PZsmVYsWLFs9qNCkd/fzH9a+bMmeI6U9qcCrd3716Eh4djxowZ+OWXX/Diiy+iZ8+euH37tqWrViE1a9bM4Ld66dIlcd2SJUuwYsUKrFmzBufOnYNKpUJQUJD4QFUylpGRgRYtWmDNmjUFrjelTcPDw7Fv3z7s2bMHp0+fxpMnTxAaGgqtVvusdqNiEIjMZMuWLYJSqTRKP3jwoGBjYyPcu3dPTNu9e7egUCiEtLQ0QRAEYe3atYJSqRSys7PFPAsXLhQ8PT0FnU5n9rpXNHXr1hVWrlxZ6HpT2pwK17ZtW+Hdd981SGvcuLEwbdo0C9Wo4vroo4+EFi1aFLhOp9MJKpVKWLRokZiWnZ0tKJVKYf369c+ohhUbAGHfvn3ie1Pa9NGjR4JMJhP27Nkj5rl3755gY2MjREZGPrO6VwQcaaJnLjY2Fr6+vgYPRgwJCUFOTg7i4+PFPIGBgQY3YQsJCcFff/2FmzdvPusqVwiLFy+Gi4sLWrZsifnz5xucejOlzalgarUa8fHxCA4ONkgPDg5GTEyMhWpVsf3+++/w9PSEt7c3BgwYgBs3bgAAEhMTkZycbNDWCoUCgYGBbOtSMqVN4+PjodFoDPJ4enrC19eX7Z4P7whOz1xycjLc3d0N0mrWrAm5XI7k5GQxT7169Qzy6MskJyfD29v7mdS1opgwYQJat26NmjVr4uzZs5g+fToSExPx+eefAzCtzalg//zzD7RarVH7ubu7s+1KoV27dvjyyy/RqFEj3L9/H/PmzUOHDh1w5coVsT0Lautbt25ZoroVniltmpycDLlcjpo1axrl4W/cEEeayCQFTd7M/zp//rzJ25NIJEZpgiAYpOfPI/xvEnhBZSujkrT5xIkTERgYiObNm+Odd97B+vXrsXnzZjx48EDcniltToUr6PfItiu5nj174j//+Q/8/PzQvXt3HDhwAACwbds2MQ/buvyVpk3Z7sY40kQmGTt2LAYMGFBknvwjQ4VRqVQ4c+aMQVpqaio0Go3415BKpTL6CyclJQWA8V9MlVVZ2rx9+/YAgD/++AMuLi4mtTkVzNXVFVKptMDfI9uu7BwdHeHn54fff/8dffr0AZA38uHh4SHmYVuXnv7KxKLaVKVSQa1WIzU11WC0KSUlBR06dHi2FbZyHGkik7i6uqJx48ZFvuzs7EzaVkBAAC5fvoykpCQxLSoqCgqFAv7+/mKekydPGszLiYqKgqenp8nBWUVXljb/5ZdfAED8T9KUNqeCyeVy+Pv7Izo62iA9OjqaB5RykJOTg2vXrsHDwwPe3t5QqVQGba1Wq3HixAm2dSmZ0qb+/v6QyWQGeZKSknD58mW2e34WnIROldStW7eEX375RZgzZ45QrVo14ZdffhF++eUX4fHjx4IgCEJubq7g6+srdOvWTbhw4YJw5MgRoXbt2sLYsWPFbTx69Ehwd3cXBg4cKFy6dEn49ttvherVqwvLli2z1G5ZrZiYGGHFihXCL7/8Ity4cUPYu3ev4OnpKfTu3VvMY0qbU+H27NkjyGQyYfPmzcLVq1eF8PBwwdHRUbh586alq1bhTJ48WTh+/Lhw48YNIS4uTggNDRWcnJzEtly0aJGgVCqFb7/9Vrh06ZIwcOBAwcPDQ0hPT7dwza3X48ePxf9nAYj/H9y6dUsQBNPa9N133xVq164tHDlyRLhw4YLw0ksvCS1atBByc3MttVtWiUETlbshQ4YIAIxex44dE/PcunVL6NWrl2Bvby84OzsLY8eONbi9gCAIwsWLF4UXX3xRUCgUgkqlEiIiIni7gQLEx8cL7dq1E5RKpWBnZyf4+PgIH330kZCRkWGQz5Q2p8J99tlnQt26dQW5XC60bt1aOHHihKWrVCG98cYbgoeHhyCTyQRPT0+hb9++wpUrV8T1Op1O+OijjwSVSiUoFAqhc+fOwqVLlyxYY+t37NixAv/PHTJkiCAIprVpVlaWMHbsWMHZ2Vmwt7cXQkNDhdu3b1tgb6ybRBB4i2UiIiKi4nBOExEREZEJGDQRERERmYBBExEREZEJGDQRERERmYBBExEREZEJGDQRERERmYBBExEREZEJGDQRUaWxdetW1KhRo0Rlhg4dKj7zzNJu3rwJiUSChIQES1eFiArAoImInrn169fDyckJubm5YtqTJ08gk8nw4osvGuQ9deoUJBIJfvvtt2K3+8Ybb5iUr6Tq1auHVatWlft2iahiYdBERM9c165d8eTJE5w/f15MO3XqFFQqFc6dO4fMzEwx/fjx4/D09ESjRo2K3a69vT3c3NzMUmciIgZNRPTM+fj4wNPTE8ePHxfTjh8/jldffRUNGjRATEyMQXrXrl0B5D2dferUqahVqxYcHR3Rrl07g20UdHpu3rx5cHNzg5OTE9555x1MmzYNLVu2NKrTsmXL4OHhARcXF4wZMwYajQYA0KVLF9y6dQsTJ06ERCKBRCIpcJ8GDhyIAQMGGKRpNBq4urpiy5YtAIDIyEh06tQJNWrUgIuLC0JDQ/Hnn38W2k4F7c/+/fuN6vDDDz/A398fdnZ2qF+/PubMmWMwikdE5YNBExFZRJcuXXDs2DHx/bFjx9ClSxcEBgaK6Wq1GrGxsWLQNGzYMPz888/Ys2cPLl68iH79+qFHjx74/fffC/yMnTt3Yv78+Vi8eDHi4+NRp04drFu3zijfsWPH8Oeff+LYsWPYtm0btm7diq1btwIAvv32W9SuXRtz585FUlISkpKSCvyswYMH4/vvv8eTJ0/EtMOHDyMjIwP/+c9/AAAZGRmYNGkSzp07h59++gk2NjZ47bXXoNPpSt6AT33Gm2++ifHjx+Pq1avYsGEDtm7divnz55d6m0RUCEs/MZiIqqaNGzcKjo6OgkajEdLT0wVbW1vh/v37wp49e4QOHToIgiAIJ06cEAAIf/75p/DHH38IEolEuHfvnsF2unXrJkyfPl0QBEHYsmWLoFQqxXXt2rUTxowZY5C/Y8eOQosWLcT3Q4YMEerWrSvk5uaKaf369RPeeOMN8X3dunWFlStXFrk/arVacHV1Fb788ksxbeDAgUK/fv0KLZOSkiIAEJ84n5iYKAAQfvnllwL3RxAEYd++fcLT/3W/+OKLwoIFCwzybN++XfDw8CiyvkRUchxpIiKL6Nq1KzIyMnDu3DmcOnUKjRo1gpubGwIDA3Hu3DlkZGTg+PHjqFOnDurXr48LFy5AEAQ0atQI1apVE18nTpwo9BTX9evX0bZtW4O0/O8BoFmzZpBKpeJ7Dw8PpKSklGh/ZDIZ+vXrh507dwLIG1X67rvvMHjwYDHPn3/+iUGDBqF+/fqoXr06vL29AQC3b98u0Wc9LT4+HnPnzjVokxEjRiApKclgbhgRlZ2tpStARFXT888/j9q1a+PYsWNITU1FYGAgAEClUsHb2xs///wzjh07hpdeegkAoNPpIJVKER8fbxDgAEC1atUK/Zz8838EQTDKI5PJjMqU5pTZ4MGDERgYiJSUFERHR8POzg49e/YU17/yyivw8vLCpk2b4OnpCZ1OB19fX6jV6gK3Z2NjY1Rf/VwrPZ1Ohzlz5qBv375G5e3s7Eq8D0RUOAZNRGQxXbt2xfHjx5Gamor3339fTA8MDMThw4cRFxeHYcOGAQBatWoFrVaLlJQUo9sSFMbHxwdnz55FWFiYmPb0FXumksvl0Gq1xebr0KEDvLy8sHfvXhw6dAj9+vWDXC4HADx48ADXrl3Dhg0bxPqfPn26yO0999xzePz4MTIyMuDo6AgARvdwat26Na5fv47nn3++xPtFRCXDoImILKZr167ilWr6kSYgL2h67733kJ2dLU4Cb9SoEQYPHoy33noLy5cvR6tWrfDPP//g6NGj8PPzw8svv2y0/XHjxmHEiBFo06YNOnTogL179+LixYuoX79+iepZr149nDx5EgMGDIBCoYCrq2uB+SQSCQYNGoT169fjt99+M5joXrNmTbi4uGDjxo3w8PDA7du3MW3atCI/t127dnBwcMCHH36IcePG4ezZs+IEdb3Zs2cjNDQUXl5e6NevH2xsbHDx4kVcunQJ8+bNK9F+ElHROKeJiCyma9euyMrKwvPPPw93d3cxPTAwEI8fP0aDBg3g5eUlpm/ZsgVvvfUWJk+eDB8fH/Tu3RtnzpwxyPO0wYMHY/r06ZgyZQpat26NxMREDB06tMSnrebOnYubN2+iQYMGeO6554rMO3jwYFy9ehW1atVCx44dxXQbGxvs2bMH8fHx8PX1xcSJE7F06dIit+Xs7IwdO3bg4MGD8PPzw+7duxEREWGQJyQkBD/++COio6PxwgsvoH379lixYgXq1q1bon0kouJJhIJO8BMRVVJBQUFQqVTYvn27patCRBUMT88RUaWVmZmJ9evXIyQkBFKpFLt378aRI0cQHR1t6aoRUQXEkSYiqrSysrLwyiuv4MKFC8jJyYGPjw9mzpxZ4JVmRETFYdBEREREZAJOBCciIiIyAYMmIiIiIhMwaCIiIiIyAYMmIiIiIhMwaCIiIiIyAYMmIiIiIhMwaCIiIiIyAYMmIiIiIhMwaCIiIiIywf8D/9f3NaH7Mm4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 64, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABuWklEQVR4nO3dd3gU1d4H8O/uZrNJSIEQySYQQugtFIN0CQhJ6CACCog0AWlSpYhA6FVEQUCUKlJ8r4J6qQEJRYJAAKkXUUMRCVFKAmnbzvtHbuZm2dTNLlvy/TzPPuzOnDNz5mQP89tzZs7IhBACRERERE5KbusCEBEREVkTgx0iIiJyagx2iIiIyKkx2CEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfGYIeIiIicGoMdcmqbNm2CTCbL9TVp0iSjtJmZmVi1ahVatmyJMmXKwNXVFeXLl0fv3r1x9OhRKd2dO3fw6quvonLlyihVqhR8fHzQsGFDrFq1CjqdLt/y/Otf/4JMJsPOnTtN1tWvXx8ymQwHDhwwWVelShW8+OKLRTr2gQMHolKlSkXKky06OhoymQz//PNPgWkXLFiA3bt3F3rbOf8GCoUCZcqUQf369TF8+HCcOnXKJP3Nmzchk8mwadOmIhwBsG3bNqxYsaJIeXLbV1HqorCuXr2K6Oho3Lx502Rdcf5ulvD7779DpVIhLi5OWta6dWvUrVu3UPllMhmio6Olz/kdq7mEEPj8888RFhYGb29vlC1bFuHh4dizZ49Rul9//RWurq44d+6cxfZNDkoQObGNGzcKAGLjxo0iLi7O6HXr1i0p3d9//y3CwsKEUqkUw4cPF7t37xbHjh0T27dvF2+88YZQKBTiwoULQgghrl27Jt566y2xYcMGcejQIbF3714xevRoAUAMGTIk3/L8/fffQiaTieHDhxstf/DggZDJZKJUqVJiypQpRuvu3LkjAIgJEyYU6dh/++03ce7cuSLlyTZr1iwBQPz9998Fpi1VqpQYMGBAobcNQPTs2VPExcWJkydPiv3794tly5aJevXqCQDi3XffNUqfkZEh4uLiRFJSUpGOoVOnTiI4OLhIeXLbV1HqorD+7//+TwAQR44cMVlXnL+bJXTv3l106tTJaFl4eLioU6dOofLHxcWJO3fuSJ/zO1ZzzZgxQwAQ77zzjjh48KD4/vvvRUREhAAgvvnmG6O0AwcOFK1atbLYvskxMdghp5Yd7Jw5cybfdB06dBAuLi7i8OHDua4/ffq0UXCUm969ewsXFxeRkZGRb7rQ0FBRo0YNo2XffvutUCqV4t133xWNGzc2WrdlyxYBQPzwww/5bteSrB3sjBo1ymS5TqcTgwcPFgDE6tWri1LcXBUl2NHpdHn+3Z53sGNLV69eFQDE/v37jZYXJdh5ljWOtXz58qJly5ZGy9LT04WPj4/o2rWr0fKzZ88KAOKnn36y2P7J8XAYi0q8+Ph47Nu3D0OGDMErr7ySa5qXXnoJFStWzHc7L7zwAuRyORQKRb7p2rRpg+vXr+PevXvSstjYWLz00kvo2LEj4uPj8eTJE6N1CoUCL7/8MoCsLvzVq1ejQYMGcHd3R5kyZdCzZ0/88ccfRvvJbTjk8ePHGDJkCHx9feHp6YlOnTrhjz/+MBl6yHb//n306dMHPj4+8Pf3x+DBg5GcnCytl8lkSE1NxebNm6WhqdatW+d7/HlRKBRYtWoV/Pz8sHTpUml5bkNLf//9N4YNG4agoCCoVCq88MILaNGiBQ4dOgQga9hlz549uHXrltGwWc7tLVmyBPPmzUNISAhUKhWOHDmS75DZnTt30KNHD3h7e8PHxwdvvvkm/v77b6M0edVjpUqVMHDgQABZQ6u9evUCkPVdyC5b9j5z+7tlZGRg2rRpCAkJkYZXR40ahcePH5vsp3Pnzti/fz9efPFFuLu7o2bNmtiwYUMBtZ9lzZo1UKvViIiIyHX98ePH0bRpU7i7u6N8+fKYMWMG9Hp9nnVQ0LGaS6lUwsfHx2iZm5ub9MopLCwMtWrVwtq1a4u1T3JsDHaoRNDr9dDpdEavbAcPHgQAdO/evUjbFEJAp9Ph0aNH2LlzJzZt2oSJEyfCxcUl33xt2rQBkBXEZDty5AjCw8PRokULyGQyHD9+3Gjdiy++KP3nPnz4cIwbNw7t2rXD7t27sXr1aly5cgXNmzfH/fv389yvwWBAly5dsG3bNkyZMgW7du1CkyZN0L59+zzzvPbaa6hevTq++eYbTJ06Fdu2bcP48eOl9XFxcXB3d0fHjh0RFxeHuLg4rF69Ot/jz4+7uzvatWuHhIQE/Pnnn3mm69+/P3bv3o2ZM2fi4MGD+OKLL9CuXTs8ePAAALB69Wq0aNECarVaKlfOa1AA4JNPPsGPP/6IZcuWYd++fahZs2a+ZXv11VdRtWpV/Otf/0J0dDR2796NqKgoaLXaIh1jp06dsGDBAgDAp59+KpWtU6dOuaYXQqB79+5YtmwZ+vfvjz179mDChAnYvHkzXnnlFWRmZhql/+WXXzBx4kSMHz8e3333HerVq4chQ4bg2LFjBZZtz549aNWqFeRy01NDYmIi3njjDfTr1w/fffcdevbsiXnz5mHs2LFmH6vBYDBpl7m9ng2oxo4di/3792P9+vV49OgR7t27hwkTJiA5ORnvvvuuSTlat26Nffv2QQhRYB2Qk7JtxxKRdWUPY+X20mq1Qggh3nnnHQFA/Oc//ynSthcuXChtSyaTienTpxcq38OHD4VcLhfDhg0TQgjxzz//CJlMJg0dNG7cWEyaNEkIIcTt27cFADF58mQhRNb1EADEhx9+aLTNO3fuCHd3dymdEEIMGDDAaBhnz549AoBYs2ZNrscxa9YsaVn20M2SJUuM0o4cOVK4ubkJg8EgLbPUMFa2KVOmCADi559/FkIIkZCQIF13lc3T01OMGzcu3/3kNYyVvb0qVaoIjUaT67qc+8qui/Hjxxul/eqrrwQAsXXrVqNjy1mP2YKDg43qKL+hnWf/bvv378/1b7Fz504BQKxbt85oP25ubkZDrunp6cLX19fkOrFn3b9/XwAQixYtMlkXHh4uAIjvvvvOaPnQoUOFXC432t+zdZDfsWbXbUGv3P6Oa9euFSqVSkrj6+srYmJicj22zz//XAAQ165dy7cOyHmxZ4dKhC1btuDMmTNGr4J6YAoycOBAnDlzBgcOHMDkyZOxdOlSjBkzpsB82XcfZffsHD16FAqFAi1atAAAhIeH48iRIwAg/ZvdG/Tvf/8bMpkMb775ptEvX7VabbTN3GTfUda7d2+j5X369MkzT9euXY0+16tXDxkZGUhKSirwOM0lCvHru3Hjxti0aRPmzZuHU6dOFbl3Bcg6NqVSWej0/fr1M/rcu3dvuLi4SH8ja/nxxx8BQBoGy9arVy+UKlUKhw8fNlreoEEDoyFXNzc3VK9eHbdu3cp3P3/99RcAoFy5crmu9/LyMvk+9O3bFwaDoVC9RrkZNmyYSbvM7fXDDz8Y5du4cSPGjh2L0aNH49ChQ9i7dy8iIyPRrVu3XO9mzD6mu3fvmlVOcnzF+9+eyEHUqlULjRo1ynVd9okhISEBNWrUKPQ21Wo11Go1ACAyMhJlypTB1KlTMXjwYDRs2DDfvG3atMHy5cvx119/4ciRIwgLC4OnpyeArGDnww8/RHJyMo4cOQIXFxe0bNkSQNY1NEII+Pv757rdypUr57nPBw8ewMXFBb6+vkbL89oWAJQtW9bos0qlAgCkp6fne3zFkX1SDgwMzDPNzp07MW/ePHzxxReYMWMGPD098eqrr2LJkiXS36QgAQEBRSrXs9t1cXFB2bJlpaEza8n+u73wwgtGy2UyGdRqtcn+n/2bAVl/t4L+Ztnrn73mJVtu35PsOjG3DtRqdZ7BVU7Z11sBwKNHjzBq1Ci8/fbbWLZsmbS8Q4cOaN26Nd555x0kJCQY5c8+Jmt+b8m+sWeHSryoqCgAKNJcMblp3LgxgKy5PQqS87qd2NhYhIeHS+uyA5tjx45JFy5nB0J+fn6QyWQ4ceJErr+A8zuGsmXLQqfT4eHDh0bLExMTi3Sc1pSeno5Dhw6hSpUqqFChQp7p/Pz8sGLFCty8eRO3bt3CwoUL8e2335r0fuQn5wm0MJ6tJ51OhwcPHhgFFyqVyuQaGsD8YAD439/t2YuhhRBITEyEn5+f2dvOKXs7z34/suV2PVh2neQWYBXGnDlzoFQqC3xVqVJFynP9+nWkp6fjpZdeMtleo0aNcPPmTTx9+tRoefYxWaquyPEw2KES78UXX0SHDh2wfv16acjgWWfPnsXt27fz3U72cEbVqlUL3GerVq2gUCjwr3/9C1euXDG6g8nHxwcNGjTA5s2bcfPmTSkwAoDOnTtDCIG7d++iUaNGJq/Q0NA895kdUD07oeGOHTsKLG9+CtNrUBh6vR6jR4/GgwcPMGXKlELnq1ixIkaPHo2IiAijyeMsVa5sX331ldHnr7/+GjqdzuhvV6lSJVy8eNEo3Y8//mhy8i1KD1nbtm0BAFu3bjVa/s033yA1NVVaX1zBwcFwd3fH77//nuv6J0+e4Pvvvzdatm3bNsjlcrRq1SrP7eZ3rOYMY2X3+D07AaUQAqdOnUKZMmVQqlQpo3V//PEH5HJ5kXpuyblwGIsIWdf0tG/fHh06dMDgwYPRoUMHlClTBvfu3cMPP/yA7du3Iz4+HhUrVsSsWbNw//59tGrVCuXLl8fjx4+xf/9+fP755+jVqxfCwsIK3J+3tzdefPFF7N69G3K5XLpeJ1t4eLg0+2/OYKdFixYYNmwYBg0ahLNnz6JVq1YoVaoU7t27hxMnTiA0NBQjRozIdZ/t27dHixYtMHHiRKSkpCAsLAxxcXHYsmULAOR6B05hhIaGIjY2Fj/88AMCAgLg5eVV4Enl/v37OHXqFIQQePLkCS5fvowtW7bgl19+wfjx4zF06NA88yYnJ6NNmzbo27cvatasCS8vL5w5cwb79+9Hjx49jMr17bffYs2aNQgLC4NcLs9zKLMwvv32W7i4uCAiIgJXrlzBjBkzUL9+faNroPr3748ZM2Zg5syZCA8Px9WrV7Fq1SqT26SzZyNet24dvLy84ObmhpCQkFx7SCIiIhAVFYUpU6YgJSUFLVq0wMWLFzFr1iw0bNgQ/fv3N/uYcnJ1dUWzZs1yncUayOq9GTFiBG7fvo3q1atj7969+PzzzzFixIh8p2XI71gDAwPzHa7MTcWKFdGjRw+sW7cOKpUKHTt2RGZmJjZv3oyffvoJc+fONem1O3XqFBo0aIAyZcoUaV/kRGx5dTSRtRV2UkEhsu5a+eSTT0SzZs2Et7e3cHFxEYGBgaJHjx5iz549Urrvv/9etGvXTvj7+wsXFxfh6ekpGjduLD755BPpDq/CmDx5sgAgGjVqZLJu9+7dAoBwdXUVqampJus3bNggmjRpIkqVKiXc3d1FlSpVxFtvvSXOnj0rpXn2rh4hsu4EGzRokChdurTw8PAQERER4tSpUwKA+Pjjj6V0eU2kl12fCQkJ0rILFy6IFi1aCA8PDwFAhIeH53vcyHGXjVwuF97e3iI0NFQMGzZMxMXFmaR/9g6pjIwM8c4774h69eoJb29v4e7uLmrUqCFmzZplVFcPHz4UPXv2FKVLlxYymUxk/3eXvb2lS5cWuK+cdREfHy+6dOkiPD09hZeXl+jTp4+4f/++Uf7MzEwxefJkERQUJNzd3UV4eLi4cOGCyd1YQgixYsUKERISIhQKhdE+c/u7paeniylTpojg4GChVCpFQECAGDFihHj06JFRuuDgYJPZj4XIupuqoL+LEEKsX79eKBQK8ddff5nkr1OnjoiNjRWNGjUSKpVKBAQEiPfff9/kO49c7kjL61jNlZ6eLpYuXSrq1asnvLy8hK+vr2jatKnYunWr0Z2CQgjx5MkT4eHhYXIHI5UsMiE48QBRSbZt2zb069cPP/30E5o3b27r4pANZWRkoGLFipg4cWKRhhLt2fr16zF27FjcuXOHPTslGIMdohJk+/btuHv3LkJDQyGXy3Hq1CksXboUDRs2NHrYKZVca9asQXR0NP744w+Ta18cjU6nQ+3atTFgwABMnz7d1sUhG+I1O0QliJeXF3bs2IF58+YhNTUVAQEBGDhwIObNm2fropGdGDZsGB4/fow//vgj3wveHcGdO3fw5ptvYuLEibYuCtkYe3aIiIjIqfHWcyIiInJqDHaIiIjIqTHYISIiIqfGC5QBGAwG/PXXX/Dy8iryFPJERERkG+K/E5MGBgbmOzEqgx1kPe03KCjI1sUgIiIiM9y5cyff5+kx2EHW7bhAVmV5e3tbZJtpGh0azz8MADg9vS08XB23qrVaLQ4ePIjIyEgolUpbF8fpsH6tj3VsXaxf63PUOrb2uTAlJQVBQUHSeTwvjnsGtqDsoStvb2+LBTsuGh3kKg9pu44e7Hh4eMDb29uhGpmjYP1aH+vYuli/1ueodfy8zoUFXYLCC5SJyOFlaPUY+VU8Rn4Vjwyt3ur5iMixMNghIodnEAJ7LyVi76VEGIowT6q5+YjIsTju2IqdU8hleO3FCtJ7IiKiksZezoUMdqxE5aLAh73r27oYREQWp9frodVqpc9arRYuLi7IyMiAXs/hQGtw5Dqe37UGAEDotMjQaQtIbUypVEKhUBS7DAx2iIioUIQQSExMxOPHj02Wq9Vq3Llzh3OVWUlJruPSpUtDrVYX67gZ7FiJEALp/73g0V2pKHFfTiJyPtmBTrly5eDh4SH9v2YwGPD06VN4enrmO7Ebmc9R61gIAcN/L4eTywq+a+rZvGlpaUhKSgIABAQEmF0OmwY7a9aswZo1a3Dz5k0AQJ06dTBz5kx06NABQNaBzp49G+vWrcOjR4/QpEkTfPrpp6hTp460jczMTEyaNAnbt29Heno62rZti9WrV+c7udDzkK7Vo/bMAwCAq3OiHPrWcyIivV4vBTply5Y1WmcwGKDRaODm5uZQJ2JH4qh1rDcIXPkrGQBQJ9CnyNftuLu7AwCSkpJQrlw5s4e0bFpjFSpUwKJFi3D27FmcPXsWr7zyCrp164YrV64AAJYsWYLly5dj1apVOHPmDNRqNSIiIvDkyRNpG+PGjcOuXbuwY8cOnDhxAk+fPkXnzp0dbkyTiMieZV+j4+HhYeOSUEmT/Z3LeZ1YUdk02OnSpQs6duyI6tWro3r16pg/fz48PT1x6tQpCCGwYsUKTJ8+HT169EDdunWxefNmpKWlYdu2bQCA5ORkrF+/Hh9++CHatWuHhg0bYuvWrbh06RIOHTpky0MjInJKHJKn580S3zm7GVvR6/X4v//7P6SmpqJZs2ZISEhAYmIiIiMjpTQqlQrh4eE4efIkhg8fjvj4eGi1WqM0gYGBqFu3Lk6ePImoqKhc95WZmYnMzEzpc0pKCoCsqLE4kWNOWq0ux3sttDLHncMju04sVTdkjPVbfAW1t7zq2JnaqbVptdqs6y8MBhgMBqN14r9zFGWvJ8tz1DrOOX1VVtmL3sYMBgOEENBqtSbDWIX9f9Pmwc6lS5fQrFkzZGRkwNPTE7t27ULt2rVx8uRJAIC/v79Ren9/f9y6dQtA1sVyrq6uKFOmjEmaxMTEPPe5cOFCzJ4922T5wYMHLdZFm6kHsqv3wIGDUBX/zjmbi4mJsXURnBrr13yFbW/P1rEztlNrcXFxgVqtxtOnT6HRaHJNk/MSA7IOS9fxw4cP0aRJExw+fBgVK1a06LYBIGdsk5KSgpyX7MyYMQMajQaLFy/OdxsajQbp6ek4duwYdDqd0bq0tLRClcPmwU6NGjVw4cIFPH78GN988w0GDBiAo0ePSuuf7b4SQhTYpVVQmmnTpmHChAnS5+wHiUVGRlr0QaCTT/8IAIiKinToC5S1Wi1iYmIQERHhUM9kcRSs3+IrqL3lVcfO1E6tLSMjA3fu3IGnpyfc3NyM1gkh8OTJE3h5edndMNegQYPw+PFj7Nq1S/q8ZcsWLFiwAFOmTJHS7d69G6+99hr0er2UJj96vR46nQ6zZ8/Gtm3bkJiYiICAAAwYMADTp0+3+EXE1qrjuXPnokuXLqhbt660bNy4cfjpp59w+fJl1KpVC+fOnTPKExsbixUrVuDMmTNISUlBtWrVMHHiRPTr109Kk1cd1q5dG5cuXQIATJ8+HdWqVcPkyZMREhKSZxkzMjLg7u6OVq1amXz3skdmCmLzlu3q6oqqVasCABo1aoQzZ87g448/lr6E2V+gbElJSVJvj1qthkajwaNHj4x6d5KSktC8efM896lSqaBSqUyWK5VKi51slOJ/X8as7dq8qovNkvVDpli/5itse3u2jp2xnVqLXq+HTCaDXC43OZFnD6tkr7cnMpnMqFwymQxubm5YsmQJ3nnnHenckb1eLpfjk08+MeptCAgIwMaNG9G+fXtpmVwux9KlS/HZZ59h8+bNqFOnDs6ePYtBgwahdOnSGDt2rEWPwxp1nJ6ejg0bNmDv3r0m2xw8eDB+/vlnXLx40WTdqVOnUL9+fUydOhX+/v7Ys2cPBg4ciNKlS6NLly4AINWh3iBwPTEFer0Ob7RvhV69eknbU6vViIyMxLp16/Lt3ZHL5ZDJZLn+H1nY/zPt61uJrOg1MzMTISEhUKvVRt3OGo0GR48elQKZsLAwKJVKozT37t3D5cuX8w12nge5TIaOoWp0DFVDbme/dIicjbntje20ZGrXrh3UajUWLlyY63ofHx+o1WrpBfxvYrucy+Li4tCtWzd06tQJlSpVQs+ePREZGYmzZ8/mue/o6Gg0aNAAGzZsQMWKFeHp6YkRI0ZAr9djyZIlUKvVKFeuHObPn2+U76OPPkLz5s3h5eWFoKAgjBw5Ek+fPpXWDx48GPXq1ZOuR9VqtQgLCzPqbXnWvn374OLigmbNmhkt/+STTzBq1ChUrlw513zvv/8+5s6di+bNm6NKlSp499130b59e6n3LGcdBqjVqBJcAQn/uYRHjx5h0KBBRtvq2rUrtm/fnmcZLcWmP2Pef/99dOjQAUFBQXjy5Al27NiB2NhY7N+/HzKZDOPGjcOCBQtQrVo1VKtWDQsWLICHhwf69u0LIKsyhwwZgokTJ6Js2bLw9fXFpEmTEBoainbt2tny0OCmVGB1vzCbloGopDC3vbGdFl+aRgeDwYB0jR4uGp1RL4BcJoObUmGUNi+FTWuJoUaFQoEFCxagb9++ePfdd82el61ly5ZYu3Ytfv31V1SvXh2//PILTpw4gRUrVuSb7/fff8e+ffuwf/9+/P777+jZsycSEhJQvXp1HD16FCdPnsTgwYPRtm1bNG3aFEBW78bixYtRu3Zt3Lp1CyNHjsTkyZOxevVqAFkBSnZvy0cffYQZM2bgn3/+kdbn5tixY2jUqJFZx/6s5ORk1KpVy2S5XC5DcNlS+OHrr9CuXTsEBwcbrW/cuDHu3LmDW7dumayzJJsGO/fv30f//v1x7949+Pj4oF69eti/fz8iIiIAAJMnT0Z6ejpGjhwpTSp48OBBeHl5Sdv46KOP4OLigt69e0uTCm7atMkiz9IgIqL8ZU+emps2NV7AxkGNpc9hcw9JM8s/q0mIL3YO/18PQ8vFR/Aw1fRC6JuLOhWjtP/z6quvokGDBpg1axbWr19v1jamTJmC5ORk1KxZEwqFAnq9HvPnz0efPn3yzWcwGLBhwwZ4eXmhdu3aaNOmDa5fvy4NJ9WoUQOLFy9GbGysFOyMHTsWKSkp8Pb2RpUqVTB37lyMGDFCCmY8PT2xdetWhIeHw8vLCx9++CEOHz4MHx+fPMtx8+ZNBAYGmnXsOf3rX//CmTNn8Nlnn+W6/t69e9i3b580bUxO5cuXl8ritMFOQV8wmUyG6OhoREdH55nGzc0NK1euxMqVKy1cOiIicmaLFy/GK6+8gokTJ5qVf+fOndi6dSu2bduGOnXq4MKFCxg3bhwCAwMxYMCAPPNVqlTJ6Ee7v78/FAqFUa+Yv7+/9JgEADhy5AjmzZuHX3/9FSkpKdDpdMjIyEBqaipKlSoFAGjWrBkmTZqEuXPnYsqUKWjVqlW+5U9PTze54LeoYmNjMXDgQHz++edGTzfIadOmTShdujS6d+9usi57huTC3lVlLl6NZyVpGh0fF0H0nJjb3thOi+/qnCgYDAY8SXkCL28vk2GsnOJn5H15wbNpT0xpY9mC5qJVq1aIiorC+++/j4EDBxY5/3vvvYepU6fijTfeAACEhobi1q1bWLhwYb7BzrMX1WZffPvssuyLkm/duoXOnTtj0KBBmD9/Pvz8/HDixAkMGTLEaJ4Zg8GAn376CQqFAjdu3Ciw/H5+fnj06FGhj/dZR48eRZcuXbB8+XK89dZbuabR6Q1Yu+4LdOjeGwoX04uJHz58CAB44YUXzC5HYbBlExGR2TxcXWAwGKBzVcDD1SXfO4WKEkw+r8Bz0aJFaNCgAapXr17kvGlpaSbHq1AoLD7p39mzZ6HT6TBv3jyULl0acrkcX3/9tUm6pUuX4tq1azh69CiioqKwceNGkwuCc8p+6oA5YmNj0blzZyxevBjDhg3LM93Ro0dx++Yf6P7Gm7muv3z5MpRKZZ69QpbCYIeIHJ67UoH4D9pJ762dj5xHaGgo+vXrZ9alEF26dMH8+fNRsWJF1KlTB+fPn8fy5csxePBgi5axSpUq0Ol0WLduHXr27Im4uDisXbvWKM2FCxcwc+ZM/Otf/0KLFi3w8ccfY+zYsQgPD8/zrqqoqChMmzbNZPqW3377DU+fPkViYiLS09Nx4cIFAFlz5Li6uiI2NhadOnXC2LFj8dprr0mT+Lq6usLX19doHxs3bEBow0aoVrN2rmU4fvw4Xn75ZWk4y1rs7tZzIqKikslkKOupQllPVZEmXDM3HzmXuXPnSo9jKIqVK1eiZ8+eGDlyJGrVqoVJkyZh+PDhmDt3rkXL16BBA3z44Yf4+OOPUa9ePXz11VdGt81nZGSgX79+GDhwoDTPzZAhQ9CuXTv0798/zwdjh4aGolGjRia9RG+//TYaNmyIzz77DL/++isaNmyIhg0b4q+//gKQdQ1OWloaFi5ciICAAOnVo0cPo+0kJyfj22+/wat59OoAwPbt2zF06FCz6qUoZMKcv7CTSUlJgY+PD5KTky06g7KzXAug1Wqxd+9edOzYkZPeWQHr1/pYx8WXkZGBhIQEhISEmFzUajAYpDuF7G1SQWdhrTreu3cvJk2ahMuXL1vlb6c3CFz5KxkAUCfQB4ocz4vYs2cP3nvvPVy8eBEuLnmfI/P77hX2/O24Z2Aiov/K1Okx79/XAAAfdK4FlUvhhqTMzUfkLDp27IgbN27g7t27CAoKeq77Tk1NxcaNG/MNdCyFwQ4ROTy9QeDLU1kPCJ7WsabV8xE5E0s/2qKwevfu/dz2xWDHSuQyGdrUeEF6T0REVNLIAHi5KaX3tsJgx0rclAqjmUOJiIhKGrlchhC/UrYuBu/GIiIiIufGYIeIiIicGoMdK0nT6FBrxn7UmrE/3yf9EhEROSu9QeDy3WRcvpsMvcF2M93wmh0ryuvpvkRERCWFwQ6m82PPDhERETk1BjtERER2TqFQYM+ePcXezo8//oiaNWta/GGl5sjMzETFihURHx9v9X0x2CEiIqc1cOBAdO/e3eizTCbDokWLjNLt3r1bej5adpr8XgCg0+nwwQcfICQkBO7u7qhcuTLmzJljlUDi7t27aNeuXbG3M3nyZEyfPj3fR0NcuXIFr732GipVqgSZTIYVK1aYpFm4cCFeeukleHl5oVy5cujevTuuX79ulObp06d4d8xoRLxUB42rBqBundpYs2aNtF6lUmHSpEmYMmVKsY+rIAx2iIioRHFzc8PixYvx6NGjXNd//PHHuHfvnvQCgI0bN5osW7x4MdauXYtVq1bh2rVrWLJkCZYuXWrWE9QLolaroVKpirWNkydP4saNG+jVq1e+6dLS0lC5cmUsWrQIarU61zRHjx7FqFGjcOrUKcTExECn0yEyMhKpqalSmvHjx+PAgQNY8Mln2HXkZ4wdOw5jxozBd999J6Xp168fjh8/jmvXrhXr2ArCYIeIiEqUdu3aQa1WGz05PCcfHx+o1WrpBQClS5c2WRYXF4du3bqhU6dOqFSpEnr27InIyEicPXs2z31HR0ejQYMG2LBhAypWrAhPT0+MGDECer0eS5YsgVqtRrly5TB//nyjfDmHsW7evAmZTIZvv/0Wbdq0gYeHB+rXr4+4uLh8j3vHjh2IjIw0eZjms1566SUsXboUb7zxRp4B1v79+zFw4EDUqVMH9evXx8aNG3H79m2jIam4uDj0f+stvNSsJcoHVcTQYcNQv359o/opW7Ysmjdvju3bt+dbpuJisGMlcpkMTUJ80STEl4+LILIyc9sb22nxpWl0SNPokK7RS++zXxnP3JH67Hpz0lqCQqHAggULsHLlSvz5559mb6dly5Y4fPgwfv31VwDAL7/8ghMnTqBjx4755vv999+xb98+7N+/H9u3b8eGDRvQqVMn/Pnnnzh69CgWL16MDz74AKdOncp3O9OnT8ekSZNw4cIFVK9eHX369IFOl3cdHTt2DI0aNSr6gRZCcnLWk819fX2lZS1btsS/f/gBTx4mwcNVgdgjR/Drr78iKirKKG/jxo1x/Phxq5QrG289txI3pQI7hzezdTGISgRz2xvbafHVnnkgz3Vtarxg9NicsLmH8pySo0mIr9HfouXiI3iYqjFJd3NRp2KU9n9effVVNGjQALNmzcL69evN2saUKVOQnJyMmjVrQqFQQK/XY/78+ejTp0+++QwGAzZs2AAvLy/Url0bbdq0wfXr17F3717I5XLUqFEDixcvRmxsLJo2bZrndiZNmoROnbLqY/bs2ahTpw5+++031KyZ+0Ntb968icDAQLOONT9CCEyYMAEtW7ZE3bp1peWffPIJhg4dipb1a8DFxQVyuRxffPEFWrZsaZS/fPnyuHnzpsXLlRN7doiIqERavHgxNm/ejKtXr5qVf+fOndi6dSu2bduGc+fOYfPmzVi2bBk2b96cb75KlSrBy8tL+uzv74/atWsbXTTs7++PpKSkfLdTr1496X1AQAAA5JsnPT3daAjr9u3b8PT0lF4LFizId395GT16NC5evGgyFPXJJ5/g1KlT+P777xEfH48PP/wQI0eOxKFDh4zSubu7Iy0tzax9FxZ7doiIyGxX50TBYDDgScoTeHl7GZ2wnx0ajJ+R991Ez6Y9MaWNZQuai1atWiEqKgrvv/8+Bg4cWOT87733HqZOnYo33ngDABAaGopbt25h4cKFGDBgQJ75lEql0WeZTJbrsoLu6sqZJ/sOsfzy+Pn5GV2UHRgYiAsXLkifcw5BFdaYMWPw/fff49ixY6hQoYK0PD09He+//z527dol9T7Vq1cPFy5cwLJly4zuLHv48CFeeOGFIu+7KBjsWEmaRoeWi48AyGq0Hq6saiJrMbe9sZ0Wn4erCwwGA3SuCni4uuR7S3NR6vd5/S0WLVqEBg0aoHr16kXOm5aWZnK8CoXCLuawyU3Dhg2NerFcXFxQtWpVs7YlhMCYMWOwa9cuxMbGIiQkxGi9VquFVquFgAxX/0oBANRQe+VaP5cvX0bDhg3NKkdhsWVbUW7jzURkHea2N7bTki00NBT9+vUz63bxLl26YP78+ahYsSLq1KmD8+fPY/ny5Rg8eLAVSlp8UVFRBQ6xAYBGo5GCIo1Gg7t37+LChQvw9PSUgqNRo0Zh27Zt+O677+Dl5YXExEQAWXeyubu7w9vbG+Hh4Zg6ZTLGz1qEgPJBOLX/HLZs2YLly5cb7e/48eOYO3euhY/WGIMdInJ4bi4KHBzfSnpv7XzkXObOnYuvv/66yPlWrlyJGTNmYOTIkUhKSkJgYCCGDx+OmTNnWqGUxffmm29iypQpuH79OmrUqJFnur/++suop2XZsmVYtmwZwsPDERsbCwDS5ICtW7c2yrtx40ZpSHDHjh2YOnUapo0ZhpTHj1CpUjDmz5+Pd955R0ofFxeH5ORk9OzZ0zIHmQeZEHbwhC4bS0lJgY+PD5KTk+Ht7W2RbaZpdNJdClfnRDl097hWq8XevXvRsWNHk3FlKj7Wr/WxjosvIyMDCQkJCAkJMZmnxWAwICUlBd7e3vkOY5H5LFXHkydPRnJyMj777DMLli5veoPAlb+ybkuvE+gDhdz42qxevXqhYcOGeP/99/PcRn7fvcKev/mtJCIiKiGmT5+O4OBg6PW5TwHwPGVmZqJ+/foYP3681ffluN0NRET/pdEZ8OmR3wAAo9pUhatL4X7HmZuPyFH5+Pjk24vyPKlUKnzwwQfPZV8MdojI4ekMBnx8+AYAYHh4ZbgWstPa3HxE5FgY7FiJXCZDvQo+0nsiIqKSRgbA3VUhvbcVBjtW4qZU4PvRLQtOSERE5KTkchmqlfMqOKG1y2HrAhARERFZE4MdIiIicmocxrKSdI0e7ZYfBQAcmhAujVkSERGVFAaDwK/3nwAAqvt7QS63zZU7DHasREDg7uN06T0REVFJIwBo9Abpva1wGIuIiIicGoMdIiIiBzN8+HDIZDKsWLGiwLTffPMNateuDZVKhdq1a2PXrl0maVavXi09jiEsLAzHjx+3Qqlth8EOERGRA9m9ezd+/vlnBAYGFpg2Li4Or7/+Ovr3749ffvkF/fv3R+/evfHzzz9LaXbu3Ilx48Zh+vTpOH/+PF5++WV06NABt2/ftuZhPFcMdoiIyGm1bt0aY8aMwbhx41CmTBn4+/tj3bp1SE1NxaBBg+Dl5YUqVapg3759Uh69Xo8hQ4YgJCQE7u7uqFGjBj7++GNpfUZGBurUqYNhw4ZJyxISEuDj44PPP//cqsdz9+5djB49Gl999VWhHmq7YsUKREREYNq0aahZsyamTZuGtm3bGvUILV++HEOGDMHbb7+NWrVqYcWKFQgKCpKebO4MGOwQEZHZ0jQ6pGl0SNfopfcFvXT/vWAVAHR6A9I0OmRo9blu99mXOTZv3gw/Pz+cPn0aY8aMwYgRI9CrVy80b94c586dQ1RUFPr374+0tDQAWU8Yr1ChAr7++mtcvXoVM2fOxPvvv4+vv/4aAODm5oavvvoKmzdvxu7du6HX69G/f3+0adMGQ4cOzbMcHTp0gKenZ76v/BgMBgwYMADvvfce6tSpU6hjj4uLQ2RkpNGyqKgonDx5EgCg0WgQHx9vkiYyMlJK4wx4N5aVyCBDtXKe0nsish5z2xvbafHVnnmgyHk+7fsiOtULAAAcuHIfo7adQ5MQX+wc3kxK03LxETxM1ZjkvbmoU5H3V79+femBk9OmTcOiRYvg5+cnBSYzZ87EmjVrcPHiRTRt2hRKpRKzZ8+W8oeEhODkyZP4+uuv0bt3bwBAgwYNMG/ePAwdOhR9+vTB77//jt27d+dbji+++ALp6elFLn+2FStWQKFQ4N133y10nsTERPj7+xst8/f3R2JiIgDgn3/+gV6vzzdNccgAuLnwcRFOy91VgZgJ4bYuBlGJYG57YzstGerVqye9VygUKFu2LEJDQ6Vl2Sf6pKQkadnatWvxxRdf4NatW0hPT4dGo0GDBg2Mtjtx4kR89913WLlyJfbt2wc/P798y1G+fHmzjyE+Ph6fffYZ4uPjISvi8xafTS+EMFlWmDTmkMtlqK62/eMiGOwQEZHZrs6JgsFgwJOUJ/Dy9oJcXvDVEa6K/6WJquOPq3OiTB6YfGJKG4uV8dlrW2QymdGy7JO6wZA1vPb1119j/Pjx+PDDD9GsWTN4eXlh6dKlRhf1AlnB0fXr16FQKHDjxg20b98+33J06NChwLucnj59muvyEydO4O+//0alSpWkZXq9HhMnTsSKFStw8+bNXPOp1WqTHpqkpCQpwPPz84NCocg3jTNgsENERGbzcHWBwWCAzlUBD1eXQgU7Obko5HBRmObxcLXd6en48eNo3rw5Ro4cKS37/fffTdINHjwYdevWxdChQzFkyBC0bdsWtWvXznO7xRnGevPNN9GkSRN4enpKdZx9rdGgQYPyzNesWTPExMRg/Pjx0rKDBw+iefPmAABXV1eEhYUhJiYGr776qpQmJiYG3bp1M6us9ojBjpWka/TouuoEAOD70S35uAgiKzK3vbGdUm6qVq2KLVu24MCBAwgJCcGXX36JM2fOICQkRErz6aefIi4uDhcvXkRQUBD27duHfv364eeff4arq2uu2y3OMFbZsmWhVCrh7e0tBTtKpRJqtRo1atSQ0r311lsoX748Fi5cCAAYO3YsWrVqhcWLF6Nbt2747rvvcOjQIZw4cULKM2HCBPTv3x+NGjVCs2bNsG7dOty+fRvvvPOO2eXNZjAI/JaU1VtVtZynzR4XwbuxrERA4EbSU9xIesrHRRBZmbntje2UcvPOO++gR48eeP3119GkSRM8ePDAqJfnP//5D9577z2sXr0aQUFBALKCn8ePH2PGjBm2KjYA4Pbt27h37570uXnz5tixYwc2btyIevXqYdOmTdi5cyeaNGkipXn99dexYsUKzJkzBw0aNMCxY8ewd+9eBAcHF7s8AkCGTo8Mnd6mLYw9O0Tk8FQuCmwf2lR6b+185DhiY2NNluV2fYsQ/zsVq1QqbNy4ERs3bjRKk91bUrNmTek29Wze3t5ISEgofoGLILfjyO14e/bsiZ49e+a7rZEjRxoFdM6GwQ4ROTyFXIZmVco+t3xE5FhsOoy1cOFCvPTSS/Dy8kK5cuXQvXt3XL9+3SjNwIEDIZPJjF5NmzY1SpOZmYkxY8bAz88PpUqVQteuXfHnn38+z0MhIiIiO2XTYOfo0aMYNWoUTp06hZiYGOh0OkRGRiI1NdUoXfv27XHv3j3ptXfvXqP148aNw65du7Bjxw6cOHECT58+RefOnaHXG8/ISUTOSas3YEvcTWyJuwltjtl5rZWPiByLTYex9u/fb/R548aNKFeuHOLj49GqVStpuUqlglqtznUbycnJWL9+Pb788ku0a9cOALB161YEBQXh0KFDiIqKst4BEJFd0OoNmPndFQBAz7AKUOZyK7Ml8xGRY7Gra3aSk5MBAL6+vkbLY2NjUa5cOZQuXRrh4eGYP38+ypUrByBrVkmtVmv0XI/AwEDUrVsXJ0+ezDXYyczMRGZmpvQ5JSUFAKDVaqHVai1yLDqtHuVLu/33vQ5amePe6ZFdJ5aqGzLG+i0+rVaX473WpL3lVccF5aP/0Wq1EELAYDBIk+9ly764N3s9WZ7D1rHIMYmkEDAYit7GDAYDhBDQarVQKIxvJCjs/5sykfMSdBsSQqBbt2549OiR0QyTO3fuhKenJ4KDg5GQkIAZM2ZAp9MhPj4eKpUK27Ztw6BBg4yCFyDrIWYhISH47LPPTPYVHR1t9NyTbNu2bYOHh4flD46IrCpTD0w+nfXbbUljHVSFvLHK3HwlkYuLC9RqNYKCgvKcR4bIGjQaDe7cuYPExETodMYPg01LS0Pfvn2RnJwMb2/vPLdhNz07o0ePxsWLF40mOgKy7v/PVrduXTRq1AjBwcHYs2cPevTokef28nuux7Rp0zBhwgTpc0pKCoKCghAZGZlvZZVUWq0WMTExiIiIMJl2nYqP9Vt8aRodJp/+EQAQFRVpMvtuXnVcUD76n4yMDNy5cweenp5wc3MzWieEwJMnT+Dl5WWR5ymRqZJcxxkZGXB3d0erVq1MvnvZIzMFsYuWPWbMGHz//fc4duwYKlSokG/agIAABAcH48aNGwCynvuh0Wjw6NEjlClTRkqXlJQkTYf9LJVKBZVKZbJcqVTyZJMP1o91sX7NpxT/+88/qx5z/6/t2ToubD7Keg6TTCaDXC43eSRE9rBK9nqyvJJcx3K5XHqe2bP/Rxb2/0yb1pgQAqNHj8a3336LH3/80Wgq7rw8ePAAd+7cQUBAAAAgLCwMSqUSMTExUpp79+7h8uXLeQY7z0OGNmsa+q6rTiBDy7vCiIio5DEYBG4kPcGNpCdmXa9jKTYNdkaNGoWtW7di27Zt8PLyQmJiIhITE6UHpT19+hSTJk1CXFwcbt68idjYWHTp0gV+fn7SA8t8fHwwZMgQTJw4EYcPH8b58+fx5ptvIjQ0VLo7yxYMQuDin8m4+GcyDPZxWRQRERVCbGwsZDIZHj9+bOuiODyBrGfQpWts+7gImwY7a9asQXJyMlq3bo2AgADptXPnTgCAQqHApUuX0K1bN1SvXh0DBgxA9erVERcXBy8vL2k7H330Ebp3747evXujRYsW8PDwwA8//GBy1TYREVFBmjdvjnv37sHHx8fWRcnTgwcPUKFChUIFZYWZePfRo0fo378/fHx84OPjg/79+ztVsGfTAeqCbgRzd3fHgQMHCtyOm5sbVq5ciZUrV1qqaEREVEK5urrmObebvRgyZAjq1auHu3fvFph23Lhx+OGHH7Bjxw6ULVsWEydOROfOnREfHy91CvTt2xd//vmnNP/dsGHD0L9/f/zwww9WPY7npWRd5URERCVK69atMWbMGIwbNw5lypSBv78/1q1bh9TUVAwaNAheXl6oUqUK9u3bJ+V5dhhr06ZNKF26NA4cOIBatWrB09NTmtnfFtasWYPHjx9j0qRJBabNnnj3ww8/RLt27dCwYUNs3boVly5dwqFDhwAA165dw/79+/HFF1+gWbNmaNasGT7//HP8+9//NnmEk6NisENERGZL0+iQptEhXaOX3hf00uV4NIdOb0CaRmdyI0deec2xefNm+Pn54fTp0xgzZgxGjBiBXr16oXnz5jh37hyioqLQv39/kyeZG5UnLQ3Lli3Dl19+iWPHjuH27dsFBhuenp75vjp06FDkY7l69SrmzJmDLVu2FOqurIIm3gWAuLg4+Pj4oEmTJlKapk2bwsfHR0rj6HifJRERma32zIIvNXjWp31fRKd6WXfUHrhyH6O2nUOTEF/sHN5MStNy8RE8TNWY5L25qFOR91e/fn188MEHALLmWVu0aBH8/PwwdOhQAMDMmTOxZs0aXLx40eRB09m0Wi3Wrl2LKlWqAMiaG27OnDn57vfChQv5rnd3dy/ScWRmZqJfv35YunQpKlasiD/++KPAPImJiXB1dTWamgUA/P39kZiYKKXJfipBTuXKlZPSODoGO1bkW4qzjBI9L+a2N7ZT51evXj3pvUKhQNmyZREaGiot8/f3B5A1P1tePDw8pEAHyJrzLb/0AFC1alVzi4wOHTpITxMIDg7GpUuXMGfOHNSsWRNvvvmm2dvN9uzEu7lNVJjf5LxF4WIH8wIx2LESD1cXnJsRYetiEJUI5rY3ttPiuzonCgaDAU9SnsDL26tQQyuuOR64GlXHH1fnREH+zEn1xJQ2FivjsxPPZU9Ql/MzgHyfOZXbNgq6ycbT0zPf9S+//LLRtUI5ffHFF9I0LNn7PnbsGK5evQoXl6xTd/b+/fz8MH369Fwfg1SYiXfVajXu379vkvfvv/+WAkFzKeQy1A60/ZMJGOwQEZHZPFxdYDAYoHNVwMPVpciz+7oo5HDJ5WnzzvDojuIMY5UvX97os8FgwJYtW6BQKKQ6PnPmDAYPHozjx48b9TrllHPi3d69ewP438S7S5YsAQA0a9YMycnJOH36NBo3bgwA+Pnnn5GcnGzTyXktyfG/TURERHaoOMNYuQkJCYG3t7cU7Pzzzz8AgFq1aqF06dIAgLt376Jt27bYsmULGjdubDTxbtmyZeHr64tJkyYZTbxbq1YttG/fHkOHDpUenj1s2DB07twZNWrUsOgx2AqDHSvJ0OoxYMNpAMDmwY3hpuQEh0TWYm57YzslZ6PVanH9+nWjO8s++ugjuLi4oHfv3khPT0fbtm2xadMmo4l3v/rqK7z77rvSXVtdu3bFqlWril0eg0Eg4UEqACCkbCnI5bZ5iCmDHSsxCIGfEx5K74nIesxtb2ynzi82NtZk2c2bN02W5bz+pnXr1kafBw4ciIEDBxql7969e4HX7Fjbs+UEgEqVKpksK8zEu76+vti6davFyygApGbqpPe2wmCHiByeq0KOT/u+KL23dj4iciwMdojI4bko5NK8Lc8jHxE5Fv6UISIiIqfGYIeIHJ5Ob8Cei/ew5+I9VJq6x6x8OR9hQETOhcNYROTwNHoDRm07V6x8V+dE5TrfCxmz9UW5VPJY4jvHlm1F7koF3HkrKxE5gexZfPN7WCZRbuQymckM2UWR/Z17dhbromDPjpV4uLrg2tz2ti4GEZFFKBQKlC5dWnoelIeHh9FjFjQaDTIyMoo8gzIVjiPXcdWyKgCAVpMJbRHyCSGQlpaGpKQklC5d2mheoKJisENERIWiVqsBmD4wUwiB9PR0uLu7W+TBkWSqJNdx6dKlpe+euRjsEBFRochkMgQEBKBcuXLQav/3G12r1eLYsWNo1apVsYYaKG8ltY6VSmWxenSyMdixkgytHiO2xgMA1rwZxmnoichpKBQKoxOQQqGATqeDm5tbiToRP0+OWsf2ci5ksGMlBiFw5Prf0nsiIqKSxl7OhY51lRMRERFRETHYISIiIqfGYIeIiIicGoMdIiIicmoMdoiIiMipMdghIiIip8Zbz63Ew9UFNxd1snUxiEqEnO2tKE89Zzslsi57aWPs2SEiIiKnxmCHiIiInBqHsawkQ6vHhK8vAACW927Ax0UQWVHO9mZuPrZTIsuzlzbGnh0rMQiBvZcSsfdSIh8XQWRlOdubufnYToksz17aGHt2iMjhKRVyzOlWBwAw87srZuVTKvjbj8hZMdghIoenVMjxVrNKAIoe7GTnIyLnxZ8yRERE5NTYs0NEDk9vEDid8LBY+RqH+EIhl1m6aERkBxjsEJHDy9Tp0efzU8XKd3VOFDxc+V8ikTPiMBYRERE5Nf6MsRJ3pQJX50RJ74mIiEoaezkXMtixEplMxi5xIiIq0ezlXMhhLCIiInJqDHasJFOnx8Svf8HEr39Bpk5v6+IQERE9d/ZyLmSwYyV6g8A35/7EN+f+hN7AaeiJiKjksZdzIYMdIiIicmoMdoiIiMipMdghIiIip8Zgh4iIiJwagx0iIiJyagx2iIiIyKnZflpDJ+WuVCD+g3bSeyKynpztLWzeIbPysZ0SWZ69tDGb9uwsXLgQL730Ery8vFCuXDl0794d169fN0ojhEB0dDQCAwPh7u6O1q1b48qVK0ZpMjMzMWbMGPj5+aFUqVLo2rUr/vzzz+d5KCZkMhnKeqpQ1lMFmUxm07IQObuc7c3cfGynRJZnL23MpsHO0aNHMWrUKJw6dQoxMTHQ6XSIjIxEamqqlGbJkiVYvnw5Vq1ahTNnzkCtViMiIgJPnjyR0owbNw67du3Cjh07cOLECTx9+hSdO3eGXs+Zi4mIiEo6mw5j7d+/3+jzxo0bUa5cOcTHx6NVq1YQQmDFihWYPn06evToAQDYvHkz/P39sW3bNgwfPhzJyclYv349vvzyS7Rrl9VVtnXrVgQFBeHQoUOIiop67scFZE2RPe/f1wAAH3SuBZULu8iJrCVnezM3H9spkeXZSxuzq2t2kpOTAQC+vr4AgISEBCQmJiIyMlJKo1KpEB4ejpMnT2L48OGIj4+HVqs1ShMYGIi6devi5MmTuQY7mZmZyMzMlD6npKQAALRaLbRarUWOJUOjw5enbgEAJkVUgVzYVVUXSXadWKpuyBjrt/hytjdXuTCpy7zq2JnaqS3xO2x9jlrH1m5jha0Pu2nZQghMmDABLVu2RN26dQEAiYmJAAB/f3+jtP7+/rh165aUxtXVFWXKlDFJk53/WQsXLsTs2bNNlh88eBAeHh7FPhYAyNQD2dV74MBBqJzgB2NMTIyti+DUWL/m0xmA9hWyRuUjyhuwd+/eXNM9W8c58x06eBAuvD+1WPgdtj5Hq2NrnwvT0tIKlc5ugp3Ro0fj4sWLOHHihMm6Zy9qEkIUeKFTfmmmTZuGCRMmSJ9TUlIQFBSEyMhIeHt7m1F6U2kaHSaf/hEAEBUVCQ9Xu6nqItNqtYiJiUFERASUSqWti+N0WL+W0fW//9aNPoDL0cY9uvnVcVdQcfE7bH2OWsfWPhdmj8wUxC7OwGPGjMH333+PY8eOoUKFCtJytVoNIKv3JiAgQFqelJQk9fao1WpoNBo8evTIqHcnKSkJzZs3z3V/KpUKKpXpXRtKpdJiXyKl+F+glbVdu6jqYrFk/ZAp1q9lZOpledYj69i6WL/W52h1bO1zYWHrwqadtkIIjB49Gt9++y1+/PFHhISEGK0PCQmBWq026rbTaDQ4evSoFMiEhYVBqVQapbl37x4uX76cZ7BDRM7FYBD49f4T/Hr/ScGJ88hnMAgrlY6IbM2m3Q2jRo3Ctm3b8N1338HLy0u6xsbHxwfu7u6QyWQYN24cFixYgGrVqqFatWpYsGABPDw80LdvXyntkCFDMHHiRJQtWxa+vr6YNGkSQkNDpbuziMi5Zej0iPzoWLHyXZ0T5dDDzUSUN5u27DVr1gAAWrdubbR848aNGDhwIABg8uTJSE9Px8iRI/Ho0SM0adIEBw8ehJeXl5T+o48+gouLC3r37o309HS0bdsWmzZtgkLhBFcFExERUbHYNNgRouBuY5lMhujoaERHR+eZxs3NDStXrsTKlSstWLricXNR4PjkNtJ7IiKiksZezoXss7USuVyGIF/L3MZORETkiOzlXMhZJYiIiMipsWfHSjQ6A5YdzHqo6aTIGnDlbGVERFTC2Mu5kGdgK9EZDFh37A+sO/YHdAaDrYtDRET03NnLuZDBDhERETk1BjtERETk1BjsEBERkVNjsENEREROjcEOEREROTUGO0REROTUOM+Olbi5KHBwfCvpPRFZT872VpQHgrKdElmXvbQxBjtWIpfLUN3fq+CERFRs5rY3tlMi67KXNsZhLCIiInJq7NmxEo3OgE+P/AYAGNWmKh8XQWRFOdubufnYToksz17aGIMdK9EZDPj48A0AwPDwynBlJxqR1eRsb+bmYzslsjx7aWMMdojI4SnkMvRvGgwA+PLULbPyKeQyq5SNiGyPwQ4ROTyViwJzu9cFULRgJ2c+InJe7LMlIiIip8aeHSJyeEIIPEzVFCufbylXyGQcyiJyRgx2iMjhpWv1CJt3qFj5rs6Jgocr/0skckYcxiIiIiKnxp8xVqJyUeC7US2k90RERCWNvZwLGexYiUIuQ/2g0rYuBhERkc3Yy7mQw1hERETk1NizYyUanQEbf0oAAAxqEcJp6ImIqMSxl3Mhgx0r0RkMWLjvPwCA/s2COQ09ERGVOPZyLuQZmIiIiJwagx0iIiJyamYFO5UrV8aDBw9Mlj9+/BiVK1cudqGIiIiILMWsYOfmzZvQ6/UmyzMzM3H37t1iF4qIiIjIUop0gfL3338vvT9w4AB8fHykz3q9HocPH0alSpUsVjgiIiKi4ipSsNO9e3cAgEwmw4ABA4zWKZVKVKpUCR9++KHFCkdERERUXEUKdgwGAwAgJCQEZ86cgZ+fn1UK5QxULgpsH9pUek9E1pOzvfX5/JRZ+dhOiSzPXtqYWfPsJCQkWLocTkchl6FZlbK2LgZRiWBue2M7JbIue2ljZk8qePjwYRw+fBhJSUlSj0+2DRs2FLtgRERERJZgVrAze/ZszJkzB40aNUJAQABkMpmly+XwtHoDtp++DQDo07gilApOaURkLTnbm7n52E6JLM9e2phZwc7atWuxadMm9O/f39LlcRpavQEzv7sCAOgZVoH/iRJZUc72Zm4+tlMiy7OXNmZWsKPRaNC8eXNLl4WIyCxymQwdQ9UAgL2XEs3KJ2cPNZHTMivEevvtt7Ft2zZLl4WIyCxuSgVW9wvD6n5hZudzU/JuLCJnZVbPTkZGBtatW4dDhw6hXr16UCqVRuuXL19ukcIRERERFZdZwc7FixfRoEEDAMDly5eN1vFiZSIiIrInZgU7R44csXQ5iIjMlqbRofbMA8XKd3VOFDxczZ6Ng4jsGG89ICIiIqdm1s+YNm3a5Dtc9eOPP5pdIGfhqpBjw8BG0nsiIqKSxl7OhWYFO9nX62TTarW4cOECLl++bPKA0JLKRSHHKzX9bV0MIiIim7GXc6FZwc5HH32U6/Lo6Gg8ffq0WAUiIiIisiSL9im9+eabfC7Wf2n1Bvzf2Tv4v7N3oNUbCs5ARETkZOzlXGjRWw/i4uLg5uZmyU06LK3egPf+dREA0KleAKehJyKiEsdezoVm7bVHjx5Gr1dffRVNmzbFoEGDMHz48EJv59ixY+jSpQsCAwMhk8mwe/duo/UDBw6ETCYzejVt2tQoTWZmJsaMGQM/Pz+UKlUKXbt2xZ9//mnOYREREZETMivY8fHxMXr5+vqidevW2Lt3L2bNmlXo7aSmpqJ+/fpYtWpVnmnat2+Pe/fuSa+9e/carR83bhx27dqFHTt24MSJE3j69Ck6d+4MvV5vzqERERGRkzFrGGvjxo0W2XmHDh3QoUOHfNOoVCqo1epc1yUnJ2P9+vX48ssv0a5dOwDA1q1bERQUhEOHDiEqKsoi5SQiIiLHVaxrduLj43Ht2jXIZDLUrl0bDRs2tFS5JLGxsShXrhxKly6N8PBwzJ8/H+XKlZP2r9VqERkZKaUPDAxE3bp1cfLkyTyDnczMTGRmZkqfU1JSAGTdQq/Vai1Sbq1Wl+O9FlqZsMh2bSG7TixVN2SM9Vt8Odubq1yY1GVedexM7dSW+B22PketY2u3scLWh1nBTlJSEt544w3ExsaidOnSEEIgOTkZbdq0wY4dO/DCCy+Ys1kTHTp0QK9evRAcHIyEhATMmDEDr7zyCuLj46FSqZCYmAhXV1eUKVPGKJ+/vz8SExPz3O7ChQsxe/Zsk+UHDx6Eh4eHRcqeqQeyq/fAgYNQOcEDlWNiYmxdBKfG+jVfzvY2r5HeZLg727N17Izt1Jb4HbY+R6tja7extLS0QqUzK9gZM2YMUlJScOXKFdSqVQsAcPXqVQwYMADvvvsutm/fbs5mTbz++uvS+7p166JRo0YIDg7Gnj170KNHjzzzCSHyneF52rRpmDBhgvQ5JSUFQUFBiIyMhLe3t0XKnqbRYfLprJmko6IiHfqZO1qtFjExMYiIiDB5wj0VH+u3+HK2tw/OKnBltnGvbl517Ezt1Jb4HbY+R61ja7ex7JGZgpi11/379+PQoUNSoAMAtWvXxqeffmo0pGRpAQEBCA4Oxo0bNwAAarUaGo0Gjx49MurdSUpKQvPmzfPcjkqlgkqlMlmuVCot9iUqJVfg074vZr13U8HFCW49t2T9kCnWr/lytrdR287lWY/P1rEztlNb4nfY+hytjq3dxgpbF2bt1WAw5LoDpVIJg8F6kwY9ePAAd+7cQUBAAAAgLCwMSqXSqFvv3r17uHz5cr7BzvPgopCjU70AdKoXwP9AiawsZ3szNx/bKZHl2UsbM2vPr7zyCsaOHYu//vpLWnb37l2MHz8ebdu2LfR2nj59igsXLuDChQsAgISEBFy4cAG3b9/G06dPMWnSJMTFxeHmzZuIjY1Fly5d4Ofnh1dffRVA1i3wQ4YMwcSJE3H48GGcP38eb775JkJDQ6W7s4iIiKhkM2sYa9WqVejWrRsqVaqEoKAgyGQy3L59G6Ghodi6dWuht3P27Fm0adNG+px9Hc2AAQOwZs0aXLp0CVu2bMHjx48REBCANm3aYOfOnfDy8pLyfPTRR3BxcUHv3r2Rnp6Otm3bYtOmTVAobHuloU5vwIEr9wEAUXX8+auRyIpytjdz87GdElmevbQxs4KdoKAgnDt3DjExMfjPf/4DIQRq165d5N6U1q1bQ4i8b0M7cOBAgdtwc3PDypUrsXLlyiLt29o0egNGbTsHALg6J4r/iRJZUc72Zm4+tlMiy7OXNlakvf7444+oXbu2dPVzREQExowZg3fffRcvvfQS6tSpg+PHj1uloEREeZHLZGgS4osmIb5m55PncwcnETm2IvXsrFixAkOHDs319mwfHx8MHz4cy5cvx8svv2yxAhIRFcRNqcDO4c0AAJWm7jErHxE5ryL17Pzyyy9o3759nusjIyMRHx9f7EIRERERWUqRgp379+/ne0+7i4sL/v7772IXioiIiMhSijSMVb58eVy6dAlVq1bNdf3FixelOXCIiJ6XNI0OLRcfKVa+E1PacAZlIidVpJ6djh07YubMmcjIyDBZl56ejlmzZqFz584WKxwRUWE9TNXgYarmueUjIsdRpJ8xH3zwAb799ltUr14do0ePRo0aNSCTyXDt2jV8+umn0Ov1mD59urXK6lCUCjmW9qwnvSciIipp7OVcWKRgx9/fHydPnsSIESMwbdo0aY4cmUyGqKgorF69Gv7+/lYpqKNRKuTo1SjI1sUgIiKyGXs5FxZ5gDo4OBh79+7Fo0eP8Ntvv0EIgWrVqhk9iJOIiIjIXph9NV6ZMmXw0ksvWbIsTkWnN+DYjaw701pVe4EzsxIRUYljL+dC3npgJRq9AYM3nQXAaeiJiKhkspdzIc/ARERE5NQY7BAREZFTY7BDRERETo3BDhERETk1BjtERETk1BjsEBERkVPjredWolTIMadbHek9EVlPzvY287srZuVjOyWyPHtpYwx2rESpkOOtZpVsXQyiEiFneytqsMN2SmQ99tLG+FOGiIiInBp7dqxEbxA4nfAQANA4xBcKuczGJSJyXjnbm7n52E6JLM9e2hiDHSvJ1OnR5/NTALKmyPZwZVUTWUvO9mZuPrZTIsuzlzbGlk1EDk8GGaqV8wQA3Eh6alY+GdirQ+SsGOwQkcNzd1UgZkI4AKDS1D1m5SMi58ULlImIiMipMdghIiIip8ZhLCJyeOkaPbquOlGsfN+Pbgl3V4Wli0ZEdoDBDhE5PAFRpAuTc8snICxdLCKyEwx2rMRFLse0DjWl90RERCWNvZwLGexYiauLHMPDq9i6GERERDZjL+dCdjkQERGRU2PPjpXoDQKX7yYDAOqW9+E09EREVOLYy7mQPTtWkqnTo9unP6Hbpz8hU6e3dXGIiIieO3s5FzLYISIiIqfGYIeIiIicGoMdIiIicmoMdoiIiMipMdghIiIip8Zgh4iIiJwa59mxEhe5HGPbVpPeE5H15GxvHx++YVY+tlMiy7OXNsZgx0pcXeQYH1Hd1sUgKhFytreiBDtsp0TWZS9tjD9liIiIyKmxZ8dKDAaB3/5+CgCo+oIn5HxcBJHV5Gxv5uZjOyWyPHtpYwx2rCRDp0fkR8cAAFfnRMHDlVVNZC0525u5+dhOiSzPXtoYWzYROQXfUq4AgIepGrPyEZHzYrBDRA7Pw9UF52ZEAAAqTd1jVj4icl68QJmIiIicGoMdIiIicmocxiIih5eh1WPAhtPFyrd5cGO4KRWWLhoR2QGb9uwcO3YMXbp0QWBgIGQyGXbv3m20XgiB6OhoBAYGwt3dHa1bt8aVK1eM0mRmZmLMmDHw8/NDqVKl0LVrV/z555/P8SiIyNYMQuDnhIf4OeGh2fkMQlipdERkazYNdlJTU1G/fn2sWrUq1/VLlizB8uXLsWrVKpw5cwZqtRoRERF48uSJlGbcuHHYtWsXduzYgRMnTuDp06fo3Lkz9Hr98zqMXLnI5RjWqjKGtarMaeiJiKhEspdzoU2HsTp06IAOHTrkuk4IgRUrVmD69Ono0aMHAGDz5s3w9/fHtm3bMHz4cCQnJ2P9+vX48ssv0a5dOwDA1q1bERQUhEOHDiEqKuq5HcuzXF3keL9jLZvtn4iIyNbs5Vxot9fsJCQkIDExEZGRkdIylUqF8PBwnDx5EsOHD0d8fDy0Wq1RmsDAQNStWxcnT57MM9jJzMxEZmam9DklJQUAoNVqodVqrXREjiu7Tlg31sH6LT6tVie9d5ULk7rMq45z5tNqtdDKOJRlDn6HrY91nLvC1ofdBjuJiYkAAH9/f6Pl/v7+uHXrlpTG1dUVZcqUMUmTnT83CxcuxOzZs02WHzx4EB4eHsUtOgDAIIBH/42nyqgAZ5iFPiYmxtZFcGqsX/Nl6oHs/87mNdJj7969uaZ7to5z5jtw4CBUvD65WPgdtj5Hq2NrnwvT0tIKlc5ug51sMplxzQghTJY9q6A006ZNw4QJE6TPKSkpCAoKQmRkJLy9vYtX4P9K0+hQf+6PAIBfZrzi0NPQa7VaxMTEICIiAkql0tbFcTqs3+JL0+gw+XRWe/vgrAJXZhv36uZVxznzRUVFOnQ7tSV+h63PUevY2ufC7JGZgthty1ar1QCyem8CAgKk5UlJSVJvj1qthkajwaNHj4x6d5KSktC8efM8t61SqaBSqUyWK5VKi32JlOJ/wVbWdu22qgvNkvVDpli/5svZ3jQGWZ71+GwdO2M7tSV+h63P0erY2m2ssHVht7cJhYSEQK1WG3XZaTQaHD16VApkwsLCoFQqjdLcu3cPly9fzjfYISIiopLDpj9jnj59it9++036nJCQgAsXLsDX1xcVK1bEuHHjsGDBAlSrVg3VqlXDggUL4OHhgb59+wIAfHx8MGTIEEycOBFly5aFr68vJk2ahNDQUOnuLCIiIirZbBrsnD17Fm3atJE+Z19HM2DAAGzatAmTJ09Geno6Ro4ciUePHqFJkyY4ePAgvLy8pDwfffQRXFxc0Lt3b6Snp6Nt27bYtGkTFApeaUhEREQ2DnZat24Nkc+spTKZDNHR0YiOjs4zjZubG1auXImVK1daoYRERETk6Oz2mh0iIiIiS+CtB1aikMvQv2mw9J6IrCdne/vy1C2z8rGdElmevbQxBjtWonJRYG73urYuBlGJkLO9FSXYYTslsi57aWMcxiIiIiKnxp4dKxFC4GGqBgDgW8q1wFmfich8OdubufnYToksz17aGIMdK0nX6hE27xAA4OqcKE5DT2RFOdubufnYToksz17aGIexiIiIyKnxZwwROTwPVxfcXNQJAFBp6h6z8hGR82LPDhERETk1BjtERETk1DiMRUQOL0Orx4SvLxQr3/LeDeCm5DP1iJwRgx0icngGIbD3UmKx8i3rlfdz+ojIsTHYsRKFXIbXXqwgvSciIipp7OVcyGDHSlQuCnzYu76ti0FERGQz9nIu5AXKRERE5NTYs2MlQgika/UAAHelgtPQExFRiWMv50L27FhJulaP2jMPoPbMA9IfmoiIqCSxl3Mhgx0iIiJyagx2iIiIyKkx2CEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqXGeHSuRy2ToGKqW3hOR9eRsb0V5RhbbKZF12UsbY7BjJW5KBVb3C7N1MYhKhJztrdLUPWblIyLLs5c2xmEsIiIicmoMdoiIiMipcRjLStI0OtSeeQAAcHVOFDxcWdVE1pKzvZmbj+2UyPLspY2xZ4eIiIicGn/GEJHDc1cqEP9BOwBA2LxDZuVzVyqsUjYisj0GO0Tk8GQyGcp6qp5bPiJyLBzGIiIiIqfGnh0icniZOj3m/ftasfJ90LkWVC4cyiJyRgx2iMjh6Q0CX566Vax80zrWtHSxiMhOMNixErlMhjY1XpDeExERlTT2ci5ksGMlbkoFNg5qbOtiEBER2Yy9nAt5gTIRERE5NQY7RERE5NQY7FhJmkaHWjP2o9aM/UjT6GxdHCIioufOXs6FvGbHitK1elsXgYiIyKbs4VzInh0iIiJyagx2iIiIyKkx2CEiIiKnxmCHiIiInBqDHSIiInJqvBvLSuQyGZqE+Ervich6cra3nxMempWP7ZTI8uyljTHYsRI3pQI7hzezdTGISoSc7a3S1D1m5SMiy7OXNsZhLCIiInJqdh3sREdHQyaTGb3UarW0XgiB6OhoBAYGwt3dHa1bt8aVK1dsWGIiIiKyN3Y/jFWnTh0cOnRI+qxQKKT3S5YswfLly7Fp0yZUr14d8+bNQ0REBK5fvw4vLy9bFFeSptGh5eIjAIATU9rAw9Xuq5rIYeVsb+bmYzslsjx7aWN237JdXFyMenOyCSGwYsUKTJ8+HT169AAAbN68Gf7+/ti2bRuGDx/+vItq4mGqxtZFICoxzG1vbKdE1mUPbczug50bN24gMDAQKpUKTZo0wYIFC1C5cmUkJCQgMTERkZGRUlqVSoXw8HCcPHky32AnMzMTmZmZ0ueUlBQAgFarhVartUi5tVpdjvdaaGXCItu1hew6sVTdkDHWb/EphMDe0c0BAN1X/2RSl3nVcc58CmHg38BM/A5bn6PWsbXPhYWtD5kQwm7Pwvv27UNaWhqqV6+O+/fvY968efjPf/6DK1eu4Pr162jRogXu3r2LwMBAKc+wYcNw69YtHDhwIM/tRkdHY/bs2SbLt23bBg8PD4uUPVMPTD6dFUsuaayDSlFABiIiIidj7XNhWloa+vbti+TkZHh7e+eZzq6DnWelpqaiSpUqmDx5Mpo2bYoWLVrgr7/+QkBAgJRm6NChuHPnDvbv35/ndnLr2QkKCsI///yTb2UVRZpGh/pzfwQA/DLjFYe+FkCr1SImJgYRERFQKpW2Lo7TYf1aVt3oA7gcHWW0jHVsXaxf63PUOrb2uTAlJQV+fn4FBjsOdQYuVaoUQkNDcePGDXTv3h0AkJiYaBTsJCUlwd/fP9/tqFQqqFQqk+VKpdJiXyKl+N/kSVnbdaiqzpUl64dMsX7Np9EZ8OmR3wAAmXpZnvX4bB3nzDeqTVW4utj1Dap2j99h63O0Orb2ubCwdeFQLTszMxPXrl1DQEAAQkJCoFarERMTI63XaDQ4evQomjdvbsNSEtHzpjMY8PHhG/j48A2z8+kMBiuVjohsza67GyZNmoQuXbqgYsWKSEpKwrx585CSkoIBAwZAJpNh3LhxWLBgAapVq4Zq1aphwYIF8PDwQN++fW1ddMhlMtSr4CO9JyIiKmns5Vxo18HOn3/+iT59+uCff/7BCy+8gKZNm+LUqVMIDg4GAEyePBnp6ekYOXIkHj16hCZNmuDgwYM2n2MHyJoi+/vRLW1dDCIiIpuxl3OhXQc7O3bsyHe9TCZDdHQ0oqOjn0+BiIiIyOE41DU7REREREXFYMdK0jV6tFj0I1os+hHpGr2ti0NERPTc2cu50K6HsRyZgMDdx+nSeyIiopLGXs6F7NkhIiIip8Zgh4iIiJwagx0iIiJyagx2iIiIyKkx2CEiIiKnxruxrEQGGaqV85TeE5H15GxvN5KempWP7ZTI8uyljTHYsRJ3VwViJoTbuhhEJULO9lZp6h6z8hGR5dlLG+MwFhERETk1BjtERETk1DiMZSXpGj26rjoBAPh+dEu4uypsXCIi55WzvZmbj+2UyPLspY0x2LESASFdKMnHRRBZV872Zm4+tlMiy7OXNsZgh4gcnspFge1DmwIA+nx+yqx8Khf26hA5KwY7ROTwFHIZmlUp+9zyEZFj4QXKRERE5NTYs0NEDk+rN2D76dvFytencUUoFfz9R+SMGOwQkcPT6g2Y+d2VYuXrGVaBwQ6Rk2KwYyUyyFC+tLv0noiIqKSxl3Mhgx0rcXdV4Kepr9i6GERERDZjL+dC9tkSERGRU2OwQ0RERE6NwY6VZGizpsjuuuoEMrR6WxeHiIjoubOXcyGv2bESgxC4+Gey9J6IiKiksZdzIXt2iIiIyKkx2CEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqfFuLCvyLeVq6yIQlRjZ7e1hqsasfERkHfbQxhjsWImHqwvOzYiwdTGISoSc7a3S1D1m5SMiy7OXNsZhLCIiInJqDHaIiIjIqXEYy0oytHoM2HAaALB5cGO4KRU2LhGR88rZ3szNx3ZKZHn20sYY7FiJQQj8nPBQek9E1pOzvZmbj+2UyPLspY0x2CEih+eqkOPTvi8CAEZtO2dWPlcFR/WJnBWDHSJyeC4KOTrVCwAAjNpmXj4icl78KUNEREROjT07ROTwdHoDDly5X6x8UXX84cKhLCKnxGCHiByeRm8o0rU6ueW7OieKwQ6Rk2KwY0XuvI2ViIhKOHs4FzLYsRIPVxdcm9ve1sUgIiKyGXs5F7LPloiIiJwagx0iIiJyagx2rCRDq8egjacxaONpZGj1ti4OERHRc2cv50Jes2MlBiFw5Prf0nsiIqKSxl7OhezZISIiIqfmNMHO6tWrERISAjc3N4SFheH48eO2LhIRERHZAacIdnbu3Ilx48Zh+vTpOH/+PF5++WV06NABt2/ftnXRiIiIyMacIthZvnw5hgwZgrfffhu1atXCihUrEBQUhDVr1ti6aERERGRjDh/saDQaxMfHIzIy0mh5ZGQkTp48aaNSERERkb1w+Lux/vnnH+j1evj7+xst9/f3R2JiYq55MjMzkZmZKX1OTk4GADx8+BBardYi5UrT6GDITAMAPHjwAOmujlvVWq0WaWlpePDgAZRKpa2L43RYv8WXs70p5QIPHjwwWp9XHTtTO7Ulfoetz1Hr2Npt7MmTJwAAUcCdXk7TsmUymdFnIYTJsmwLFy7E7NmzTZaHhIRYpWwVV1hls0SUB7/lRc/DdkpkXdZsY0+ePIGPj0+e6x0+2PHz84NCoTDpxUlKSjLp7ck2bdo0TJgwQfpsMBjw8OFDlC1bNs8AqSRLSUlBUFAQ7ty5A29vb1sXx+mwfq2PdWxdrF/rYx3nTgiBJ0+eIDAwMN90Dh/suLq6IiwsDDExMXj11Vel5TExMejWrVuueVQqFVQqldGy0qVLW7OYTsHb25uNzIpYv9bHOrYu1q/1sY5N5dejk83hgx0AmDBhAvr3749GjRqhWbNmWLduHW7fvo133nnH1kUjIiIiG3OKYOf111/HgwcPMGfOHNy7dw9169bF3r17ERwcbOuiERERkY05RbADACNHjsTIkSNtXQynpFKpMGvWLJOhP7IM1q/1sY6ti/Vrfazj4pGJgu7XIiIiInJgDj+pIBEREVF+GOwQERGRU2OwQ0RERE6NwQ4RERE5NQY7JJk/fz6aN28ODw+PPCdZvH37Nrp06YJSpUrBz88P7777LjQajVGaS5cuITw8HO7u7ihfvjzmzJlT4HNLSqpKlSpBJpMZvaZOnWqUpjB1TnlbvXo1QkJC4ObmhrCwMBw/ftzWRXJI0dHRJt9VtVotrRdCIDo6GoGBgXB3d0fr1q1x5coVG5bY/h07dgxdunRBYGAgZDIZdu/ebbS+MHWamZmJMWPGwM/PD6VKlULXrl3x559/PsejcAwMdkii0WjQq1cvjBgxItf1er0enTp1QmpqKk6cOIEdO3bgm2++wcSJE6U0KSkpiIiIQGBgIM6cOYOVK1di2bJlWL7cjIcVlRDZ80Nlvz744ANpXWHqnPK2c+dOjBs3DtOnT8f58+fx8ssvo0OHDrh9+7ati+aQ6tSpY/RdvXTpkrRuyZIlWL58OVatWoUzZ85ArVYjIiJCelAjmUpNTUX9+vWxatWqXNcXpk7HjRuHXbt2YceOHThx4gSePn2Kzp07Q6/XP6/DcAyC6BkbN24UPj4+Jsv37t0r5HK5uHv3rrRs+/btQqVSieTkZCGEEKtXrxY+Pj4iIyNDSrNw4UIRGBgoDAaD1cvuaIKDg8VHH32U5/rC1DnlrXHjxuKdd94xWlazZk0xdepUG5XIcc2aNUvUr18/13UGg0Go1WqxaNEiaVlGRobw8fERa9eufU4ldGwAxK5du6TPhanTx48fC6VSKXbs2CGluXv3rpDL5WL//v3PreyOgD07VGhxcXGoW7eu0QPXoqKikJmZifj4eClNeHi40cRXUVFR+Ouvv3Dz5s3nXWSHsHjxYpQtWxYNGjTA/PnzjYaoClPnlDuNRoP4+HhERkYaLY+MjMTJkydtVCrHduPGDQQGBiIkJARvvPEG/vjjDwBAQkICEhMTjepapVIhPDycdW2mwtRpfHw8tFqtUZrAwEDUrVuX9f4Mp5lBmawvMTHR5EnyZcqUgaurq/TU+cTERFSqVMkoTXaexMREhISEPJeyOoqxY8fixRdfRJkyZXD69GlMmzYNCQkJ+OKLLwAUrs4pd//88w/0er1J/fn7+7PuzNCkSRNs2bIF1atXx/379zFv3jw0b94cV65ckeozt7q+deuWLYrr8ApTp4mJiXB1dUWZMmVM0vA7bow9O04ut4sKn32dPXu20NuTyWQmy4QQRsufTSP+e3FybnmdUVHqfPz48QgPD0e9evXw9ttvY+3atVi/fj0ePHggba8wdU55y+37yLorug4dOuC1115DaGgo2rVrhz179gAANm/eLKVhXVueOXXKejfFnh0nN3r0aLzxxhv5pnm2JyYvarUaP//8s9GyR48eQavVSr8+1Gq1yS+KpKQkAKa/UJxVceq8adOmAIDffvsNZcuWLVSdU+78/PygUChy/T6y7oqvVKlSCA0NxY0bN9C9e3cAWT0NAQEBUhrWtfmy73TLr07VajU0Gg0ePXpk1LuTlJSE5s2bP98C2zn27Dg5Pz8/1KxZM9+Xm5tbobbVrFkzXL58Gffu3ZOWHTx4ECqVCmFhYVKaY8eOGV13cvDgQQQGBhY6qHJ0xanz8+fPA4D0n1th6pxy5+rqirCwMMTExBgtj4mJ4YnAAjIzM3Ht2jUEBAQgJCQEarXaqK41Gg2OHj3KujZTYeo0LCwMSqXSKM29e/dw+fJl1vuzbHhxNNmZW7duifPnz4vZs2cLT09Pcf78eXH+/Hnx5MkTIYQQOp1O1K1bV7Rt21acO3dOHDp0SFSoUEGMHj1a2sbjx4+Fv7+/6NOnj7h06ZL49ttvhbe3t1i2bJmtDstunTx5UixfvlycP39e/PHHH2Lnzp0iMDBQdO3aVUpTmDqnvO3YsUMolUqxfv16cfXqVTFu3DhRqlQpcfPmTVsXzeFMnDhRxMbGij/++EOcOnVKdO7cWXh5eUl1uWjRIuHj4yO+/fZbcenSJdGnTx8REBAgUlJSbFxy+/XkyRPp/1kA0v8Ht27dEkIUrk7feecdUaFCBXHo0CFx7tw58corr4j69esLnU5nq8OySwx2SDJgwAABwOR15MgRKc2tW7dEp06dhLu7u/D19RWjR482us1cCCEuXrwoXn75ZaFSqYRarRbR0dG87TwX8fHxokmTJsLHx0e4ubmJGjVqiFmzZonU1FSjdIWpc8rbp59+KoKDg4Wrq6t48cUXxdGjR21dJIf0+uuvi4CAAKFUKkVgYKDo0aOHuHLlirTeYDCIWbNmCbVaLVQqlWjVqpW4dOmSDUts/44cOZLr/7kDBgwQQhSuTtPT08Xo0aOFr6+vcHd3F507dxa3b9+2wdHYN5kQnNqWiIiInBev2SEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfGYIeIiIicGoMdIiIicmoMdojILmzatAmlS5cuUp6BAwdKz2WytZs3b0Imk+HChQu2LgoRPYPBDhEVydq1a+Hl5QWdTicte/r0KZRKJV5++WWjtMePH4dMJsOvv/5a4HZff/31QqUrqkqVKmHFihUW3y4ROQ4GO0RUJG3atMHTp09x9uxZadnx48ehVqtx5swZpKWlSctjY2MRGBiI6tWrF7hdd3d3lCtXziplJqKSjcEOERVJjRo1EBgYiNjYWGlZbGwsunXrhipVquDkyZNGy9u0aQMg64nNkydPRvny5VGqVCk0adLEaBu5DWPNmzcP5cqVg5eXF95++21MnToVDRo0MCnTsmXLEBAQgLJly2LUqFHQarUAgNatW+PWrVsYP348ZDIZZDJZrsfUp08fvPHGG0bLtFot/Pz8sHHjRgDA/v370bJlS5QuXRply5ZF586d8fvvv+dZT7kdz+7du03K8MMPPyAsLAxubm6oXLkyZs+ebdRrRkTFx2CHiIqsdevWOHLkiPT5yJEjaN26NcLDw6XlGo0GcXFxUrAzaNAg/PTTT9ixYwcuXryIXr16oX379rhx40au+/jqq68wf/58LF68GPHx8ahYsSLWrFljku7IkSP4/fffceTIEWzevBmbNm3Cpk2bAADffvstKlSogDlz5uDevXu4d+9ervvq168fvv/+ezx9+lRaduDAAaSmpuK1114DAKSmpmLChAk4c+YMDh8+DLlcjldffRUGg6HoFZhjH2+++SbeffddXL16FZ999hk2bdqE+fPnm71NIsqFrZ9ESkSOZ926daJUqVJCq9WKlJQU4eLiIu7fvy927NghmjdvLoQQ4ujRowKA+P3338Vvv/0mZDKZuHv3rtF22rZtK6ZNmyaEEGLjxo3Cx8dHWtekSRMxatQoo/QtWrQQ9evXlz4PGDBABAcHC51OJy3r1auXeP3116XPwcHB4qOPPsr3eDQajfDz8xNbtmyRlvXp00f06tUrzzxJSUkCgPQU6oSEBAFAnD9/PtfjEUKIXbt2iZz/7b788stiwYIFRmm+/PJLERAQkG95iaho2LNDREXWpk0bpKam4syZMzh+/DiqV6+OcuXKITw8HGfOnEFqaipiY2NRsWJFVK5cGefOnYMQAtWrV4enp6f0Onr0aJ5DQdevX0fjxo2Nlj37GQDq1KkDhUIhfQ4ICEBSUlKRjkepVKJXr1746quvAGT14nz33Xfo16+flOb3339H3759UblyZXh7eyMkJAQAcPv27SLtK6f4+HjMmTPHqE6GDh2Ke/fuGV37RETF42LrAhCR46latSoqVKiAI0eO4NGjRwgPDwcAqNVqhISE4KeffsKRI0fwyiuvAAAMBgMUCgXi4+ONAhMA8PT0zHM/z17fIoQwSaNUKk3ymDO01K9fP4SHhyMpKQkxMTFwc3NDhw4dpPVdunRBUFAQPv/8cwQGBsJgMKBu3brQaDS5bk8ul5uUN/taomwGgwGzZ89Gjx49TPK7ubkV+RiIKHcMdojILG3atEFsbCwePXqE9957T1oeHh6OAwcO4NSpUxg0aBAAoGHDhtDr9UhKSjK5PT0vNWrUwOnTp9G/f39pWc47wArL1dUVer2+wHTNmzdHUFAQdu7ciX379qFXr15wdXUFADx48ADXrl3DZ599JpX/xIkT+W7vhRdewJMnT5CamopSpUoBgMkcPC+++CKuX7+OqlWrFvm4iKjwGOwQkVnatGkj3fmU3bMDZAU7I0aMQEZGhnRxcvXq1dGvXz+89dZb+PDDD9GwYUP8888/+PHHHxEaGoqOHTuabH/MmDEYOnQoGjVqhObNm2Pnzp24ePEiKleuXKRyVqpUCceOHcMbb7wBlUoFPz+/XNPJZDL07dsXa9euxa+//mp0AXaZMmVQtmxZrFu3DgEBAbh9+zamTp2a736bNGkCDw8PvP/++xgzZgxOnz4tXTidbebMmejcuTOCgoLQq1cvyOVyXLx4EZcuXcK8efOKdJxElDdes0NEZmnTpg3S09NRtWpV+Pv7S8vDw8Px5MkTVKlSBUFBQdLyjRs34q233sLEiRNRo0YNdO3aFT///LNRmpz69euHadOmYdKkSXjxxReRkJCAgQMHFnl4Z86cObh58yaqVKmCF154Id+0/fr1w9WrV1G+fHm0aNFCWi6Xy7Fjxw7Ex8ejbt26GD9+PJYuXZrvtnx9fbF161bs3bsXoaGh2L59O6Kjo43SREVF4d///jdiYmLw0ksvoWnTpli+fDmCg4OLdIxElD+ZyG0QnIjIDkVERECtVuPLL7+0dVGIyIFwGIuI7FJaWhrWrl2LqKgoKBQKbN++HYcOHUJMTIyti0ZEDoY9O0Rkl9LT09GlSxecO3cOmZmZqFGjBj744INc71wiIsoPgx0iIiJyarxAmYiIiJwagx0iIiJyagx2iIiIyKkx2CEiIiKnxmCHiIiInBqDHSIiInJqDHaIiIjIqTHYISIiIqfGYIeIiIic2v8DqmYK81UN0CMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=512, out_features=200, TIME=6, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.03125, 0.03125])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=6, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=6, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.03125, 0.03125])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=6, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=6, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.03125, 0.03125])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 144,400\n",
      "========================================================\n",
      "\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "ÏûëÏùÄÍ±∏ÌÅ¨Í≤å\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 40.0\n",
      "lif layer 1 self.abs_max_v: 40.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 102.0\n",
      "lif layer 1 self.abs_max_v: 106.5\n",
      "fc layer 2 self.abs_max_out: 56.0\n",
      "lif layer 2 self.abs_max_v: 56.0\n",
      "fc layer 1 self.abs_max_out: 108.0\n",
      "lif layer 1 self.abs_max_v: 151.5\n",
      "lif layer 2 self.abs_max_v: 68.5\n",
      "lif layer 2 self.abs_max_v: 76.5\n",
      "fc layer 3 self.abs_max_out: 4.0\n",
      "layer   1  Sparsity: 74.1211%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 87.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 99.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 255.0\n",
      "fc layer 2 self.abs_max_out: 64.0\n",
      "lif layer 2 self.abs_max_v: 78.0\n",
      "fc layer 2 self.abs_max_out: 66.0\n",
      "lif layer 1 self.abs_max_v: 306.5\n",
      "lif layer 2 self.abs_max_v: 84.0\n",
      "fc layer 1 self.abs_max_out: 455.0\n",
      "lif layer 1 self.abs_max_v: 456.5\n",
      "fc layer 2 self.abs_max_out: 109.0\n",
      "lif layer 2 self.abs_max_v: 109.0\n",
      "fc layer 3 self.abs_max_out: 8.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 131.0\n",
      "fc layer 3 self.abs_max_out: 12.0\n",
      "lif layer 2 self.abs_max_v: 147.0\n",
      "fc layer 2 self.abs_max_out: 139.0\n",
      "fc layer 2 self.abs_max_out: 143.0\n",
      "fc layer 2 self.abs_max_out: 145.0\n",
      "fc layer 2 self.abs_max_out: 146.0\n",
      "fc layer 2 self.abs_max_out: 167.0\n",
      "lif layer 2 self.abs_max_v: 167.0\n",
      "lif layer 1 self.abs_max_v: 488.5\n",
      "lif layer 1 self.abs_max_v: 535.5\n",
      "fc layer 2 self.abs_max_out: 176.0\n",
      "lif layer 2 self.abs_max_v: 180.5\n",
      "fc layer 1 self.abs_max_out: 568.0\n",
      "lif layer 1 self.abs_max_v: 568.0\n",
      "fc layer 2 self.abs_max_out: 179.0\n",
      "fc layer 2 self.abs_max_out: 182.0\n",
      "lif layer 2 self.abs_max_v: 182.0\n",
      "fc layer 2 self.abs_max_out: 188.0\n",
      "lif layer 2 self.abs_max_v: 188.0\n",
      "fc layer 1 self.abs_max_out: 606.0\n",
      "lif layer 1 self.abs_max_v: 606.0\n",
      "fc layer 3 self.abs_max_out: 13.0\n",
      "fc layer 3 self.abs_max_out: 15.0\n",
      "lif layer 2 self.abs_max_v: 208.0\n",
      "fc layer 1 self.abs_max_out: 610.0\n",
      "lif layer 1 self.abs_max_v: 610.0\n",
      "fc layer 2 self.abs_max_out: 189.0\n",
      "fc layer 1 self.abs_max_out: 661.0\n",
      "lif layer 1 self.abs_max_v: 676.0\n",
      "fc layer 2 self.abs_max_out: 191.0\n",
      "lif layer 2 self.abs_max_v: 213.5\n",
      "fc layer 2 self.abs_max_out: 193.0\n",
      "fc layer 2 self.abs_max_out: 198.0\n",
      "fc layer 2 self.abs_max_out: 212.0\n",
      "fc layer 2 self.abs_max_out: 214.0\n",
      "lif layer 2 self.abs_max_v: 214.0\n",
      "fc layer 2 self.abs_max_out: 215.0\n",
      "lif layer 2 self.abs_max_v: 215.0\n",
      "fc layer 2 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 216.0\n",
      "fc layer 2 self.abs_max_out: 219.0\n",
      "lif layer 2 self.abs_max_v: 219.0\n",
      "fc layer 2 self.abs_max_out: 265.0\n",
      "lif layer 2 self.abs_max_v: 264.5\n",
      "fc layer 1 self.abs_max_out: 662.0\n",
      "fc layer 1 self.abs_max_out: 682.0\n",
      "lif layer 1 self.abs_max_v: 682.0\n",
      "lif layer 2 self.abs_max_v: 292.5\n",
      "fc layer 2 self.abs_max_out: 266.0\n",
      "lif layer 2 self.abs_max_v: 293.5\n",
      "fc layer 2 self.abs_max_out: 268.0\n",
      "fc layer 2 self.abs_max_out: 271.0\n",
      "fc layer 2 self.abs_max_out: 272.0\n",
      "lif layer 2 self.abs_max_v: 294.0\n",
      "fc layer 1 self.abs_max_out: 704.0\n",
      "lif layer 1 self.abs_max_v: 704.0\n",
      "fc layer 1 self.abs_max_out: 725.0\n",
      "lif layer 1 self.abs_max_v: 725.0\n",
      "fc layer 1 self.abs_max_out: 755.0\n",
      "lif layer 1 self.abs_max_v: 755.0\n",
      "fc layer 1 self.abs_max_out: 761.0\n",
      "lif layer 1 self.abs_max_v: 774.0\n",
      "fc layer 2 self.abs_max_out: 275.0\n",
      "fc layer 2 self.abs_max_out: 277.0\n",
      "fc layer 1 self.abs_max_out: 802.0\n",
      "lif layer 1 self.abs_max_v: 802.0\n",
      "fc layer 2 self.abs_max_out: 282.0\n",
      "fc layer 2 self.abs_max_out: 306.0\n",
      "lif layer 2 self.abs_max_v: 306.0\n",
      "fc layer 1 self.abs_max_out: 857.0\n",
      "lif layer 1 self.abs_max_v: 857.0\n",
      "fc layer 2 self.abs_max_out: 314.0\n",
      "lif layer 2 self.abs_max_v: 314.0\n",
      "fc layer 2 self.abs_max_out: 315.0\n",
      "lif layer 2 self.abs_max_v: 315.0\n",
      "lif layer 2 self.abs_max_v: 316.0\n",
      "fc layer 2 self.abs_max_out: 326.0\n",
      "lif layer 2 self.abs_max_v: 326.0\n",
      "fc layer 1 self.abs_max_out: 862.0\n",
      "lif layer 1 self.abs_max_v: 862.0\n",
      "fc layer 1 self.abs_max_out: 898.0\n",
      "lif layer 1 self.abs_max_v: 898.0\n",
      "fc layer 1 self.abs_max_out: 927.0\n",
      "lif layer 1 self.abs_max_v: 927.0\n",
      "fc layer 2 self.abs_max_out: 330.0\n",
      "lif layer 2 self.abs_max_v: 330.0\n",
      "lif layer 2 self.abs_max_v: 334.0\n",
      "fc layer 2 self.abs_max_out: 334.0\n",
      "fc layer 2 self.abs_max_out: 354.0\n",
      "lif layer 2 self.abs_max_v: 354.0\n",
      "fc layer 2 self.abs_max_out: 356.0\n",
      "lif layer 2 self.abs_max_v: 356.0\n",
      "fc layer 2 self.abs_max_out: 358.0\n",
      "lif layer 2 self.abs_max_v: 358.0\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 359.0\n",
      "fc layer 2 self.abs_max_out: 378.0\n",
      "lif layer 2 self.abs_max_v: 378.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "lif layer 1 self.abs_max_v: 932.0\n",
      "fc layer 1 self.abs_max_out: 936.0\n",
      "lif layer 1 self.abs_max_v: 936.0\n",
      "fc layer 1 self.abs_max_out: 937.0\n",
      "lif layer 1 self.abs_max_v: 937.0\n",
      "fc layer 1 self.abs_max_out: 943.0\n",
      "lif layer 1 self.abs_max_v: 943.0\n",
      "fc layer 1 self.abs_max_out: 959.0\n",
      "lif layer 1 self.abs_max_v: 959.0\n",
      "fc layer 1 self.abs_max_out: 1003.0\n",
      "lif layer 1 self.abs_max_v: 1003.0\n",
      "fc layer 1 self.abs_max_out: 1043.0\n",
      "lif layer 1 self.abs_max_v: 1043.0\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "fc layer 1 self.abs_max_out: 1050.0\n",
      "lif layer 1 self.abs_max_v: 1050.0\n",
      "fc layer 1 self.abs_max_out: 1057.0\n",
      "lif layer 1 self.abs_max_v: 1057.0\n",
      "fc layer 3 self.abs_max_out: 19.0\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 1 self.abs_max_out: 1081.0\n",
      "lif layer 1 self.abs_max_v: 1081.0\n",
      "fc layer 2 self.abs_max_out: 382.0\n",
      "lif layer 2 self.abs_max_v: 382.0\n",
      "fc layer 3 self.abs_max_out: 24.0\n",
      "fc layer 1 self.abs_max_out: 1099.0\n",
      "lif layer 1 self.abs_max_v: 1099.0\n",
      "fc layer 2 self.abs_max_out: 384.0\n",
      "lif layer 2 self.abs_max_v: 384.0\n",
      "lif layer 2 self.abs_max_v: 400.0\n",
      "fc layer 2 self.abs_max_out: 387.0\n",
      "fc layer 2 self.abs_max_out: 390.0\n",
      "lif layer 2 self.abs_max_v: 415.0\n",
      "fc layer 2 self.abs_max_out: 392.0\n",
      "lif layer 2 self.abs_max_v: 418.0\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "fc layer 1 self.abs_max_out: 1173.0\n",
      "lif layer 1 self.abs_max_v: 1173.0\n",
      "lif layer 1 self.abs_max_v: 1232.5\n",
      "lif layer 1 self.abs_max_v: 1259.5\n",
      "fc layer 2 self.abs_max_out: 400.0\n",
      "fc layer 1 self.abs_max_out: 1224.0\n",
      "fc layer 2 self.abs_max_out: 401.0\n",
      "fc layer 3 self.abs_max_out: 26.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 2 self.abs_max_out: 407.0\n",
      "fc layer 2 self.abs_max_out: 410.0\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "fc layer 2 self.abs_max_out: 419.0\n",
      "lif layer 2 self.abs_max_v: 419.0\n",
      "fc layer 2 self.abs_max_out: 420.0\n",
      "lif layer 2 self.abs_max_v: 420.0\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "lif layer 2 self.abs_max_v: 421.0\n",
      "fc layer 2 self.abs_max_out: 423.0\n",
      "lif layer 2 self.abs_max_v: 423.0\n",
      "fc layer 1 self.abs_max_out: 1226.0\n",
      "lif layer 1 self.abs_max_v: 1299.5\n",
      "lif layer 1 self.abs_max_v: 1400.0\n",
      "lif layer 1 self.abs_max_v: 1492.0\n",
      "fc layer 1 self.abs_max_out: 1320.0\n",
      "fc layer 2 self.abs_max_out: 425.0\n",
      "lif layer 2 self.abs_max_v: 425.0\n",
      "lif layer 2 self.abs_max_v: 435.0\n",
      "fc layer 2 self.abs_max_out: 427.0\n",
      "fc layer 2 self.abs_max_out: 428.0\n",
      "lif layer 1 self.abs_max_v: 1559.5\n",
      "fc layer 2 self.abs_max_out: 434.0\n",
      "lif layer 2 self.abs_max_v: 448.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "fc layer 2 self.abs_max_out: 451.0\n",
      "lif layer 2 self.abs_max_v: 451.0\n",
      "fc layer 2 self.abs_max_out: 453.0\n",
      "lif layer 2 self.abs_max_v: 453.0\n",
      "lif layer 1 self.abs_max_v: 1563.5\n",
      "fc layer 2 self.abs_max_out: 463.0\n",
      "lif layer 2 self.abs_max_v: 463.0\n",
      "fc layer 2 self.abs_max_out: 465.0\n",
      "lif layer 2 self.abs_max_v: 465.0\n",
      "lif layer 2 self.abs_max_v: 475.0\n",
      "fc layer 2 self.abs_max_out: 470.0\n",
      "fc layer 2 self.abs_max_out: 480.0\n",
      "lif layer 2 self.abs_max_v: 480.0\n",
      "fc layer 2 self.abs_max_out: 487.0\n",
      "lif layer 2 self.abs_max_v: 487.0\n",
      "fc layer 2 self.abs_max_out: 488.0\n",
      "lif layer 2 self.abs_max_v: 488.0\n",
      "fc layer 2 self.abs_max_out: 489.0\n",
      "lif layer 2 self.abs_max_v: 489.0\n",
      "fc layer 2 self.abs_max_out: 497.0\n",
      "lif layer 2 self.abs_max_v: 497.0\n",
      "fc layer 1 self.abs_max_out: 1338.0\n",
      "fc layer 2 self.abs_max_out: 503.0\n",
      "lif layer 2 self.abs_max_v: 503.0\n",
      "lif layer 1 self.abs_max_v: 1691.5\n",
      "lif layer 1 self.abs_max_v: 1820.0\n",
      "fc layer 1 self.abs_max_out: 1345.0\n",
      "lif layer 1 self.abs_max_v: 1853.0\n",
      "lif layer 1 self.abs_max_v: 1909.5\n",
      "lif layer 2 self.abs_max_v: 511.0\n",
      "lif layer 2 self.abs_max_v: 514.5\n",
      "fc layer 1 self.abs_max_out: 1368.0\n",
      "train - Value 0: 1938 occurrences\n",
      "train - Value 1: 2094 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 84.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 85.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 93.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 99.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 101.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 104.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 106.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 109.00 at epoch 0, iter 4031\n",
      "max_activation_accul updated: 115.00 at epoch 0, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss:  8.693631/ 11.877891, val:  50.00%, val_best:  50.00%, tr:  89.93%, tr_best:  89.93%, epoch time: 191.03 seconds, 3.18 minutes\n",
      "layer   1  Sparsity: 81.4672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.1616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 24192 real_backward_count 5486  22.677%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "layer   1  Sparsity: 75.7161%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 531.0\n",
      "lif layer 2 self.abs_max_v: 531.0\n",
      "fc layer 2 self.abs_max_out: 533.0\n",
      "lif layer 2 self.abs_max_v: 533.0\n",
      "fc layer 2 self.abs_max_out: 558.0\n",
      "lif layer 2 self.abs_max_v: 558.0\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "lif layer 2 self.abs_max_v: 571.0\n",
      "fc layer 2 self.abs_max_out: 577.0\n",
      "lif layer 2 self.abs_max_v: 577.0\n",
      "fc layer 2 self.abs_max_out: 584.0\n",
      "lif layer 2 self.abs_max_v: 584.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "lif layer 2 self.abs_max_v: 588.0\n",
      "fc layer 2 self.abs_max_out: 607.0\n",
      "lif layer 2 self.abs_max_v: 607.0\n",
      "fc layer 2 self.abs_max_out: 616.0\n",
      "lif layer 2 self.abs_max_v: 616.0\n",
      "fc layer 1 self.abs_max_out: 1375.0\n",
      "fc layer 2 self.abs_max_out: 623.0\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "fc layer 1 self.abs_max_out: 1410.0\n",
      "fc layer 1 self.abs_max_out: 1467.0\n",
      "fc layer 1 self.abs_max_out: 1469.0\n",
      "lif layer 1 self.abs_max_v: 1998.5\n",
      "fc layer 2 self.abs_max_out: 641.0\n",
      "lif layer 2 self.abs_max_v: 641.0\n",
      "fc layer 1 self.abs_max_out: 1472.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 1 self.abs_max_out: 1482.0\n",
      "fc layer 1 self.abs_max_out: 1515.0\n",
      "fc layer 1 self.abs_max_out: 1539.0\n",
      "fc layer 2 self.abs_max_out: 658.0\n",
      "lif layer 2 self.abs_max_v: 658.0\n",
      "fc layer 2 self.abs_max_out: 667.0\n",
      "lif layer 2 self.abs_max_v: 667.0\n",
      "fc layer 2 self.abs_max_out: 675.0\n",
      "lif layer 2 self.abs_max_v: 675.0\n",
      "fc layer 2 self.abs_max_out: 678.0\n",
      "lif layer 2 self.abs_max_v: 678.0\n",
      "fc layer 1 self.abs_max_out: 1555.0\n",
      "fc layer 1 self.abs_max_out: 1556.0\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "lif layer 2 self.abs_max_v: 698.0\n",
      "fc layer 1 self.abs_max_out: 1561.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 714.0\n",
      "fc layer 1 self.abs_max_out: 1601.0\n",
      "fc layer 1 self.abs_max_out: 1637.0\n",
      "train - Value 0: 1934 occurrences\n",
      "train - Value 1: 2098 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 122.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 124.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 126.00 at epoch 1, iter 4031\n",
      "max_activation_accul updated: 128.00 at epoch 1, iter 4031\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "max_activation_accul updated: 129.00 at epoch 1, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.825543/ 14.491260, val:  50.00%, val_best:  50.00%, tr:  90.58%, tr_best:  90.58%, epoch time: 190.55 seconds, 3.18 minutes\n",
      "layer   1  Sparsity: 81.4669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1803%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48384 real_backward_count 10477  21.654%\n",
      "layer   1  Sparsity: 84.7982%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "lif layer 2 self.abs_max_v: 717.5\n",
      "lif layer 2 self.abs_max_v: 726.0\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 1 self.abs_max_out: 1645.0\n",
      "lif layer 2 self.abs_max_v: 737.0\n",
      "lif layer 2 self.abs_max_v: 775.5\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "fc layer 1 self.abs_max_out: 1662.0\n",
      "lif layer 1 self.abs_max_v: 2134.5\n",
      "lif layer 1 self.abs_max_v: 2159.5\n",
      "fc layer 3 self.abs_max_out: 38.0\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 1 self.abs_max_out: 1707.0\n",
      "fc layer 2 self.abs_max_out: 721.0\n",
      "fc layer 2 self.abs_max_out: 771.0\n",
      "fc layer 2 self.abs_max_out: 772.0\n",
      "fc layer 2 self.abs_max_out: 774.0\n",
      "fc layer 2 self.abs_max_out: 782.0\n",
      "lif layer 2 self.abs_max_v: 782.0\n",
      "train - Value 0: 1956 occurrences\n",
      "train - Value 1: 2076 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 156.00 at epoch 2, iter 4031\n",
      "fc layer 2 self.abs_max_out: 790.0\n",
      "lif layer 2 self.abs_max_v: 790.0\n",
      "fc layer 2 self.abs_max_out: 793.0\n",
      "lif layer 2 self.abs_max_v: 793.0\n",
      "fc layer 2 self.abs_max_out: 808.0\n",
      "lif layer 2 self.abs_max_v: 808.0\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 12.721142/ 15.311130, val:  50.22%, val_best:  50.22%, tr:  91.57%, tr_best:  91.57%, epoch time: 188.60 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.3190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.2045%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 72576 real_backward_count 15273  21.044%\n",
      "layer   1  Sparsity: 74.7721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 810.0\n",
      "lif layer 2 self.abs_max_v: 810.0\n",
      "fc layer 1 self.abs_max_out: 1716.0\n",
      "fc layer 2 self.abs_max_out: 812.0\n",
      "lif layer 2 self.abs_max_v: 812.0\n",
      "fc layer 2 self.abs_max_out: 817.0\n",
      "lif layer 2 self.abs_max_v: 817.0\n",
      "fc layer 1 self.abs_max_out: 1758.0\n",
      "fc layer 2 self.abs_max_out: 821.0\n",
      "lif layer 2 self.abs_max_v: 821.0\n",
      "lif layer 1 self.abs_max_v: 2193.0\n",
      "lif layer 1 self.abs_max_v: 2271.5\n",
      "fc layer 2 self.abs_max_out: 829.0\n",
      "lif layer 2 self.abs_max_v: 829.0\n",
      "fc layer 1 self.abs_max_out: 1760.0\n",
      "lif layer 2 self.abs_max_v: 841.5\n",
      "lif layer 2 self.abs_max_v: 887.0\n",
      "lif layer 2 self.abs_max_v: 895.5\n",
      "fc layer 1 self.abs_max_out: 1796.0\n",
      "lif layer 2 self.abs_max_v: 897.5\n",
      "fc layer 1 self.abs_max_out: 1799.0\n",
      "fc layer 1 self.abs_max_out: 1820.0\n",
      "fc layer 1 self.abs_max_out: 1836.0\n",
      "fc layer 2 self.abs_max_out: 835.0\n",
      "fc layer 1 self.abs_max_out: 1867.0\n",
      "fc layer 1 self.abs_max_out: 1871.0\n",
      "train - Value 0: 1930 occurrences\n",
      "train - Value 1: 2102 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "lif layer 2 self.abs_max_v: 907.0\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 13.312750/ 23.694731, val:  50.00%, val_best:  50.22%, tr:  92.21%, tr_best:  92.21%, epoch time: 189.13 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3154%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 96768 real_backward_count 19784  20.445%\n",
      "layer   1  Sparsity: 76.6602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 955.0\n",
      "lif layer 2 self.abs_max_v: 973.5\n",
      "fc layer 1 self.abs_max_out: 1875.0\n",
      "lif layer 2 self.abs_max_v: 981.0\n",
      "fc layer 1 self.abs_max_out: 1899.0\n",
      "lif layer 1 self.abs_max_v: 2290.5\n",
      "lif layer 1 self.abs_max_v: 2449.5\n",
      "fc layer 1 self.abs_max_out: 1903.0\n",
      "fc layer 1 self.abs_max_out: 1910.0\n",
      "lif layer 1 self.abs_max_v: 2471.0\n",
      "fc layer 1 self.abs_max_out: 1922.0\n",
      "fc layer 1 self.abs_max_out: 1976.0\n",
      "fc layer 1 self.abs_max_out: 1994.0\n",
      "lif layer 1 self.abs_max_v: 2657.0\n",
      "fc layer 1 self.abs_max_out: 2026.0\n",
      "fc layer 1 self.abs_max_out: 2031.0\n",
      "train - Value 0: 1919 occurrences\n",
      "train - Value 1: 2113 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 165.00 at epoch 4, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 11.779834/ 14.287580, val:  50.00%, val_best:  50.22%, tr:  92.98%, tr_best:  92.98%, epoch time: 189.19 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.0075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 120960 real_backward_count 24366  20.144%\n",
      "layer   1  Sparsity: 83.4310%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 843.0\n",
      "fc layer 2 self.abs_max_out: 858.0\n",
      "fc layer 2 self.abs_max_out: 877.0\n",
      "fc layer 2 self.abs_max_out: 886.0\n",
      "fc layer 2 self.abs_max_out: 912.0\n",
      "fc layer 2 self.abs_max_out: 924.0\n",
      "fc layer 1 self.abs_max_out: 2042.0\n",
      "fc layer 2 self.abs_max_out: 932.0\n",
      "lif layer 1 self.abs_max_v: 2684.5\n",
      "fc layer 1 self.abs_max_out: 2068.0\n",
      "fc layer 2 self.abs_max_out: 935.0\n",
      "lif layer 1 self.abs_max_v: 2704.0\n",
      "fc layer 2 self.abs_max_out: 963.0\n",
      "lif layer 1 self.abs_max_v: 2807.0\n",
      "fc layer 1 self.abs_max_out: 2110.0\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "train - Value 0: 1937 occurrences\n",
      "train - Value 1: 2095 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 13.349651/ 22.349001, val:  50.00%, val_best:  50.22%, tr:  91.79%, tr_best:  92.98%, epoch time: 190.46 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 145152 real_backward_count 29052  20.015%\n",
      "layer   1  Sparsity: 84.4401%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 2870.0\n",
      "fc layer 1 self.abs_max_out: 2124.0\n",
      "lif layer 1 self.abs_max_v: 2968.5\n",
      "fc layer 1 self.abs_max_out: 2218.0\n",
      "train - Value 0: 1952 occurrences\n",
      "train - Value 1: 2080 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 12.103290/ 10.193588, val:  50.00%, val_best:  50.22%, tr:  91.87%, tr_best:  92.98%, epoch time: 189.33 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 169344 real_backward_count 33751  19.930%\n",
      "layer   1  Sparsity: 85.6120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2221.0\n",
      "fc layer 1 self.abs_max_out: 2238.0\n",
      "lif layer 2 self.abs_max_v: 1003.5\n",
      "lif layer 2 self.abs_max_v: 1039.5\n",
      "lif layer 2 self.abs_max_v: 1102.0\n",
      "lif layer 2 self.abs_max_v: 1138.0\n",
      "lif layer 1 self.abs_max_v: 3022.5\n",
      "lif layer 2 self.abs_max_v: 1183.5\n",
      "train - Value 0: 1964 occurrences\n",
      "train - Value 1: 2068 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 446 occurrences\n",
      "test - Value 1: 6 occurrences\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 13.170190/  8.692383, val:  50.88%, val_best:  50.88%, tr:  90.97%, tr_best:  92.98%, epoch time: 188.03 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.2920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 193536 real_backward_count 38547  19.917%\n",
      "layer   1  Sparsity: 81.7383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 977.0\n",
      "fc layer 2 self.abs_max_out: 1013.0\n",
      "fc layer 2 self.abs_max_out: 1057.0\n",
      "fc layer 2 self.abs_max_out: 1080.0\n",
      "lif layer 1 self.abs_max_v: 3062.5\n",
      "fc layer 2 self.abs_max_out: 1085.0\n",
      "fc layer 2 self.abs_max_out: 1100.0\n",
      "fc layer 1 self.abs_max_out: 2264.0\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "fc layer 2 self.abs_max_out: 1116.0\n",
      "fc layer 1 self.abs_max_out: 2324.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 12.642490/  7.404207, val:  50.00%, val_best:  50.88%, tr:  92.09%, tr_best:  92.98%, epoch time: 189.59 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7256%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 217728 real_backward_count 42977  19.739%\n",
      "layer   1  Sparsity: 82.6823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2358.0\n",
      "fc layer 1 self.abs_max_out: 2366.0\n",
      "train - Value 0: 1964 occurrences\n",
      "train - Value 1: 2068 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "fc layer 1 self.abs_max_out: 2408.0\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 12.336708/ 18.273434, val:  50.22%, val_best:  50.88%, tr:  93.35%, tr_best:  93.35%, epoch time: 189.05 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.6362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 241920 real_backward_count 47534  19.649%\n",
      "layer   1  Sparsity: 79.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.8333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2444.0\n",
      "fc layer 1 self.abs_max_out: 2490.0\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "lif layer 1 self.abs_max_v: 3063.0\n",
      "lif layer 1 self.abs_max_v: 3230.5\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 439 occurrences\n",
      "test - Value 1: 13 occurrences\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 11.309951/  4.445770, val:  52.43%, val_best:  52.43%, tr:  93.11%, tr_best:  93.35%, epoch time: 189.97 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2444%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 266112 real_backward_count 52015  19.546%\n",
      "layer   1  Sparsity: 86.7188%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1144.0\n",
      "fc layer 1 self.abs_max_out: 2498.0\n",
      "fc layer 2 self.abs_max_out: 1162.0\n",
      "fc layer 1 self.abs_max_out: 2538.0\n",
      "fc layer 1 self.abs_max_out: 2547.0\n",
      "lif layer 1 self.abs_max_v: 3427.5\n",
      "fc layer 1 self.abs_max_out: 2588.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 12.406547/ 22.567007, val:  50.00%, val_best:  52.43%, tr:  93.77%, tr_best:  93.77%, epoch time: 189.53 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 290304 real_backward_count 56532  19.473%\n",
      "layer   1  Sparsity: 83.8542%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2589.0\n",
      "fc layer 1 self.abs_max_out: 2602.0\n",
      "fc layer 1 self.abs_max_out: 2605.0\n",
      "lif layer 1 self.abs_max_v: 3492.5\n",
      "fc layer 1 self.abs_max_out: 2637.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 12.581203/ 11.001747, val:  51.11%, val_best:  52.43%, tr:  93.45%, tr_best:  93.77%, epoch time: 190.45 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 314496 real_backward_count 61101  19.428%\n",
      "layer   1  Sparsity: 88.2487%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.8333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2673.0\n",
      "lif layer 1 self.abs_max_v: 3502.5\n",
      "train - Value 0: 1921 occurrences\n",
      "train - Value 1: 2111 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 13.272777/ 13.592961, val:  50.00%, val_best:  52.43%, tr:  92.93%, tr_best:  93.77%, epoch time: 188.63 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.8250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 338688 real_backward_count 65592  19.366%\n",
      "layer   1  Sparsity: 90.3971%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 3577.5\n",
      "lif layer 1 self.abs_max_v: 3592.0\n",
      "fc layer 1 self.abs_max_out: 2684.0\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 276 occurrences\n",
      "test - Value 1: 176 occurrences\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 12.023979/  7.843003, val:  68.58%, val_best:  68.58%, tr:  93.08%, tr_best:  93.77%, epoch time: 188.92 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.9720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.2857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362880 real_backward_count 70092  19.315%\n",
      "layer   1  Sparsity: 93.8151%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2758.0\n",
      "train - Value 0: 1989 occurrences\n",
      "train - Value 1: 2043 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 14.245744/  9.920036, val:  50.00%, val_best:  68.58%, tr:  94.47%, tr_best:  94.47%, epoch time: 188.76 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1903%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 387072 real_backward_count 74465  19.238%\n",
      "layer   1  Sparsity: 84.9935%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 3653.5\n",
      "lif layer 2 self.abs_max_v: 1250.0\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "lif layer 1 self.abs_max_v: 3716.5\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 402 occurrences\n",
      "test - Value 1: 50 occurrences\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 13.527411/  6.776297, val:  60.62%, val_best:  68.58%, tr:  93.63%, tr_best:  94.47%, epoch time: 188.63 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.7817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411264 real_backward_count 79128  19.240%\n",
      "layer   1  Sparsity: 75.6510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 46.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 3823.5\n",
      "lif layer 1 self.abs_max_v: 3934.5\n",
      "lif layer 1 self.abs_max_v: 3987.5\n",
      "lif layer 1 self.abs_max_v: 4037.0\n",
      "fc layer 2 self.abs_max_out: 1171.0\n",
      "fc layer 2 self.abs_max_out: 1220.0\n",
      "fc layer 2 self.abs_max_out: 1235.0\n",
      "fc layer 2 self.abs_max_out: 1278.0\n",
      "lif layer 2 self.abs_max_v: 1278.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 12.963839/ 10.966323, val:  50.00%, val_best:  68.58%, tr:  93.70%, tr_best:  94.47%, epoch time: 189.35 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0188%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 435456 real_backward_count 83764  19.236%\n",
      "layer   1  Sparsity: 75.2930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1307.0\n",
      "lif layer 2 self.abs_max_v: 1307.0\n",
      "fc layer 2 self.abs_max_out: 1364.0\n",
      "lif layer 2 self.abs_max_v: 1364.0\n",
      "fc layer 3 self.abs_max_out: 50.0\n",
      "train - Value 0: 2013 occurrences\n",
      "train - Value 1: 2019 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 11.768681/  9.817477, val:  50.66%, val_best:  68.58%, tr:  92.98%, tr_best:  94.47%, epoch time: 189.51 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.6987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 459648 real_backward_count 88492  19.252%\n",
      "layer   1  Sparsity: 88.3789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4111.0\n",
      "fc layer 1 self.abs_max_out: 2777.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss: 10.517426/ 15.641665, val:  50.00%, val_best:  68.58%, tr:  93.50%, tr_best:  94.47%, epoch time: 187.95 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.3160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 483840 real_backward_count 93108  19.244%\n",
      "layer   1  Sparsity: 81.2826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 58.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2788.0\n",
      "lif layer 1 self.abs_max_v: 4181.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 326 occurrences\n",
      "test - Value 1: 126 occurrences\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss: 10.632514/  3.424207, val:  68.58%, val_best:  68.58%, tr:  95.21%, tr_best:  95.21%, epoch time: 189.05 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.6513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.5996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 508032 real_backward_count 97546  19.201%\n",
      "layer   1  Sparsity: 85.8724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2843.0\n",
      "fc layer 1 self.abs_max_out: 2871.0\n",
      "fc layer 1 self.abs_max_out: 2873.0\n",
      "fc layer 1 self.abs_max_out: 2874.0\n",
      "fc layer 1 self.abs_max_out: 2886.0\n",
      "fc layer 1 self.abs_max_out: 2908.0\n",
      "fc layer 1 self.abs_max_out: 2946.0\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 443 occurrences\n",
      "test - Value 1: 9 occurrences\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 12.476889/  6.040219, val:  51.55%, val_best:  68.58%, tr:  94.67%, tr_best:  95.21%, epoch time: 189.99 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.2808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 532224 real_backward_count 102015  19.168%\n",
      "layer   1  Sparsity: 77.2461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 2966.0\n",
      "fc layer 1 self.abs_max_out: 2997.0\n",
      "fc layer 1 self.abs_max_out: 3073.0\n",
      "fc layer 1 self.abs_max_out: 3083.0\n",
      "fc layer 1 self.abs_max_out: 3101.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss: 12.103344/ 25.721634, val:  50.00%, val_best:  68.58%, tr:  94.05%, tr_best:  95.21%, epoch time: 190.86 seconds, 3.18 minutes\n",
      "layer   1  Sparsity: 81.4665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.4953%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 556416 real_backward_count 106543  19.148%\n",
      "layer   1  Sparsity: 76.0091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3105.0\n",
      "fc layer 1 self.abs_max_out: 3134.0\n",
      "fc layer 1 self.abs_max_out: 3178.0\n",
      "lif layer 1 self.abs_max_v: 4331.0\n",
      "fc layer 1 self.abs_max_out: 3179.0\n",
      "fc layer 1 self.abs_max_out: 3193.0\n",
      "fc layer 1 self.abs_max_out: 3198.0\n",
      "fc layer 1 self.abs_max_out: 3205.0\n",
      "fc layer 1 self.abs_max_out: 3212.0\n",
      "fc layer 1 self.abs_max_out: 3228.0\n",
      "fc layer 1 self.abs_max_out: 3239.0\n",
      "fc layer 1 self.abs_max_out: 3242.0\n",
      "train - Value 0: 2000 occurrences\n",
      "train - Value 1: 2032 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss: 11.188235/  1.752338, val:  50.00%, val_best:  68.58%, tr:  94.59%, tr_best:  95.21%, epoch time: 189.92 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8718%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.6315%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 580608 real_backward_count 111193  19.151%\n",
      "layer   1  Sparsity: 75.5208%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.8333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4387.5\n",
      "fc layer 1 self.abs_max_out: 3300.0\n",
      "lif layer 2 self.abs_max_v: 1397.5\n",
      "fc layer 1 self.abs_max_out: 3304.0\n",
      "fc layer 1 self.abs_max_out: 3310.0\n",
      "train - Value 0: 1975 occurrences\n",
      "train - Value 1: 2057 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 369 occurrences\n",
      "test - Value 1: 83 occurrences\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss: 10.208690/  8.725185, val:  64.38%, val_best:  68.58%, tr:  93.18%, tr_best:  95.21%, epoch time: 189.81 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.9939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 604800 real_backward_count 115702  19.131%\n",
      "fc layer 1 self.abs_max_out: 3353.0\n",
      "layer   1  Sparsity: 72.7865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3360.0\n",
      "fc layer 1 self.abs_max_out: 3364.0\n",
      "lif layer 1 self.abs_max_v: 4508.5\n",
      "fc layer 1 self.abs_max_out: 3382.0\n",
      "fc layer 1 self.abs_max_out: 3430.0\n",
      "train - Value 0: 2034 occurrences\n",
      "train - Value 1: 1998 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 6 occurrences\n",
      "test - Value 1: 446 occurrences\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss: 11.021993/  8.879026, val:  51.33%, val_best:  68.58%, tr:  94.69%, tr_best:  95.21%, epoch time: 189.57 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 628992 real_backward_count 120102  19.094%\n",
      "layer   1  Sparsity: 77.2135%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3446.0\n",
      "fc layer 1 self.abs_max_out: 3457.0\n",
      "lif layer 1 self.abs_max_v: 4621.0\n",
      "fc layer 1 self.abs_max_out: 3460.0\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss: 11.159974/ 18.449682, val:  50.00%, val_best:  68.58%, tr:  93.95%, tr_best:  95.21%, epoch time: 189.44 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.6820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 653184 real_backward_count 124589  19.074%\n",
      "layer   1  Sparsity: 72.9818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 11.044559/ 13.644814, val:  51.11%, val_best:  68.58%, tr:  92.81%, tr_best:  95.21%, epoch time: 188.32 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.0414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 677376 real_backward_count 128982  19.041%\n",
      "layer   1  Sparsity: 80.7617%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4837.0\n",
      "lif layer 2 self.abs_max_v: 1497.0\n",
      "train - Value 0: 2076 occurrences\n",
      "train - Value 1: 1956 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 410 occurrences\n",
      "test - Value 1: 42 occurrences\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss: 10.077435/  5.108707, val:  53.98%, val_best:  68.58%, tr:  89.78%, tr_best:  95.21%, epoch time: 189.87 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.5153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 701568 real_backward_count 133198  18.986%\n",
      "layer   1  Sparsity: 80.3385%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3467.0\n",
      "fc layer 1 self.abs_max_out: 3489.0\n",
      "train - Value 0: 1990 occurrences\n",
      "train - Value 1: 2042 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 68 occurrences\n",
      "test - Value 1: 384 occurrences\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  9.977698/  5.791970, val:  63.72%, val_best:  68.58%, tr:  93.50%, tr_best:  95.21%, epoch time: 189.24 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7928%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3290%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 725760 real_backward_count 137403  18.932%\n",
      "layer   1  Sparsity: 89.8763%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3490.0\n",
      "train - Value 0: 1992 occurrences\n",
      "train - Value 1: 2040 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 10.396514/  2.740561, val:  50.00%, val_best:  68.58%, tr:  90.67%, tr_best:  95.21%, epoch time: 188.96 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 749952 real_backward_count 141864  18.916%\n",
      "layer   1  Sparsity: 69.0104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3528.0\n",
      "fc layer 2 self.abs_max_out: 1424.0\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "train - Value 0: 1968 occurrences\n",
      "train - Value 1: 2064 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 410 occurrences\n",
      "test - Value 1: 42 occurrences\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss: 11.127031/  9.394864, val:  57.96%, val_best:  68.58%, tr:  93.65%, tr_best:  95.21%, epoch time: 187.63 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 774144 real_backward_count 146224  18.888%\n",
      "layer   1  Sparsity: 74.7070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3529.0\n",
      "fc layer 1 self.abs_max_out: 3563.0\n",
      "fc layer 1 self.abs_max_out: 3575.0\n",
      "train - Value 0: 1978 occurrences\n",
      "train - Value 1: 2054 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 451 occurrences\n",
      "test - Value 1: 1 occurrences\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 11.711882/ 10.127434, val:  50.22%, val_best:  68.58%, tr:  94.35%, tr_best:  95.21%, epoch time: 188.38 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 798336 real_backward_count 150696  18.876%\n",
      "layer   1  Sparsity: 90.4297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3587.0\n",
      "fc layer 1 self.abs_max_out: 3595.0\n",
      "train - Value 0: 2004 occurrences\n",
      "train - Value 1: 2028 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 426 occurrences\n",
      "test - Value 1: 26 occurrences\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 11.203500/  4.779709, val:  53.10%, val_best:  68.58%, tr:  93.45%, tr_best:  95.21%, epoch time: 188.34 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0118%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822528 real_backward_count 154961  18.840%\n",
      "layer   1  Sparsity: 74.7070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3607.0\n",
      "fc layer 1 self.abs_max_out: 3612.0\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 361 occurrences\n",
      "test - Value 1: 91 occurrences\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 12.200467/  6.197419, val:  64.82%, val_best:  68.58%, tr:  93.53%, tr_best:  95.21%, epoch time: 189.65 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.1818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 846720 real_backward_count 159446  18.831%\n",
      "layer   1  Sparsity: 87.9232%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3629.0\n",
      "fc layer 1 self.abs_max_out: 3675.0\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 19 occurrences\n",
      "test - Value 1: 433 occurrences\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss: 11.519239/  7.689518, val:  53.76%, val_best:  68.58%, tr:  93.43%, tr_best:  95.21%, epoch time: 188.55 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 870912 real_backward_count 163878  18.817%\n",
      "layer   1  Sparsity: 93.9779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 80.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3762.0\n",
      "fc layer 1 self.abs_max_out: 3764.0\n",
      "fc layer 1 self.abs_max_out: 3793.0\n",
      "fc layer 1 self.abs_max_out: 3812.0\n",
      "fc layer 1 self.abs_max_out: 3816.0\n",
      "fc layer 1 self.abs_max_out: 3823.0\n",
      "fc layer 1 self.abs_max_out: 3835.0\n",
      "train - Value 0: 2089 occurrences\n",
      "train - Value 1: 1943 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss: 10.405990/ 11.153720, val:  50.00%, val_best:  68.58%, tr:  92.53%, tr_best:  95.21%, epoch time: 189.57 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 895104 real_backward_count 168272  18.799%\n",
      "layer   1  Sparsity: 80.7292%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3843.0\n",
      "fc layer 1 self.abs_max_out: 3847.0\n",
      "fc layer 1 self.abs_max_out: 3857.0\n",
      "fc layer 1 self.abs_max_out: 3869.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss: 13.170280/ 22.969141, val:  50.00%, val_best:  68.58%, tr:  93.70%, tr_best:  95.21%, epoch time: 189.47 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 919296 real_backward_count 172794  18.796%\n",
      "layer   1  Sparsity: 70.4753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 13.352699/ 16.848433, val:  50.00%, val_best:  68.58%, tr:  94.05%, tr_best:  95.21%, epoch time: 189.77 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5649%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 943488 real_backward_count 177241  18.786%\n",
      "layer   1  Sparsity: 82.8451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1464.0\n",
      "fc layer 2 self.abs_max_out: 1566.0\n",
      "lif layer 2 self.abs_max_v: 1566.0\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "lif layer 2 self.abs_max_v: 1589.0\n",
      "train - Value 0: 2038 occurrences\n",
      "train - Value 1: 1994 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 13 occurrences\n",
      "test - Value 1: 439 occurrences\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss: 11.091532/  7.149167, val:  52.88%, val_best:  68.58%, tr:  91.37%, tr_best:  95.21%, epoch time: 188.46 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 967680 real_backward_count 181666  18.773%\n",
      "layer   1  Sparsity: 78.8086%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1967 occurrences\n",
      "train - Value 1: 2065 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 372 occurrences\n",
      "test - Value 1: 80 occurrences\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss: 13.491875/  5.774887, val:  64.16%, val_best:  68.58%, tr:  93.38%, tr_best:  95.21%, epoch time: 188.70 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 991872 real_backward_count 185989  18.751%\n",
      "layer   1  Sparsity: 83.3333%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2051 occurrences\n",
      "train - Value 1: 1981 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 174.00 at epoch 41, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss: 13.552836/ 27.240406, val:  50.00%, val_best:  68.58%, tr:  94.42%, tr_best:  95.21%, epoch time: 189.91 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6826%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1016064 real_backward_count 190498  18.749%\n",
      "layer   1  Sparsity: 69.5312%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 416 occurrences\n",
      "test - Value 1: 36 occurrences\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss: 12.074281/  8.222102, val:  57.96%, val_best:  68.58%, tr:  94.79%, tr_best:  95.21%, epoch time: 190.42 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4031%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1040256 real_backward_count 194965  18.742%\n",
      "layer   1  Sparsity: 79.9805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1967 occurrences\n",
      "train - Value 1: 2065 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 394 occurrences\n",
      "test - Value 1: 58 occurrences\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss: 12.428450/  2.497445, val:  61.95%, val_best:  68.58%, tr:  93.68%, tr_best:  95.21%, epoch time: 188.01 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1064448 real_backward_count 199271  18.721%\n",
      "layer   1  Sparsity: 89.1276%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 2 self.abs_max_v: 1675.0\n",
      "train - Value 0: 2052 occurrences\n",
      "train - Value 1: 1980 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 10.576956/ 20.189220, val:  50.00%, val_best:  68.58%, tr:  93.01%, tr_best:  95.21%, epoch time: 187.65 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.2380%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1088640 real_backward_count 203460  18.689%\n",
      "layer   1  Sparsity: 82.4544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1666.0\n",
      "fc layer 2 self.abs_max_out: 1702.0\n",
      "lif layer 2 self.abs_max_v: 1702.0\n",
      "fc layer 2 self.abs_max_out: 1720.0\n",
      "lif layer 2 self.abs_max_v: 1720.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 417 occurrences\n",
      "test - Value 1: 35 occurrences\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss: 14.063506/ 10.416978, val:  56.86%, val_best:  68.58%, tr:  93.92%, tr_best:  95.21%, epoch time: 188.69 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1112832 real_backward_count 207865  18.679%\n",
      "layer   1  Sparsity: 93.5872%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2061 occurrences\n",
      "train - Value 1: 1971 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 12.518679/  6.611478, val:  50.00%, val_best:  68.58%, tr:  92.73%, tr_best:  95.21%, epoch time: 186.52 seconds, 3.11 minutes\n",
      "layer   1  Sparsity: 81.4629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.8107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1137024 real_backward_count 212209  18.664%\n",
      "layer   1  Sparsity: 74.0234%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 51.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 13.116580/  7.986937, val:  50.00%, val_best:  68.58%, tr:  94.69%, tr_best:  95.21%, epoch time: 189.29 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1161216 real_backward_count 216640  18.656%\n",
      "layer   1  Sparsity: 84.3099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2055 occurrences\n",
      "train - Value 1: 1977 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 11.121651/ 16.205740, val:  50.00%, val_best:  68.58%, tr:  93.43%, tr_best:  95.21%, epoch time: 190.43 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7150%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1185408 real_backward_count 220828  18.629%\n",
      "layer   1  Sparsity: 89.3555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3903.0\n",
      "lif layer 1 self.abs_max_v: 4891.5\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 16 occurrences\n",
      "test - Value 1: 436 occurrences\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 12.137922/ 13.908634, val:  53.54%, val_best:  68.58%, tr:  93.23%, tr_best:  95.21%, epoch time: 190.14 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4668%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1209600 real_backward_count 224963  18.598%\n",
      "layer   1  Sparsity: 81.9336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2155 occurrences\n",
      "train - Value 1: 1877 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 11.839425/ 25.215364, val:  50.44%, val_best:  68.58%, tr:  91.54%, tr_best:  95.21%, epoch time: 189.59 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.6483%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233792 real_backward_count 229041  18.564%\n",
      "layer   1  Sparsity: 75.7812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2093 occurrences\n",
      "train - Value 1: 1939 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 183.00 at epoch 51, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 11.762190/ 29.477406, val:  50.00%, val_best:  68.58%, tr:  92.68%, tr_best:  95.21%, epoch time: 186.02 seconds, 3.10 minutes\n",
      "layer   1  Sparsity: 81.4669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.4628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1257984 real_backward_count 233258  18.542%\n",
      "layer   1  Sparsity: 87.6953%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 43 occurrences\n",
      "test - Value 1: 409 occurrences\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 13.922979/ 12.916448, val:  58.19%, val_best:  68.58%, tr:  94.52%, tr_best:  95.21%, epoch time: 189.44 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.7662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282176 real_backward_count 237597  18.531%\n",
      "layer   1  Sparsity: 91.9922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3929.0\n",
      "train - Value 0: 2059 occurrences\n",
      "train - Value 1: 1973 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 407 occurrences\n",
      "test - Value 1: 45 occurrences\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 13.421863/  4.447556, val:  58.63%, val_best:  68.58%, tr:  94.67%, tr_best:  95.21%, epoch time: 188.63 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1798%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1306368 real_backward_count 241993  18.524%\n",
      "layer   1  Sparsity: 86.3281%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2051 occurrences\n",
      "train - Value 1: 1981 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 11.057303/  4.360019, val:  50.00%, val_best:  68.58%, tr:  93.33%, tr_best:  95.21%, epoch time: 189.43 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.9844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.8819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1330560 real_backward_count 246280  18.509%\n",
      "layer   1  Sparsity: 81.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 11.571117/  1.364357, val:  50.00%, val_best:  68.58%, tr:  93.90%, tr_best:  95.21%, epoch time: 189.57 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1354752 real_backward_count 250624  18.500%\n",
      "layer   1  Sparsity: 81.0872%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2043 occurrences\n",
      "train - Value 1: 1989 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 12.303600/ 17.378571, val:  50.00%, val_best:  68.58%, tr:  91.39%, tr_best:  95.21%, epoch time: 187.54 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.3248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.6029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1378944 real_backward_count 254831  18.480%\n",
      "layer   1  Sparsity: 89.7135%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2108 occurrences\n",
      "train - Value 1: 1924 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 331 occurrences\n",
      "test - Value 1: 121 occurrences\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 11.327476/ 16.106050, val:  69.25%, val_best:  69.25%, tr:  91.62%, tr_best:  95.21%, epoch time: 187.99 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0161%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1403136 real_backward_count 259133  18.468%\n",
      "layer   1  Sparsity: 92.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 93.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1750.0\n",
      "lif layer 2 self.abs_max_v: 1750.0\n",
      "train - Value 0: 1961 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 364 occurrences\n",
      "test - Value 1: 88 occurrences\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 12.000862/  4.490020, val:  65.04%, val_best:  69.25%, tr:  90.95%, tr_best:  95.21%, epoch time: 190.28 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.9680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.1507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1427328 real_backward_count 263612  18.469%\n",
      "layer   1  Sparsity: 78.4831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3940.0\n",
      "fc layer 1 self.abs_max_out: 3973.0\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 215 occurrences\n",
      "test - Value 1: 237 occurrences\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 11.231650/  5.538472, val:  70.58%, val_best:  70.58%, tr:  92.58%, tr_best:  95.21%, epoch time: 189.60 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1451520 real_backward_count 267836  18.452%\n",
      "layer   1  Sparsity: 83.3333%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 3983.0\n",
      "train - Value 0: 1914 occurrences\n",
      "train - Value 1: 2118 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 451 occurrences\n",
      "test - Value 1: 1 occurrences\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 12.329491/  6.983428, val:  50.22%, val_best:  70.58%, tr:  92.96%, tr_best:  95.21%, epoch time: 191.22 seconds, 3.19 minutes\n",
      "layer   1  Sparsity: 81.4652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.1321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.0715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1475712 real_backward_count 272094  18.438%\n",
      "layer   1  Sparsity: 75.0000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1961 occurrences\n",
      "train - Value 1: 2071 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 10.668974/  8.219986, val:  68.81%, val_best:  70.58%, tr:  90.10%, tr_best:  95.21%, epoch time: 190.23 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.0533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1499904 real_backward_count 276304  18.421%\n",
      "layer   1  Sparsity: 78.5807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 4963.5\n",
      "train - Value 0: 2133 occurrences\n",
      "train - Value 1: 1899 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 448 occurrences\n",
      "test - Value 1: 4 occurrences\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  9.224579/  7.874543, val:  50.88%, val_best:  70.58%, tr:  91.10%, tr_best:  95.21%, epoch time: 190.52 seconds, 3.18 minutes\n",
      "layer   1  Sparsity: 81.4662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.5347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1524096 real_backward_count 280394  18.397%\n",
      "layer   1  Sparsity: 80.9896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4050.0\n",
      "lif layer 1 self.abs_max_v: 4971.0\n",
      "lif layer 1 self.abs_max_v: 5195.5\n",
      "fc layer 1 self.abs_max_out: 4053.0\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss: 12.853394/  4.653336, val:  50.00%, val_best:  70.58%, tr:  92.21%, tr_best:  95.21%, epoch time: 190.06 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 60.6371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1548288 real_backward_count 284654  18.385%\n",
      "layer   1  Sparsity: 79.3294%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4086.0\n",
      "train - Value 0: 1977 occurrences\n",
      "train - Value 1: 2055 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 253 occurrences\n",
      "test - Value 1: 199 occurrences\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 12.064206/ 13.991927, val:  73.23%, val_best:  73.23%, tr:  91.00%, tr_best:  95.21%, epoch time: 190.10 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.1149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1572480 real_backward_count 288947  18.375%\n",
      "layer   1  Sparsity: 83.6589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4111.0\n",
      "lif layer 1 self.abs_max_v: 5200.0\n",
      "fc layer 1 self.abs_max_out: 4125.0\n",
      "train - Value 0: 1982 occurrences\n",
      "train - Value 1: 2050 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss: 13.951016/ 10.374480, val:  50.00%, val_best:  73.23%, tr:  92.26%, tr_best:  95.21%, epoch time: 189.78 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.4713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1596672 real_backward_count 293420  18.377%\n",
      "layer   1  Sparsity: 89.3555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2087 occurrences\n",
      "train - Value 1: 1945 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 12.498944/  1.279805, val:  50.44%, val_best:  73.23%, tr:  93.87%, tr_best:  95.21%, epoch time: 190.16 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1620864 real_backward_count 297659  18.364%\n",
      "layer   1  Sparsity: 79.1341%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2113 occurrences\n",
      "train - Value 1: 1919 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 379 occurrences\n",
      "test - Value 1: 73 occurrences\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 11.479948/ 13.493358, val:  63.05%, val_best:  73.23%, tr:  92.53%, tr_best:  95.21%, epoch time: 189.26 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1645056 real_backward_count 302213  18.371%\n",
      "layer   1  Sparsity: 82.8451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4144.0\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 1 self.abs_max_out: 4290.0\n",
      "train - Value 0: 1998 occurrences\n",
      "train - Value 1: 2034 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 11.437428/ 14.217278, val:  50.00%, val_best:  73.23%, tr:  93.30%, tr_best:  95.21%, epoch time: 188.55 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1669248 real_backward_count 306779  18.378%\n",
      "layer   1  Sparsity: 89.8112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.8333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4343.0\n",
      "fc layer 1 self.abs_max_out: 4405.0\n",
      "lif layer 1 self.abs_max_v: 5222.0\n",
      "fc layer 1 self.abs_max_out: 4454.0\n",
      "train - Value 0: 2100 occurrences\n",
      "train - Value 1: 1932 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 219 occurrences\n",
      "test - Value 1: 233 occurrences\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss: 11.463521/  9.841763, val:  76.77%, val_best:  76.77%, tr:  91.27%, tr_best:  95.21%, epoch time: 188.74 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1544%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693440 real_backward_count 311144  18.373%\n",
      "layer   1  Sparsity: 73.4701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5227.5\n",
      "fc layer 1 self.abs_max_out: 4553.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "train - Value 0: 2153 occurrences\n",
      "train - Value 1: 1879 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 122 occurrences\n",
      "test - Value 1: 330 occurrences\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 11.803931/ 10.718160, val:  70.35%, val_best:  76.77%, tr:  90.60%, tr_best:  95.21%, epoch time: 189.55 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.2211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.8908%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1717632 real_backward_count 315323  18.358%\n",
      "layer   1  Sparsity: 73.0143%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4701.0\n",
      "fc layer 2 self.abs_max_out: 1809.0\n",
      "lif layer 2 self.abs_max_v: 1809.0\n",
      "train - Value 0: 2017 occurrences\n",
      "train - Value 1: 2015 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 13.709616/ 12.431950, val:  50.00%, val_best:  76.77%, tr:  94.12%, tr_best:  95.21%, epoch time: 188.80 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.3302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1741824 real_backward_count 319707  18.355%\n",
      "layer   1  Sparsity: 79.9805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4748.0\n",
      "train - Value 0: 2027 occurrences\n",
      "train - Value 1: 2005 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 11.673008/ 27.984116, val:  50.00%, val_best:  76.77%, tr:  94.42%, tr_best:  95.21%, epoch time: 188.58 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7775%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1766016 real_backward_count 324130  18.354%\n",
      "layer   1  Sparsity: 81.8685%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4755.0\n",
      "train - Value 0: 2029 occurrences\n",
      "train - Value 1: 2003 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 12.960920/  9.852095, val:  50.00%, val_best:  76.77%, tr:  92.04%, tr_best:  95.21%, epoch time: 189.31 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0209%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1790208 real_backward_count 328399  18.344%\n",
      "layer   1  Sparsity: 87.5000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2090 occurrences\n",
      "train - Value 1: 1942 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 3 occurrences\n",
      "test - Value 1: 449 occurrences\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 13.023491/ 20.172112, val:  50.66%, val_best:  76.77%, tr:  92.66%, tr_best:  95.21%, epoch time: 188.73 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9778%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1814400 real_backward_count 332685  18.336%\n",
      "layer   1  Sparsity: 85.6120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4824.0\n",
      "fc layer 2 self.abs_max_out: 1893.0\n",
      "lif layer 2 self.abs_max_v: 1893.0\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 426 occurrences\n",
      "test - Value 1: 26 occurrences\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 15.555650/  6.004097, val:  55.75%, val_best:  76.77%, tr:  94.05%, tr_best:  95.21%, epoch time: 189.00 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1838592 real_backward_count 337104  18.335%\n",
      "layer   1  Sparsity: 87.1419%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5267.0\n",
      "fc layer 1 self.abs_max_out: 4883.0\n",
      "train - Value 0: 2055 occurrences\n",
      "train - Value 1: 1977 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 450 occurrences\n",
      "test - Value 1: 2 occurrences\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 12.885704/  1.753151, val:  50.44%, val_best:  76.77%, tr:  93.08%, tr_best:  95.21%, epoch time: 187.69 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.8330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1862784 real_backward_count 341477  18.332%\n",
      "layer   1  Sparsity: 77.6367%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "lif layer 1 self.abs_max_v: 5333.5\n",
      "train - Value 0: 2012 occurrences\n",
      "train - Value 1: 2020 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 437 occurrences\n",
      "test - Value 1: 15 occurrences\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 13.805933/ 13.019477, val:  53.32%, val_best:  76.77%, tr:  93.80%, tr_best:  95.21%, epoch time: 188.97 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1886976 real_backward_count 345703  18.320%\n",
      "layer   1  Sparsity: 78.4180%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 15 occurrences\n",
      "test - Value 1: 437 occurrences\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 12.666365/  8.890550, val:  53.32%, val_best:  76.77%, tr:  93.97%, tr_best:  95.21%, epoch time: 188.81 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3408%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1911168 real_backward_count 349884  18.307%\n",
      "layer   1  Sparsity: 86.5560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 184.00 at epoch 79, iter 4031\n",
      "max_activation_accul updated: 198.00 at epoch 79, iter 4031\n",
      "max_activation_accul updated: 202.00 at epoch 79, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 12.878782/  9.882059, val:  50.00%, val_best:  76.77%, tr:  93.50%, tr_best:  95.21%, epoch time: 190.21 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0937%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1935360 real_backward_count 354129  18.298%\n",
      "layer   1  Sparsity: 84.8307%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2024 occurrences\n",
      "train - Value 1: 2008 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 178 occurrences\n",
      "test - Value 1: 274 occurrences\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss: 14.749398/  7.767520, val:  76.11%, val_best:  76.77%, tr:  94.89%, tr_best:  95.21%, epoch time: 187.86 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1959552 real_backward_count 358521  18.296%\n",
      "layer   1  Sparsity: 89.2253%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 14.659119/ 19.123436, val:  50.22%, val_best:  76.77%, tr:  95.06%, tr_best:  95.21%, epoch time: 188.48 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1983744 real_backward_count 362863  18.292%\n",
      "layer   1  Sparsity: 80.7943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1999 occurrences\n",
      "train - Value 1: 2033 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 59 occurrences\n",
      "test - Value 1: 393 occurrences\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 14.539955/ 15.335045, val:  60.84%, val_best:  76.77%, tr:  95.76%, tr_best:  95.76%, epoch time: 189.02 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2007936 real_backward_count 367334  18.294%\n",
      "layer   1  Sparsity: 77.1159%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 13.627628/ 18.376970, val:  50.88%, val_best:  76.77%, tr:  95.68%, tr_best:  95.76%, epoch time: 190.35 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2032128 real_backward_count 371691  18.291%\n",
      "layer   1  Sparsity: 71.6471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 53.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1983 occurrences\n",
      "train - Value 1: 2049 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 13.033704/ 28.395170, val:  50.00%, val_best:  76.77%, tr:  94.17%, tr_best:  95.76%, epoch time: 190.12 seconds, 3.17 minutes\n",
      "layer   1  Sparsity: 81.4678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2056320 real_backward_count 376060  18.288%\n",
      "layer   1  Sparsity: 73.8281%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2003 occurrences\n",
      "train - Value 1: 2029 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 26 occurrences\n",
      "test - Value 1: 426 occurrences\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 13.416852/ 14.010242, val:  55.75%, val_best:  76.77%, tr:  94.97%, tr_best:  95.76%, epoch time: 187.96 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2080512 real_backward_count 380575  18.292%\n",
      "layer   1  Sparsity: 73.6328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2006 occurrences\n",
      "train - Value 1: 2026 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 13.989112/ 25.719133, val:  50.00%, val_best:  76.77%, tr:  95.04%, tr_best:  95.76%, epoch time: 188.95 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2104704 real_backward_count 384977  18.291%\n",
      "layer   1  Sparsity: 81.3477%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2001 occurrences\n",
      "train - Value 1: 2031 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 109 occurrences\n",
      "test - Value 1: 343 occurrences\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 13.641894/  7.914555, val:  70.58%, val_best:  76.77%, tr:  95.81%, tr_best:  95.81%, epoch time: 188.80 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.8036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.4456%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2128896 real_backward_count 389314  18.287%\n",
      "layer   1  Sparsity: 79.4596%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 1933.0\n",
      "lif layer 2 self.abs_max_v: 1933.0\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 32 occurrences\n",
      "test - Value 1: 420 occurrences\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 14.029073/ 14.443243, val:  56.64%, val_best:  76.77%, tr:  96.03%, tr_best:  96.03%, epoch time: 188.07 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2153088 real_backward_count 393575  18.280%\n",
      "layer   1  Sparsity: 91.8945%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2053 occurrences\n",
      "train - Value 1: 1979 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 327 occurrences\n",
      "test - Value 1: 125 occurrences\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 13.783550/  5.301569, val:  69.25%, val_best:  76.77%, tr:  95.76%, tr_best:  96.03%, epoch time: 188.59 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.1196%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2177280 real_backward_count 397929  18.276%\n",
      "layer   1  Sparsity: 84.6029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 5 occurrences\n",
      "test - Value 1: 447 occurrences\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 14.683471/ 15.475290, val:  51.11%, val_best:  76.77%, tr:  94.89%, tr_best:  96.03%, epoch time: 189.09 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2201472 real_backward_count 402304  18.274%\n",
      "layer   1  Sparsity: 82.7474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 27 occurrences\n",
      "test - Value 1: 425 occurrences\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss: 13.690104/ 11.643188, val:  55.97%, val_best:  76.77%, tr:  95.09%, tr_best:  96.03%, epoch time: 189.12 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6072%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2225664 real_backward_count 406721  18.274%\n",
      "layer   1  Sparsity: 81.3151%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1992 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "lif layer 2 self.abs_max_v: 1971.5\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 8 occurrences\n",
      "test - Value 1: 444 occurrences\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 13.837070/ 13.787492, val:  51.77%, val_best:  76.77%, tr:  96.23%, tr_best:  96.23%, epoch time: 187.93 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.4720%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2249856 real_backward_count 411121  18.273%\n",
      "layer   1  Sparsity: 80.9570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2030 occurrences\n",
      "train - Value 1: 2002 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss: 13.793635/ 22.070904, val:  50.22%, val_best:  76.77%, tr:  96.28%, tr_best:  96.28%, epoch time: 188.04 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.5322%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2274048 real_backward_count 415509  18.272%\n",
      "layer   1  Sparsity: 77.0182%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2024.0\n",
      "lif layer 2 self.abs_max_v: 2024.0\n",
      "train - Value 0: 2010 occurrences\n",
      "train - Value 1: 2022 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "lif layer 2 self.abs_max_v: 2027.5\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 421 occurrences\n",
      "test - Value 1: 31 occurrences\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss: 16.369276/ 10.896657, val:  56.86%, val_best:  76.77%, tr:  96.73%, tr_best:  96.73%, epoch time: 187.83 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.7558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2298240 real_backward_count 419872  18.269%\n",
      "layer   1  Sparsity: 68.0013%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2049.0\n",
      "lif layer 2 self.abs_max_v: 2049.0\n",
      "train - Value 0: 1972 occurrences\n",
      "train - Value 1: 2060 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 19 occurrences\n",
      "test - Value 1: 433 occurrences\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 14.642706/ 13.272715, val:  54.20%, val_best:  76.77%, tr:  95.59%, tr_best:  96.73%, epoch time: 189.62 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4686%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2322432 real_backward_count 424160  18.264%\n",
      "layer   1  Sparsity: 90.1693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 92.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2014 occurrences\n",
      "train - Value 1: 2018 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 11 occurrences\n",
      "test - Value 1: 441 occurrences\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss: 14.489540/  9.091625, val:  51.99%, val_best:  76.77%, tr:  96.23%, tr_best:  96.73%, epoch time: 189.14 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.2872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2346624 real_backward_count 428524  18.261%\n",
      "layer   1  Sparsity: 72.0378%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 50.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2066 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "lif layer 2 self.abs_max_v: 2282.5\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 431 occurrences\n",
      "test - Value 1: 21 occurrences\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss: 14.314161/  9.611998, val:  54.65%, val_best:  76.77%, tr:  95.04%, tr_best:  96.73%, epoch time: 188.46 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.9913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2370816 real_backward_count 432876  18.259%\n",
      "layer   1  Sparsity: 78.7435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2035 occurrences\n",
      "train - Value 1: 1997 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 431 occurrences\n",
      "test - Value 1: 21 occurrences\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss: 13.855842/ 17.289494, val:  54.20%, val_best:  76.77%, tr:  94.92%, tr_best:  96.73%, epoch time: 189.17 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.3613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2395008 real_backward_count 437316  18.259%\n",
      "layer   1  Sparsity: 79.8503%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 13.774399/ 21.029696, val:  50.00%, val_best:  76.77%, tr:  96.25%, tr_best:  96.73%, epoch time: 188.64 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3718%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7380%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2419200 real_backward_count 441636  18.255%\n",
      "layer   1  Sparsity: 83.6589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.8333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2127.0\n",
      "fc layer 2 self.abs_max_out: 2144.0\n",
      "train - Value 0: 1979 occurrences\n",
      "train - Value 1: 2053 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "lif layer 2 self.abs_max_v: 2410.5\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss: 13.841175/ 16.630892, val:  50.00%, val_best:  76.77%, tr:  93.87%, tr_best:  96.73%, epoch time: 188.37 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2072%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2443392 real_backward_count 446030  18.255%\n",
      "layer   1  Sparsity: 68.5221%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 54.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2042 occurrences\n",
      "train - Value 1: 1990 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 267 occurrences\n",
      "test - Value 1: 185 occurrences\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 15.972365/ 10.097870, val:  78.98%, val_best:  78.98%, tr:  96.03%, tr_best:  96.73%, epoch time: 188.01 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4685%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5369%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5821%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2467584 real_backward_count 450502  18.257%\n",
      "layer   1  Sparsity: 85.3516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1973 occurrences\n",
      "train - Value 1: 2059 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 213.00 at epoch 102, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss: 15.803941/  9.856773, val:  50.00%, val_best:  78.98%, tr:  95.96%, tr_best:  96.73%, epoch time: 188.01 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5768%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.4820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2491776 real_backward_count 454759  18.250%\n",
      "layer   1  Sparsity: 69.0430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 52.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2040 occurrences\n",
      "train - Value 1: 1992 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 260 occurrences\n",
      "test - Value 1: 192 occurrences\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss: 16.743355/  9.786242, val:  77.88%, val_best:  78.98%, tr:  97.22%, tr_best:  97.22%, epoch time: 188.58 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2515968 real_backward_count 459072  18.246%\n",
      "layer   1  Sparsity: 79.3294%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 64.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2015 occurrences\n",
      "train - Value 1: 2017 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 16.650545/ 36.946522, val:  50.00%, val_best:  78.98%, tr:  96.45%, tr_best:  97.22%, epoch time: 189.81 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.7723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2540160 real_backward_count 463429  18.244%\n",
      "layer   1  Sparsity: 76.0091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4885.0\n",
      "train - Value 0: 1951 occurrences\n",
      "train - Value 1: 2081 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 17.749638/ 30.890564, val:  50.00%, val_best:  78.98%, tr:  96.16%, tr_best:  97.22%, epoch time: 186.89 seconds, 3.11 minutes\n",
      "layer   1  Sparsity: 81.4668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.4112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2564352 real_backward_count 467884  18.246%\n",
      "layer   1  Sparsity: 84.8958%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2002 occurrences\n",
      "train - Value 1: 2030 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss: 17.440718/ 26.052664, val:  50.00%, val_best:  78.98%, tr:  95.98%, tr_best:  97.22%, epoch time: 189.34 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2588544 real_backward_count 472245  18.244%\n",
      "layer   1  Sparsity: 90.8203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2152.0\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "fc layer 2 self.abs_max_out: 2200.0\n",
      "fc layer 2 self.abs_max_out: 2244.0\n",
      "fc layer 2 self.abs_max_out: 2276.0\n",
      "fc layer 1 self.abs_max_out: 4912.0\n",
      "train - Value 0: 1996 occurrences\n",
      "train - Value 1: 2036 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 30 occurrences\n",
      "test - Value 1: 422 occurrences\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss: 15.750145/ 16.836538, val:  55.75%, val_best:  78.98%, tr:  96.13%, tr_best:  97.22%, epoch time: 189.63 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2612736 real_backward_count 476516  18.238%\n",
      "layer   1  Sparsity: 86.2305%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.1667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2011 occurrences\n",
      "train - Value 1: 2021 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 384 occurrences\n",
      "test - Value 1: 68 occurrences\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 14.430242/ 22.802725, val:  64.16%, val_best:  78.98%, tr:  96.01%, tr_best:  97.22%, epoch time: 189.09 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2636928 real_backward_count 480904  18.237%\n",
      "layer   1  Sparsity: 85.7096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 2 self.abs_max_out: 2296.0\n",
      "fc layer 1 self.abs_max_out: 4945.0\n",
      "train - Value 0: 1997 occurrences\n",
      "train - Value 1: 2035 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 1 occurrences\n",
      "test - Value 1: 451 occurrences\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss: 17.189167/ 29.208176, val:  50.22%, val_best:  78.98%, tr:  96.70%, tr_best:  97.22%, epoch time: 189.70 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6437%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2661120 real_backward_count 485323  18.238%\n",
      "layer   1  Sparsity: 88.2161%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.5833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4946.0\n",
      "train - Value 0: 2020 occurrences\n",
      "train - Value 1: 2012 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss: 17.095846/ 36.595737, val:  50.00%, val_best:  78.98%, tr:  97.57%, tr_best:  97.57%, epoch time: 189.23 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2685312 real_backward_count 489564  18.231%\n",
      "layer   1  Sparsity: 80.5013%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 59.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4978.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 327 occurrences\n",
      "test - Value 1: 125 occurrences\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss: 16.625402/ 19.852976, val:  70.58%, val_best:  78.98%, tr:  96.95%, tr_best:  97.57%, epoch time: 189.38 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.2535%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2709504 real_backward_count 493796  18.225%\n",
      "layer   1  Sparsity: 79.5898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 86.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4991.0\n",
      "train - Value 0: 2045 occurrences\n",
      "train - Value 1: 1987 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 218.00 at epoch 112, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 377 occurrences\n",
      "test - Value 1: 75 occurrences\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss: 14.493268/ 16.583391, val:  65.27%, val_best:  78.98%, tr:  95.26%, tr_best:  97.57%, epoch time: 189.30 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.8289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.4026%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2733696 real_backward_count 498216  18.225%\n",
      "layer   1  Sparsity: 85.4818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 4996.0\n",
      "train - Value 0: 1977 occurrences\n",
      "train - Value 1: 2055 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 440 occurrences\n",
      "test - Value 1: 12 occurrences\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss: 14.143232/ 14.241902, val:  52.65%, val_best:  78.98%, tr:  95.01%, tr_best:  97.57%, epoch time: 187.91 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0126%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2757888 real_backward_count 502698  18.228%\n",
      "layer   1  Sparsity: 70.6380%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.4167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5048.0\n",
      "train - Value 0: 1962 occurrences\n",
      "train - Value 1: 2070 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 444 occurrences\n",
      "test - Value 1: 8 occurrences\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss: 17.099325/ 16.079062, val:  51.77%, val_best:  78.98%, tr:  95.63%, tr_best:  97.57%, epoch time: 188.33 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.7991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2782080 real_backward_count 507146  18.229%\n",
      "layer   1  Sparsity: 78.0273%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2008 occurrences\n",
      "train - Value 1: 2024 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 241 occurrences\n",
      "test - Value 1: 211 occurrences\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss: 19.245970/ 10.082391, val:  78.10%, val_best:  78.98%, tr:  96.63%, tr_best:  97.57%, epoch time: 189.35 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.0352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2806272 real_backward_count 511425  18.224%\n",
      "layer   1  Sparsity: 86.7513%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 91.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2033 occurrences\n",
      "train - Value 1: 1999 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 405 occurrences\n",
      "test - Value 1: 47 occurrences\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss: 14.544251/ 11.491054, val:  60.40%, val_best:  78.98%, tr:  95.01%, tr_best:  97.57%, epoch time: 189.64 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9798%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.6853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2830464 real_backward_count 515801  18.223%\n",
      "layer   1  Sparsity: 70.9635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.3333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2032 occurrences\n",
      "train - Value 1: 2000 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 4 occurrences\n",
      "test - Value 1: 448 occurrences\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss: 14.852697/ 26.597898, val:  50.88%, val_best:  78.98%, tr:  95.78%, tr_best:  97.57%, epoch time: 188.45 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2854656 real_backward_count 520052  18.218%\n",
      "layer   1  Sparsity: 87.2721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.4167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "fc layer 1 self.abs_max_out: 5080.0\n",
      "train - Value 0: 2078 occurrences\n",
      "train - Value 1: 1954 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss: 12.314691/ 21.848019, val:  50.44%, val_best:  78.98%, tr:  94.39%, tr_best:  97.57%, epoch time: 187.75 seconds, 3.13 minutes\n",
      "layer   1  Sparsity: 81.4643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2878848 real_backward_count 524490  18.219%\n",
      "layer   1  Sparsity: 88.2487%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.2500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2082 occurrences\n",
      "train - Value 1: 1950 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 36 occurrences\n",
      "test - Value 1: 416 occurrences\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss: 13.981410/ 12.367021, val:  57.52%, val_best:  78.98%, tr:  95.68%, tr_best:  97.57%, epoch time: 188.97 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2903040 real_backward_count 528870  18.218%\n",
      "layer   1  Sparsity: 87.2070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2019 occurrences\n",
      "train - Value 1: 2013 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 431 occurrences\n",
      "test - Value 1: 21 occurrences\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss: 14.586164/ 13.595133, val:  54.65%, val_best:  78.98%, tr:  96.16%, tr_best:  97.57%, epoch time: 188.60 seconds, 3.14 minutes\n",
      "layer   1  Sparsity: 81.4643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.1948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.2674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2927232 real_backward_count 533333  18.220%\n",
      "layer   1  Sparsity: 81.5104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 57.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2025 occurrences\n",
      "train - Value 1: 2007 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss: 15.699704/ 33.221878, val:  50.00%, val_best:  78.98%, tr:  96.80%, tr_best:  97.57%, epoch time: 189.39 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2951424 real_backward_count 537594  18.215%\n",
      "layer   1  Sparsity: 77.2461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.0000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2009 occurrences\n",
      "train - Value 1: 2023 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 1: 452 occurrences\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss: 17.155613/ 22.679159, val:  50.00%, val_best:  78.98%, tr:  97.50%, tr_best:  97.57%, epoch time: 188.81 seconds, 3.15 minutes\n",
      "layer   1  Sparsity: 81.4665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.9720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2975616 real_backward_count 541808  18.208%\n",
      "layer   1  Sparsity: 88.1510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 65.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1966 occurrences\n",
      "train - Value 1: 2066 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 20 occurrences\n",
      "test - Value 1: 432 occurrences\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss: 15.299057/ 14.233187, val:  54.42%, val_best:  78.98%, tr:  96.88%, tr_best:  97.57%, epoch time: 189.66 seconds, 3.16 minutes\n",
      "layer   1  Sparsity: 81.4641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.5074%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.8373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 2999808 real_backward_count 546087  18.204%\n",
      "layer   1  Sparsity: 82.1615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 63.6667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.0000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1988 occurrences\n",
      "train - Value 1: 2044 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 385 occurrences\n",
      "test - Value 1: 67 occurrences\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss: 17.873302/ 16.835253, val:  64.82%, val_best:  78.98%, tr:  97.02%, tr_best:  97.57%, epoch time: 170.83 seconds, 2.85 minutes\n",
      "layer   1  Sparsity: 81.4654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.3092%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.7526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3024000 real_backward_count 550346  18.199%\n",
      "layer   1  Sparsity: 85.6120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 1995 occurrences\n",
      "train - Value 1: 2037 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 375 occurrences\n",
      "test - Value 1: 77 occurrences\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss: 19.086306/ 13.752745, val:  66.59%, val_best:  78.98%, tr:  97.74%, tr_best:  97.74%, epoch time: 157.86 seconds, 2.63 minutes\n",
      "layer   1  Sparsity: 81.4647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 62.2408%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.5730%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3048192 real_backward_count 554458  18.190%\n",
      "layer   1  Sparsity: 72.4284%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 56.7500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 87.0833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2007 occurrences\n",
      "train - Value 1: 2025 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "max_activation_accul updated: 237.00 at epoch 126, iter 4031\n",
      "max_activation_accul updated: 261.00 at epoch 126, iter 4031\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 2 occurrences\n",
      "test - Value 1: 450 occurrences\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss: 16.028162/ 29.247879, val:  50.44%, val_best:  78.98%, tr:  96.60%, tr_best:  97.74%, epoch time: 153.94 seconds, 2.57 minutes\n",
      "layer   1  Sparsity: 81.4676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.1635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3072384 real_backward_count 558653  18.183%\n",
      "layer   1  Sparsity: 75.6185%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 55.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 88.1667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "train - Value 0: 2037 occurrences\n",
      "train - Value 1: 1995 occurrences\n",
      "train_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test_spike_distribution.mean 6.000000, min 6, max 6\n",
      "test - Value 0: 452 occurrences\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss: 14.185839/ 13.719619, val:  50.00%, val_best:  78.98%, tr:  95.41%, tr_best:  97.74%, epoch time: 152.66 seconds, 2.54 minutes\n",
      "layer   1  Sparsity: 81.4669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 61.9282%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 89.5381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 3096576 real_backward_count 562970  18.180%\n",
      "layer   1  Sparsity: 89.9414%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 90.6667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "target_word=0\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_targetword{target_word}_new251129',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [4,6,8]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [8]},\n",
    "        \"which_data\": {\"values\": ['n_tidigits_tonic']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [4096.0,2048.0,1024.0,512.0,256.0,128.0,64.0,32.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [4.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1.0, 2.0, 4.0, 8.0]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [1]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [target_word]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [False]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [False]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [8]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "\n",
    "        \"timestep_sums_threshold\": {\"values\": [0]},\n",
    "\n",
    "        \"loser_encourage_mode\": {\"values\": [True, False]},\n",
    "        \n",
    "        \"lif_layer_sg_width2\": {\"values\": [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [4096.0,2048.0,1024.0,512.0,256.0,128.0,64.0,32.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0, 2.0, 4.0, 8.0]},\n",
    "        \"init_scaling_0\": {\"values\": [1.0, 1/2, 1/4, 1/8, 1/16, 1/32]},\n",
    "        \"init_scaling_1\": {\"values\": [1.0, 1/2, 1/4, 1/8, 1/16, 1/32]},\n",
    "        \"init_scaling_2\": {\"values\": [1.0, 1/2, 1/4, 1/8, 1/16, 1/32]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"4\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        timestep_sums_threshold  =  wandb.config.timestep_sums_threshold,\n",
    "        loser_encourage_mode  =  wandb.config.loser_encourage_mode,\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = '9m2jgqar'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn NTIDIGITS SWEEP LOSER ONOFF new251129')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
