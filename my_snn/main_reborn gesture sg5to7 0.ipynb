{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35528/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAIhCAYAAACfVbSSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA77UlEQVR4nO3deXhU5f3//9ckMROWJKwJQUKIS0sENZi4sPnDhbQUEOsCRWURsGBYZPkopFpRUCKoSCuCIpvIYqSAoCKaShVUkBgRrEsRQRKUGEFMACEhM+f3ByXfDgmYjDP3YWaej+s612XunLnPe8aFt69zz30clmVZAgAAgN+F2V0AAABAqKDxAgAAMITGCwAAwBAaLwAAAENovAAAAAyh8QIAADCExgsAAMAQGi8AAABDaLwAAAAMofECvLBw4UI5HI7KIyIiQgkJCfrTn/6kr776yra6HnroITkcDtuuf6r8/HwNHz5cF198saKjoxUfH6/rr79e69evr3LuwIEDPT7TevXqqVWrVrrhhhu0YMEClZWV1fr6Y8eOlcPhUI8ePXzxdgDgV6PxAn6FBQsWaNOmTfrnP/+pESNGaM2aNerUqZMOHjxod2lnhWXLlmnLli0aNGiQVq9erblz58rpdOq6667TokWLqpxfp04dbdq0SZs2bdJrr72mSZMmqV69errrrruUlpamvXv31vjax48f1+LFiyVJ69at07fffuuz9wUAXrMA1NqCBQssSVZeXp7H+MMPP2xJsubPn29LXRMnTrTOpn+tv//++ypjFRUV1iWXXGKdf/75HuMDBgyw6tWrV+08b775pnXOOedYV155ZY2vvXz5ckuS1b17d0uS9eijj9bodeXl5dbx48er/d2RI0dqfH0AqA6JF+BD6enpkqTvv/++cuzYsWMaN26cUlNTFRsbq0aNGql9+/ZavXp1ldc7HA6NGDFCL774olJSUlS3bl1deumleu2116qc+/rrrys1NVVOp1PJycl64oknqq3p2LFjysrKUnJysiIjI3Xuuedq+PDh+umnnzzOa9WqlXr06KHXXntN7dq1U506dZSSklJ57YULFyolJUX16tXTFVdcoY8++ugXP4+4uLgqY+Hh4UpLS1NhYeEvvv6kjIwM3XXXXfrwww+1YcOGGr1m3rx5ioyM1IIFC5SYmKgFCxbIsiyPc9555x05HA69+OKLGjdunM4991w5nU7t3LlTAwcOVP369fXpp58qIyND0dHRuu666yRJubm56tWrl1q0aKGoqChdcMEFGjp0qPbv318598aNG+VwOLRs2bIqtS1atEgOh0N5eXk1/gwABAcaL8CHdu/eLUn6zW9+UzlWVlamH3/8Uf/3f/+nV155RcuWLVOnTp100003VXu77fXXX9fMmTM1adIkrVixQo0aNdIf//hH7dq1q/Kct99+W7169VJ0dLReeuklPf7443r55Ze1YMECj7ksy9KNN96oJ554Qv369dPrr7+usWPH6oUXXtC1115bZd3Utm3blJWVpfHjx2vlypWKjY3VTTfdpIkTJ2ru3LmaMmWKlixZopKSEvXo0UNHjx6t9WdUUVGhjRs3qk2bNrV63Q033CBJNWq89u7dq7feeku9evVS06ZNNWDAAO3cufO0r83KylJBQYGeffZZvfrqq5UNY3l5uW644QZde+21Wr16tR5++GFJ0tdff6327dtr9uzZeuutt/Tggw/qww8/VKdOnXT8+HFJUufOndWuXTs988wzVa43c+ZMXX755br88str9RkACAJ2R25AIDp5q3Hz5s3W8ePHrUOHDlnr1q2zmjVrZl199dWnvVVlWSdutR0/ftwaPHiw1a5dO4/fSbLi4+Ot0tLSyrGioiIrLCzMys7Orhy78sorrebNm1tHjx6tHCstLbUaNWrkcatx3bp1liRr2rRpHtfJycmxJFlz5sypHEtKSrLq1Klj7d27t3Lsk08+sSRZCQkJHrfZXnnlFUuStWbNmpp8XB7uv/9+S5L1yiuveIyf6VajZVnWF198YUmy7r777l+8xqRJkyxJ1rp16yzLsqxdu3ZZDofD6tevn8d5//rXvyxJ1tVXX11ljgEDBtTotrHb7baOHz9u7dmzx5JkrV69uvJ3J/852bp1a+XYli1bLEnWCy+88IvvA0DwIfECfoWrrrpK55xzjqKjo/X73/9eDRs21OrVqxUREeFx3vLly9WxY0fVr19fEREROuecczRv3jx98cUXVea85pprFB0dXflzfHy84uLitGfPHknSkSNHlJeXp5tuuklRUVGV50VHR6tnz54ec5389uDAgQM9xm+99VbVq1dPb7/9tsd4amqqzj333MqfU1JSJEldunRR3bp1q4yfrKmm5s6dq0cffVTjxo1Tr169avVa65TbhGc67+Ttxa5du0qSkpOT1aVLF61YsUKlpaVVXnPzzTefdr7qfldcXKxhw4YpMTGx8u9nUlKSJHn8Pe3bt6/i4uI8Uq+nn35aTZs2VZ8+fWr0fgAEFxov4FdYtGiR8vLytH79eg0dOlRffPGF+vbt63HOypUr1bt3b5177rlavHixNm3apLy8PA0aNEjHjh2rMmfjxo2rjDmdzsrbegcPHpTb7VazZs2qnHfq2IEDBxQREaGmTZt6jDscDjVr1kwHDhzwGG/UqJHHz5GRkWccr67+01mwYIGGDh2qP//5z3r88cdr/LqTTjZ5zZs3P+N569ev1+7du3XrrbeqtLRUP/30k3766Sf17t1bP//8c7VrrhISEqqdq27duoqJifEYc7vdysjI0MqVK3Xffffp7bff1pYtW7R582ZJ8rj96nQ6NXToUC1dulQ//fSTfvjhB7388ssaMmSInE5nrd4/gOAQ8cunADidlJSUygX111xzjVwul+bOnat//OMfuuWWWyRJixcvVnJysnJycjz22PJmXypJatiwoRwOh4qKiqr87tSxxo0bq6KiQj/88INH82VZloqKioytMVqwYIGGDBmiAQMG6Nlnn/Vqr7E1a9ZIOpG+ncm8efMkSdOnT9f06dOr/f3QoUM9xk5XT3Xj//73v7Vt2zYtXLhQAwYMqBzfuXNntXPcfffdeuyxxzR//nwdO3ZMFRUVGjZs2BnfA4DgReIF+NC0adPUsGFDPfjgg3K73ZJO/OEdGRnp8Yd4UVFRtd9qrImT3ypcuXKlR+J06NAhvfrqqx7nnvwW3sn9rE5asWKFjhw5Uvl7f1q4cKGGDBmiO+64Q3PnzvWq6crNzdXcuXPVoUMHderU6bTnHTx4UKtWrVLHjh31r3/9q8px++23Ky8vT//+97+9fj8n6z81sXruueeqPT8hIUG33nqrZs2apWeffVY9e/ZUy5Ytvb4+gMBG4gX4UMOGDZWVlaX77rtPS5cu1R133KEePXpo5cqVyszM1C233KLCwkJNnjxZCQkJXu9yP3nyZP3+979X165dNW7cOLlcLk2dOlX16tXTjz/+WHle165d9bvf/U7jx49XaWmpOnbsqO3bt2vixIlq166d+vXr56u3Xq3ly5dr8ODBSk1N1dChQ7VlyxaP37dr186jgXG73ZW37MrKylRQUKA33nhDL7/8slJSUvTyyy+f8XpLlizRsWPHNGrUqGqTscaNG2vJkiWaN2+ennrqKa/eU+vWrXX++edrwoQJsixLjRo10quvvqrc3NzTvuaee+7RlVdeKUlVvnkKIMTYu7YfCEyn20DVsizr6NGjVsuWLa0LL7zQqqiosCzLsh577DGrVatWltPptFJSUqznn3++2s1OJVnDhw+vMmdSUpI1YMAAj7E1a9ZYl1xyiRUZGWm1bNnSeuyxx6qd8+jRo9b48eOtpKQk65xzzrESEhKsu+++2zp48GCVa3Tv3r3Ktauraffu3ZYk6/HHHz/tZ2RZ/++bgac7du/efdpz69SpY7Vs2dLq2bOnNX/+fKusrOyM17Isy0pNTbXi4uLOeO5VV11lNWnSxCorK6v8VuPy5currf1037L8/PPPra5du1rR0dFWw4YNrVtvvdUqKCiwJFkTJ06s9jWtWrWyUlJSfvE9AAhuDsuq4VeFAABe2b59uy699FI988wzyszMtLscADai8QIAP/n666+1Z88e/eUvf1FBQYF27tzpsS0HgNDD4noA8JPJkyera9euOnz4sJYvX07TBYDECwAAwBQSLwAAAENovAAAAAyh8QIAADAkoDdQdbvd+u677xQdHe3VbtgAAIQSy7J06NAhNW/eXGFh5rOXY8eOqby83C9zR0ZGKioqyi9z+1JAN17fffedEhMT7S4DAICAUlhYqBYtWhi95rFjx5ScVF9FxS6/zN+sWTPt3r37rG++Arrxio6OliRd0WWCIiLO7g/6VHU/32d3CV755vYku0vw2qt3zrC7BK9k9h1kdwle2dmvnt0leO2CF4/YXYJXEqYX2F2CV97ffJHdJXjtjms32F1CrZQdqdAT16+v/PPTpPLychUVu7Qnv5Vion2btpUecisp7RuVl5fTePnTyduLERFRAdd4RYQ5f/mks1C4M7A+5/8V7eN/0U2JCA/Mf1bC6gTuPysR4RV2l+CVyPqRdpfglbCz/A/KM4mqf47dJXjFzuU59aMdqh/t2+u7FTjLjQK68QIAAIHFZbnl8vEOoi7L7dsJ/SgwIwAAAIAAROIFAACMccuSW76NvHw9nz+ReAEAABhC4gUAAIxxyy1fr8jy/Yz+Q+IFAABgCIkXAAAwxmVZclm+XZPl6/n8icQLAADAEBIvAABgTKh/q5HGCwAAGOOWJVcIN17cagQAADCExAsAABgT6rcaSbwAAAAMIfECAADGsJ0EAAAAjCDxAgAAxrj/e/h6zkBhe+I1a9YsJScnKyoqSmlpadq4caPdJQEAAPiFrY1XTk6ORo8erfvvv19bt25V586d1a1bNxUUFNhZFgAA8BPXf/fx8vURKGxtvKZPn67BgwdryJAhSklJ0YwZM5SYmKjZs2fbWRYAAPATl+WfI1DY1niVl5crPz9fGRkZHuMZGRn64IMPqn1NWVmZSktLPQ4AAIBAYVvjtX//frlcLsXHx3uMx8fHq6ioqNrXZGdnKzY2tvJITEw0USoAAPARt5+OQGH74nqHw+Hxs2VZVcZOysrKUklJSeVRWFhookQAAACfsG07iSZNmig8PLxKulVcXFwlBTvJ6XTK6XSaKA8AAPiBWw65VH3A8mvmDBS2JV6RkZFKS0tTbm6ux3hubq46dOhgU1UAAAD+Y+sGqmPHjlW/fv2Unp6u9u3ba86cOSooKNCwYcPsLAsAAPiJ2zpx+HrOQGFr49WnTx8dOHBAkyZN0r59+9S2bVutXbtWSUlJdpYFAADgF7Y/MigzM1OZmZl2lwEAAAxw+WGNl6/n8yfbGy8AABA6Qr3xsn07CQAAgFBB4gUAAIxxWw65LR9vJ+Hj+fyJxAsAAMAQEi8AAGAMa7wAAABgBIkXAAAwxqUwuXyc+7h8Opt/kXgBAAAYQuIFAACMsfzwrUYrgL7VSOMFAACMYXE9AAAAjCDxAgAAxrisMLksHy+ut3w6nV+ReAEAABhC4gUAAIxxyyG3j3MftwIn8iLxAgAAMCQoEq+6O/crIsxpdxm18tyml+0uwSs3fXqn3SV4rW5YuN0leGXlqwvsLsEr/zoWY3cJXnvk/YF2l+CV8gdb212CVz6Y96TdJXjt1cPn211CrRw9p8LuEvhWo90FAAAAhIqgSLwAAEBg8M+3GgNnjReNFwAAMObE4nrf3hr09Xz+xK1GAAAAQ0i8AACAMW6FycV2EgAAAPA3Ei8AAGBMqC+uJ/ECAAAwhMQLAAAY41YYjwwCAACA/5F4AQAAY1yWQy7Lx48M8vF8/kTjBQAAjHH5YTsJF7caAQAAcCoSLwAAYIzbCpPbx9tJuNlOAgAAAKci8QIAAMawxgsAAABGkHgBAABj3PL99g9un87mXyReAAAAhpB4AQAAY/zzyKDAyZFovAAAgDEuK0wuH28n4ev5/ClwKgUAAAhwJF4AAMAYtxxyy9eL6wPnWY0kXgAAAIaQeAEAAGNY4wUAAAAjSLwAAIAx/nlkUODkSIFTKQAAQIAj8QIAAMa4LYfcvn5kkI/n8ycSLwAAAENIvAAAgDFuP6zx4pFBAAAA1XBbYXL7ePsHX8/nT4FTKQAAQIAj8QIAAMa45JDLx4/48fV8/kTiBQAAYAiJFwAAMIY1XgAAADCCxAsAABjjku/XZLl8Opt/kXgBAAAYQuIFAACMCfU1XjReAADAGJcVJpePGyVfz+dPgVMpAABAgKPxAgAAxlhyyO3jw/Jysf6sWbOUnJysqKgopaWlaePGjWc8f8mSJbr00ktVt25dJSQk6M4779SBAwdqdU0aLwAAEHJycnI0evRo3X///dq6das6d+6sbt26qaCgoNrz33vvPfXv31+DBw/WZ599puXLlysvL09Dhgyp1XVpvAAAgDEn13j5+qit6dOna/DgwRoyZIhSUlI0Y8YMJSYmavbs2dWev3nzZrVq1UqjRo1ScnKyOnXqpKFDh+qjjz6q1XVpvAAAQFAoLS31OMrKyqo9r7y8XPn5+crIyPAYz8jI0AcffFDtazp06KC9e/dq7dq1sixL33//vf7xj3+oe/futaoxKL7VuOv2BIVHRdldRq1sK29idwleafRgpN0leO3DnBi7S/DKdxUN7S7BK9Ofv8XuErzW4pMf7C7BKz9d0tjuErzS46//Z3cJXiu50O4Kasd97JikzfbWYDnktny7gerJ+RITEz3GJ06cqIceeqjK+fv375fL5VJ8fLzHeHx8vIqKiqq9RocOHbRkyRL16dNHx44dU0VFhW644QY9/fTTtaqVxAsAAASFwsJClZSUVB5ZWVlnPN/h8GwALcuqMnbS559/rlGjRunBBx9Ufn6+1q1bp927d2vYsGG1qjEoEi8AABAYXAqTy8e5z8n5YmJiFBPzy3c3mjRpovDw8CrpVnFxcZUU7KTs7Gx17NhR9957ryTpkksuUb169dS5c2c98sgjSkhIqFGtJF4AAMCYk7cafX3URmRkpNLS0pSbm+sxnpubqw4dOlT7mp9//llhYZ5tU3h4uKQTSVlN0XgBAICQM3bsWM2dO1fz58/XF198oTFjxqigoKDy1mFWVpb69+9feX7Pnj21cuVKzZ49W7t27dL777+vUaNG6YorrlDz5s1rfF1uNQIAAGPcCpPbx7mPN/P16dNHBw4c0KRJk7Rv3z61bdtWa9euVVJSkiRp3759Hnt6DRw4UIcOHdLMmTM1btw4NWjQQNdee62mTp1aq+vSeAEAgJCUmZmpzMzMan+3cOHCKmMjR47UyJEjf9U1abwAAIAxLsshl4+3k/D1fP7EGi8AAABDSLwAAIAx/txANRCQeAEAABhC4gUAAIyxrDC5vXio9S/NGShovAAAgDEuOeSSjxfX+3g+fwqcFhEAACDAkXgBAABj3JbvF8O7a/7EHtuReAEAABhC4gUAAIxx+2Fxva/n86fAqRQAACDAkXgBAABj3HLI7eNvIfp6Pn+yNfHKzs7W5ZdfrujoaMXFxenGG2/Uf/7zHztLAgAA8BtbG693331Xw4cP1+bNm5Wbm6uKigplZGToyJEjdpYFAAD85ORDsn19BApbbzWuW7fO4+cFCxYoLi5O+fn5uvrqq22qCgAA+EuoL64/q9Z4lZSUSJIaNWpU7e/LyspUVlZW+XNpaamRugAAAHzhrGkRLcvS2LFj1alTJ7Vt27bac7KzsxUbG1t5JCYmGq4SAAD8Gm455LZ8fLC4vvZGjBih7du3a9myZac9JysrSyUlJZVHYWGhwQoBAAB+nbPiVuPIkSO1Zs0abdiwQS1atDjteU6nU06n02BlAADAlyw/bCdhBVDiZWvjZVmWRo4cqVWrVumdd95RcnKyneUAAAD4la2N1/Dhw7V06VKtXr1a0dHRKioqkiTFxsaqTp06dpYGAAD84OS6LF/PGShsXeM1e/ZslZSUqEuXLkpISKg8cnJy7CwLAADAL2y/1QgAAEIH+3gBAAAYwq1GAAAAGEHiBQAAjHH7YTsJNlAFAABAFSReAADAGNZ4AQAAwAgSLwAAYAyJFwAAAIwg8QIAAMaEeuJF4wUAAIwJ9caLW40AAACGkHgBAABjLPl+w9NAevIziRcAAIAhJF4AAMAY1ngBAADACBIvAABgTKgnXkHReLW8aq8i6jntLqNW/n5Ba7tL8Iq70zl2l+C1xzIH2F2CV1zOwAymyy8PpOWunpblLrK7BK90fmqc3SV4ZebwWXaX4LUhHwbWf1esn4/ZXULIC4rGCwAABAYSLwAAAENCvfEKzHsYAAAAAYjECwAAGGNZDlk+Tqh8PZ8/kXgBAAAYQuIFAACMccvh80cG+Xo+fyLxAgAAMITECwAAGMO3GgEAAGAEiRcAADCGbzUCAADACBIvAABgTKiv8aLxAgAAxnCrEQAAAEaQeAEAAGMsP9xqJPECAABAFSReAADAGEuSZfl+zkBB4gUAAGAIiRcAADDGLYccPCQbAAAA/kbiBQAAjAn1fbxovAAAgDFuyyFHCO9cz61GAAAAQ0i8AACAMZblh+0kAmg/CRIvAAAAQ0i8AACAMaG+uJ7ECwAAwBASLwAAYAyJFwAAAIwg8QIAAMaE+j5eNF4AAMAYtpMAAACAESReAADAmBOJl68X1/t0Or8i8QIAADCExAsAABjDdhIAAAAwgsQLAAAYY/338PWcgYLECwAAwBASLwAAYEyor/Gi8QIAAOaE+L1GbjUCAAAYQuIFAADM8cOtRgXQrUYSLwAAEJJmzZql5ORkRUVFKS0tTRs3bjzj+WVlZbr//vuVlJQkp9Op888/X/Pnz6/VNUm8AACAMWfLQ7JzcnI0evRozZo1Sx07dtRzzz2nbt266fPPP1fLli2rfU3v3r31/fffa968ebrgggtUXFysioqKWl2XxgsAAISc6dOna/DgwRoyZIgkacaMGXrzzTc1e/ZsZWdnVzl/3bp1evfdd7Vr1y41atRIktSqVataXzcoGq+IcLfOCXfZXUatDPlqp90leOUcx5d2l+C1rNmD7C7BKxXtS+0uwSutm/5gdwleS33tHrtL8Eq9jiV2l+CVgW8MtbsEr7W88Hu7S6iViqgy7ba5Bn9uJ1Fa6vnfS6fTKafTWeX88vJy5efna8KECR7jGRkZ+uCDD6q9xpo1a5Senq5p06bpxRdfVL169XTDDTdo8uTJqlOnTo1rDYrGCwAAIDEx0ePniRMn6qGHHqpy3v79++VyuRQfH+8xHh8fr6Kiomrn3rVrl9577z1FRUVp1apV2r9/vzIzM/Xjjz/Wap0XjRcAADDHcvj+W4j/na+wsFAxMTGVw9WlXf/L4fCsw7KsKmMnud1uORwOLVmyRLGxsZJO3K685ZZb9Mwzz9Q49aLxAgAAxvhzcX1MTIxH43U6TZo0UXh4eJV0q7i4uEoKdlJCQoLOPffcyqZLklJSUmRZlvbu3asLL7ywRrWynQQAAAgpkZGRSktLU25ursd4bm6uOnToUO1rOnbsqO+++06HDx+uHNuxY4fCwsLUokWLGl+bxgsAAJhj+emopbFjx2ru3LmaP3++vvjiC40ZM0YFBQUaNmyYJCkrK0v9+/evPP+2225T48aNdeedd+rzzz/Xhg0bdO+992rQoEEsrgcAADiTPn366MCBA5o0aZL27duntm3bau3atUpKSpIk7du3TwUFBZXn169fX7m5uRo5cqTS09PVuHFj9e7dW4888kitrkvjBQAAjPHndhK1lZmZqczMzGp/t3DhwipjrVu3rnJ7sra41QgAAGAIiRcAADDLx99qDCQkXgAAAIaQeAEAAGPOpjVedqDxAgAA5ni5/cMvzhkguNUIAABgCIkXAAAwyPHfw9dzBgYSLwAAAENIvAAAgDms8QIAAIAJJF4AAMAcEi8AAACYcNY0XtnZ2XI4HBo9erTdpQAAAH+xHP45AsRZcasxLy9Pc+bM0SWXXGJ3KQAAwI8s68Th6zkDhe2J1+HDh3X77bfr+eefV8OGDe0uBwAAwG9sb7yGDx+u7t276/rrr//Fc8vKylRaWupxAACAAGL56QgQtt5qfOmll/Txxx8rLy+vRudnZ2fr4Ycf9nNVAAAA/mFb4lVYWKh77rlHixcvVlRUVI1ek5WVpZKSksqjsLDQz1UCAACfYnG9PfLz81VcXKy0tLTKMZfLpQ0bNmjmzJkqKytTeHi4x2ucTqecTqfpUgEAAHzCtsbruuuu06effuoxduedd6p169YaP358laYLAAAEPod14vD1nIHCtsYrOjpabdu29RirV6+eGjduXGUcAAAgGNR6jdcLL7yg119/vfLn++67Tw0aNFCHDh20Z88enxYHAACCTIh/q7HWjdeUKVNUp04dSdKmTZs0c+ZMTZs2TU2aNNGYMWN+VTHvvPOOZsyY8avmAAAAZzEW19dOYWGhLrjgAknSK6+8oltuuUV//vOf1bFjR3Xp0sXX9QEAAASNWide9evX14EDByRJb731VuXGp1FRUTp69KhvqwMAAMElxG811jrx6tq1q4YMGaJ27dppx44d6t69uyTps88+U6tWrXxdHwAAQNCodeL1zDPPqH379vrhhx+0YsUKNW7cWNKJfbn69u3r8wIBAEAQIfGqnQYNGmjmzJlVxnmUDwAAwJnVqPHavn272rZtq7CwMG3fvv2M515yySU+KQwAAAQhfyRUwZZ4paamqqioSHFxcUpNTZXD4ZBl/b93efJnh8Mhl8vlt2IBAAACWY0ar927d6tp06aVfw0AAOAVf+y7FWz7eCUlJVX716f63xQMAAAAnmr9rcZ+/frp8OHDVca/+eYbXX311T4pCgAABKeTD8n29REoat14ff7557r44ov1/vvvV4698MILuvTSSxUfH+/T4gAAQJBhO4na+fDDD/XAAw/o2muv1bhx4/TVV19p3bp1+tvf/qZBgwb5o0YAAICgUOvGKyIiQo899picTqcmT56siIgIvfvuu2rfvr0/6gMAAAgatb7VePz4cY0bN05Tp05VVlaW2rdvrz/+8Y9au3atP+oDAAAIGrVOvNLT0/Xzzz/rnXfe0VVXXSXLsjRt2jTddNNNGjRokGbNmuWPOgEAQBBwyPeL4QNnMwkvG6+///3vqlevnqQTm6eOHz9ev/vd73THHXf4vMCa2FsSq/DjTluu7a0xX9xmdwleyfn9M3aX4LW637vtLsErVyV/aXcJXhnb9B27S/Dan8cNs7sEr+S8MtfuErzS58a77C7Ba+F7j9pdQq1UuMvtLiHk1brxmjdvXrXjqampys/P/9UFAQCAIMYGqt47evSojh8/7jHmdAZW8gQAAGBKrRfXHzlyRCNGjFBcXJzq16+vhg0behwAAACnFeL7eNW68brvvvu0fv16zZo1S06nU3PnztXDDz+s5s2ba9GiRf6oEQAABIsQb7xqfavx1Vdf1aJFi9SlSxcNGjRInTt31gUXXKCkpCQtWbJEt99+uz/qBAAACHi1Trx+/PFHJScnS5JiYmL0448/SpI6deqkDRs2+LY6AAAQVHhWYy2dd955+uabbyRJF110kV5++WVJJ5KwBg0a+LI2AACAoFLrxuvOO+/Utm3bJElZWVmVa73GjBmje++91+cFAgCAIMIar9oZM2ZM5V9fc801+vLLL/XRRx/p/PPP16WXXurT4gAAAILJr9rHS5Jatmypli1b+qIWAAAQ7PyRUAVQ4lXrW40AAADwzq9OvAAAAGrKH99CDMpvNe7du9efdQAAgFBw8lmNvj4CRI0br7Zt2+rFF1/0Zy0AAABBrcaN15QpUzR8+HDdfPPNOnDggD9rAgAAwSrEt5OoceOVmZmpbdu26eDBg2rTpo3WrFnjz7oAAACCTq0W1ycnJ2v9+vWaOXOmbr75ZqWkpCgiwnOKjz/+2KcFAgCA4BHqi+tr/a3GPXv2aMWKFWrUqJF69epVpfECAABA9WrVNT3//PMaN26crr/+ev373/9W06ZN/VUXAAAIRiG+gWqNG6/f//732rJli2bOnKn+/fv7syYAAICgVOPGy+Vyafv27WrRooU/6wEAAMHMD2u8gjLxys3N9WcdAAAgFIT4rUae1QgAAGAIX0kEAADmkHgBAADABBIvAABgTKhvoEriBQAAYAiNFwAAgCE0XgAAAIawxgsAAJgT4t9qpPECAADGsLgeAAAARpB4AQAAswIoofI1Ei8AAABDSLwAAIA5Ib64nsQLAADAEBIvAABgDN9qBAAAgBEkXgAAwJwQX+NF4wUAAIzhViMAAACMIPECAADmhPitRhIvAAAAQ2i8AACAOZafDi/MmjVLycnJioqKUlpamjZu3Fij173//vuKiIhQampqra9J4wUAAEJOTk6ORo8erfvvv19bt25V586d1a1bNxUUFJzxdSUlJerfv7+uu+46r65L4wUAAIw5+a1GXx+1NX36dA0ePFhDhgxRSkqKZsyYocTERM2ePfuMrxs6dKhuu+02tW/f3qv3HxSL6yu2NpDljLK7jFpp8ZnL7hK8MvC7e+wuwWutcnfaXYJX3rzgCrtL8MqGy863uwSvxX3ypd0leKVvuxvsLsErt21cZ3cJXrup/l67S6iV0kNuJba2uwr/KS0t9fjZ6XTK6XRWOa+8vFz5+fmaMGGCx3hGRoY++OCD086/YMECff3111q8eLEeeeQRr2ok8QIAAOb4cY1XYmKiYmNjK4/s7OxqS9i/f79cLpfi4+M9xuPj41VUVFTta7766itNmDBBS5YsUUSE97lVUCReAAAgQPhxO4nCwkLFxMRUDleXdv0vh8PhOY1lVRmTJJfLpdtuu00PP/ywfvOb3/yqUmm8AABAUIiJifFovE6nSZMmCg8Pr5JuFRcXV0nBJOnQoUP66KOPtHXrVo0YMUKS5Ha7ZVmWIiIi9NZbb+naa6+tUY00XgAAwJiz4ZFBkZGRSktLU25urv74xz9Wjufm5qpXr15Vzo+JidGnn37qMTZr1iytX79e//jHP5ScnFzja9N4AQCAkDN27Fj169dP6enpat++vebMmaOCggINGzZMkpSVlaVvv/1WixYtUlhYmNq2bevx+ri4OEVFRVUZ/yU0XgAAwJyz5JFBffr00YEDBzRp0iTt27dPbdu21dq1a5WUlCRJ2rdv3y/u6eUNGi8AABCSMjMzlZmZWe3vFi5ceMbXPvTQQ3rooYdqfU0aLwAAYMzZsMbLTuzjBQAAYAiJFwAAMOcsWeNlFxovAABgTog3XtxqBAAAMITECwAAGOP47+HrOQMFiRcAAIAhJF4AAMAc1ngBAADABBIvAABgDBuoAgAAwAjbG69vv/1Wd9xxhxo3bqy6desqNTVV+fn5dpcFAAD8wfLTESBsvdV48OBBdezYUddcc43eeOMNxcXF6euvv1aDBg3sLAsAAPhTADVKvmZr4zV16lQlJiZqwYIFlWOtWrWyryAAAAA/svVW45o1a5Senq5bb71VcXFxateunZ5//vnTnl9WVqbS0lKPAwAABI6Ti+t9fQQKWxuvXbt2afbs2brwwgv15ptvatiwYRo1apQWLVpU7fnZ2dmKjY2tPBITEw1XDAAA4D1bGy+3263LLrtMU6ZMUbt27TR06FDdddddmj17drXnZ2VlqaSkpPIoLCw0XDEAAPhVQnxxva2NV0JCgi666CKPsZSUFBUUFFR7vtPpVExMjMcBAAAQKGxdXN+xY0f95z//8RjbsWOHkpKSbKoIAAD4Exuo2mjMmDHavHmzpkyZop07d2rp0qWaM2eOhg8fbmdZAAAAfmFr43X55Zdr1apVWrZsmdq2bavJkydrxowZuv322+0sCwAA+EuIr/Gy/VmNPXr0UI8ePewuAwAAwO9sb7wAAEDoCPU1XjReAADAHH/cGgygxsv2h2QDAACEChIvAABgDokXAAAATCDxAgAAxoT64noSLwAAAENIvAAAgDms8QIAAIAJJF4AAMAYh2XJYfk2ovL1fP5E4wUAAMzhViMAAABMIPECAADGsJ0EAAAAjCDxAgAA5rDGCwAAACYEReJ1161vqE79wHorr9/S3u4SvBJeHmt3CV47d80Ru0vwyrnaZncJXtm8/FK7S/Dazsca2l2CV2bdOM/uErzy8M6edpfgtX3nfmp3CbVy7PBxSd/ZWgNrvAAAAGBEYMVEAAAgsIX4Gi8aLwAAYAy3GgEAAGAEiRcAADAnxG81kngBAAAYQuIFAACMCqQ1Wb5G4gUAAGAIiRcAADDHsk4cvp4zQJB4AQAAGELiBQAAjAn1fbxovAAAgDlsJwEAAAATSLwAAIAxDveJw9dzBgoSLwAAAENIvAAAgDms8QIAAIAJJF4AAMCYUN9OgsQLAADAEBIvAABgTog/MojGCwAAGMOtRgAAABhB4gUAAMxhOwkAAACYQOIFAACMYY0XAAAAjCDxAgAA5oT4dhIkXgAAAIaQeAEAAGNCfY0XjRcAADCH7SQAAABgAokXAAAwJtRvNZJ4AQAAGELiBQAAzHFbJw5fzxkgSLwAAAAMIfECAADm8K1GAAAAmEDiBQAAjHHID99q9O10fkXjBQAAzOFZjQAAADCBxAsAABjDBqoAAAAwgsQLAACYw3YSAAAAoWfWrFlKTk5WVFSU0tLStHHjxtOeu3LlSnXt2lVNmzZVTEyM2rdvrzfffLPW16TxAgAAxjgsyy9HbeXk5Gj06NG6//77tXXrVnXu3FndunVTQUFBtedv2LBBXbt21dq1a5Wfn69rrrlGPXv21NatW2t13aC41fh60cWKqOe0u4xa2fXnRnaX4JXWj1f/D2Qg+LZ7lN0leGVH1oV2l+CV85/4wO4SvHZgSHu7S/DKxZEH7S7BKyX/amZ3CV57NrmJ3SXUivvoMUn/tLsMvyktLfX42el0yumsvj+YPn26Bg8erCFDhkiSZsyYoTfffFOzZ89WdnZ2lfNnzJjh8fOUKVO0evVqvfrqq2rXrl2NayTxAgAA5rj9dEhKTExUbGxs5VFdAyVJ5eXlys/PV0ZGhsd4RkaGPvigZv/T6Ha7dejQITVqVLsgJSgSLwAAEBi8vTX4S3NKUmFhoWJiYirHT5d27d+/Xy6XS/Hx8R7j8fHxKioqqtE1n3zySR05ckS9e/euVa00XgAAICjExMR4NF6/xOHwfNiQZVlVxqqzbNkyPfTQQ1q9erXi4uJqVSONFwAAMOcs2E6iSZMmCg8Pr5JuFRcXV0nBTpWTk6PBgwdr+fLluv7662tbKWu8AABAaImMjFRaWppyc3M9xnNzc9WhQ4fTvm7ZsmUaOHCgli5dqu7du3t1bRIvAABgzlnykOyxY8eqX79+Sk9PV/v27TVnzhwVFBRo2LBhkqSsrCx9++23WrRokaQTTVf//v31t7/9TVdddVVlWlanTh3FxsbW+Lo0XgAAIOT06dNHBw4c0KRJk7Rv3z61bdtWa9euVVJSkiRp3759Hnt6Pffcc6qoqNDw4cM1fPjwyvEBAwZo4cKFNb4ujRcAADDmbHpIdmZmpjIzM6v93anN1DvvvOPdRU7BGi8AAABDSLwAAIA5Z8kaL7uQeAEAABhC4gUAAIxxuE8cvp4zUNB4AQAAc7jVCAAAABNIvAAAgDlnwSOD7ETiBQAAYAiJFwAAMMZhWXL4eE2Wr+fzJxIvAAAAQ0i8AACAOXyr0T4VFRV64IEHlJycrDp16ui8887TpEmT5HYH0IYcAAAANWRr4jV16lQ9++yzeuGFF9SmTRt99NFHuvPOOxUbG6t77rnHztIAAIA/WJJ8na8ETuBlb+O1adMm9erVS927d5cktWrVSsuWLdNHH31U7fllZWUqKyur/Lm0tNRInQAAwDdYXG+jTp066e2339aOHTskSdu2bdN7772nP/zhD9Wen52drdjY2MojMTHRZLkAAAC/iq2J1/jx41VSUqLWrVsrPDxcLpdLjz76qPr27Vvt+VlZWRo7dmzlz6WlpTRfAAAEEkt+WFzv2+n8ydbGKycnR4sXL9bSpUvVpk0bffLJJxo9erSaN2+uAQMGVDnf6XTK6XTaUCkAAMCvZ2vjde+992rChAn605/+JEm6+OKLtWfPHmVnZ1fbeAEAgADHdhL2+fnnnxUW5llCeHg420kAAICgZGvi1bNnTz366KNq2bKl2rRpo61bt2r69OkaNGiQnWUBAAB/cUty+GHOAGFr4/X000/rr3/9qzIzM1VcXKzmzZtr6NChevDBB+0sCwAAwC9sbbyio6M1Y8YMzZgxw84yAACAIaG+jxfPagQAAOawuB4AAAAmkHgBAABzSLwAAABgAokXAAAwh8QLAAAAJpB4AQAAc0J8A1USLwAAAENIvAAAgDFsoAoAAGAKi+sBAABgAokXAAAwx21JDh8nVG4SLwAAAJyCxAsAAJjDGi8AAACYQOIFAAAM8kPipcBJvIKi8Qp7KFZhEVF2l1Er54cftbsErxR1T7K7BK/5ei2nKRdMyLe7BK8c7Nfe7hK89v/d/aHdJXjlj1n/Z3cJXmm6v9zuErz273vm2l1CrZQecqvhaLurCG1B0XgBAIAAEeJrvGi8AACAOW5LPr81yHYSAAAAOBWJFwAAMMdynzh8PWeAIPECAAAwhMQLAACYE+KL60m8AAAADCHxAgAA5vCtRgAAAJhA4gUAAMwJ8TVeNF4AAMAcS35ovHw7nT9xqxEAAMAQEi8AAGBOiN9qJPECAAAwhMQLAACY43ZL8vEjftw8MggAAACnIPECAADmsMYLAAAAJpB4AQAAc0I88aLxAgAA5vCsRgAAAJhA4gUAAIyxLLcsy7fbP/h6Pn8i8QIAADCExAsAAJhjWb5fkxVAi+tJvAAAAAwh8QIAAOZYfvhWI4kXAAAATkXiBQAAzHG7JYePv4UYQN9qpPECAADmcKsRAAAAJpB4AQAAYyy3W5aPbzWygSoAAACqIPECAADmsMYLAAAAJpB4AQAAc9yW5CDxAgAAgJ+ReAEAAHMsS5KvN1Al8QIAAMApSLwAAIAxltuS5eM1XlYAJV40XgAAwBzLLd/famQDVQAAAJyCxAsAABgT6rcaSbwAAAAMIfECAADmhPgar4BuvE5GixWuMpsrqT3LCsyw0VUemHVLvt8o2ZQK67jdJXjFVX7M7hK8VnaYz9ykiooKu0vwWumhwPkDX5JKD5+o185bcxU67vNHNVYocP6ddViBdGP0FHv37lViYqLdZQAAEFAKCwvVokULo9c8duyYkpOTVVRU5Jf5mzVrpt27dysqKsov8/tKQDdebrdb3333naKjo+VwOHw6d2lpqRITE1VYWKiYmBifzo3q8ZmbxedtFp+3eXzmVVmWpUOHDql58+YKCzN/B+PYsWMqLy/3y9yRkZFnfdMlBfitxrCwML937DExMfwLaxifuVl83mbxeZvHZ+4pNjbWtmtHRUUFRHPkT4G7YAcAACDA0HgBAAAYQuN1Gk6nUxMnTpTT6bS7lJDBZ24Wn7dZfN7m8ZnjbBTQi+sBAAACCYkXAACAITReAAAAhtB4AQAAGELjBQAAYAiN12nMmjVLycnJioqKUlpamjZu3Gh3SUEpOztbl19+uaKjoxUXF6cbb7xR//nPf+wuK2RkZ2fL4XBo9OjRdpcS1L799lvdcccdaty4serWravU1FTl5+fbXVZQqqio0AMPPKDk5GTVqVNH5513niZNmiS3O7CeqYjgReNVjZycHI0ePVr333+/tm7dqs6dO6tbt24qKCiwu7Sg8+6772r48OHavHmzcnNzVVFRoYyMDB05csTu0oJeXl6e5syZo0suucTuUoLawYMH1bFjR51zzjl644039Pnnn+vJJ59UgwYN7C4tKE2dOlXPPvusZs6cqS+++ELTpk3T448/rqefftru0gBJbCdRrSuvvFKXXXaZZs+eXTmWkpKiG2+8UdnZ2TZWFvx++OEHxcXF6d1339XVV19tdzlB6/Dhw7rssss0a9YsPfLII0pNTdWMGTPsLisoTZgwQe+//z6puSE9evRQfHy85s2bVzl28803q27dunrxxRdtrAw4gcTrFOXl5crPz1dGRobHeEZGhj744AObqgodJSUlkqRGjRrZXElwGz58uLp3767rr7/e7lKC3po1a5Senq5bb71VcXFxateunZ5//nm7ywpanTp10ttvv60dO3ZIkrZt26b33ntPf/jDH2yuDDghoB+S7Q/79++Xy+VSfHy8x3h8fLyKiopsqio0WJalsWPHqlOnTmrbtq3d5QStl156SR9//LHy8vLsLiUk7Nq1S7Nnz9bYsWP1l7/8RVu2bNGoUaPkdDrVv39/u8sLOuPHj1dJSYlat26t8PBwuVwuPfroo+rbt6/dpQGSaLxOy+FwePxsWVaVMfjWiBEjtH37dr333nt2lxK0CgsLdc899+itt95SVFSU3eWEBLfbrfT0dE2ZMkWS1K5dO3322WeaPXs2jZcf5OTkaPHixVq6dKnatGmjTz75RKNHj1bz5s01YMAAu8sDaLxO1aRJE4WHh1dJt4qLi6ukYPCdkSNHas2aNdqwYYNatGhhdzlBKz8/X8XFxUpLS6scc7lc2rBhg2bOnKmysjKFh4fbWGHwSUhI0EUXXeQxlpKSohUrVthUUXC79957NWHCBP3pT3+SJF188cXas2ePsrOzabxwVmCN1ykiIyOVlpam3Nxcj/Hc3Fx16NDBpqqCl2VZGjFihFauXKn169crOTnZ7pKC2nXXXadPP/1Un3zySeWRnp6u22+/XZ988glNlx907NixyhYpO3bsUFJSkk0VBbeff/5ZYWGef7SFh4eznQTOGiRe1Rg7dqz69eun9PR0tW/fXnPmzFFBQYGGDRtmd2lBZ/jw4Vq6dKlWr16t6OjoyqQxNjZWderUsbm64BMdHV1l/Vy9evXUuHFj1tX5yZgxY9ShQwdNmTJFvXv31pYtWzRnzhzNmTPH7tKCUs+ePfXoo4+qZcuWatOmjbZu3arp06dr0KBBdpcGSGI7idOaNWuWpk2bpn379qlt27Z66qmn2N7AD063bm7BggUaOHCg2WJCVJcuXdhOws9ee+01ZWVl6auvvlJycrLGjh2ru+66y+6ygtKhQ4f017/+VatWrVJxcbGaN2+uvn376sEHH1RkZKTd5QE0XgAAAKawxgsAAMAQGi8AAABDaLwAAAAMofECAAAwhMYLAADAEBovAAAAQ2i8AAAADKHxAgAAMITGC4DtHA6HXnnlFbvLAAC/o/ECIJfLpQ4dOujmm2/2GC8pKVFiYqIeeOABv15/37596tatm1+vAQBnAx4ZBECS9NVXXyk1NVVz5szR7bffLknq37+/tm3bpry8PJ5zBwA+QOIFQJJ04YUXKjs7WyNHjtR3332n1atX66WXXtILL7xwxqZr8eLFSk9PV3R0tJo1a6bbbrtNxcXFlb+fNGmSmjdvrgMHDlSO3XDDDbr66qvldrsled5qLC8v14gRI5SQkKCoqCi1atVK2dnZ/nnTAGAYiReASpZl6dprr1V4eLg+/fRTjRw58hdvM86fP18JCQn67W9/q+LiYo0ZM0YNGzbU2rVrJZ24jdm5c2fFx8dr1apVevbZZzVhwgRt27ZNSUlJkk40XqtWrdKNN96oJ554Qn//+9+1ZMkStWzZUoWFhSosLFTfvn39/v4BwN9ovAB4+PLLL5WSkqKLL75YH3/8sSIiImr1+ry8PF1xxRU6dOiQ6tevL0natWuXUlNTlZmZqaefftrjdqbk2XiNGjVKn332mf75z3/K4XD49L0BgN241QjAw/z581W3bl3t3r1be/fu/cXzt27dql69eikpKUnR0dHq0qWLJKmgoKDynPPOO09PPPGEpk6dqp49e3o0XacaOHCgPvnkE/32t7/VqFGj9NZbb/3q9wQAZwsaLwCVNm3apKeeekqrV69W+/btNXjwYJ0pFD9y5IgyMjJUv359LV68WHl5eVq1apWkE2u1/teGDRsUHh6ub775RhUVFaed87LLLtPu3bs1efJkHT16VL1799Ytt9zimzcIADaj8QIgSTp69KgGDBigoUOH6vrrr9fcuXOVl5en55577rSv+fLLL7V//3499thj6ty5s1q3bu2xsP6knJwcrVy5Uu+8844KCws1efLkM9YSExOjPn366Pnnn1dOTo5WrFihH3/88Ve/RwCwG40XAEnShAkT5Ha7NXXqVElSy5Yt9eSTT+ree+/VN998U+1rWrZsqcjISD399NPatWuX1qxZU6Wp2rt3r+6++25NnTpVnTp10sKFC5Wdna3NmzdXO+dTTz2ll156SV9++aV27Nih5cuXq1mzZmrQoIEv3y4A2ILGC4DeffddPfPMM1q4cKHq1atXOX7XXXepQ4cOp73l2LRpUy1cuFDLly/XRRddpMcee0xPPPFE5e8ty9LAgQN1xRVXaMSIEZKkrl27asSIEbrjjjt0+PDhKnPWr19fU6dOVXp6ui6//HJ98803Wrt2rcLC+M8VgMDHtxoBAAAM4X8hAQAADKHxAgAAMITGCwAAwBAaLwAAAENovAAAAAyh8QIAADCExgsAAMAQGi8AAABDaLwAAAAMofECAAAwhMYLAADAkP8fVnHQY4NURxoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 20664,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 300,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: t0h80lho\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4q7jhaem with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 14852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251008_205923-4q7jhaem</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4q7jhaem' target=\"_blank\">flowing-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4q7jhaem' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4q7jhaem</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251008_205931_074', 'my_seed': 14852, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 634.0\n",
      "lif layer 1 self.abs_max_v: 634.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 251.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 653.0\n",
      "lif layer 1 self.abs_max_v: 873.0\n",
      "fc layer 2 self.abs_max_out: 413.0\n",
      "lif layer 2 self.abs_max_v: 470.5\n",
      "fc layer 1 self.abs_max_out: 676.0\n",
      "lif layer 1 self.abs_max_v: 1112.5\n",
      "fc layer 1 self.abs_max_out: 760.0\n",
      "lif layer 1 self.abs_max_v: 1255.5\n",
      "lif layer 2 self.abs_max_v: 478.0\n",
      "fc layer 1 self.abs_max_out: 971.0\n",
      "lif layer 1 self.abs_max_v: 1365.5\n",
      "fc layer 2 self.abs_max_out: 538.0\n",
      "lif layer 2 self.abs_max_v: 540.5\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "lif layer 1 self.abs_max_v: 1393.0\n",
      "lif layer 2 self.abs_max_v: 641.5\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "fc layer 1 self.abs_max_out: 1543.0\n",
      "lif layer 1 self.abs_max_v: 1543.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 2 self.abs_max_out: 620.0\n",
      "lif layer 2 self.abs_max_v: 751.0\n",
      "fc layer 3 self.abs_max_out: 160.0\n",
      "lif layer 2 self.abs_max_v: 797.0\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 2 self.abs_max_out: 726.0\n",
      "fc layer 2 self.abs_max_out: 763.0\n",
      "lif layer 2 self.abs_max_v: 1065.5\n",
      "fc layer 3 self.abs_max_out: 191.0\n",
      "lif layer 2 self.abs_max_v: 1084.5\n",
      "fc layer 1 self.abs_max_out: 1603.0\n",
      "lif layer 1 self.abs_max_v: 1603.0\n",
      "lif layer 2 self.abs_max_v: 1113.0\n",
      "fc layer 1 self.abs_max_out: 1856.0\n",
      "lif layer 1 self.abs_max_v: 1856.0\n",
      "fc layer 2 self.abs_max_out: 777.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "fc layer 1 self.abs_max_out: 1928.0\n",
      "lif layer 1 self.abs_max_v: 1928.0\n",
      "lif layer 2 self.abs_max_v: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1338.0\n",
      "lif layer 2 self.abs_max_v: 1365.5\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 2 self.abs_max_out: 1036.0\n",
      "lif layer 2 self.abs_max_v: 1639.0\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 2 self.abs_max_out: 1061.0\n",
      "fc layer 2 self.abs_max_out: 1105.0\n",
      "fc layer 1 self.abs_max_out: 1969.0\n",
      "lif layer 1 self.abs_max_v: 1969.0\n",
      "fc layer 2 self.abs_max_out: 1232.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 2 self.abs_max_out: 1270.0\n",
      "fc layer 2 self.abs_max_out: 1379.0\n",
      "lif layer 2 self.abs_max_v: 1773.0\n",
      "fc layer 1 self.abs_max_out: 2116.0\n",
      "lif layer 1 self.abs_max_v: 2116.0\n",
      "fc layer 1 self.abs_max_out: 2121.0\n",
      "lif layer 1 self.abs_max_v: 2121.0\n",
      "fc layer 1 self.abs_max_out: 2659.0\n",
      "lif layer 1 self.abs_max_v: 2659.0\n",
      "fc layer 3 self.abs_max_out: 277.0\n",
      "fc layer 2 self.abs_max_out: 1603.0\n",
      "fc layer 2 self.abs_max_out: 1923.0\n",
      "lif layer 2 self.abs_max_v: 1923.0\n",
      "lif layer 2 self.abs_max_v: 2043.5\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "lif layer 2 self.abs_max_v: 2453.0\n",
      "fc layer 3 self.abs_max_out: 297.0\n",
      "fc layer 1 self.abs_max_out: 3001.0\n",
      "lif layer 1 self.abs_max_v: 3001.0\n",
      "fc layer 3 self.abs_max_out: 390.0\n",
      "fc layer 1 self.abs_max_out: 3284.0\n",
      "lif layer 1 self.abs_max_v: 3284.0\n",
      "fc layer 1 self.abs_max_out: 3356.0\n",
      "lif layer 1 self.abs_max_v: 3356.0\n",
      "fc layer 1 self.abs_max_out: 3775.0\n",
      "lif layer 1 self.abs_max_v: 3775.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "fc layer 2 self.abs_max_out: 2011.0\n",
      "lif layer 2 self.abs_max_v: 2497.5\n",
      "fc layer 1 self.abs_max_out: 3890.0\n",
      "lif layer 1 self.abs_max_v: 3890.0\n",
      "fc layer 2 self.abs_max_out: 2101.0\n",
      "lif layer 1 self.abs_max_v: 3977.0\n",
      "fc layer 2 self.abs_max_out: 2139.0\n",
      "fc layer 2 self.abs_max_out: 2185.0\n",
      "fc layer 1 self.abs_max_out: 4148.0\n",
      "lif layer 1 self.abs_max_v: 4148.0\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "fc layer 1 self.abs_max_out: 4408.0\n",
      "lif layer 1 self.abs_max_v: 4408.0\n",
      "fc layer 2 self.abs_max_out: 2319.0\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "lif layer 2 self.abs_max_v: 2498.0\n",
      "fc layer 2 self.abs_max_out: 2370.0\n",
      "fc layer 2 self.abs_max_out: 2371.0\n",
      "lif layer 2 self.abs_max_v: 2546.0\n",
      "fc layer 2 self.abs_max_out: 2425.0\n",
      "fc layer 1 self.abs_max_out: 4459.0\n",
      "lif layer 1 self.abs_max_v: 4459.0\n",
      "fc layer 2 self.abs_max_out: 2486.0\n",
      "fc layer 2 self.abs_max_out: 2734.0\n",
      "lif layer 2 self.abs_max_v: 2734.0\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "fc layer 1 self.abs_max_out: 4573.0\n",
      "lif layer 1 self.abs_max_v: 4573.0\n",
      "fc layer 2 self.abs_max_out: 2808.0\n",
      "lif layer 2 self.abs_max_v: 2808.0\n",
      "fc layer 1 self.abs_max_out: 4700.0\n",
      "lif layer 1 self.abs_max_v: 4700.0\n",
      "fc layer 1 self.abs_max_out: 4970.0\n",
      "lif layer 1 self.abs_max_v: 4970.0\n",
      "fc layer 1 self.abs_max_out: 5241.0\n",
      "lif layer 1 self.abs_max_v: 5241.0\n",
      "fc layer 1 self.abs_max_out: 5275.0\n",
      "lif layer 1 self.abs_max_v: 5275.0\n",
      "fc layer 1 self.abs_max_out: 5331.0\n",
      "lif layer 1 self.abs_max_v: 5331.0\n",
      "fc layer 2 self.abs_max_out: 2854.0\n",
      "lif layer 2 self.abs_max_v: 2854.0\n",
      "fc layer 2 self.abs_max_out: 2906.0\n",
      "lif layer 2 self.abs_max_v: 2906.0\n",
      "fc layer 2 self.abs_max_out: 2950.0\n",
      "lif layer 2 self.abs_max_v: 2950.0\n",
      "fc layer 2 self.abs_max_out: 3008.0\n",
      "lif layer 2 self.abs_max_v: 3008.0\n",
      "fc layer 1 self.abs_max_out: 5833.0\n",
      "lif layer 1 self.abs_max_v: 5833.0\n",
      "lif layer 2 self.abs_max_v: 3225.5\n",
      "fc layer 1 self.abs_max_out: 6023.0\n",
      "lif layer 1 self.abs_max_v: 6023.0\n",
      "fc layer 2 self.abs_max_out: 3037.0\n",
      "fc layer 2 self.abs_max_out: 3180.0\n",
      "fc layer 2 self.abs_max_out: 3189.0\n",
      "lif layer 1 self.abs_max_v: 6603.0\n",
      "lif layer 1 self.abs_max_v: 6603.5\n",
      "fc layer 1 self.abs_max_out: 6085.0\n",
      "fc layer 1 self.abs_max_out: 6306.0\n",
      "lif layer 2 self.abs_max_v: 3289.5\n",
      "lif layer 2 self.abs_max_v: 3393.5\n",
      "lif layer 1 self.abs_max_v: 7521.5\n",
      "lif layer 1 self.abs_max_v: 7701.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.035561/  2.117048, val:  52.92%, val_best:  52.92%, tr:  94.23%, tr_best:  94.23%, epoch time: 276.38 seconds, 4.61 minutes\n",
      "total_backward_count 44360 real_backward_count 11135  25.101%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3462.0\n",
      "lif layer 2 self.abs_max_v: 3615.5\n",
      "fc layer 1 self.abs_max_out: 6808.0\n",
      "fc layer 1 self.abs_max_out: 6920.0\n",
      "fc layer 1 self.abs_max_out: 7008.0\n",
      "lif layer 1 self.abs_max_v: 7856.0\n",
      "fc layer 1 self.abs_max_out: 7199.0\n",
      "lif layer 1 self.abs_max_v: 8828.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.013220/  2.106874, val:  47.08%, val_best:  52.92%, tr:  99.35%, tr_best:  99.35%, epoch time: 275.85 seconds, 4.60 minutes\n",
      "total_backward_count 88720 real_backward_count 18665  21.038%\n",
      "fc layer 1 self.abs_max_out: 7263.0\n",
      "fc layer 1 self.abs_max_out: 7664.0\n",
      "lif layer 1 self.abs_max_v: 9014.0\n",
      "lif layer 2 self.abs_max_v: 3729.0\n",
      "lif layer 1 self.abs_max_v: 9862.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.010522/  2.091845, val:  67.08%, val_best:  67.08%, tr:  99.48%, tr_best:  99.48%, epoch time: 274.06 seconds, 4.57 minutes\n",
      "total_backward_count 133080 real_backward_count 25488  19.152%\n",
      "lif layer 2 self.abs_max_v: 3741.5\n",
      "lif layer 2 self.abs_max_v: 3890.0\n",
      "fc layer 1 self.abs_max_out: 8195.0\n",
      "lif layer 2 self.abs_max_v: 3920.5\n",
      "lif layer 2 self.abs_max_v: 4025.5\n",
      "lif layer 2 self.abs_max_v: 4100.5\n",
      "lif layer 2 self.abs_max_v: 4202.5\n",
      "lif layer 2 self.abs_max_v: 4403.5\n",
      "lif layer 1 self.abs_max_v: 11027.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.005547/  2.087742, val:  48.33%, val_best:  67.08%, tr:  99.71%, tr_best:  99.71%, epoch time: 275.26 seconds, 4.59 minutes\n",
      "total_backward_count 177440 real_backward_count 32030  18.051%\n",
      "fc layer 1 self.abs_max_out: 8322.0\n",
      "fc layer 2 self.abs_max_out: 3230.0\n",
      "lif layer 2 self.abs_max_v: 4605.0\n",
      "lif layer 2 self.abs_max_v: 4699.5\n",
      "fc layer 2 self.abs_max_out: 3306.0\n",
      "fc layer 2 self.abs_max_out: 3341.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.991459/  2.077217, val:  63.75%, val_best:  67.08%, tr:  99.77%, tr_best:  99.77%, epoch time: 277.17 seconds, 4.62 minutes\n",
      "total_backward_count 221800 real_backward_count 38094  17.175%\n",
      "fc layer 1 self.abs_max_out: 8346.0\n",
      "fc layer 1 self.abs_max_out: 8767.0\n",
      "fc layer 2 self.abs_max_out: 3362.0\n",
      "lif layer 1 self.abs_max_v: 11112.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.977837/  2.070972, val:  67.50%, val_best:  67.50%, tr:  99.82%, tr_best:  99.82%, epoch time: 270.80 seconds, 4.51 minutes\n",
      "total_backward_count 266160 real_backward_count 44107  16.572%\n",
      "fc layer 2 self.abs_max_out: 3515.0\n",
      "fc layer 1 self.abs_max_out: 8974.0\n",
      "lif layer 1 self.abs_max_v: 11923.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.970885/  2.038238, val:  64.58%, val_best:  67.50%, tr:  99.91%, tr_best:  99.91%, epoch time: 274.46 seconds, 4.57 minutes\n",
      "total_backward_count 310520 real_backward_count 49747  16.021%\n",
      "fc layer 2 self.abs_max_out: 3607.0\n",
      "fc layer 2 self.abs_max_out: 3899.0\n",
      "fc layer 1 self.abs_max_out: 9063.0\n",
      "lif layer 1 self.abs_max_v: 12538.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.969048/  2.064363, val:  61.67%, val_best:  67.50%, tr:  99.84%, tr_best:  99.91%, epoch time: 272.59 seconds, 4.54 minutes\n",
      "total_backward_count 354880 real_backward_count 55211  15.558%\n",
      "fc layer 1 self.abs_max_out: 9164.0\n",
      "lif layer 1 self.abs_max_v: 13072.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.955884/  2.036613, val:  66.67%, val_best:  67.50%, tr:  99.84%, tr_best:  99.91%, epoch time: 273.76 seconds, 4.56 minutes\n",
      "total_backward_count 399240 real_backward_count 60327  15.110%\n",
      "fc layer 1 self.abs_max_out: 9436.0\n",
      "lif layer 1 self.abs_max_v: 13308.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.944809/  2.021624, val:  75.42%, val_best:  75.42%, tr:  99.93%, tr_best:  99.93%, epoch time: 273.23 seconds, 4.55 minutes\n",
      "total_backward_count 443600 real_backward_count 65406  14.744%\n",
      "fc layer 1 self.abs_max_out: 9437.0\n",
      "lif layer 1 self.abs_max_v: 13643.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.948834/  2.037753, val:  81.67%, val_best:  81.67%, tr:  99.95%, tr_best:  99.95%, epoch time: 268.66 seconds, 4.48 minutes\n",
      "total_backward_count 487960 real_backward_count 70379  14.423%\n",
      "fc layer 1 self.abs_max_out: 9580.0\n",
      "lif layer 1 self.abs_max_v: 14164.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.937472/  2.005131, val:  77.92%, val_best:  81.67%, tr:  99.98%, tr_best:  99.98%, epoch time: 272.04 seconds, 4.53 minutes\n",
      "total_backward_count 532320 real_backward_count 75096  14.107%\n",
      "fc layer 1 self.abs_max_out: 9595.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.914894/  2.013509, val:  64.17%, val_best:  81.67%, tr:  99.93%, tr_best:  99.98%, epoch time: 272.85 seconds, 4.55 minutes\n",
      "total_backward_count 576680 real_backward_count 79501  13.786%\n",
      "fc layer 1 self.abs_max_out: 9661.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.907684/  1.986122, val:  73.75%, val_best:  81.67%, tr:  99.98%, tr_best:  99.98%, epoch time: 271.10 seconds, 4.52 minutes\n",
      "total_backward_count 621040 real_backward_count 83907  13.511%\n",
      "fc layer 1 self.abs_max_out: 9783.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.896106/  1.996391, val:  66.25%, val_best:  81.67%, tr:  99.98%, tr_best:  99.98%, epoch time: 268.95 seconds, 4.48 minutes\n",
      "total_backward_count 665400 real_backward_count 88193  13.254%\n",
      "fc layer 1 self.abs_max_out: 9811.0\n",
      "fc layer 2 self.abs_max_out: 3929.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.894691/  1.984971, val:  75.83%, val_best:  81.67%, tr:  99.95%, tr_best:  99.98%, epoch time: 272.40 seconds, 4.54 minutes\n",
      "total_backward_count 709760 real_backward_count 92410  13.020%\n",
      "fc layer 1 self.abs_max_out: 9900.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.889972/  1.977735, val:  74.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.65 seconds, 4.51 minutes\n",
      "total_backward_count 754120 real_backward_count 96435  12.788%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.879212/  1.969629, val:  81.67%, val_best:  81.67%, tr:  99.95%, tr_best: 100.00%, epoch time: 272.51 seconds, 4.54 minutes\n",
      "total_backward_count 798480 real_backward_count 100326  12.565%\n",
      "fc layer 1 self.abs_max_out: 9926.0\n",
      "lif layer 1 self.abs_max_v: 14437.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.878005/  1.979406, val:  76.25%, val_best:  81.67%, tr:  99.95%, tr_best: 100.00%, epoch time: 274.73 seconds, 4.58 minutes\n",
      "total_backward_count 842840 real_backward_count 104113  12.353%\n",
      "lif layer 2 self.abs_max_v: 4719.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.872928/  1.967607, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.29 seconds, 4.54 minutes\n",
      "total_backward_count 887200 real_backward_count 107853  12.157%\n",
      "lif layer 2 self.abs_max_v: 4877.5\n",
      "fc layer 1 self.abs_max_out: 10000.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.875252/  1.956564, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.24 seconds, 4.57 minutes\n",
      "total_backward_count 931560 real_backward_count 111568  11.976%\n",
      "fc layer 1 self.abs_max_out: 10059.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.852447/  1.949708, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.83 seconds, 4.51 minutes\n",
      "total_backward_count 975920 real_backward_count 115094  11.793%\n",
      "lif layer 1 self.abs_max_v: 14706.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.858234/  1.938878, val:  85.00%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 272.46 seconds, 4.54 minutes\n",
      "total_backward_count 1020280 real_backward_count 118539  11.618%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.848843/  1.942794, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.93 seconds, 4.55 minutes\n",
      "total_backward_count 1064640 real_backward_count 121979  11.457%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.838507/  1.928345, val:  72.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.02 seconds, 4.52 minutes\n",
      "total_backward_count 1109000 real_backward_count 125279  11.297%\n",
      "fc layer 1 self.abs_max_out: 10118.0\n",
      "lif layer 1 self.abs_max_v: 14831.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.838331/  1.948830, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.84 seconds, 4.48 minutes\n",
      "total_backward_count 1153360 real_backward_count 128526  11.144%\n",
      "lif layer 2 self.abs_max_v: 4923.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.828506/  1.913652, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.10 seconds, 4.57 minutes\n",
      "total_backward_count 1197720 real_backward_count 131747  11.000%\n",
      "fc layer 1 self.abs_max_out: 10146.0\n",
      "lif layer 1 self.abs_max_v: 14852.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.829989/  1.924322, val:  91.25%, val_best:  91.25%, tr:  99.95%, tr_best: 100.00%, epoch time: 271.21 seconds, 4.52 minutes\n",
      "total_backward_count 1242080 real_backward_count 134783  10.851%\n",
      "fc layer 1 self.abs_max_out: 10154.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.828394/  1.921953, val:  81.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.99 seconds, 4.57 minutes\n",
      "total_backward_count 1286440 real_backward_count 137781  10.710%\n",
      "fc layer 1 self.abs_max_out: 10221.0\n",
      "lif layer 1 self.abs_max_v: 15161.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.824007/  1.919601, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.62 seconds, 4.53 minutes\n",
      "total_backward_count 1330800 real_backward_count 140622  10.567%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.820910/  1.921003, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.64 seconds, 4.56 minutes\n",
      "total_backward_count 1375160 real_backward_count 143370  10.426%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.813299/  1.897535, val:  81.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.01 seconds, 4.53 minutes\n",
      "total_backward_count 1419520 real_backward_count 146080  10.291%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.797015/  1.901239, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.41 seconds, 4.49 minutes\n",
      "total_backward_count 1463880 real_backward_count 148821  10.166%\n",
      "fc layer 1 self.abs_max_out: 10279.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.790902/  1.889913, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.06 seconds, 4.50 minutes\n",
      "total_backward_count 1508240 real_backward_count 151550  10.048%\n",
      "fc layer 1 self.abs_max_out: 10389.0\n",
      "lif layer 1 self.abs_max_v: 15220.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.776872/  1.880299, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.92 seconds, 4.57 minutes\n",
      "total_backward_count 1552600 real_backward_count 154142   9.928%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.775887/  1.878154, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.37 seconds, 4.54 minutes\n",
      "total_backward_count 1596960 real_backward_count 156685   9.811%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.769000/  1.864128, val:  86.67%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 268.22 seconds, 4.47 minutes\n",
      "total_backward_count 1641320 real_backward_count 159086   9.693%\n",
      "fc layer 2 self.abs_max_out: 3982.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.772741/  1.868757, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.64 seconds, 4.58 minutes\n",
      "total_backward_count 1685680 real_backward_count 161536   9.583%\n",
      "lif layer 1 self.abs_max_v: 15330.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.767667/  1.881318, val:  77.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.58 seconds, 4.53 minutes\n",
      "total_backward_count 1730040 real_backward_count 163978   9.478%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.759586/  1.873994, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.70 seconds, 4.56 minutes\n",
      "total_backward_count 1774400 real_backward_count 166174   9.365%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.755314/  1.861661, val:  83.75%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 272.83 seconds, 4.55 minutes\n",
      "total_backward_count 1818760 real_backward_count 168425   9.260%\n",
      "fc layer 1 self.abs_max_out: 10412.0\n",
      "lif layer 1 self.abs_max_v: 15348.5\n",
      "lif layer 1 self.abs_max_v: 15538.5\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.757258/  1.865836, val:  84.58%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 273.42 seconds, 4.56 minutes\n",
      "total_backward_count 1863120 real_backward_count 170622   9.158%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.762192/  1.873639, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.14 seconds, 4.55 minutes\n",
      "total_backward_count 1907480 real_backward_count 172789   9.058%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.756917/  1.859324, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.82 seconds, 4.50 minutes\n",
      "total_backward_count 1951840 real_backward_count 174865   8.959%\n",
      "fc layer 1 self.abs_max_out: 10421.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.749658/  1.856755, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.47 seconds, 4.52 minutes\n",
      "total_backward_count 1996200 real_backward_count 176983   8.866%\n",
      "fc layer 1 self.abs_max_out: 10434.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.749926/  1.862131, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.09 seconds, 4.55 minutes\n",
      "total_backward_count 2040560 real_backward_count 179081   8.776%\n",
      "fc layer 1 self.abs_max_out: 10439.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.737353/  1.866299, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.07 seconds, 4.57 minutes\n",
      "total_backward_count 2084920 real_backward_count 181080   8.685%\n",
      "fc layer 1 self.abs_max_out: 10458.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.732540/  1.838397, val:  85.83%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 268.68 seconds, 4.48 minutes\n",
      "total_backward_count 2129280 real_backward_count 183022   8.595%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.733977/  1.850191, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.34 seconds, 4.54 minutes\n",
      "total_backward_count 2173640 real_backward_count 185078   8.515%\n",
      "fc layer 1 self.abs_max_out: 10518.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.733515/  1.850075, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.08 seconds, 4.52 minutes\n",
      "total_backward_count 2218000 real_backward_count 187010   8.431%\n",
      "fc layer 1 self.abs_max_out: 10547.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.735932/  1.849574, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.83 seconds, 4.55 minutes\n",
      "total_backward_count 2262360 real_backward_count 188990   8.354%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.733847/  1.842991, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.43 seconds, 4.56 minutes\n",
      "total_backward_count 2306720 real_backward_count 190910   8.276%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.727355/  1.837781, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.98 seconds, 4.53 minutes\n",
      "total_backward_count 2351080 real_backward_count 192700   8.196%\n",
      "lif layer 2 self.abs_max_v: 4939.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.723779/  1.842504, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.92 seconds, 4.53 minutes\n",
      "total_backward_count 2395440 real_backward_count 194412   8.116%\n",
      "lif layer 2 self.abs_max_v: 4942.0\n",
      "fc layer 1 self.abs_max_out: 10602.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.724238/  1.843008, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.17 seconds, 4.52 minutes\n",
      "total_backward_count 2439800 real_backward_count 196088   8.037%\n",
      "lif layer 2 self.abs_max_v: 4961.0\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "lif layer 2 self.abs_max_v: 4966.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.724105/  1.851326, val:  80.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.03 seconds, 4.50 minutes\n",
      "total_backward_count 2484160 real_backward_count 197779   7.962%\n",
      "fc layer 1 self.abs_max_out: 10609.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.714685/  1.843773, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.87 seconds, 4.53 minutes\n",
      "total_backward_count 2528520 real_backward_count 199508   7.890%\n",
      "fc layer 1 self.abs_max_out: 10623.0\n",
      "lif layer 2 self.abs_max_v: 5087.5\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.716356/  1.826006, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.20 seconds, 4.57 minutes\n",
      "total_backward_count 2572880 real_backward_count 201263   7.822%\n",
      "fc layer 1 self.abs_max_out: 10631.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.714151/  1.834922, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.75 seconds, 4.50 minutes\n",
      "total_backward_count 2617240 real_backward_count 202926   7.753%\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "lif layer 2 self.abs_max_v: 5108.5\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.711370/  1.833656, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.71 seconds, 4.56 minutes\n",
      "total_backward_count 2661600 real_backward_count 204539   7.685%\n",
      "fc layer 1 self.abs_max_out: 10634.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.717225/  1.839432, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.90 seconds, 4.53 minutes\n",
      "total_backward_count 2705960 real_backward_count 206137   7.618%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.714053/  1.839022, val:  80.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.00 seconds, 4.53 minutes\n",
      "total_backward_count 2750320 real_backward_count 207738   7.553%\n",
      "fc layer 1 self.abs_max_out: 10700.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.708113/  1.833701, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.12 seconds, 4.55 minutes\n",
      "total_backward_count 2794680 real_backward_count 209298   7.489%\n",
      "fc layer 3 self.abs_max_out: 627.0\n",
      "fc layer 1 self.abs_max_out: 10723.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.713627/  1.844702, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.48 seconds, 4.54 minutes\n",
      "total_backward_count 2839040 real_backward_count 210872   7.428%\n",
      "lif layer 2 self.abs_max_v: 5152.5\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.709883/  1.822004, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.65 seconds, 4.54 minutes\n",
      "total_backward_count 2883400 real_backward_count 212384   7.366%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.709812/  1.824377, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.43 seconds, 4.52 minutes\n",
      "total_backward_count 2927760 real_backward_count 213876   7.305%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.712556/  1.826537, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.07 seconds, 4.50 minutes\n",
      "total_backward_count 2972120 real_backward_count 215399   7.247%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.715672/  1.839976, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.78 seconds, 4.56 minutes\n",
      "total_backward_count 3016480 real_backward_count 216880   7.190%\n",
      "fc layer 1 self.abs_max_out: 10725.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.701164/  1.815805, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.93 seconds, 4.53 minutes\n",
      "total_backward_count 3060840 real_backward_count 218248   7.130%\n",
      "fc layer 1 self.abs_max_out: 10754.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.692503/  1.822995, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.98 seconds, 4.48 minutes\n",
      "total_backward_count 3105200 real_backward_count 219687   7.075%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.696721/  1.817019, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.87 seconds, 4.53 minutes\n",
      "total_backward_count 3149560 real_backward_count 221051   7.018%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.683194/  1.801199, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.76 seconds, 4.51 minutes\n",
      "total_backward_count 3193920 real_backward_count 222360   6.962%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.688921/  1.809679, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.99 seconds, 4.55 minutes\n",
      "total_backward_count 3238280 real_backward_count 223719   6.909%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.676623/  1.802868, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.33 seconds, 4.54 minutes\n",
      "total_backward_count 3282640 real_backward_count 225010   6.855%\n",
      "fc layer 1 self.abs_max_out: 10761.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.674518/  1.799902, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.47 seconds, 4.54 minutes\n",
      "total_backward_count 3327000 real_backward_count 226371   6.804%\n",
      "fc layer 1 self.abs_max_out: 10765.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.672495/  1.808600, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.73 seconds, 4.56 minutes\n",
      "total_backward_count 3371360 real_backward_count 227717   6.754%\n",
      "fc layer 1 self.abs_max_out: 10815.0\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.673873/  1.800094, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.73 seconds, 4.53 minutes\n",
      "total_backward_count 3415720 real_backward_count 229016   6.705%\n",
      "fc layer 1 self.abs_max_out: 10841.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.664279/  1.780444, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.06 seconds, 4.52 minutes\n",
      "total_backward_count 3460080 real_backward_count 230262   6.655%\n",
      "fc layer 1 self.abs_max_out: 10907.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.655368/  1.793679, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.99 seconds, 4.55 minutes\n",
      "total_backward_count 3504440 real_backward_count 231509   6.606%\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 1 self.abs_max_out: 10932.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.665088/  1.790770, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.28 seconds, 4.55 minutes\n",
      "total_backward_count 3548800 real_backward_count 232766   6.559%\n",
      "fc layer 1 self.abs_max_out: 10942.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.659925/  1.787750, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.96 seconds, 4.48 minutes\n",
      "total_backward_count 3593160 real_backward_count 234002   6.512%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.656828/  1.782048, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.76 seconds, 4.53 minutes\n",
      "total_backward_count 3637520 real_backward_count 235202   6.466%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.659638/  1.789995, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.98 seconds, 4.53 minutes\n",
      "total_backward_count 3681880 real_backward_count 236342   6.419%\n",
      "fc layer 1 self.abs_max_out: 10949.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.642745/  1.778135, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.07 seconds, 4.55 minutes\n",
      "total_backward_count 3726240 real_backward_count 237420   6.372%\n",
      "fc layer 1 self.abs_max_out: 10962.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.645592/  1.775915, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.33 seconds, 4.57 minutes\n",
      "total_backward_count 3770600 real_backward_count 238545   6.326%\n",
      "fc layer 1 self.abs_max_out: 10983.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.643127/  1.787728, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.49 seconds, 4.54 minutes\n",
      "total_backward_count 3814960 real_backward_count 239621   6.281%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.648744/  1.786052, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.48 seconds, 4.52 minutes\n",
      "total_backward_count 3859320 real_backward_count 240659   6.236%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.637586/  1.758150, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.85 seconds, 4.55 minutes\n",
      "total_backward_count 3903680 real_backward_count 241647   6.190%\n",
      "fc layer 3 self.abs_max_out: 663.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.634948/  1.777636, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.30 seconds, 4.52 minutes\n",
      "total_backward_count 3948040 real_backward_count 242697   6.147%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.640039/  1.774201, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.57 seconds, 4.54 minutes\n",
      "total_backward_count 3992400 real_backward_count 243781   6.106%\n",
      "fc layer 1 self.abs_max_out: 11001.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.630671/  1.770420, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.50 seconds, 4.58 minutes\n",
      "total_backward_count 4036760 real_backward_count 244838   6.065%\n",
      "fc layer 1 self.abs_max_out: 11007.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.635649/  1.773885, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.77 seconds, 4.53 minutes\n",
      "total_backward_count 4081120 real_backward_count 245834   6.024%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.637698/  1.775288, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.70 seconds, 4.53 minutes\n",
      "total_backward_count 4125480 real_backward_count 246856   5.984%\n",
      "fc layer 1 self.abs_max_out: 11034.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.637047/  1.775174, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.52 seconds, 4.53 minutes\n",
      "total_backward_count 4169840 real_backward_count 247973   5.947%\n",
      "fc layer 1 self.abs_max_out: 11052.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.634380/  1.770406, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.79 seconds, 4.56 minutes\n",
      "total_backward_count 4214200 real_backward_count 249027   5.909%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.631316/  1.760976, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.20 seconds, 4.57 minutes\n",
      "total_backward_count 4258560 real_backward_count 250022   5.871%\n",
      "fc layer 1 self.abs_max_out: 11065.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.631560/  1.785756, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.88 seconds, 4.55 minutes\n",
      "total_backward_count 4302920 real_backward_count 251055   5.835%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.636742/  1.762840, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.38 seconds, 4.57 minutes\n",
      "total_backward_count 4347280 real_backward_count 252049   5.798%\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.627224/  1.766333, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.22 seconds, 4.55 minutes\n",
      "total_backward_count 4391640 real_backward_count 252973   5.760%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.622696/  1.767973, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.67 seconds, 4.53 minutes\n",
      "total_backward_count 4436000 real_backward_count 253901   5.724%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.626396/  1.765994, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.82 seconds, 4.58 minutes\n",
      "total_backward_count 4480360 real_backward_count 254915   5.690%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.629759/  1.764637, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.89 seconds, 4.56 minutes\n",
      "total_backward_count 4524720 real_backward_count 255816   5.654%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.629318/  1.763740, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.72 seconds, 4.53 minutes\n",
      "total_backward_count 4569080 real_backward_count 256765   5.620%\n",
      "fc layer 1 self.abs_max_out: 11113.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.622755/  1.770897, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.20 seconds, 4.52 minutes\n",
      "total_backward_count 4613440 real_backward_count 257642   5.585%\n",
      "fc layer 1 self.abs_max_out: 11118.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.625384/  1.760270, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.99 seconds, 4.53 minutes\n",
      "total_backward_count 4657800 real_backward_count 258527   5.550%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.613125/  1.738911, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.01 seconds, 4.55 minutes\n",
      "total_backward_count 4702160 real_backward_count 259332   5.515%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.603987/  1.747198, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.70 seconds, 4.56 minutes\n",
      "total_backward_count 4746520 real_backward_count 260197   5.482%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.603910/  1.759923, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.43 seconds, 4.52 minutes\n",
      "total_backward_count 4790880 real_backward_count 261103   5.450%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.620119/  1.756210, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.50 seconds, 4.56 minutes\n",
      "total_backward_count 4835240 real_backward_count 262019   5.419%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.620255/  1.760466, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.13 seconds, 4.54 minutes\n",
      "total_backward_count 4879600 real_backward_count 262857   5.387%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.619225/  1.751054, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.78 seconds, 4.50 minutes\n",
      "total_backward_count 4923960 real_backward_count 263699   5.355%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.614198/  1.753243, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.14 seconds, 4.55 minutes\n",
      "total_backward_count 4968320 real_backward_count 264509   5.324%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.614945/  1.739014, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.48 seconds, 4.59 minutes\n",
      "total_backward_count 5012680 real_backward_count 265357   5.294%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.606153/  1.752558, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.46 seconds, 4.57 minutes\n",
      "total_backward_count 5057040 real_backward_count 266143   5.263%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.598688/  1.734243, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.08 seconds, 4.50 minutes\n",
      "total_backward_count 5101400 real_backward_count 266973   5.233%\n",
      "fc layer 1 self.abs_max_out: 11126.0\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.596817/  1.730629, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.87 seconds, 4.55 minutes\n",
      "total_backward_count 5145760 real_backward_count 267767   5.204%\n",
      "fc layer 1 self.abs_max_out: 11132.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.604315/  1.747816, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.28 seconds, 4.55 minutes\n",
      "total_backward_count 5190120 real_backward_count 268611   5.175%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.601961/  1.744246, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.27 seconds, 4.55 minutes\n",
      "total_backward_count 5234480 real_backward_count 269402   5.147%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.598959/  1.740376, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.30 seconds, 4.59 minutes\n",
      "total_backward_count 5278840 real_backward_count 270228   5.119%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.607557/  1.747557, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.25 seconds, 4.59 minutes\n",
      "total_backward_count 5323200 real_backward_count 270977   5.090%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.595013/  1.743145, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.75 seconds, 4.53 minutes\n",
      "total_backward_count 5367560 real_backward_count 271732   5.062%\n",
      "fc layer 1 self.abs_max_out: 11155.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.597269/  1.735566, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.50 seconds, 4.49 minutes\n",
      "total_backward_count 5411920 real_backward_count 272488   5.035%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.592690/  1.741919, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.90 seconds, 4.55 minutes\n",
      "total_backward_count 5456280 real_backward_count 273246   5.008%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.589158/  1.736413, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.73 seconds, 4.60 minutes\n",
      "total_backward_count 5500640 real_backward_count 273959   4.980%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.592202/  1.742292, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.67 seconds, 4.54 minutes\n",
      "total_backward_count 5545000 real_backward_count 274631   4.953%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.587712/  1.747465, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.20 seconds, 4.50 minutes\n",
      "total_backward_count 5589360 real_backward_count 275367   4.927%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.592643/  1.750647, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.66 seconds, 4.54 minutes\n",
      "total_backward_count 5633720 real_backward_count 276090   4.901%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.596537/  1.741767, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.28 seconds, 4.57 minutes\n",
      "total_backward_count 5678080 real_backward_count 276830   4.875%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.593326/  1.741152, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.67 seconds, 4.56 minutes\n",
      "total_backward_count 5722440 real_backward_count 277572   4.851%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.591606/  1.745715, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.19 seconds, 4.55 minutes\n",
      "total_backward_count 5766800 real_backward_count 278290   4.826%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.596283/  1.739197, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.12 seconds, 4.55 minutes\n",
      "total_backward_count 5811160 real_backward_count 279026   4.802%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.595021/  1.742154, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.22 seconds, 4.54 minutes\n",
      "total_backward_count 5855520 real_backward_count 279744   4.777%\n",
      "fc layer 1 self.abs_max_out: 11164.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.594800/  1.735350, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.47 seconds, 4.51 minutes\n",
      "total_backward_count 5899880 real_backward_count 280452   4.754%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.591505/  1.735211, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.37 seconds, 4.57 minutes\n",
      "total_backward_count 5944240 real_backward_count 281146   4.730%\n",
      "fc layer 1 self.abs_max_out: 11166.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.590791/  1.737522, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.95 seconds, 4.57 minutes\n",
      "total_backward_count 5988600 real_backward_count 281825   4.706%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.584678/  1.726370, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.68 seconds, 4.56 minutes\n",
      "total_backward_count 6032960 real_backward_count 282417   4.681%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.580762/  1.724984, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.40 seconds, 4.49 minutes\n",
      "total_backward_count 6077320 real_backward_count 283083   4.658%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.567973/  1.723772, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.29 seconds, 4.55 minutes\n",
      "total_backward_count 6121680 real_backward_count 283701   4.634%\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.573434/  1.725952, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.86 seconds, 4.55 minutes\n",
      "total_backward_count 6166040 real_backward_count 284375   4.612%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.574888/  1.729296, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.93 seconds, 4.57 minutes\n",
      "total_backward_count 6210400 real_backward_count 284957   4.588%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.577597/  1.728861, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.97 seconds, 4.58 minutes\n",
      "total_backward_count 6254760 real_backward_count 285602   4.566%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.573636/  1.725391, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.74 seconds, 4.56 minutes\n",
      "total_backward_count 6299120 real_backward_count 286222   4.544%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.573822/  1.721667, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.99 seconds, 4.53 minutes\n",
      "total_backward_count 6343480 real_backward_count 286905   4.523%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.574263/  1.729953, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.83 seconds, 4.50 minutes\n",
      "total_backward_count 6387840 real_backward_count 287562   4.502%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.572725/  1.728169, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.87 seconds, 4.58 minutes\n",
      "total_backward_count 6432200 real_backward_count 288122   4.479%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.576916/  1.721097, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.34 seconds, 4.57 minutes\n",
      "total_backward_count 6476560 real_backward_count 288757   4.458%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.568927/  1.716826, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.65 seconds, 4.59 minutes\n",
      "total_backward_count 6520920 real_backward_count 289319   4.437%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.562745/  1.718433, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.11 seconds, 4.50 minutes\n",
      "total_backward_count 6565280 real_backward_count 289902   4.416%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.562715/  1.711682, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.92 seconds, 4.55 minutes\n",
      "total_backward_count 6609640 real_backward_count 290470   4.395%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.566607/  1.721309, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.72 seconds, 4.56 minutes\n",
      "total_backward_count 6654000 real_backward_count 291071   4.374%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.559349/  1.708915, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.79 seconds, 4.53 minutes\n",
      "total_backward_count 6698360 real_backward_count 291676   4.354%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.561434/  1.720788, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.99 seconds, 4.57 minutes\n",
      "total_backward_count 6742720 real_backward_count 292305   4.335%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.553276/  1.721306, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.57 seconds, 4.56 minutes\n",
      "total_backward_count 6787080 real_backward_count 292902   4.316%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.570833/  1.725548, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.56 seconds, 4.53 minutes\n",
      "total_backward_count 6831440 real_backward_count 293522   4.297%\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.561885/  1.716661, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.64 seconds, 4.49 minutes\n",
      "total_backward_count 6875800 real_backward_count 294102   4.277%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.564636/  1.726374, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.89 seconds, 4.56 minutes\n",
      "total_backward_count 6920160 real_backward_count 294620   4.257%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.560859/  1.726737, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.31 seconds, 4.57 minutes\n",
      "total_backward_count 6964520 real_backward_count 295184   4.238%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.563921/  1.722790, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.44 seconds, 4.56 minutes\n",
      "total_backward_count 7008880 real_backward_count 295791   4.220%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.569263/  1.731981, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.16 seconds, 4.49 minutes\n",
      "total_backward_count 7053240 real_backward_count 296378   4.202%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.573629/  1.726615, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.23 seconds, 4.54 minutes\n",
      "total_backward_count 7097600 real_backward_count 296946   4.184%\n",
      "fc layer 1 self.abs_max_out: 11176.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.572466/  1.727673, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.47 seconds, 4.54 minutes\n",
      "total_backward_count 7141960 real_backward_count 297510   4.166%\n",
      "fc layer 1 self.abs_max_out: 11201.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.576607/  1.732168, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.13 seconds, 4.57 minutes\n",
      "total_backward_count 7186320 real_backward_count 298122   4.148%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.569078/  1.723246, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.07 seconds, 4.53 minutes\n",
      "total_backward_count 7230680 real_backward_count 298630   4.130%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.561074/  1.721115, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.46 seconds, 4.56 minutes\n",
      "total_backward_count 7275040 real_backward_count 299104   4.111%\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.564262/  1.716631, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.09 seconds, 4.57 minutes\n",
      "total_backward_count 7319400 real_backward_count 299641   4.094%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.560107/  1.714388, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.75 seconds, 4.50 minutes\n",
      "total_backward_count 7363760 real_backward_count 300196   4.077%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.556391/  1.709246, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.74 seconds, 4.55 minutes\n",
      "total_backward_count 7408120 real_backward_count 300748   4.060%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.550229/  1.707603, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.72 seconds, 4.55 minutes\n",
      "total_backward_count 7452480 real_backward_count 301286   4.043%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.546934/  1.710027, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.22 seconds, 4.57 minutes\n",
      "total_backward_count 7496840 real_backward_count 301872   4.027%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.550317/  1.719115, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.42 seconds, 4.51 minutes\n",
      "total_backward_count 7541200 real_backward_count 302422   4.010%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.555557/  1.711531, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.65 seconds, 4.53 minutes\n",
      "total_backward_count 7585560 real_backward_count 303006   3.995%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.551145/  1.700663, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.33 seconds, 4.54 minutes\n",
      "total_backward_count 7629920 real_backward_count 303569   3.979%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.546301/  1.708646, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.69 seconds, 4.56 minutes\n",
      "total_backward_count 7674280 real_backward_count 304045   3.962%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.548630/  1.704639, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.64 seconds, 4.56 minutes\n",
      "total_backward_count 7718640 real_backward_count 304598   3.946%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.547668/  1.710132, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.03 seconds, 4.57 minutes\n",
      "total_backward_count 7763000 real_backward_count 305184   3.931%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.549238/  1.718617, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.99 seconds, 4.57 minutes\n",
      "total_backward_count 7807360 real_backward_count 305683   3.915%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.547604/  1.712569, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.82 seconds, 4.50 minutes\n",
      "total_backward_count 7851720 real_backward_count 306203   3.900%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.552856/  1.713114, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.17 seconds, 4.57 minutes\n",
      "total_backward_count 7896080 real_backward_count 306668   3.884%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.555643/  1.719521, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.01 seconds, 4.60 minutes\n",
      "total_backward_count 7940440 real_backward_count 307142   3.868%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.557944/  1.731168, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.53 seconds, 4.63 minutes\n",
      "total_backward_count 7984800 real_backward_count 307672   3.853%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.561535/  1.718595, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.08 seconds, 4.57 minutes\n",
      "total_backward_count 8029160 real_backward_count 308170   3.838%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.565528/  1.716886, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.72 seconds, 4.56 minutes\n",
      "total_backward_count 8073520 real_backward_count 308679   3.823%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.560248/  1.720709, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.66 seconds, 4.61 minutes\n",
      "total_backward_count 8117880 real_backward_count 309148   3.808%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.557352/  1.718524, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.45 seconds, 4.62 minutes\n",
      "total_backward_count 8162240 real_backward_count 309654   3.794%\n",
      "fc layer 1 self.abs_max_out: 11210.0\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.565115/  1.709766, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.87 seconds, 4.61 minutes\n",
      "total_backward_count 8206600 real_backward_count 310124   3.779%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.550290/  1.718198, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.06 seconds, 4.62 minutes\n",
      "total_backward_count 8250960 real_backward_count 310615   3.765%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.548283/  1.709316, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.17 seconds, 4.59 minutes\n",
      "total_backward_count 8295320 real_backward_count 311098   3.750%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.545940/  1.703627, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.90 seconds, 4.53 minutes\n",
      "total_backward_count 8339680 real_backward_count 311556   3.736%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.546542/  1.707450, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.75 seconds, 4.60 minutes\n",
      "total_backward_count 8384040 real_backward_count 312058   3.722%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.545948/  1.705471, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.52 seconds, 4.63 minutes\n",
      "total_backward_count 8428400 real_backward_count 312528   3.708%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.545572/  1.707248, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.29 seconds, 4.59 minutes\n",
      "total_backward_count 8472760 real_backward_count 312949   3.694%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.549865/  1.714949, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.81 seconds, 4.56 minutes\n",
      "total_backward_count 8517120 real_backward_count 313432   3.680%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.549097/  1.709050, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.87 seconds, 4.53 minutes\n",
      "total_backward_count 8561480 real_backward_count 313868   3.666%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.536565/  1.695632, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.97 seconds, 4.62 minutes\n",
      "total_backward_count 8605840 real_backward_count 314282   3.652%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.539490/  1.697676, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.33 seconds, 4.51 minutes\n",
      "total_backward_count 8650200 real_backward_count 314740   3.639%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.544829/  1.706870, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.31 seconds, 4.62 minutes\n",
      "total_backward_count 8694560 real_backward_count 315215   3.625%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.538556/  1.706250, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.44 seconds, 4.61 minutes\n",
      "total_backward_count 8738920 real_backward_count 315685   3.612%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.537947/  1.698134, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.39 seconds, 4.61 minutes\n",
      "total_backward_count 8783280 real_backward_count 316094   3.599%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.538679/  1.706671, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.62 seconds, 4.53 minutes\n",
      "total_backward_count 8827640 real_backward_count 316598   3.586%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.539796/  1.704521, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.10 seconds, 4.62 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71395fc6d1c9433fbc9ecfe5ed4e07f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.5398</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90417</td></tr><tr><td>val_loss</td><td>1.70452</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4q7jhaem' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4q7jhaem</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251008_205923-4q7jhaem/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: geu9o40i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 34382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251009_121218-geu9o40i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/geu9o40i' target=\"_blank\">young-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/geu9o40i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/geu9o40i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251009_121226_424', 'my_seed': 34382, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 566.0\n",
      "lif layer 1 self.abs_max_v: 566.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 140.0\n",
      "lif layer 2 self.abs_max_v: 140.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 590.5\n",
      "lif layer 2 self.abs_max_v: 178.5\n",
      "lif layer 1 self.abs_max_v: 596.0\n",
      "lif layer 2 self.abs_max_v: 189.0\n",
      "lif layer 1 self.abs_max_v: 654.5\n",
      "lif layer 2 self.abs_max_v: 202.5\n",
      "fc layer 1 self.abs_max_out: 567.0\n",
      "lif layer 1 self.abs_max_v: 813.0\n",
      "fc layer 2 self.abs_max_out: 304.0\n",
      "lif layer 2 self.abs_max_v: 278.0\n",
      "fc layer 1 self.abs_max_out: 686.0\n",
      "lif layer 1 self.abs_max_v: 844.5\n",
      "lif layer 2 self.abs_max_v: 344.5\n",
      "lif layer 1 self.abs_max_v: 924.5\n",
      "fc layer 2 self.abs_max_out: 426.0\n",
      "lif layer 2 self.abs_max_v: 519.5\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "fc layer 1 self.abs_max_out: 727.0\n",
      "lif layer 1 self.abs_max_v: 963.5\n",
      "fc layer 1 self.abs_max_out: 810.0\n",
      "lif layer 1 self.abs_max_v: 1001.0\n",
      "fc layer 1 self.abs_max_out: 1098.0\n",
      "lif layer 1 self.abs_max_v: 1098.0\n",
      "fc layer 2 self.abs_max_out: 535.0\n",
      "lif layer 2 self.abs_max_v: 665.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 1 self.abs_max_out: 1110.0\n",
      "lif layer 1 self.abs_max_v: 1126.5\n",
      "fc layer 2 self.abs_max_out: 537.0\n",
      "lif layer 2 self.abs_max_v: 698.5\n",
      "fc layer 2 self.abs_max_out: 639.0\n",
      "lif layer 2 self.abs_max_v: 722.5\n",
      "fc layer 3 self.abs_max_out: 84.0\n",
      "fc layer 1 self.abs_max_out: 1371.0\n",
      "lif layer 1 self.abs_max_v: 1371.0\n",
      "lif layer 2 self.abs_max_v: 749.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 1 self.abs_max_out: 1662.0\n",
      "lif layer 1 self.abs_max_v: 1662.0\n",
      "fc layer 2 self.abs_max_out: 717.0\n",
      "lif layer 2 self.abs_max_v: 989.0\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 2 self.abs_max_out: 743.0\n",
      "fc layer 2 self.abs_max_out: 842.0\n",
      "lif layer 2 self.abs_max_v: 1059.5\n",
      "fc layer 1 self.abs_max_out: 1781.0\n",
      "lif layer 1 self.abs_max_v: 1781.0\n",
      "fc layer 1 self.abs_max_out: 1821.0\n",
      "lif layer 1 self.abs_max_v: 1821.0\n",
      "lif layer 2 self.abs_max_v: 1133.5\n",
      "fc layer 2 self.abs_max_out: 849.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 1 self.abs_max_out: 1851.0\n",
      "lif layer 1 self.abs_max_v: 1851.0\n",
      "fc layer 2 self.abs_max_out: 971.0\n",
      "lif layer 2 self.abs_max_v: 1179.5\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 2 self.abs_max_v: 1293.5\n",
      "fc layer 2 self.abs_max_out: 977.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 2 self.abs_max_out: 1088.0\n",
      "fc layer 1 self.abs_max_out: 1868.0\n",
      "lif layer 1 self.abs_max_v: 1868.0\n",
      "lif layer 2 self.abs_max_v: 1319.5\n",
      "lif layer 2 self.abs_max_v: 1355.5\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "lif layer 2 self.abs_max_v: 1459.5\n",
      "lif layer 2 self.abs_max_v: 1518.0\n",
      "fc layer 2 self.abs_max_out: 1218.0\n",
      "lif layer 2 self.abs_max_v: 1642.0\n",
      "fc layer 2 self.abs_max_out: 1327.0\n",
      "fc layer 2 self.abs_max_out: 1330.0\n",
      "fc layer 2 self.abs_max_out: 1358.0\n",
      "fc layer 2 self.abs_max_out: 1534.0\n",
      "fc layer 1 self.abs_max_out: 2119.0\n",
      "lif layer 1 self.abs_max_v: 2119.0\n",
      "fc layer 1 self.abs_max_out: 2155.0\n",
      "lif layer 1 self.abs_max_v: 2155.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 2635.0\n",
      "lif layer 1 self.abs_max_v: 2635.0\n",
      "fc layer 1 self.abs_max_out: 2714.0\n",
      "lif layer 1 self.abs_max_v: 2714.0\n",
      "fc layer 1 self.abs_max_out: 2836.0\n",
      "lif layer 1 self.abs_max_v: 2836.0\n",
      "fc layer 2 self.abs_max_out: 1602.0\n",
      "fc layer 1 self.abs_max_out: 3008.0\n",
      "lif layer 1 self.abs_max_v: 3008.0\n",
      "fc layer 1 self.abs_max_out: 3332.0\n",
      "lif layer 1 self.abs_max_v: 3332.0\n",
      "fc layer 1 self.abs_max_out: 3407.0\n",
      "lif layer 1 self.abs_max_v: 3407.0\n",
      "lif layer 2 self.abs_max_v: 1694.5\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 2 self.abs_max_out: 1630.0\n",
      "lif layer 2 self.abs_max_v: 1841.0\n",
      "lif layer 2 self.abs_max_v: 1933.0\n",
      "fc layer 2 self.abs_max_out: 1725.0\n",
      "lif layer 2 self.abs_max_v: 1949.0\n",
      "lif layer 2 self.abs_max_v: 2022.0\n",
      "fc layer 2 self.abs_max_out: 1728.0\n",
      "lif layer 2 self.abs_max_v: 2058.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "lif layer 2 self.abs_max_v: 2079.0\n",
      "lif layer 2 self.abs_max_v: 2335.5\n",
      "fc layer 2 self.abs_max_out: 1735.0\n",
      "fc layer 2 self.abs_max_out: 1966.0\n",
      "fc layer 2 self.abs_max_out: 2034.0\n",
      "fc layer 2 self.abs_max_out: 2199.0\n",
      "fc layer 1 self.abs_max_out: 3643.0\n",
      "lif layer 1 self.abs_max_v: 3643.0\n",
      "fc layer 3 self.abs_max_out: 475.0\n",
      "lif layer 2 self.abs_max_v: 2554.0\n",
      "fc layer 1 self.abs_max_out: 3770.0\n",
      "lif layer 1 self.abs_max_v: 3770.0\n",
      "fc layer 1 self.abs_max_out: 3851.0\n",
      "lif layer 1 self.abs_max_v: 3851.0\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "fc layer 2 self.abs_max_out: 2287.0\n",
      "fc layer 2 self.abs_max_out: 2288.0\n",
      "lif layer 2 self.abs_max_v: 2592.0\n",
      "fc layer 2 self.abs_max_out: 2349.0\n",
      "lif layer 2 self.abs_max_v: 2596.5\n",
      "fc layer 1 self.abs_max_out: 3920.0\n",
      "lif layer 1 self.abs_max_v: 3920.0\n",
      "fc layer 1 self.abs_max_out: 4140.0\n",
      "lif layer 1 self.abs_max_v: 4140.0\n",
      "fc layer 2 self.abs_max_out: 2386.0\n",
      "fc layer 1 self.abs_max_out: 4364.0\n",
      "lif layer 1 self.abs_max_v: 4364.0\n",
      "fc layer 2 self.abs_max_out: 2472.0\n",
      "fc layer 2 self.abs_max_out: 2486.0\n",
      "lif layer 2 self.abs_max_v: 2726.0\n",
      "fc layer 2 self.abs_max_out: 2510.0\n",
      "fc layer 2 self.abs_max_out: 2543.0\n",
      "fc layer 2 self.abs_max_out: 2661.0\n",
      "fc layer 2 self.abs_max_out: 2672.0\n",
      "fc layer 2 self.abs_max_out: 2787.0\n",
      "lif layer 2 self.abs_max_v: 2787.0\n",
      "lif layer 2 self.abs_max_v: 2918.5\n",
      "fc layer 2 self.abs_max_out: 2817.0\n",
      "lif layer 2 self.abs_max_v: 2993.5\n",
      "fc layer 2 self.abs_max_out: 2864.0\n",
      "fc layer 1 self.abs_max_out: 4428.0\n",
      "lif layer 1 self.abs_max_v: 4428.0\n",
      "fc layer 1 self.abs_max_out: 4916.0\n",
      "lif layer 1 self.abs_max_v: 4916.0\n",
      "fc layer 2 self.abs_max_out: 3047.0\n",
      "lif layer 2 self.abs_max_v: 3047.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "fc layer 1 self.abs_max_out: 5116.0\n",
      "lif layer 1 self.abs_max_v: 5116.0\n",
      "fc layer 1 self.abs_max_out: 5137.0\n",
      "lif layer 1 self.abs_max_v: 5137.0\n",
      "fc layer 1 self.abs_max_out: 5752.0\n",
      "lif layer 1 self.abs_max_v: 5752.0\n",
      "lif layer 2 self.abs_max_v: 3066.5\n",
      "fc layer 2 self.abs_max_out: 3246.0\n",
      "lif layer 2 self.abs_max_v: 3246.0\n",
      "fc layer 1 self.abs_max_out: 6339.0\n",
      "lif layer 1 self.abs_max_v: 6339.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.019861/  2.106999, val:  58.33%, val_best:  58.33%, tr:  94.88%, tr_best:  94.88%, epoch time: 276.54 seconds, 4.61 minutes\n",
      "total_backward_count 44360 real_backward_count 10657  24.024%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 6419.0\n",
      "fc layer 1 self.abs_max_out: 6536.0\n",
      "lif layer 1 self.abs_max_v: 6536.0\n",
      "fc layer 1 self.abs_max_out: 6747.0\n",
      "lif layer 1 self.abs_max_v: 6747.0\n",
      "fc layer 2 self.abs_max_out: 3353.0\n",
      "lif layer 2 self.abs_max_v: 3353.0\n",
      "lif layer 1 self.abs_max_v: 6752.5\n",
      "lif layer 1 self.abs_max_v: 6943.5\n",
      "lif layer 2 self.abs_max_v: 3379.5\n",
      "fc layer 1 self.abs_max_out: 7072.0\n",
      "lif layer 1 self.abs_max_v: 7072.0\n",
      "lif layer 1 self.abs_max_v: 7126.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.004963/  2.091040, val:  48.75%, val_best:  58.33%, tr:  99.32%, tr_best:  99.32%, epoch time: 274.39 seconds, 4.57 minutes\n",
      "total_backward_count 88720 real_backward_count 18056  20.352%\n",
      "lif layer 2 self.abs_max_v: 3444.5\n",
      "fc layer 2 self.abs_max_out: 3366.0\n",
      "lif layer 2 self.abs_max_v: 3515.5\n",
      "fc layer 2 self.abs_max_out: 3488.0\n",
      "lif layer 2 self.abs_max_v: 3563.0\n",
      "lif layer 2 self.abs_max_v: 3592.0\n",
      "lif layer 2 self.abs_max_v: 3756.5\n",
      "fc layer 1 self.abs_max_out: 7085.0\n",
      "lif layer 2 self.abs_max_v: 3820.0\n",
      "lif layer 1 self.abs_max_v: 7320.0\n",
      "lif layer 1 self.abs_max_v: 7501.0\n",
      "lif layer 2 self.abs_max_v: 4023.5\n",
      "lif layer 2 self.abs_max_v: 4065.5\n",
      "fc layer 1 self.abs_max_out: 7362.0\n",
      "fc layer 2 self.abs_max_out: 3493.0\n",
      "fc layer 2 self.abs_max_out: 3507.0\n",
      "lif layer 1 self.abs_max_v: 7536.5\n",
      "lif layer 1 self.abs_max_v: 7740.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.994391/  2.073970, val:  55.83%, val_best:  58.33%, tr:  99.48%, tr_best:  99.48%, epoch time: 271.02 seconds, 4.52 minutes\n",
      "total_backward_count 133080 real_backward_count 24931  18.734%\n",
      "fc layer 1 self.abs_max_out: 7567.0\n",
      "lif layer 2 self.abs_max_v: 4094.0\n",
      "lif layer 2 self.abs_max_v: 4188.0\n",
      "lif layer 2 self.abs_max_v: 4194.5\n",
      "lif layer 2 self.abs_max_v: 4222.5\n",
      "fc layer 2 self.abs_max_out: 3514.0\n",
      "lif layer 2 self.abs_max_v: 4358.0\n",
      "lif layer 2 self.abs_max_v: 4394.0\n",
      "fc layer 2 self.abs_max_out: 3545.0\n",
      "fc layer 1 self.abs_max_out: 7735.0\n",
      "lif layer 1 self.abs_max_v: 8253.5\n",
      "lif layer 1 self.abs_max_v: 8432.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.981017/  2.064814, val:  66.67%, val_best:  66.67%, tr:  99.73%, tr_best:  99.73%, epoch time: 272.38 seconds, 4.54 minutes\n",
      "total_backward_count 177440 real_backward_count 31489  17.746%\n",
      "lif layer 2 self.abs_max_v: 4543.0\n",
      "fc layer 2 self.abs_max_out: 3577.0\n",
      "lif layer 2 self.abs_max_v: 4601.0\n",
      "fc layer 1 self.abs_max_out: 7856.0\n",
      "fc layer 1 self.abs_max_out: 7875.0\n",
      "fc layer 1 self.abs_max_out: 7928.0\n",
      "lif layer 2 self.abs_max_v: 4671.5\n",
      "fc layer 1 self.abs_max_out: 7986.0\n",
      "lif layer 2 self.abs_max_v: 4774.5\n",
      "lif layer 1 self.abs_max_v: 8565.5\n",
      "lif layer 1 self.abs_max_v: 9046.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.967348/  2.083138, val:  63.33%, val_best:  66.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 275.95 seconds, 4.60 minutes\n",
      "total_backward_count 221800 real_backward_count 37824  17.053%\n",
      "fc layer 1 self.abs_max_out: 8064.0\n",
      "lif layer 2 self.abs_max_v: 4896.0\n",
      "lif layer 1 self.abs_max_v: 9448.0\n",
      "lif layer 1 self.abs_max_v: 9537.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.959082/  2.026858, val:  59.17%, val_best:  66.67%, tr:  99.86%, tr_best:  99.86%, epoch time: 275.71 seconds, 4.60 minutes\n",
      "total_backward_count 266160 real_backward_count 43648  16.399%\n",
      "fc layer 1 self.abs_max_out: 8179.0\n",
      "fc layer 2 self.abs_max_out: 3578.0\n",
      "fc layer 3 self.abs_max_out: 506.0\n",
      "fc layer 1 self.abs_max_out: 8218.0\n",
      "fc layer 1 self.abs_max_out: 8246.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.937525/  2.030934, val:  68.33%, val_best:  68.33%, tr:  99.89%, tr_best:  99.89%, epoch time: 275.52 seconds, 4.59 minutes\n",
      "total_backward_count 310520 real_backward_count 49241  15.858%\n",
      "fc layer 1 self.abs_max_out: 8281.0\n",
      "lif layer 2 self.abs_max_v: 4916.0\n",
      "fc layer 1 self.abs_max_out: 8368.0\n",
      "fc layer 1 self.abs_max_out: 8429.0\n",
      "lif layer 1 self.abs_max_v: 9638.0\n",
      "lif layer 1 self.abs_max_v: 9656.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.930866/  2.020647, val:  76.25%, val_best:  76.25%, tr:  99.86%, tr_best:  99.89%, epoch time: 274.74 seconds, 4.58 minutes\n",
      "total_backward_count 354880 real_backward_count 54577  15.379%\n",
      "fc layer 1 self.abs_max_out: 8459.0\n",
      "lif layer 2 self.abs_max_v: 4946.5\n",
      "lif layer 2 self.abs_max_v: 4947.5\n",
      "lif layer 2 self.abs_max_v: 4956.0\n",
      "lif layer 1 self.abs_max_v: 9811.0\n",
      "lif layer 1 self.abs_max_v: 9894.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.917661/  1.999156, val:  80.83%, val_best:  80.83%, tr:  99.91%, tr_best:  99.91%, epoch time: 272.43 seconds, 4.54 minutes\n",
      "total_backward_count 399240 real_backward_count 59670  14.946%\n",
      "lif layer 2 self.abs_max_v: 5033.0\n",
      "lif layer 2 self.abs_max_v: 5167.5\n",
      "lif layer 2 self.abs_max_v: 5198.0\n",
      "lif layer 1 self.abs_max_v: 9995.5\n",
      "fc layer 1 self.abs_max_out: 8492.0\n",
      "fc layer 1 self.abs_max_out: 8551.0\n",
      "fc layer 1 self.abs_max_out: 8626.0\n",
      "fc layer 2 self.abs_max_out: 3636.0\n",
      "lif layer 1 self.abs_max_v: 10048.0\n",
      "lif layer 1 self.abs_max_v: 10107.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.918807/  1.995767, val:  76.67%, val_best:  80.83%, tr:  99.93%, tr_best:  99.93%, epoch time: 273.41 seconds, 4.56 minutes\n",
      "total_backward_count 443600 real_backward_count 64640  14.572%\n",
      "lif layer 2 self.abs_max_v: 5266.0\n",
      "lif layer 2 self.abs_max_v: 5346.5\n",
      "lif layer 1 self.abs_max_v: 10550.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.912654/  2.001091, val:  68.75%, val_best:  80.83%, tr:  99.93%, tr_best:  99.93%, epoch time: 275.55 seconds, 4.59 minutes\n",
      "total_backward_count 487960 real_backward_count 69486  14.240%\n",
      "fc layer 1 self.abs_max_out: 8698.0\n",
      "fc layer 2 self.abs_max_out: 3657.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.907829/  2.001966, val:  70.83%, val_best:  80.83%, tr:  99.98%, tr_best:  99.98%, epoch time: 275.17 seconds, 4.59 minutes\n",
      "total_backward_count 532320 real_backward_count 74128  13.925%\n",
      "fc layer 2 self.abs_max_out: 3750.0\n",
      "fc layer 1 self.abs_max_out: 8805.0\n",
      "fc layer 1 self.abs_max_out: 8883.0\n",
      "lif layer 1 self.abs_max_v: 10662.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.892686/  1.984546, val:  80.42%, val_best:  80.83%, tr:  99.91%, tr_best:  99.98%, epoch time: 275.14 seconds, 4.59 minutes\n",
      "total_backward_count 576680 real_backward_count 78651  13.639%\n",
      "fc layer 1 self.abs_max_out: 9022.0\n",
      "lif layer 1 self.abs_max_v: 10705.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.898976/  1.994619, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.58 seconds, 4.54 minutes\n",
      "total_backward_count 621040 real_backward_count 83050  13.373%\n",
      "fc layer 1 self.abs_max_out: 9137.0\n",
      "fc layer 2 self.abs_max_out: 3978.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.898064/  1.992550, val:  80.00%, val_best:  80.83%, tr:  99.95%, tr_best: 100.00%, epoch time: 274.53 seconds, 4.58 minutes\n",
      "total_backward_count 665400 real_backward_count 87299  13.120%\n",
      "fc layer 1 self.abs_max_out: 9237.0\n",
      "lif layer 1 self.abs_max_v: 10900.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.883138/  1.980409, val:  77.92%, val_best:  80.83%, tr:  99.95%, tr_best: 100.00%, epoch time: 275.07 seconds, 4.58 minutes\n",
      "total_backward_count 709760 real_backward_count 91504  12.892%\n",
      "lif layer 1 self.abs_max_v: 11245.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.882204/  1.980924, val:  71.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.63 seconds, 4.61 minutes\n",
      "total_backward_count 754120 real_backward_count 95460  12.658%\n",
      "fc layer 1 self.abs_max_out: 9307.0\n",
      "fc layer 3 self.abs_max_out: 508.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.875069/  1.958778, val:  81.67%, val_best:  81.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 274.01 seconds, 4.57 minutes\n",
      "total_backward_count 798480 real_backward_count 99234  12.428%\n",
      "lif layer 1 self.abs_max_v: 11282.5\n",
      "fc layer 1 self.abs_max_out: 9359.0\n",
      "fc layer 3 self.abs_max_out: 513.0\n",
      "fc layer 2 self.abs_max_out: 4040.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.864014/  1.959643, val:  74.58%, val_best:  81.67%, tr:  99.95%, tr_best: 100.00%, epoch time: 274.07 seconds, 4.57 minutes\n",
      "total_backward_count 842840 real_backward_count 103003  12.221%\n",
      "lif layer 1 self.abs_max_v: 11301.5\n",
      "fc layer 2 self.abs_max_out: 4187.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.856350/  1.943959, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.15 seconds, 4.54 minutes\n",
      "total_backward_count 887200 real_backward_count 106634  12.019%\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 1 self.abs_max_out: 9473.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.863487/  1.961934, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.67 seconds, 4.51 minutes\n",
      "total_backward_count 931560 real_backward_count 110163  11.826%\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 1 self.abs_max_out: 9519.0\n",
      "lif layer 1 self.abs_max_v: 11352.5\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.867230/  1.957602, val:  78.75%, val_best:  82.50%, tr:  99.98%, tr_best: 100.00%, epoch time: 271.01 seconds, 4.52 minutes\n",
      "total_backward_count 975920 real_backward_count 113577  11.638%\n",
      "fc layer 1 self.abs_max_out: 9644.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.850827/  1.946754, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.30 seconds, 4.56 minutes\n",
      "total_backward_count 1020280 real_backward_count 116882  11.456%\n",
      "fc layer 1 self.abs_max_out: 9651.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.849056/  1.938565, val:  83.75%, val_best:  83.75%, tr:  99.95%, tr_best: 100.00%, epoch time: 272.07 seconds, 4.53 minutes\n",
      "total_backward_count 1064640 real_backward_count 120081  11.279%\n",
      "fc layer 1 self.abs_max_out: 9736.0\n",
      "lif layer 1 self.abs_max_v: 11482.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.841177/  1.938648, val:  80.83%, val_best:  83.75%, tr:  99.95%, tr_best: 100.00%, epoch time: 270.69 seconds, 4.51 minutes\n",
      "total_backward_count 1109000 real_backward_count 123287  11.117%\n",
      "fc layer 1 self.abs_max_out: 9803.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.839295/  1.932143, val:  82.50%, val_best:  83.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 269.48 seconds, 4.49 minutes\n",
      "total_backward_count 1153360 real_backward_count 126270  10.948%\n",
      "fc layer 1 self.abs_max_out: 9836.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.826171/  1.928867, val:  85.00%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 272.91 seconds, 4.55 minutes\n",
      "total_backward_count 1197720 real_backward_count 129250  10.791%\n",
      "fc layer 1 self.abs_max_out: 9886.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "lif layer 1 self.abs_max_v: 11538.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.829723/  1.938641, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.71 seconds, 4.56 minutes\n",
      "total_backward_count 1242080 real_backward_count 132165  10.641%\n",
      "fc layer 1 self.abs_max_out: 9966.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.831703/  1.923630, val:  79.17%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 273.60 seconds, 4.56 minutes\n",
      "total_backward_count 1286440 real_backward_count 135105  10.502%\n",
      "lif layer 1 self.abs_max_v: 11986.5\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.820923/  1.918040, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.46 seconds, 4.56 minutes\n",
      "total_backward_count 1330800 real_backward_count 137922  10.364%\n",
      "fc layer 1 self.abs_max_out: 9992.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.811488/  1.920789, val:  85.00%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 270.90 seconds, 4.51 minutes\n",
      "total_backward_count 1375160 real_backward_count 140566  10.222%\n",
      "fc layer 1 self.abs_max_out: 10043.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.803000/  1.897156, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.32 seconds, 4.51 minutes\n",
      "total_backward_count 1419520 real_backward_count 143307  10.095%\n",
      "fc layer 1 self.abs_max_out: 10103.0\n",
      "lif layer 2 self.abs_max_v: 5497.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.801544/  1.909032, val:  88.33%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 270.59 seconds, 4.51 minutes\n",
      "total_backward_count 1463880 real_backward_count 145958   9.971%\n",
      "fc layer 1 self.abs_max_out: 10116.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.801180/  1.902444, val:  80.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.71 seconds, 4.55 minutes\n",
      "total_backward_count 1508240 real_backward_count 148528   9.848%\n",
      "fc layer 1 self.abs_max_out: 10230.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.795090/  1.900521, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.91 seconds, 4.58 minutes\n",
      "total_backward_count 1552600 real_backward_count 151132   9.734%\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.786446/  1.911387, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.18 seconds, 4.50 minutes\n",
      "total_backward_count 1596960 real_backward_count 153657   9.622%\n",
      "fc layer 1 self.abs_max_out: 10240.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.787666/  1.887923, val:  87.92%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 270.07 seconds, 4.50 minutes\n",
      "total_backward_count 1641320 real_backward_count 156117   9.512%\n",
      "fc layer 3 self.abs_max_out: 593.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.780634/  1.901444, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.07 seconds, 4.52 minutes\n",
      "total_backward_count 1685680 real_backward_count 158558   9.406%\n",
      "fc layer 1 self.abs_max_out: 10248.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.791996/  1.892931, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.47 seconds, 4.54 minutes\n",
      "total_backward_count 1730040 real_backward_count 160869   9.299%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.778536/  1.888664, val:  85.42%, val_best:  88.33%, tr:  99.95%, tr_best: 100.00%, epoch time: 273.69 seconds, 4.56 minutes\n",
      "total_backward_count 1774400 real_backward_count 163162   9.195%\n",
      "fc layer 1 self.abs_max_out: 10279.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.782034/  1.875593, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.60 seconds, 4.53 minutes\n",
      "total_backward_count 1818760 real_backward_count 165448   9.097%\n",
      "fc layer 1 self.abs_max_out: 10309.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.781613/  1.915362, val:  85.00%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%, epoch time: 270.73 seconds, 4.51 minutes\n",
      "total_backward_count 1863120 real_backward_count 167595   8.995%\n",
      "lif layer 2 self.abs_max_v: 5518.5\n",
      "lif layer 2 self.abs_max_v: 5532.0\n",
      "fc layer 1 self.abs_max_out: 10369.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.775922/  1.890776, val:  78.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.97 seconds, 4.48 minutes\n",
      "total_backward_count 1907480 real_backward_count 169715   8.897%\n",
      "lif layer 1 self.abs_max_v: 12183.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.767030/  1.889237, val:  82.50%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%, epoch time: 272.14 seconds, 4.54 minutes\n",
      "total_backward_count 1951840 real_backward_count 171875   8.806%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.768881/  1.873434, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.80 seconds, 4.53 minutes\n",
      "total_backward_count 1996200 real_backward_count 173957   8.714%\n",
      "fc layer 1 self.abs_max_out: 10386.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.766381/  1.886984, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.76 seconds, 4.53 minutes\n",
      "total_backward_count 2040560 real_backward_count 175975   8.624%\n",
      "fc layer 1 self.abs_max_out: 10422.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.764621/  1.879833, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.00 seconds, 4.52 minutes\n",
      "total_backward_count 2084920 real_backward_count 177979   8.536%\n",
      "fc layer 1 self.abs_max_out: 10482.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.749678/  1.856233, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.84 seconds, 4.48 minutes\n",
      "total_backward_count 2129280 real_backward_count 179869   8.447%\n",
      "fc layer 1 self.abs_max_out: 10535.0\n",
      "fc layer 3 self.abs_max_out: 594.0\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.749827/  1.868440, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.86 seconds, 4.56 minutes\n",
      "total_backward_count 2173640 real_backward_count 181744   8.361%\n",
      "fc layer 1 self.abs_max_out: 10554.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.747657/  1.863846, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.60 seconds, 4.54 minutes\n",
      "total_backward_count 2218000 real_backward_count 183627   8.279%\n",
      "fc layer 1 self.abs_max_out: 10589.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.747926/  1.869680, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.25 seconds, 4.57 minutes\n",
      "total_backward_count 2262360 real_backward_count 185447   8.197%\n",
      "fc layer 1 self.abs_max_out: 10626.0\n",
      "fc layer 3 self.abs_max_out: 604.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.747054/  1.851682, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.48 seconds, 4.56 minutes\n",
      "total_backward_count 2306720 real_backward_count 187227   8.117%\n",
      "fc layer 1 self.abs_max_out: 10655.0\n",
      "lif layer 1 self.abs_max_v: 12479.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.737235/  1.858334, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.52 seconds, 4.53 minutes\n",
      "total_backward_count 2351080 real_backward_count 189084   8.042%\n",
      "lif layer 1 self.abs_max_v: 12526.5\n",
      "fc layer 1 self.abs_max_out: 10686.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.734674/  1.855079, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.89 seconds, 4.50 minutes\n",
      "total_backward_count 2395440 real_backward_count 190907   7.970%\n",
      "fc layer 3 self.abs_max_out: 605.0\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.736248/  1.846796, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.69 seconds, 4.56 minutes\n",
      "total_backward_count 2439800 real_backward_count 192552   7.892%\n",
      "fc layer 1 self.abs_max_out: 10695.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.735332/  1.851239, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.51 seconds, 4.58 minutes\n",
      "total_backward_count 2484160 real_backward_count 194126   7.815%\n",
      "fc layer 1 self.abs_max_out: 10697.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.736630/  1.859356, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.33 seconds, 4.56 minutes\n",
      "total_backward_count 2528520 real_backward_count 195766   7.742%\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "fc layer 1 self.abs_max_out: 10704.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.728248/  1.845854, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.40 seconds, 4.54 minutes\n",
      "total_backward_count 2572880 real_backward_count 197410   7.673%\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.731285/  1.856725, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.69 seconds, 4.46 minutes\n",
      "total_backward_count 2617240 real_backward_count 199014   7.604%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.732925/  1.838349, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.14 seconds, 4.52 minutes\n",
      "total_backward_count 2661600 real_backward_count 200652   7.539%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.719287/  1.846648, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.05 seconds, 4.53 minutes\n",
      "total_backward_count 2705960 real_backward_count 202207   7.473%\n",
      "fc layer 1 self.abs_max_out: 10725.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.719984/  1.842321, val:  81.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.01 seconds, 4.55 minutes\n",
      "total_backward_count 2750320 real_backward_count 203753   7.408%\n",
      "fc layer 3 self.abs_max_out: 657.0\n",
      "fc layer 1 self.abs_max_out: 10767.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.714705/  1.833160, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.62 seconds, 4.56 minutes\n",
      "total_backward_count 2794680 real_backward_count 205144   7.341%\n",
      "fc layer 1 self.abs_max_out: 10796.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.711448/  1.826860, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.15 seconds, 4.54 minutes\n",
      "total_backward_count 2839040 real_backward_count 206643   7.279%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.715614/  1.838698, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.78 seconds, 4.46 minutes\n",
      "total_backward_count 2883400 real_backward_count 208067   7.216%\n",
      "fc layer 1 self.abs_max_out: 10805.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.717139/  1.838190, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.73 seconds, 4.56 minutes\n",
      "total_backward_count 2927760 real_backward_count 209597   7.159%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.715056/  1.838311, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.45 seconds, 4.54 minutes\n",
      "total_backward_count 2972120 real_backward_count 211025   7.100%\n",
      "fc layer 1 self.abs_max_out: 10842.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.711519/  1.831843, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.15 seconds, 4.54 minutes\n",
      "total_backward_count 3016480 real_backward_count 212459   7.043%\n",
      "fc layer 1 self.abs_max_out: 10852.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.704211/  1.833919, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.99 seconds, 4.53 minutes\n",
      "total_backward_count 3060840 real_backward_count 213877   6.988%\n",
      "fc layer 1 self.abs_max_out: 10861.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.702610/  1.828232, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.69 seconds, 4.48 minutes\n",
      "total_backward_count 3105200 real_backward_count 215343   6.935%\n",
      "fc layer 1 self.abs_max_out: 10876.0\n",
      "lif layer 1 self.abs_max_v: 12567.5\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.707460/  1.830830, val:  84.58%, val_best:  89.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 273.26 seconds, 4.55 minutes\n",
      "total_backward_count 3149560 real_backward_count 216647   6.879%\n",
      "fc layer 1 self.abs_max_out: 10881.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.705034/  1.833098, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.73 seconds, 4.55 minutes\n",
      "total_backward_count 3193920 real_backward_count 217989   6.825%\n",
      "lif layer 1 self.abs_max_v: 13048.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.706800/  1.836845, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.39 seconds, 4.56 minutes\n",
      "total_backward_count 3238280 real_backward_count 219393   6.775%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.701047/  1.832869, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.76 seconds, 4.56 minutes\n",
      "total_backward_count 3282640 real_backward_count 220708   6.723%\n",
      "fc layer 1 self.abs_max_out: 10882.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.698657/  1.817914, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.35 seconds, 4.52 minutes\n",
      "total_backward_count 3327000 real_backward_count 222122   6.676%\n",
      "fc layer 1 self.abs_max_out: 10897.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.695666/  1.804427, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.87 seconds, 4.48 minutes\n",
      "total_backward_count 3371360 real_backward_count 223336   6.625%\n",
      "fc layer 1 self.abs_max_out: 10915.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.685741/  1.818243, val:  86.25%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 273.36 seconds, 4.56 minutes\n",
      "total_backward_count 3415720 real_backward_count 224510   6.573%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.690750/  1.814747, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.50 seconds, 4.52 minutes\n",
      "total_backward_count 3460080 real_backward_count 225732   6.524%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.687593/  1.820284, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.72 seconds, 4.56 minutes\n",
      "total_backward_count 3504440 real_backward_count 226969   6.477%\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.682809/  1.808798, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.00 seconds, 4.53 minutes\n",
      "total_backward_count 3548800 real_backward_count 228160   6.429%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.679542/  1.810931, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.20 seconds, 4.45 minutes\n",
      "total_backward_count 3593160 real_backward_count 229311   6.382%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.683051/  1.808118, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.52 seconds, 4.56 minutes\n",
      "total_backward_count 3637520 real_backward_count 230442   6.335%\n",
      "fc layer 1 self.abs_max_out: 10931.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.678268/  1.809467, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.97 seconds, 4.57 minutes\n",
      "total_backward_count 3681880 real_backward_count 231562   6.289%\n",
      "fc layer 1 self.abs_max_out: 10959.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.677946/  1.798697, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.27 seconds, 4.54 minutes\n",
      "total_backward_count 3726240 real_backward_count 232648   6.244%\n",
      "fc layer 1 self.abs_max_out: 10983.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.663828/  1.825908, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.49 seconds, 4.54 minutes\n",
      "total_backward_count 3770600 real_backward_count 233744   6.199%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.674490/  1.800835, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.15 seconds, 4.54 minutes\n",
      "total_backward_count 3814960 real_backward_count 234861   6.156%\n",
      "fc layer 1 self.abs_max_out: 11016.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.670949/  1.797131, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.03 seconds, 4.48 minutes\n",
      "total_backward_count 3859320 real_backward_count 235870   6.112%\n",
      "fc layer 1 self.abs_max_out: 11017.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.679341/  1.806351, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.77 seconds, 4.55 minutes\n",
      "total_backward_count 3903680 real_backward_count 236958   6.070%\n",
      "fc layer 1 self.abs_max_out: 11018.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.677344/  1.796665, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.60 seconds, 4.54 minutes\n",
      "total_backward_count 3948040 real_backward_count 237939   6.027%\n",
      "fc layer 1 self.abs_max_out: 11020.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.659447/  1.794858, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.91 seconds, 4.53 minutes\n",
      "total_backward_count 3992400 real_backward_count 238948   5.985%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.667090/  1.793765, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.60 seconds, 4.54 minutes\n",
      "total_backward_count 4036760 real_backward_count 239949   5.944%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.665018/  1.801420, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.16 seconds, 4.47 minutes\n",
      "total_backward_count 4081120 real_backward_count 241040   5.906%\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "fc layer 1 self.abs_max_out: 11025.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.668632/  1.798378, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.91 seconds, 4.53 minutes\n",
      "total_backward_count 4125480 real_backward_count 242020   5.866%\n",
      "fc layer 1 self.abs_max_out: 11053.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.660852/  1.777903, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.38 seconds, 4.52 minutes\n",
      "total_backward_count 4169840 real_backward_count 242925   5.826%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.661991/  1.802090, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.10 seconds, 4.55 minutes\n",
      "total_backward_count 4214200 real_backward_count 243916   5.788%\n",
      "fc layer 1 self.abs_max_out: 11065.0\n",
      "lif layer 1 self.abs_max_v: 13116.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.663551/  1.805552, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.77 seconds, 4.58 minutes\n",
      "total_backward_count 4258560 real_backward_count 244815   5.749%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.659772/  1.792572, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.79 seconds, 4.53 minutes\n",
      "total_backward_count 4302920 real_backward_count 245728   5.711%\n",
      "fc layer 1 self.abs_max_out: 11118.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.661103/  1.800868, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.78 seconds, 4.55 minutes\n",
      "total_backward_count 4347280 real_backward_count 246657   5.674%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.669932/  1.800099, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.83 seconds, 4.58 minutes\n",
      "total_backward_count 4391640 real_backward_count 247629   5.639%\n",
      "fc layer 1 self.abs_max_out: 11138.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.663233/  1.802112, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.02 seconds, 4.57 minutes\n",
      "total_backward_count 4436000 real_backward_count 248527   5.603%\n",
      "fc layer 1 self.abs_max_out: 11149.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.653112/  1.781083, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.73 seconds, 4.60 minutes\n",
      "total_backward_count 4480360 real_backward_count 249460   5.568%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.648630/  1.793067, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.48 seconds, 4.59 minutes\n",
      "total_backward_count 4524720 real_backward_count 250382   5.534%\n",
      "fc layer 1 self.abs_max_out: 11154.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.643689/  1.786685, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.93 seconds, 4.52 minutes\n",
      "total_backward_count 4569080 real_backward_count 251262   5.499%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.654108/  1.803737, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.01 seconds, 4.57 minutes\n",
      "total_backward_count 4613440 real_backward_count 252159   5.466%\n",
      "fc layer 1 self.abs_max_out: 11188.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.652652/  1.798115, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 277.01 seconds, 4.62 minutes\n",
      "total_backward_count 4657800 real_backward_count 253074   5.433%\n",
      "fc layer 1 self.abs_max_out: 11198.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.657216/  1.789699, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.97 seconds, 4.58 minutes\n",
      "total_backward_count 4702160 real_backward_count 253955   5.401%\n",
      "fc layer 1 self.abs_max_out: 11221.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.658052/  1.796471, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.93 seconds, 4.58 minutes\n",
      "total_backward_count 4746520 real_backward_count 254797   5.368%\n",
      "fc layer 1 self.abs_max_out: 11227.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.646639/  1.784603, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.80 seconds, 4.60 minutes\n",
      "total_backward_count 4790880 real_backward_count 255608   5.335%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.653165/  1.792157, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.87 seconds, 4.55 minutes\n",
      "total_backward_count 4835240 real_backward_count 256530   5.305%\n",
      "fc layer 1 self.abs_max_out: 11248.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.650068/  1.788442, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.95 seconds, 4.62 minutes\n",
      "total_backward_count 4879600 real_backward_count 257361   5.274%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.644861/  1.792646, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.44 seconds, 4.59 minutes\n",
      "total_backward_count 4923960 real_backward_count 258207   5.244%\n",
      "fc layer 1 self.abs_max_out: 11269.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.639519/  1.782128, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.08 seconds, 4.58 minutes\n",
      "total_backward_count 4968320 real_backward_count 259059   5.214%\n",
      "fc layer 1 self.abs_max_out: 11285.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.640359/  1.782972, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.99 seconds, 4.57 minutes\n",
      "total_backward_count 5012680 real_backward_count 259936   5.186%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.641080/  1.776871, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.81 seconds, 4.53 minutes\n",
      "total_backward_count 5057040 real_backward_count 260740   5.156%\n",
      "fc layer 1 self.abs_max_out: 11329.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.638213/  1.784834, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.54 seconds, 4.58 minutes\n",
      "total_backward_count 5101400 real_backward_count 261498   5.126%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.642991/  1.795030, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.40 seconds, 4.59 minutes\n",
      "total_backward_count 5145760 real_backward_count 262313   5.098%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.644737/  1.784611, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.07 seconds, 4.60 minutes\n",
      "total_backward_count 5190120 real_backward_count 263089   5.069%\n",
      "fc layer 3 self.abs_max_out: 703.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.638176/  1.781684, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.10 seconds, 4.60 minutes\n",
      "total_backward_count 5234480 real_backward_count 263900   5.042%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.639089/  1.782070, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.97 seconds, 4.58 minutes\n",
      "total_backward_count 5278840 real_backward_count 264740   5.015%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.645689/  1.782539, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.34 seconds, 4.52 minutes\n",
      "total_backward_count 5323200 real_backward_count 265534   4.988%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.636185/  1.781484, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.24 seconds, 4.60 minutes\n",
      "total_backward_count 5367560 real_backward_count 266368   4.963%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.634503/  1.763407, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.27 seconds, 4.59 minutes\n",
      "total_backward_count 5411920 real_backward_count 267113   4.936%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.623294/  1.769206, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.32 seconds, 4.59 minutes\n",
      "total_backward_count 5456280 real_backward_count 267922   4.910%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.617208/  1.766690, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.66 seconds, 4.56 minutes\n",
      "total_backward_count 5500640 real_backward_count 268667   4.884%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.614450/  1.765416, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.70 seconds, 4.56 minutes\n",
      "total_backward_count 5545000 real_backward_count 269367   4.858%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.612846/  1.757122, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.98 seconds, 4.52 minutes\n",
      "total_backward_count 5589360 real_backward_count 270032   4.831%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.618572/  1.772754, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.51 seconds, 4.58 minutes\n",
      "total_backward_count 5633720 real_backward_count 270795   4.807%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.616305/  1.761314, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.79 seconds, 4.58 minutes\n",
      "total_backward_count 5678080 real_backward_count 271520   4.782%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.612166/  1.762239, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.85 seconds, 4.60 minutes\n",
      "total_backward_count 5722440 real_backward_count 272231   4.757%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.599349/  1.755988, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 276.28 seconds, 4.60 minutes\n",
      "total_backward_count 5766800 real_backward_count 272894   4.732%\n",
      "lif layer 2 self.abs_max_v: 5564.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.606669/  1.775385, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.99 seconds, 4.52 minutes\n",
      "total_backward_count 5811160 real_backward_count 273527   4.707%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.610138/  1.750699, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.50 seconds, 4.59 minutes\n",
      "total_backward_count 5855520 real_backward_count 274144   4.682%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.605535/  1.755302, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.92 seconds, 4.58 minutes\n",
      "total_backward_count 5899880 real_backward_count 274823   4.658%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.600222/  1.744437, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 275.34 seconds, 4.59 minutes\n",
      "total_backward_count 5944240 real_backward_count 275473   4.634%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.600083/  1.747115, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.27 seconds, 4.55 minutes\n",
      "total_backward_count 5988600 real_backward_count 276148   4.611%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.600237/  1.756837, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.37 seconds, 4.57 minutes\n",
      "total_backward_count 6032960 real_backward_count 276827   4.589%\n",
      "fc layer 3 self.abs_max_out: 707.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.595947/  1.750831, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.58 seconds, 4.53 minutes\n",
      "total_backward_count 6077320 real_backward_count 277508   4.566%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.590877/  1.740843, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.95 seconds, 4.58 minutes\n",
      "total_backward_count 6121680 real_backward_count 278124   4.543%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.594102/  1.743561, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.02 seconds, 4.57 minutes\n",
      "total_backward_count 6166040 real_backward_count 278802   4.522%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.589036/  1.743521, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.14 seconds, 4.54 minutes\n",
      "total_backward_count 6210400 real_backward_count 279406   4.499%\n",
      "fc layer 2 self.abs_max_out: 4220.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.590519/  1.741065, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.42 seconds, 4.56 minutes\n",
      "total_backward_count 6254760 real_backward_count 280043   4.477%\n",
      "lif layer 1 self.abs_max_v: 13219.5\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.588003/  1.749685, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.85 seconds, 4.48 minutes\n",
      "total_backward_count 6299120 real_backward_count 280696   4.456%\n",
      "lif layer 1 self.abs_max_v: 13321.5\n",
      "lif layer 1 self.abs_max_v: 13564.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.582732/  1.740691, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.87 seconds, 4.55 minutes\n",
      "total_backward_count 6343480 real_backward_count 281303   4.435%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.582726/  1.737507, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.89 seconds, 4.53 minutes\n",
      "total_backward_count 6387840 real_backward_count 281865   4.413%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.577213/  1.733808, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.56 seconds, 4.53 minutes\n",
      "total_backward_count 6432200 real_backward_count 282510   4.392%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.576999/  1.739730, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.66 seconds, 4.54 minutes\n",
      "total_backward_count 6476560 real_backward_count 283149   4.372%\n",
      "fc layer 3 self.abs_max_out: 723.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.570763/  1.723823, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.04 seconds, 4.53 minutes\n",
      "total_backward_count 6520920 real_backward_count 283808   4.352%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.569938/  1.740137, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.35 seconds, 4.47 minutes\n",
      "total_backward_count 6565280 real_backward_count 284406   4.332%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.574791/  1.731300, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.07 seconds, 4.53 minutes\n",
      "total_backward_count 6609640 real_backward_count 284965   4.311%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.578887/  1.735441, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.24 seconds, 4.54 minutes\n",
      "total_backward_count 6654000 real_backward_count 285538   4.291%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.576482/  1.735318, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.53 seconds, 4.51 minutes\n",
      "total_backward_count 6698360 real_backward_count 286115   4.271%\n",
      "lif layer 2 self.abs_max_v: 5656.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.572450/  1.729212, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.93 seconds, 4.55 minutes\n",
      "total_backward_count 6742720 real_backward_count 286729   4.252%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.571689/  1.721722, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.21 seconds, 4.47 minutes\n",
      "total_backward_count 6787080 real_backward_count 287371   4.234%\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "lif layer 1 self.abs_max_v: 13652.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.563551/  1.717595, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.31 seconds, 4.57 minutes\n",
      "total_backward_count 6831440 real_backward_count 287970   4.215%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.556929/  1.718182, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.39 seconds, 4.54 minutes\n",
      "total_backward_count 6875800 real_backward_count 288498   4.196%\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.557229/  1.705718, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.98 seconds, 4.55 minutes\n",
      "total_backward_count 6920160 real_backward_count 289074   4.177%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.549245/  1.721392, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.22 seconds, 4.52 minutes\n",
      "total_backward_count 6964520 real_backward_count 289682   4.159%\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.557823/  1.730537, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.45 seconds, 4.54 minutes\n",
      "total_backward_count 7008880 real_backward_count 290196   4.140%\n",
      "lif layer 1 self.abs_max_v: 13796.5\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.559494/  1.727028, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.59 seconds, 4.48 minutes\n",
      "total_backward_count 7053240 real_backward_count 290804   4.123%\n",
      "fc layer 1 self.abs_max_out: 11335.0\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.557624/  1.713166, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.08 seconds, 4.57 minutes\n",
      "total_backward_count 7097600 real_backward_count 291334   4.105%\n",
      "fc layer 1 self.abs_max_out: 11352.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.553879/  1.718989, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.71 seconds, 4.58 minutes\n",
      "total_backward_count 7141960 real_backward_count 291847   4.086%\n",
      "fc layer 3 self.abs_max_out: 740.0\n",
      "fc layer 1 self.abs_max_out: 11375.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.554430/  1.716920, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.29 seconds, 4.55 minutes\n",
      "total_backward_count 7186320 real_backward_count 292411   4.069%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.555637/  1.726483, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.90 seconds, 4.55 minutes\n",
      "total_backward_count 7230680 real_backward_count 292941   4.051%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.557664/  1.716798, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.39 seconds, 4.47 minutes\n",
      "total_backward_count 7275040 real_backward_count 293535   4.035%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.557476/  1.723049, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.93 seconds, 4.55 minutes\n",
      "total_backward_count 7319400 real_backward_count 294076   4.018%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.550946/  1.711621, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.23 seconds, 4.54 minutes\n",
      "total_backward_count 7363760 real_backward_count 294615   4.001%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.554655/  1.724740, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.27 seconds, 4.54 minutes\n",
      "total_backward_count 7408120 real_backward_count 295179   3.985%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.559571/  1.719143, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.04 seconds, 4.52 minutes\n",
      "total_backward_count 7452480 real_backward_count 295760   3.969%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.555623/  1.718428, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.84 seconds, 4.51 minutes\n",
      "total_backward_count 7496840 real_backward_count 296268   3.952%\n",
      "fc layer 3 self.abs_max_out: 747.0\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.551146/  1.709887, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.52 seconds, 4.44 minutes\n",
      "total_backward_count 7541200 real_backward_count 296862   3.937%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.547840/  1.712270, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.08 seconds, 4.55 minutes\n",
      "total_backward_count 7585560 real_backward_count 297379   3.920%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.548082/  1.710110, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.65 seconds, 4.54 minutes\n",
      "total_backward_count 7629920 real_backward_count 297891   3.904%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.538086/  1.698841, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.41 seconds, 4.52 minutes\n",
      "total_backward_count 7674280 real_backward_count 298392   3.888%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.535532/  1.698875, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.52 seconds, 4.56 minutes\n",
      "total_backward_count 7718640 real_backward_count 298904   3.872%\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.536380/  1.702707, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.85 seconds, 4.48 minutes\n",
      "total_backward_count 7763000 real_backward_count 299396   3.857%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.534030/  1.699837, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.27 seconds, 4.52 minutes\n",
      "total_backward_count 7807360 real_backward_count 299941   3.842%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.535051/  1.692198, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.52 seconds, 4.54 minutes\n",
      "total_backward_count 7851720 real_backward_count 300446   3.826%\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.527934/  1.703302, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.34 seconds, 4.54 minutes\n",
      "total_backward_count 7896080 real_backward_count 301009   3.812%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.533593/  1.701421, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.88 seconds, 4.53 minutes\n",
      "total_backward_count 7940440 real_backward_count 301465   3.797%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.538404/  1.708887, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.54 seconds, 4.56 minutes\n",
      "total_backward_count 7984800 real_backward_count 301977   3.782%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.534833/  1.698774, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.22 seconds, 4.47 minutes\n",
      "total_backward_count 8029160 real_backward_count 302506   3.768%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.534272/  1.703378, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.21 seconds, 4.54 minutes\n",
      "total_backward_count 8073520 real_backward_count 302987   3.753%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.534335/  1.701692, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.33 seconds, 4.56 minutes\n",
      "total_backward_count 8117880 real_backward_count 303514   3.739%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.536343/  1.706852, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.83 seconds, 4.51 minutes\n",
      "total_backward_count 8162240 real_backward_count 303966   3.724%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.538949/  1.703714, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.45 seconds, 4.54 minutes\n",
      "total_backward_count 8206600 real_backward_count 304472   3.710%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.539668/  1.711368, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.01 seconds, 4.48 minutes\n",
      "total_backward_count 8250960 real_backward_count 304937   3.696%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.532261/  1.696201, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.03 seconds, 4.57 minutes\n",
      "total_backward_count 8295320 real_backward_count 305432   3.682%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.529613/  1.696665, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.18 seconds, 4.54 minutes\n",
      "total_backward_count 8339680 real_backward_count 305990   3.669%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.530524/  1.705899, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.90 seconds, 4.53 minutes\n",
      "total_backward_count 8384040 real_backward_count 306415   3.655%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.529186/  1.695478, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.54 seconds, 4.43 minutes\n",
      "total_backward_count 8428400 real_backward_count 306852   3.641%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.524101/  1.701289, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.65 seconds, 4.53 minutes\n",
      "total_backward_count 8472760 real_backward_count 307299   3.627%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.529142/  1.691180, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.16 seconds, 4.47 minutes\n",
      "total_backward_count 8517120 real_backward_count 307787   3.614%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.527686/  1.707025, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.13 seconds, 4.54 minutes\n",
      "total_backward_count 8561480 real_backward_count 308240   3.600%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.531814/  1.702493, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.95 seconds, 4.55 minutes\n",
      "total_backward_count 8605840 real_backward_count 308654   3.587%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.527179/  1.698818, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.88 seconds, 4.55 minutes\n",
      "total_backward_count 8650200 real_backward_count 309106   3.573%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.526037/  1.696406, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.10 seconds, 4.55 minutes\n",
      "total_backward_count 8694560 real_backward_count 309528   3.560%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.523763/  1.696003, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.92 seconds, 4.48 minutes\n",
      "total_backward_count 8738920 real_backward_count 309984   3.547%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.516912/  1.698570, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.52 seconds, 4.54 minutes\n",
      "total_backward_count 8783280 real_backward_count 310475   3.535%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.524063/  1.699767, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.00 seconds, 4.53 minutes\n",
      "total_backward_count 8827640 real_backward_count 310955   3.523%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.521347/  1.688936, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.52 seconds, 4.58 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4faaacf8f7942818f2473a8adce2740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.52135</td></tr><tr><td>val_acc_best</td><td>0.92083</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>1.68894</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/geu9o40i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/geu9o40i</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251009_121218-geu9o40i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: amz50fe3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 26027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251010_032234-amz50fe3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/amz50fe3' target=\"_blank\">solar-sweep-6</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/amz50fe3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/amz50fe3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251010_032242_335', 'my_seed': 26027, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=5.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=5.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 556.0\n",
      "lif layer 1 self.abs_max_v: 556.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 669.0\n",
      "fc layer 2 self.abs_max_out: 200.0\n",
      "lif layer 2 self.abs_max_v: 232.5\n",
      "fc layer 1 self.abs_max_out: 686.0\n",
      "lif layer 1 self.abs_max_v: 815.0\n",
      "fc layer 2 self.abs_max_out: 237.0\n",
      "lif layer 2 self.abs_max_v: 319.5\n",
      "fc layer 2 self.abs_max_out: 289.0\n",
      "lif layer 2 self.abs_max_v: 336.0\n",
      "lif layer 1 self.abs_max_v: 954.5\n",
      "fc layer 2 self.abs_max_out: 314.0\n",
      "lif layer 2 self.abs_max_v: 398.5\n",
      "fc layer 2 self.abs_max_out: 346.0\n",
      "lif layer 2 self.abs_max_v: 492.5\n",
      "fc layer 1 self.abs_max_out: 784.0\n",
      "fc layer 2 self.abs_max_out: 492.0\n",
      "lif layer 2 self.abs_max_v: 583.0\n",
      "fc layer 1 self.abs_max_out: 1109.0\n",
      "lif layer 1 self.abs_max_v: 1364.0\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "lif layer 2 self.abs_max_v: 702.0\n",
      "fc layer 2 self.abs_max_out: 558.0\n",
      "lif layer 2 self.abs_max_v: 728.0\n",
      "fc layer 3 self.abs_max_out: 60.0\n",
      "fc layer 1 self.abs_max_out: 1206.0\n",
      "fc layer 2 self.abs_max_out: 573.0\n",
      "lif layer 2 self.abs_max_v: 805.5\n",
      "fc layer 3 self.abs_max_out: 85.0\n",
      "fc layer 2 self.abs_max_out: 823.0\n",
      "lif layer 2 self.abs_max_v: 1110.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "fc layer 2 self.abs_max_out: 831.0\n",
      "lif layer 2 self.abs_max_v: 1178.0\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "fc layer 1 self.abs_max_out: 1316.0\n",
      "lif layer 2 self.abs_max_v: 1230.5\n",
      "lif layer 2 self.abs_max_v: 1346.5\n",
      "fc layer 2 self.abs_max_out: 887.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 1419.0\n",
      "lif layer 2 self.abs_max_v: 1398.0\n",
      "fc layer 1 self.abs_max_out: 1354.0\n",
      "fc layer 1 self.abs_max_out: 1378.0\n",
      "fc layer 2 self.abs_max_out: 948.0\n",
      "fc layer 1 self.abs_max_out: 1381.0\n",
      "fc layer 2 self.abs_max_out: 1052.0\n",
      "fc layer 1 self.abs_max_out: 1443.0\n",
      "lif layer 1 self.abs_max_v: 1443.0\n",
      "lif layer 2 self.abs_max_v: 1417.0\n",
      "lif layer 2 self.abs_max_v: 1515.5\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "fc layer 1 self.abs_max_out: 1469.0\n",
      "lif layer 1 self.abs_max_v: 1469.0\n",
      "fc layer 1 self.abs_max_out: 1621.0\n",
      "lif layer 1 self.abs_max_v: 1621.0\n",
      "fc layer 1 self.abs_max_out: 1914.0\n",
      "lif layer 1 self.abs_max_v: 1914.0\n",
      "lif layer 2 self.abs_max_v: 1557.0\n",
      "lif layer 2 self.abs_max_v: 1716.5\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "fc layer 2 self.abs_max_out: 1191.0\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "lif layer 2 self.abs_max_v: 1837.0\n",
      "lif layer 2 self.abs_max_v: 1926.5\n",
      "lif layer 2 self.abs_max_v: 2131.0\n",
      "fc layer 2 self.abs_max_out: 1311.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 1 self.abs_max_out: 2153.0\n",
      "lif layer 1 self.abs_max_v: 2153.0\n",
      "fc layer 2 self.abs_max_out: 1318.0\n",
      "fc layer 2 self.abs_max_out: 1347.0\n",
      "fc layer 2 self.abs_max_out: 1401.0\n",
      "fc layer 2 self.abs_max_out: 1444.0\n",
      "fc layer 1 self.abs_max_out: 2203.0\n",
      "lif layer 1 self.abs_max_v: 2203.0\n",
      "fc layer 1 self.abs_max_out: 2474.0\n",
      "lif layer 1 self.abs_max_v: 2474.0\n",
      "fc layer 2 self.abs_max_out: 1504.0\n",
      "lif layer 2 self.abs_max_v: 2453.0\n",
      "fc layer 1 self.abs_max_out: 2617.0\n",
      "lif layer 1 self.abs_max_v: 2617.0\n",
      "fc layer 2 self.abs_max_out: 1786.0\n",
      "fc layer 2 self.abs_max_out: 1919.0\n",
      "fc layer 3 self.abs_max_out: 361.0\n",
      "fc layer 2 self.abs_max_out: 2015.0\n",
      "lif layer 1 self.abs_max_v: 2658.5\n",
      "fc layer 1 self.abs_max_out: 2825.0\n",
      "lif layer 1 self.abs_max_v: 2825.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "fc layer 1 self.abs_max_out: 2909.0\n",
      "lif layer 1 self.abs_max_v: 2909.0\n",
      "fc layer 1 self.abs_max_out: 3059.0\n",
      "lif layer 1 self.abs_max_v: 3059.0\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "fc layer 1 self.abs_max_out: 3131.0\n",
      "lif layer 1 self.abs_max_v: 3131.0\n",
      "fc layer 1 self.abs_max_out: 3269.0\n",
      "lif layer 1 self.abs_max_v: 3269.0\n",
      "fc layer 2 self.abs_max_out: 2101.0\n",
      "fc layer 3 self.abs_max_out: 420.0\n",
      "fc layer 1 self.abs_max_out: 3508.0\n",
      "lif layer 1 self.abs_max_v: 3508.0\n",
      "lif layer 2 self.abs_max_v: 2586.0\n",
      "lif layer 2 self.abs_max_v: 2616.5\n",
      "fc layer 1 self.abs_max_out: 3574.0\n",
      "lif layer 1 self.abs_max_v: 3574.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "fc layer 1 self.abs_max_out: 3953.0\n",
      "lif layer 1 self.abs_max_v: 3953.0\n",
      "fc layer 2 self.abs_max_out: 2123.0\n",
      "fc layer 1 self.abs_max_out: 3958.0\n",
      "lif layer 1 self.abs_max_v: 3958.0\n",
      "fc layer 1 self.abs_max_out: 4503.0\n",
      "lif layer 1 self.abs_max_v: 4503.0\n",
      "fc layer 1 self.abs_max_out: 4655.0\n",
      "lif layer 1 self.abs_max_v: 4655.0\n",
      "fc layer 2 self.abs_max_out: 2161.0\n",
      "fc layer 2 self.abs_max_out: 2187.0\n",
      "fc layer 2 self.abs_max_out: 2253.0\n",
      "fc layer 2 self.abs_max_out: 2290.0\n",
      "fc layer 2 self.abs_max_out: 2305.0\n",
      "fc layer 2 self.abs_max_out: 2335.0\n",
      "fc layer 2 self.abs_max_out: 2467.0\n",
      "fc layer 1 self.abs_max_out: 4680.0\n",
      "lif layer 1 self.abs_max_v: 4680.0\n",
      "fc layer 1 self.abs_max_out: 4724.0\n",
      "lif layer 1 self.abs_max_v: 4724.0\n",
      "fc layer 1 self.abs_max_out: 5034.0\n",
      "lif layer 1 self.abs_max_v: 5034.0\n",
      "fc layer 2 self.abs_max_out: 2521.0\n",
      "fc layer 1 self.abs_max_out: 5095.0\n",
      "lif layer 1 self.abs_max_v: 5095.0\n",
      "fc layer 1 self.abs_max_out: 5162.0\n",
      "lif layer 1 self.abs_max_v: 5162.0\n",
      "lif layer 2 self.abs_max_v: 2656.5\n",
      "fc layer 2 self.abs_max_out: 2541.0\n",
      "lif layer 2 self.abs_max_v: 2741.5\n",
      "lif layer 2 self.abs_max_v: 2776.5\n",
      "lif layer 2 self.abs_max_v: 3004.0\n",
      "fc layer 1 self.abs_max_out: 5548.0\n",
      "lif layer 1 self.abs_max_v: 5548.0\n",
      "fc layer 2 self.abs_max_out: 2595.0\n",
      "fc layer 2 self.abs_max_out: 2667.0\n",
      "fc layer 1 self.abs_max_out: 5611.0\n",
      "lif layer 1 self.abs_max_v: 5611.0\n",
      "fc layer 2 self.abs_max_out: 2668.0\n",
      "fc layer 1 self.abs_max_out: 5946.0\n",
      "lif layer 1 self.abs_max_v: 5946.0\n",
      "fc layer 2 self.abs_max_out: 2776.0\n",
      "fc layer 2 self.abs_max_out: 2900.0\n",
      "fc layer 1 self.abs_max_out: 6106.0\n",
      "lif layer 1 self.abs_max_v: 6106.0\n",
      "fc layer 1 self.abs_max_out: 6144.0\n",
      "lif layer 1 self.abs_max_v: 6144.0\n",
      "fc layer 1 self.abs_max_out: 6590.0\n",
      "lif layer 1 self.abs_max_v: 6590.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.056324/  2.140692, val:  34.17%, val_best:  34.17%, tr:  94.93%, tr_best:  94.93%, epoch time: 271.92 seconds, 4.53 minutes\n",
      "total_backward_count 44360 real_backward_count 11133  25.097%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3024.0\n",
      "fc layer 1 self.abs_max_out: 6831.0\n",
      "lif layer 1 self.abs_max_v: 6831.0\n",
      "lif layer 2 self.abs_max_v: 3043.5\n",
      "lif layer 2 self.abs_max_v: 3286.0\n",
      "fc layer 1 self.abs_max_out: 6994.0\n",
      "lif layer 1 self.abs_max_v: 6994.0\n",
      "fc layer 2 self.abs_max_out: 2905.0\n",
      "lif layer 2 self.abs_max_v: 3488.5\n",
      "lif layer 2 self.abs_max_v: 3767.5\n",
      "fc layer 1 self.abs_max_out: 7159.0\n",
      "lif layer 1 self.abs_max_v: 7159.0\n",
      "fc layer 1 self.abs_max_out: 7160.0\n",
      "lif layer 1 self.abs_max_v: 7160.0\n",
      "lif layer 1 self.abs_max_v: 7476.0\n",
      "lif layer 1 self.abs_max_v: 8430.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.048911/  2.144540, val:  46.25%, val_best:  46.25%, tr:  99.19%, tr_best:  99.19%, epoch time: 271.26 seconds, 4.52 minutes\n",
      "total_backward_count 88720 real_backward_count 18813  21.205%\n",
      "fc layer 2 self.abs_max_out: 2938.0\n",
      "fc layer 1 self.abs_max_out: 7230.0\n",
      "fc layer 1 self.abs_max_out: 7402.0\n",
      "fc layer 2 self.abs_max_out: 3165.0\n",
      "fc layer 1 self.abs_max_out: 7432.0\n",
      "fc layer 1 self.abs_max_out: 7613.0\n",
      "fc layer 1 self.abs_max_out: 7827.0\n",
      "lif layer 1 self.abs_max_v: 8719.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.038504/  2.118897, val:  51.67%, val_best:  51.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 268.26 seconds, 4.47 minutes\n",
      "total_backward_count 133080 real_backward_count 25860  19.432%\n",
      "lif layer 2 self.abs_max_v: 3844.0\n",
      "fc layer 1 self.abs_max_out: 7975.0\n",
      "lif layer 2 self.abs_max_v: 3846.5\n",
      "fc layer 2 self.abs_max_out: 3230.0\n",
      "lif layer 1 self.abs_max_v: 9865.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.025299/  2.091653, val:  52.92%, val_best:  52.92%, tr:  99.50%, tr_best:  99.59%, epoch time: 273.30 seconds, 4.55 minutes\n",
      "total_backward_count 177440 real_backward_count 32592  18.368%\n",
      "lif layer 2 self.abs_max_v: 3872.0\n",
      "lif layer 2 self.abs_max_v: 4132.5\n",
      "lif layer 2 self.abs_max_v: 4316.5\n",
      "lif layer 2 self.abs_max_v: 4361.5\n",
      "lif layer 2 self.abs_max_v: 4395.0\n",
      "fc layer 1 self.abs_max_out: 8003.0\n",
      "fc layer 1 self.abs_max_out: 8337.0\n",
      "lif layer 1 self.abs_max_v: 10431.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.012170/  2.088464, val:  52.50%, val_best:  52.92%, tr:  99.89%, tr_best:  99.89%, epoch time: 272.23 seconds, 4.54 minutes\n",
      "total_backward_count 221800 real_backward_count 38942  17.557%\n",
      "fc layer 2 self.abs_max_out: 3243.0\n",
      "lif layer 2 self.abs_max_v: 4549.0\n",
      "fc layer 1 self.abs_max_out: 8575.0\n",
      "fc layer 2 self.abs_max_out: 3248.0\n",
      "fc layer 2 self.abs_max_out: 3274.0\n",
      "lif layer 1 self.abs_max_v: 11259.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.014014/  2.088286, val:  55.42%, val_best:  55.42%, tr:  99.73%, tr_best:  99.89%, epoch time: 271.34 seconds, 4.52 minutes\n",
      "total_backward_count 266160 real_backward_count 45024  16.916%\n",
      "fc layer 2 self.abs_max_out: 3433.0\n",
      "fc layer 1 self.abs_max_out: 8584.0\n",
      "lif layer 2 self.abs_max_v: 4601.5\n",
      "lif layer 2 self.abs_max_v: 4637.0\n",
      "fc layer 2 self.abs_max_out: 3481.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.001759/  2.074510, val:  75.42%, val_best:  75.42%, tr:  99.75%, tr_best:  99.89%, epoch time: 272.83 seconds, 4.55 minutes\n",
      "total_backward_count 310520 real_backward_count 50721  16.334%\n",
      "lif layer 2 self.abs_max_v: 4688.5\n",
      "lif layer 2 self.abs_max_v: 4743.5\n",
      "lif layer 2 self.abs_max_v: 4887.5\n",
      "lif layer 2 self.abs_max_v: 4890.5\n",
      "lif layer 2 self.abs_max_v: 4965.0\n",
      "fc layer 1 self.abs_max_out: 8824.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.996550/  2.075610, val:  57.08%, val_best:  75.42%, tr:  99.93%, tr_best:  99.93%, epoch time: 268.16 seconds, 4.47 minutes\n",
      "total_backward_count 354880 real_backward_count 56190  15.834%\n",
      "fc layer 1 self.abs_max_out: 8961.0\n",
      "lif layer 1 self.abs_max_v: 11543.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.995429/  2.061539, val:  66.25%, val_best:  75.42%, tr:  99.84%, tr_best:  99.93%, epoch time: 272.78 seconds, 4.55 minutes\n",
      "total_backward_count 399240 real_backward_count 61468  15.396%\n",
      "fc layer 1 self.abs_max_out: 9019.0\n",
      "fc layer 2 self.abs_max_out: 3591.0\n",
      "lif layer 1 self.abs_max_v: 11555.5\n",
      "lif layer 2 self.abs_max_v: 4966.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.985303/  2.071391, val:  60.42%, val_best:  75.42%, tr:  99.84%, tr_best:  99.93%, epoch time: 272.22 seconds, 4.54 minutes\n",
      "total_backward_count 443600 real_backward_count 66540  15.000%\n",
      "lif layer 2 self.abs_max_v: 5032.0\n",
      "fc layer 1 self.abs_max_out: 9148.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.980102/  2.052989, val:  70.83%, val_best:  75.42%, tr:  99.91%, tr_best:  99.93%, epoch time: 273.09 seconds, 4.55 minutes\n",
      "total_backward_count 487960 real_backward_count 71503  14.653%\n",
      "fc layer 1 self.abs_max_out: 9166.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.969257/  2.039580, val:  73.75%, val_best:  75.42%, tr:  99.95%, tr_best:  99.95%, epoch time: 271.24 seconds, 4.52 minutes\n",
      "total_backward_count 532320 real_backward_count 76173  14.310%\n",
      "fc layer 2 self.abs_max_out: 3615.0\n",
      "fc layer 1 self.abs_max_out: 9270.0\n",
      "fc layer 3 self.abs_max_out: 464.0\n",
      "lif layer 1 self.abs_max_v: 11670.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.973058/  2.046086, val:  75.42%, val_best:  75.42%, tr:  99.98%, tr_best:  99.98%, epoch time: 271.54 seconds, 4.53 minutes\n",
      "total_backward_count 576680 real_backward_count 80859  14.021%\n",
      "fc layer 1 self.abs_max_out: 9332.0\n",
      "lif layer 1 self.abs_max_v: 12294.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.968022/  2.037531, val:  80.00%, val_best:  80.00%, tr:  99.98%, tr_best:  99.98%, epoch time: 268.76 seconds, 4.48 minutes\n",
      "total_backward_count 621040 real_backward_count 85290  13.733%\n",
      "lif layer 2 self.abs_max_v: 5033.5\n",
      "lif layer 2 self.abs_max_v: 5090.0\n",
      "lif layer 1 self.abs_max_v: 12548.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.962207/  2.028982, val:  86.25%, val_best:  86.25%, tr:  99.95%, tr_best:  99.98%, epoch time: 271.71 seconds, 4.53 minutes\n",
      "total_backward_count 665400 real_backward_count 89721  13.484%\n",
      "fc layer 1 self.abs_max_out: 9347.0\n",
      "lif layer 1 self.abs_max_v: 12622.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.948007/  2.034354, val:  77.92%, val_best:  86.25%, tr:  99.93%, tr_best:  99.98%, epoch time: 272.27 seconds, 4.54 minutes\n",
      "total_backward_count 709760 real_backward_count 93863  13.225%\n",
      "fc layer 1 self.abs_max_out: 9453.0\n",
      "lif layer 1 self.abs_max_v: 12811.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.958415/  2.038182, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.64 seconds, 4.53 minutes\n",
      "total_backward_count 754120 real_backward_count 97893  12.981%\n",
      "fc layer 1 self.abs_max_out: 9458.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.945935/  2.024750, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.68 seconds, 4.54 minutes\n",
      "total_backward_count 798480 real_backward_count 101935  12.766%\n",
      "lif layer 2 self.abs_max_v: 5195.5\n",
      "lif layer 2 self.abs_max_v: 5204.0\n",
      "fc layer 1 self.abs_max_out: 9469.0\n",
      "lif layer 1 self.abs_max_v: 12903.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.945564/  2.032373, val:  77.08%, val_best:  86.25%, tr:  99.93%, tr_best: 100.00%, epoch time: 269.33 seconds, 4.49 minutes\n",
      "total_backward_count 842840 real_backward_count 105825  12.556%\n",
      "fc layer 1 self.abs_max_out: 9536.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.943998/  2.032434, val:  77.92%, val_best:  86.25%, tr:  99.95%, tr_best: 100.00%, epoch time: 272.93 seconds, 4.55 minutes\n",
      "total_backward_count 887200 real_backward_count 109588  12.352%\n",
      "fc layer 1 self.abs_max_out: 9546.0\n",
      "lif layer 1 self.abs_max_v: 13160.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.938361/  2.016758, val:  75.42%, val_best:  86.25%, tr:  99.93%, tr_best: 100.00%, epoch time: 271.40 seconds, 4.52 minutes\n",
      "total_backward_count 931560 real_backward_count 113211  12.153%\n",
      "fc layer 1 self.abs_max_out: 9574.0\n",
      "lif layer 1 self.abs_max_v: 13552.5\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.935712/  2.018417, val:  72.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.43 seconds, 4.54 minutes\n",
      "total_backward_count 975920 real_backward_count 116860  11.974%\n",
      "fc layer 1 self.abs_max_out: 9687.0\n",
      "lif layer 1 self.abs_max_v: 13772.5\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.922828/  2.003047, val:  80.83%, val_best:  86.25%, tr:  99.95%, tr_best: 100.00%, epoch time: 270.00 seconds, 4.50 minutes\n",
      "total_backward_count 1020280 real_backward_count 120360  11.797%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.917443/  2.005504, val:  77.92%, val_best:  86.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 270.84 seconds, 4.51 minutes\n",
      "total_backward_count 1064640 real_backward_count 123720  11.621%\n",
      "fc layer 1 self.abs_max_out: 9723.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.917259/  1.999331, val:  79.17%, val_best:  86.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 268.18 seconds, 4.47 minutes\n",
      "total_backward_count 1109000 real_backward_count 127128  11.463%\n",
      "fc layer 2 self.abs_max_out: 3728.0\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "lif layer 1 self.abs_max_v: 14128.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.914488/  1.991364, val:  85.00%, val_best:  86.25%, tr:  99.95%, tr_best: 100.00%, epoch time: 270.93 seconds, 4.52 minutes\n",
      "total_backward_count 1153360 real_backward_count 130378  11.304%\n",
      "lif layer 2 self.abs_max_v: 5222.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.910788/  1.995196, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.51 seconds, 4.56 minutes\n",
      "total_backward_count 1197720 real_backward_count 133610  11.155%\n",
      "fc layer 2 self.abs_max_out: 3737.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.905311/  2.011622, val:  74.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.98 seconds, 4.53 minutes\n",
      "total_backward_count 1242080 real_backward_count 136683  11.004%\n",
      "fc layer 1 self.abs_max_out: 9734.0\n",
      "lif layer 1 self.abs_max_v: 14376.5\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.912401/  2.001060, val:  77.50%, val_best:  86.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 272.76 seconds, 4.55 minutes\n",
      "total_backward_count 1286440 real_backward_count 139768  10.865%\n",
      "fc layer 1 self.abs_max_out: 9807.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.906443/  1.984702, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.42 seconds, 4.46 minutes\n",
      "total_backward_count 1330800 real_backward_count 142733  10.725%\n",
      "fc layer 1 self.abs_max_out: 9856.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.897264/  1.985709, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.76 seconds, 4.56 minutes\n",
      "total_backward_count 1375160 real_backward_count 145661  10.592%\n",
      "lif layer 1 self.abs_max_v: 14616.5\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.891700/  1.979703, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.79 seconds, 4.51 minutes\n",
      "total_backward_count 1419520 real_backward_count 148547  10.465%\n",
      "fc layer 1 self.abs_max_out: 9978.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.893882/  1.978533, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.56 seconds, 4.53 minutes\n",
      "total_backward_count 1463880 real_backward_count 151431  10.344%\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "fc layer 1 self.abs_max_out: 10047.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.881055/  1.960994, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.06 seconds, 4.50 minutes\n",
      "total_backward_count 1508240 real_backward_count 154268  10.228%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.880689/  1.974219, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.32 seconds, 4.54 minutes\n",
      "total_backward_count 1552600 real_backward_count 156875  10.104%\n",
      "lif layer 1 self.abs_max_v: 15018.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.883950/  1.973866, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.52 seconds, 4.49 minutes\n",
      "total_backward_count 1596960 real_backward_count 159460   9.985%\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.883386/  1.980473, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.86 seconds, 4.51 minutes\n",
      "total_backward_count 1641320 real_backward_count 162081   9.875%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.878470/  1.962997, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.15 seconds, 4.54 minutes\n",
      "total_backward_count 1685680 real_backward_count 164611   9.765%\n",
      "fc layer 1 self.abs_max_out: 10084.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.874376/  1.966692, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.08 seconds, 4.55 minutes\n",
      "total_backward_count 1730040 real_backward_count 167200   9.665%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.880256/  1.978962, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.04 seconds, 4.53 minutes\n",
      "total_backward_count 1774400 real_backward_count 169678   9.563%\n",
      "lif layer 2 self.abs_max_v: 5257.5\n",
      "lif layer 2 self.abs_max_v: 5297.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.882981/  1.979450, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.53 seconds, 4.48 minutes\n",
      "total_backward_count 1818760 real_backward_count 172081   9.461%\n",
      "lif layer 2 self.abs_max_v: 5322.0\n",
      "lif layer 2 self.abs_max_v: 5371.0\n",
      "fc layer 1 self.abs_max_out: 10085.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.868715/  1.960716, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.88 seconds, 4.55 minutes\n",
      "total_backward_count 1863120 real_backward_count 174445   9.363%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.880141/  1.962361, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.07 seconds, 4.53 minutes\n",
      "total_backward_count 1907480 real_backward_count 176799   9.269%\n",
      "lif layer 2 self.abs_max_v: 5373.5\n",
      "lif layer 2 self.abs_max_v: 5388.5\n",
      "lif layer 1 self.abs_max_v: 15084.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.861217/  1.956021, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.05 seconds, 4.55 minutes\n",
      "total_backward_count 1951840 real_backward_count 179100   9.176%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.856659/  1.955419, val:  85.83%, val_best:  86.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 270.77 seconds, 4.51 minutes\n",
      "total_backward_count 1996200 real_backward_count 181280   9.081%\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "fc layer 1 self.abs_max_out: 10188.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.856709/  1.943248, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.09 seconds, 4.52 minutes\n",
      "total_backward_count 2040560 real_backward_count 183583   8.997%\n",
      "fc layer 1 self.abs_max_out: 10264.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.851397/  1.947635, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.63 seconds, 4.51 minutes\n",
      "total_backward_count 2084920 real_backward_count 185851   8.914%\n",
      "fc layer 1 self.abs_max_out: 10274.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.857830/  1.956492, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.80 seconds, 4.51 minutes\n",
      "total_backward_count 2129280 real_backward_count 187960   8.827%\n",
      "fc layer 1 self.abs_max_out: 10354.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.860386/  1.967695, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.40 seconds, 4.54 minutes\n",
      "total_backward_count 2173640 real_backward_count 190070   8.744%\n",
      "fc layer 1 self.abs_max_out: 10380.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.858819/  1.949372, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.45 seconds, 4.56 minutes\n",
      "total_backward_count 2218000 real_backward_count 192108   8.661%\n",
      "lif layer 2 self.abs_max_v: 5471.5\n",
      "lif layer 1 self.abs_max_v: 15210.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.850281/  1.924172, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.06 seconds, 4.55 minutes\n",
      "total_backward_count 2262360 real_backward_count 194167   8.582%\n",
      "fc layer 3 self.abs_max_out: 508.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.835637/  1.938591, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.46 seconds, 4.44 minutes\n",
      "total_backward_count 2306720 real_backward_count 196077   8.500%\n",
      "lif layer 1 self.abs_max_v: 15322.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.835495/  1.934609, val:  86.25%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%, epoch time: 272.20 seconds, 4.54 minutes\n",
      "total_backward_count 2351080 real_backward_count 198068   8.425%\n",
      "lif layer 1 self.abs_max_v: 15326.5\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.831718/  1.920674, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.67 seconds, 4.51 minutes\n",
      "total_backward_count 2395440 real_backward_count 200028   8.350%\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "fc layer 1 self.abs_max_out: 10407.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.832424/  1.925751, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.39 seconds, 4.52 minutes\n",
      "total_backward_count 2439800 real_backward_count 201903   8.275%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.826499/  1.928361, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.44 seconds, 4.52 minutes\n",
      "total_backward_count 2484160 real_backward_count 203683   8.199%\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "lif layer 1 self.abs_max_v: 15483.5\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.825114/  1.934810, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.54 seconds, 4.51 minutes\n",
      "total_backward_count 2528520 real_backward_count 205510   8.128%\n",
      "lif layer 2 self.abs_max_v: 5557.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.825901/  1.922330, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.79 seconds, 4.51 minutes\n",
      "total_backward_count 2572880 real_backward_count 207366   8.060%\n",
      "lif layer 1 self.abs_max_v: 15546.5\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.822743/  1.926949, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.36 seconds, 4.51 minutes\n",
      "total_backward_count 2617240 real_backward_count 209132   7.991%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.827260/  1.937618, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.24 seconds, 4.52 minutes\n",
      "total_backward_count 2661600 real_backward_count 210878   7.923%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.824299/  1.933078, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.44 seconds, 4.56 minutes\n",
      "total_backward_count 2705960 real_backward_count 212589   7.856%\n",
      "fc layer 1 self.abs_max_out: 10415.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.824900/  1.940464, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.23 seconds, 4.55 minutes\n",
      "total_backward_count 2750320 real_backward_count 214326   7.793%\n",
      "fc layer 1 self.abs_max_out: 10590.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.831163/  1.938284, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.17 seconds, 4.47 minutes\n",
      "total_backward_count 2794680 real_backward_count 216051   7.731%\n",
      "fc layer 1 self.abs_max_out: 10665.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.827272/  1.922661, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.34 seconds, 4.54 minutes\n",
      "total_backward_count 2839040 real_backward_count 217739   7.669%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.811141/  1.921793, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.35 seconds, 4.54 minutes\n",
      "total_backward_count 2883400 real_backward_count 219402   7.609%\n",
      "lif layer 1 self.abs_max_v: 15645.5\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.815983/  1.917878, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.55 seconds, 4.56 minutes\n",
      "total_backward_count 2927760 real_backward_count 221000   7.548%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.812019/  1.922477, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.48 seconds, 4.54 minutes\n",
      "total_backward_count 2972120 real_backward_count 222566   7.488%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.819726/  1.919484, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.59 seconds, 4.53 minutes\n",
      "total_backward_count 3016480 real_backward_count 224131   7.430%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.812170/  1.926669, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.55 seconds, 4.53 minutes\n",
      "total_backward_count 3060840 real_backward_count 225732   7.375%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.816660/  1.929412, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.72 seconds, 4.48 minutes\n",
      "total_backward_count 3105200 real_backward_count 227331   7.321%\n",
      "fc layer 1 self.abs_max_out: 10673.0\n",
      "fc layer 3 self.abs_max_out: 544.0\n",
      "lif layer 1 self.abs_max_v: 15922.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.818214/  1.930887, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.26 seconds, 4.54 minutes\n",
      "total_backward_count 3149560 real_backward_count 228942   7.269%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.816950/  1.923353, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.74 seconds, 4.53 minutes\n",
      "total_backward_count 3193920 real_backward_count 230412   7.214%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.804879/  1.913613, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.73 seconds, 4.53 minutes\n",
      "total_backward_count 3238280 real_backward_count 231985   7.164%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.801338/  1.908834, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.52 seconds, 4.46 minutes\n",
      "total_backward_count 3282640 real_backward_count 233517   7.114%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.792122/  1.906385, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.13 seconds, 4.54 minutes\n",
      "total_backward_count 3327000 real_backward_count 234994   7.063%\n",
      "fc layer 1 self.abs_max_out: 10688.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.788301/  1.904404, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.23 seconds, 4.55 minutes\n",
      "total_backward_count 3371360 real_backward_count 236404   7.012%\n",
      "fc layer 1 self.abs_max_out: 10708.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.790647/  1.909400, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.49 seconds, 4.56 minutes\n",
      "total_backward_count 3415720 real_backward_count 237847   6.963%\n",
      "fc layer 1 self.abs_max_out: 10714.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.793428/  1.905165, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.29 seconds, 4.54 minutes\n",
      "total_backward_count 3460080 real_backward_count 239228   6.914%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.796631/  1.901792, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.02 seconds, 4.50 minutes\n",
      "total_backward_count 3504440 real_backward_count 240627   6.866%\n",
      "fc layer 1 self.abs_max_out: 10828.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.790487/  1.906071, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.67 seconds, 4.54 minutes\n",
      "total_backward_count 3548800 real_backward_count 242000   6.819%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.785998/  1.895172, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.65 seconds, 4.48 minutes\n",
      "total_backward_count 3593160 real_backward_count 243368   6.773%\n",
      "fc layer 1 self.abs_max_out: 10829.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.788585/  1.899971, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.68 seconds, 4.56 minutes\n",
      "total_backward_count 3637520 real_backward_count 244661   6.726%\n",
      "fc layer 1 self.abs_max_out: 10838.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.785402/  1.894185, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.90 seconds, 4.55 minutes\n",
      "total_backward_count 3681880 real_backward_count 245948   6.680%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.777677/  1.895911, val:  84.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.79 seconds, 4.53 minutes\n",
      "total_backward_count 3726240 real_backward_count 247207   6.634%\n",
      "fc layer 1 self.abs_max_out: 10839.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.784352/  1.901961, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.93 seconds, 4.48 minutes\n",
      "total_backward_count 3770600 real_backward_count 248495   6.590%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.777356/  1.887455, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.46 seconds, 4.56 minutes\n",
      "total_backward_count 3814960 real_backward_count 249801   6.548%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.764801/  1.878517, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.92 seconds, 4.53 minutes\n",
      "total_backward_count 3859320 real_backward_count 251038   6.505%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.760968/  1.873013, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.29 seconds, 4.55 minutes\n",
      "total_backward_count 3903680 real_backward_count 252309   6.463%\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.766786/  1.884553, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.92 seconds, 4.52 minutes\n",
      "total_backward_count 3948040 real_backward_count 253523   6.421%\n",
      "fc layer 1 self.abs_max_out: 10860.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.766825/  1.886303, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.19 seconds, 4.52 minutes\n",
      "total_backward_count 3992400 real_backward_count 254771   6.381%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.764946/  1.880014, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.55 seconds, 4.53 minutes\n",
      "total_backward_count 4036760 real_backward_count 255909   6.339%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.763014/  1.889470, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.61 seconds, 4.48 minutes\n",
      "total_backward_count 4081120 real_backward_count 257078   6.299%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.756271/  1.884496, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.22 seconds, 4.52 minutes\n",
      "total_backward_count 4125480 real_backward_count 258359   6.263%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.767017/  1.882711, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.87 seconds, 4.51 minutes\n",
      "total_backward_count 4169840 real_backward_count 259488   6.223%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.765364/  1.876332, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.90 seconds, 4.56 minutes\n",
      "total_backward_count 4214200 real_backward_count 260665   6.185%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.760242/  1.882386, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.59 seconds, 4.48 minutes\n",
      "total_backward_count 4258560 real_backward_count 261767   6.147%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.759549/  1.867891, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.02 seconds, 4.57 minutes\n",
      "total_backward_count 4302920 real_backward_count 262878   6.109%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.756878/  1.875363, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.74 seconds, 4.56 minutes\n",
      "total_backward_count 4347280 real_backward_count 263999   6.073%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.754166/  1.875207, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.90 seconds, 4.55 minutes\n",
      "total_backward_count 4391640 real_backward_count 265137   6.037%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.745604/  1.867148, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.60 seconds, 4.54 minutes\n",
      "total_backward_count 4436000 real_backward_count 266268   6.002%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.750245/  1.873537, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.36 seconds, 4.51 minutes\n",
      "total_backward_count 4480360 real_backward_count 267373   5.968%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.747884/  1.867430, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.04 seconds, 4.53 minutes\n",
      "total_backward_count 4524720 real_backward_count 268441   5.933%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.746282/  1.866209, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.37 seconds, 4.49 minutes\n",
      "total_backward_count 4569080 real_backward_count 269486   5.898%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.744374/  1.872053, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.53 seconds, 4.51 minutes\n",
      "total_backward_count 4613440 real_backward_count 270501   5.863%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.740699/  1.867455, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.01 seconds, 4.57 minutes\n",
      "total_backward_count 4657800 real_backward_count 271533   5.830%\n",
      "fc layer 3 self.abs_max_out: 572.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.739408/  1.856269, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.61 seconds, 4.56 minutes\n",
      "total_backward_count 4702160 real_backward_count 272515   5.796%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.732258/  1.870275, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.39 seconds, 4.47 minutes\n",
      "total_backward_count 4746520 real_backward_count 273538   5.763%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.743924/  1.870971, val:  81.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.70 seconds, 4.54 minutes\n",
      "total_backward_count 4790880 real_backward_count 274626   5.732%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.740264/  1.861372, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.88 seconds, 4.55 minutes\n",
      "total_backward_count 4835240 real_backward_count 275621   5.700%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.740929/  1.861279, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.96 seconds, 4.55 minutes\n",
      "total_backward_count 4879600 real_backward_count 276621   5.669%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.735460/  1.860124, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.98 seconds, 4.57 minutes\n",
      "total_backward_count 4923960 real_backward_count 277644   5.639%\n",
      "lif layer 2 self.abs_max_v: 5640.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.736102/  1.858636, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.61 seconds, 4.49 minutes\n",
      "total_backward_count 4968320 real_backward_count 278680   5.609%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.736522/  1.864752, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.12 seconds, 4.55 minutes\n",
      "total_backward_count 5012680 real_backward_count 279717   5.580%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.736418/  1.861755, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.15 seconds, 4.47 minutes\n",
      "total_backward_count 5057040 real_backward_count 280627   5.549%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.731103/  1.862890, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.97 seconds, 4.53 minutes\n",
      "total_backward_count 5101400 real_backward_count 281638   5.521%\n",
      "lif layer 2 self.abs_max_v: 5741.5\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.736386/  1.856970, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.84 seconds, 4.53 minutes\n",
      "total_backward_count 5145760 real_backward_count 282639   5.493%\n",
      "lif layer 2 self.abs_max_v: 5743.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.733077/  1.864923, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.63 seconds, 4.54 minutes\n",
      "total_backward_count 5190120 real_backward_count 283548   5.463%\n",
      "lif layer 2 self.abs_max_v: 5889.5\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.735824/  1.859247, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.48 seconds, 4.46 minutes\n",
      "total_backward_count 5234480 real_backward_count 284452   5.434%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.739944/  1.859911, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.04 seconds, 4.53 minutes\n",
      "total_backward_count 5278840 real_backward_count 285432   5.407%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.739433/  1.866036, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.63 seconds, 4.54 minutes\n",
      "total_backward_count 5323200 real_backward_count 286435   5.381%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.736086/  1.859837, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.94 seconds, 4.55 minutes\n",
      "total_backward_count 5367560 real_backward_count 287380   5.354%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.738086/  1.863783, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.47 seconds, 4.54 minutes\n",
      "total_backward_count 5411920 real_backward_count 288361   5.328%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.744209/  1.866504, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.77 seconds, 4.53 minutes\n",
      "total_backward_count 5456280 real_backward_count 289275   5.302%\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.735377/  1.862444, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.03 seconds, 4.55 minutes\n",
      "total_backward_count 5500640 real_backward_count 290212   5.276%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.733733/  1.856714, val:  84.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.02 seconds, 4.50 minutes\n",
      "total_backward_count 5545000 real_backward_count 291108   5.250%\n",
      "fc layer 1 self.abs_max_out: 10878.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.732510/  1.862164, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.01 seconds, 4.50 minutes\n",
      "total_backward_count 5589360 real_backward_count 292016   5.224%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.735843/  1.860219, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.45 seconds, 4.56 minutes\n",
      "total_backward_count 5633720 real_backward_count 292912   5.199%\n",
      "lif layer 2 self.abs_max_v: 5931.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.720970/  1.846411, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.66 seconds, 4.56 minutes\n",
      "total_backward_count 5678080 real_backward_count 293776   5.174%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.720454/  1.847163, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.99 seconds, 4.45 minutes\n",
      "total_backward_count 5722440 real_backward_count 294669   5.149%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.720431/  1.841137, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.04 seconds, 4.38 minutes\n",
      "total_backward_count 5766800 real_backward_count 295535   5.125%\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.722029/  1.849887, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.59 seconds, 4.39 minutes\n",
      "total_backward_count 5811160 real_backward_count 296367   5.100%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.723408/  1.852138, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.14 seconds, 4.30 minutes\n",
      "total_backward_count 5855520 real_backward_count 297227   5.076%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.726427/  1.853619, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 250.56 seconds, 4.18 minutes\n",
      "total_backward_count 5899880 real_backward_count 298110   5.053%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.725282/  1.846480, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 246.29 seconds, 4.10 minutes\n",
      "total_backward_count 5944240 real_backward_count 299021   5.030%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.718040/  1.843910, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 245.97 seconds, 4.10 minutes\n",
      "total_backward_count 5988600 real_backward_count 299846   5.007%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.718964/  1.845162, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 244.72 seconds, 4.08 minutes\n",
      "total_backward_count 6032960 real_backward_count 300705   4.984%\n",
      "fc layer 3 self.abs_max_out: 594.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.707958/  1.841277, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 247.06 seconds, 4.12 minutes\n",
      "total_backward_count 6077320 real_backward_count 301447   4.960%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [5.0, 5.5, 6.0, 6.5, 7.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"0\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = 't0h80lho'\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
