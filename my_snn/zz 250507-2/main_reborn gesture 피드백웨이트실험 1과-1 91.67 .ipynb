{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40675/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250507_190152-bld34l0e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bld34l0e' target=\"_blank\">serene-paper-8265</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bld34l0e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bld34l0e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.01, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.141395/  3.664883, val:  43.75%, val_best:  43.75%, tr:  47.27%, tr_best:  47.27%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.912074/  2.002065, val:  60.00%, val_best:  60.00%, tr:  58.00%, tr_best:  58.00%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.667315/  1.828756, val:  51.67%, val_best:  60.00%, tr:  59.72%, tr_best:  59.72%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.659223/  1.836548, val:  60.83%, val_best:  60.83%, tr:  64.07%, tr_best:  64.07%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.577251/  2.561759, val:  59.58%, val_best:  60.83%, tr:  65.92%, tr_best:  65.92%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.663566/  1.671288, val:  60.83%, val_best:  60.83%, tr:  67.79%, tr_best:  67.79%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.399959/  1.661472, val:  65.83%, val_best:  65.83%, tr:  74.21%, tr_best:  74.21%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.230597/  1.947674, val:  62.92%, val_best:  65.83%, tr:  75.97%, tr_best:  75.97%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.059059/  1.912820, val:  67.50%, val_best:  67.50%, tr:  81.38%, tr_best:  81.38%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.960093/  1.530707, val:  79.17%, val_best:  79.17%, tr:  84.78%, tr_best:  84.78%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.898578/  1.362385, val:  79.17%, val_best:  79.17%, tr:  85.93%, tr_best:  85.93%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.864920/  1.781298, val:  70.83%, val_best:  79.17%, tr:  85.73%, tr_best:  85.93%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.796967/  1.281806, val:  83.33%, val_best:  83.33%, tr:  88.12%, tr_best:  88.12%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.677496/  1.334127, val:  80.00%, val_best:  83.33%, tr:  91.70%, tr_best:  91.70%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.752050/  1.616400, val:  73.75%, val_best:  83.33%, tr:  89.81%, tr_best:  91.70%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.680960/  1.433987, val:  77.08%, val_best:  83.33%, tr:  91.52%, tr_best:  91.70%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.605240/  1.273800, val:  87.50%, val_best:  87.50%, tr:  93.78%, tr_best:  93.78%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.545622/  1.725056, val:  74.58%, val_best:  87.50%, tr:  94.21%, tr_best:  94.21%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.550925/  1.742284, val:  74.17%, val_best:  87.50%, tr:  94.77%, tr_best:  94.77%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.507880/  1.352402, val:  84.17%, val_best:  87.50%, tr:  94.75%, tr_best:  94.77%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.490162/  1.993120, val:  78.33%, val_best:  87.50%, tr:  95.72%, tr_best:  95.72%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.418266/  1.552527, val:  78.75%, val_best:  87.50%, tr:  97.11%, tr_best:  97.11%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.419453/  1.418730, val:  85.83%, val_best:  87.50%, tr:  97.05%, tr_best:  97.11%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.385612/  1.377355, val:  82.50%, val_best:  87.50%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.404783/  1.524435, val:  85.83%, val_best:  87.50%, tr:  97.11%, tr_best:  97.43%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.401530/  1.485074, val:  81.67%, val_best:  87.50%, tr:  97.07%, tr_best:  97.43%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.353116/  1.440816, val:  82.50%, val_best:  87.50%, tr:  98.33%, tr_best:  98.33%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.328781/  1.540929, val:  86.25%, val_best:  87.50%, tr:  98.56%, tr_best:  98.56%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.294509/  1.417010, val:  85.83%, val_best:  87.50%, tr:  98.81%, tr_best:  98.81%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.299515/  1.710613, val:  85.42%, val_best:  87.50%, tr:  98.44%, tr_best:  98.81%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.304841/  1.573695, val:  84.17%, val_best:  87.50%, tr:  98.78%, tr_best:  98.81%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.250367/  1.553599, val:  87.08%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.247164/  1.593852, val:  82.92%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.234886/  1.671181, val:  85.00%, val_best:  87.50%, tr:  99.14%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.234362/  1.616885, val:  86.67%, val_best:  87.50%, tr:  99.37%, tr_best:  99.37%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.210035/  1.579130, val:  88.75%, val_best:  88.75%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.204860/  1.643326, val:  87.92%, val_best:  88.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.195281/  1.632056, val:  87.50%, val_best:  88.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.184907/  1.555196, val:  88.75%, val_best:  88.75%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.161310/  1.613369, val:  83.75%, val_best:  88.75%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.161678/  1.732232, val:  84.58%, val_best:  88.75%, tr:  99.71%, tr_best:  99.84%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.149960/  1.717912, val:  88.33%, val_best:  88.75%, tr:  99.80%, tr_best:  99.84%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.134426/  1.656700, val:  90.42%, val_best:  90.42%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.119627/  1.813776, val:  85.83%, val_best:  90.42%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.107212/  1.689991, val:  89.58%, val_best:  90.42%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.109686/  1.779022, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.105539/  1.823481, val:  87.92%, val_best:  90.42%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.100848/  1.800300, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.093986/  1.805291, val:  90.00%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.091171/  1.809266, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.079866/  1.905432, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.081003/  1.847452, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.076782/  1.945377, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.074223/  1.896528, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.066792/  1.942799, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.057957/  1.986079, val:  90.00%, val_best:  90.42%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.060205/  2.049443, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.065988/  2.119761, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.059407/  1.997170, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.048467/  1.975665, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.051305/  2.113616, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.043430/  2.021899, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.048286/  2.115474, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.041898/  2.151161, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.043325/  2.119197, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.038750/  2.229352, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.036749/  2.184390, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.033449/  2.162169, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.034158/  2.215625, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.031588/  2.212856, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.027528/  2.214272, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.028989/  2.280840, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.026805/  2.233630, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.025234/  2.238068, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.027703/  2.237757, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.023677/  2.247757, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.023711/  2.243355, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.020979/  2.285697, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.026091/  2.438470, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.020991/  2.380117, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.020408/  2.347901, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.021660/  2.317267, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.024702/  2.398099, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.020048/  2.366579, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.020463/  2.360762, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.017238/  2.426220, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.017275/  2.421669, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.016018/  2.456013, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.013634/  2.542358, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.011742/  2.452416, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.014204/  2.458351, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.012431/  2.513390, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.011861/  2.460240, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.012302/  2.471559, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.012626/  2.503075, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.014730/  2.536759, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.011857/  2.512488, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.012607/  2.531030, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.010971/  2.532419, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.008980/  2.576474, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.008714/  2.524242, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.009473/  2.622217, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.010704/  2.576098, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.009929/  2.560202, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.010131/  2.568753, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.009599/  2.698266, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.008215/  2.635012, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.007587/  2.639433, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.007578/  2.628839, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.007520/  2.647523, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.007149/  2.593892, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.007762/  2.702775, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.007467/  2.629822, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.008056/  2.632171, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.007506/  2.608248, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.007689/  2.651434, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.008214/  2.638673, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.008797/  2.703406, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.008446/  2.723996, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.006200/  2.658114, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.005805/  2.657326, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.006492/  2.661175, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.006562/  2.665668, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.006552/  2.654818, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.008090/  2.734961, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.005658/  2.723546, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.006350/  2.713949, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.005633/  2.743282, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.005862/  2.716665, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.005558/  2.761009, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.005886/  2.715418, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.005226/  2.774817, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.006298/  2.811870, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.006650/  2.814159, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.006405/  2.746365, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.006292/  2.727538, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.005869/  2.864491, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.006916/  2.715059, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.005745/  2.774966, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.006270/  2.792682, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.006967/  2.835627, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.006876/  2.842979, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.006569/  2.871781, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.006194/  2.856938, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.006016/  2.812826, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.006234/  2.822481, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.005359/  2.850170, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.006087/  2.878211, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.003957/  2.836365, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.004971/  2.880366, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.004753/  2.889224, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.004189/  2.884648, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.003430/  2.835008, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.003324/  2.860341, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.002978/  2.874539, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.002503/  2.899387, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.002413/  2.839853, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.002786/  2.891502, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.002831/  2.854744, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.003145/  2.910331, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.003334/  2.876827, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.003849/  2.894062, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.003687/  2.914214, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.003568/  2.919523, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.003320/  2.921095, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.002685/  2.913115, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.002835/  2.993841, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.002789/  2.928457, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.004004/  2.968379, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.003956/  2.918752, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.004991/  3.002751, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.006787/  2.969361, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.006037/  2.983776, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.006441/  2.895763, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.005516/  2.979874, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.006956/  2.931006, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.007272/  2.942523, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.007216/  2.967657, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.007928/  2.950256, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.005246/  2.987379, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.009426/  2.944250, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.007704/  3.014209, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.006235/  2.994909, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.005378/  2.904093, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.005361/  3.025634, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.003908/  2.978712, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.004328/  3.008475, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.005156/  2.985669, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.003149/  3.004843, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.003837/  3.006301, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.004302/  2.977849, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.003725/  2.997572, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.003610/  2.994374, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.003368/  2.957877, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.002897/  2.982365, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.003218/  3.056741, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.003129/  3.033713, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.003235/  3.009091, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.003458/  2.987547, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.004749/  3.038496, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-200 lr=['0.0100000'], tr/val_loss:  0.003775/  3.032574, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-201 lr=['0.0100000'], tr/val_loss:  0.003330/  3.068464, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-202 lr=['0.0100000'], tr/val_loss:  0.003525/  3.012607, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-203 lr=['0.0100000'], tr/val_loss:  0.003501/  3.054902, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-204 lr=['0.0100000'], tr/val_loss:  0.003727/  3.064508, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-205 lr=['0.0100000'], tr/val_loss:  0.003541/  3.015849, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-206 lr=['0.0100000'], tr/val_loss:  0.003602/  3.055662, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-207 lr=['0.0100000'], tr/val_loss:  0.003002/  3.040123, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-208 lr=['0.0100000'], tr/val_loss:  0.003020/  3.018439, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-209 lr=['0.0100000'], tr/val_loss:  0.003343/  3.052445, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-210 lr=['0.0100000'], tr/val_loss:  0.003059/  3.091996, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-211 lr=['0.0100000'], tr/val_loss:  0.003698/  3.153855, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-212 lr=['0.0100000'], tr/val_loss:  0.004344/  3.112410, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-213 lr=['0.0100000'], tr/val_loss:  0.004690/  3.040883, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-214 lr=['0.0100000'], tr/val_loss:  0.003645/  3.111161, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-215 lr=['0.0100000'], tr/val_loss:  0.004497/  3.155421, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-216 lr=['0.0100000'], tr/val_loss:  0.004212/  3.065999, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-217 lr=['0.0100000'], tr/val_loss:  0.003152/  3.103396, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-218 lr=['0.0100000'], tr/val_loss:  0.002742/  3.125673, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-219 lr=['0.0100000'], tr/val_loss:  0.002557/  3.148555, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-220 lr=['0.0100000'], tr/val_loss:  0.004009/  3.086336, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-221 lr=['0.0100000'], tr/val_loss:  0.003288/  3.108818, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-222 lr=['0.0100000'], tr/val_loss:  0.003521/  3.217739, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-223 lr=['0.0100000'], tr/val_loss:  0.005277/  3.208992, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-224 lr=['0.0100000'], tr/val_loss:  0.004256/  3.153459, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-225 lr=['0.0100000'], tr/val_loss:  0.003250/  3.106571, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-226 lr=['0.0100000'], tr/val_loss:  0.003182/  3.145378, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-227 lr=['0.0100000'], tr/val_loss:  0.002471/  3.179765, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-228 lr=['0.0100000'], tr/val_loss:  0.002418/  3.189631, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-229 lr=['0.0100000'], tr/val_loss:  0.002776/  3.118979, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-230 lr=['0.0100000'], tr/val_loss:  0.002913/  3.129704, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-231 lr=['0.0100000'], tr/val_loss:  0.001858/  3.153771, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-232 lr=['0.0100000'], tr/val_loss:  0.001797/  3.171114, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-233 lr=['0.0100000'], tr/val_loss:  0.001652/  3.148087, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-234 lr=['0.0100000'], tr/val_loss:  0.001544/  3.159719, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-235 lr=['0.0100000'], tr/val_loss:  0.001443/  3.151262, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-236 lr=['0.0100000'], tr/val_loss:  0.001727/  3.142921, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-237 lr=['0.0100000'], tr/val_loss:  0.001553/  3.197656, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-238 lr=['0.0100000'], tr/val_loss:  0.002058/  3.113575, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-239 lr=['0.0100000'], tr/val_loss:  0.001891/  3.150310, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-240 lr=['0.0100000'], tr/val_loss:  0.001655/  3.178679, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-241 lr=['0.0100000'], tr/val_loss:  0.001461/  3.160223, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-242 lr=['0.0100000'], tr/val_loss:  0.001410/  3.141176, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-243 lr=['0.0100000'], tr/val_loss:  0.001453/  3.170270, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-244 lr=['0.0100000'], tr/val_loss:  0.001691/  3.192751, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-245 lr=['0.0100000'], tr/val_loss:  0.001788/  3.200083, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-246 lr=['0.0100000'], tr/val_loss:  0.001776/  3.135548, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-247 lr=['0.0100000'], tr/val_loss:  0.001671/  3.186274, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-248 lr=['0.0100000'], tr/val_loss:  0.001385/  3.219199, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-249 lr=['0.0100000'], tr/val_loss:  0.001551/  3.216544, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-250 lr=['0.0100000'], tr/val_loss:  0.001326/  3.210292, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-251 lr=['0.0100000'], tr/val_loss:  0.001567/  3.200378, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-252 lr=['0.0100000'], tr/val_loss:  0.001375/  3.225755, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-253 lr=['0.0100000'], tr/val_loss:  0.001399/  3.214968, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-254 lr=['0.0100000'], tr/val_loss:  0.001254/  3.217769, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-255 lr=['0.0100000'], tr/val_loss:  0.001426/  3.209170, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-256 lr=['0.0100000'], tr/val_loss:  0.001373/  3.201526, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-257 lr=['0.0100000'], tr/val_loss:  0.001428/  3.193228, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-258 lr=['0.0100000'], tr/val_loss:  0.001345/  3.224930, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-259 lr=['0.0100000'], tr/val_loss:  0.001261/  3.243358, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-260 lr=['0.0100000'], tr/val_loss:  0.001267/  3.216222, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-261 lr=['0.0100000'], tr/val_loss:  0.001201/  3.182147, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-262 lr=['0.0100000'], tr/val_loss:  0.001303/  3.168847, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-263 lr=['0.0100000'], tr/val_loss:  0.001558/  3.211471, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-264 lr=['0.0100000'], tr/val_loss:  0.001550/  3.218774, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-265 lr=['0.0100000'], tr/val_loss:  0.001557/  3.252871, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-266 lr=['0.0100000'], tr/val_loss:  0.001940/  3.226165, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-267 lr=['0.0100000'], tr/val_loss:  0.002556/  3.251122, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-268 lr=['0.0100000'], tr/val_loss:  0.001858/  3.222306, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-269 lr=['0.0100000'], tr/val_loss:  0.002389/  3.253892, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-270 lr=['0.0100000'], tr/val_loss:  0.002138/  3.242260, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-271 lr=['0.0100000'], tr/val_loss:  0.001551/  3.208220, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-272 lr=['0.0100000'], tr/val_loss:  0.002410/  3.203320, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-273 lr=['0.0100000'], tr/val_loss:  0.002843/  3.246442, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-274 lr=['0.0100000'], tr/val_loss:  0.002058/  3.184916, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-275 lr=['0.0100000'], tr/val_loss:  0.003013/  3.219568, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-276 lr=['0.0100000'], tr/val_loss:  0.003219/  3.246346, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-277 lr=['0.0100000'], tr/val_loss:  0.002881/  3.250889, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-278 lr=['0.0100000'], tr/val_loss:  0.002165/  3.270901, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-279 lr=['0.0100000'], tr/val_loss:  0.001775/  3.286616, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-280 lr=['0.0100000'], tr/val_loss:  0.001531/  3.239614, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-281 lr=['0.0100000'], tr/val_loss:  0.001770/  3.299120, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-282 lr=['0.0100000'], tr/val_loss:  0.001507/  3.258331, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-283 lr=['0.0100000'], tr/val_loss:  0.001877/  3.264002, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-284 lr=['0.0100000'], tr/val_loss:  0.001743/  3.278013, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-285 lr=['0.0100000'], tr/val_loss:  0.001657/  3.266778, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-286 lr=['0.0100000'], tr/val_loss:  0.001394/  3.299082, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-287 lr=['0.0100000'], tr/val_loss:  0.001307/  3.298511, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-288 lr=['0.0100000'], tr/val_loss:  0.001281/  3.275713, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-289 lr=['0.0100000'], tr/val_loss:  0.001205/  3.304539, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-290 lr=['0.0100000'], tr/val_loss:  0.001155/  3.333758, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-291 lr=['0.0100000'], tr/val_loss:  0.001096/  3.305629, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-292 lr=['0.0100000'], tr/val_loss:  0.001136/  3.290887, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-293 lr=['0.0100000'], tr/val_loss:  0.001422/  3.339337, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-294 lr=['0.0100000'], tr/val_loss:  0.001137/  3.344545, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-295 lr=['0.0100000'], tr/val_loss:  0.001118/  3.318829, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-296 lr=['0.0100000'], tr/val_loss:  0.001087/  3.318938, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-297 lr=['0.0100000'], tr/val_loss:  0.001096/  3.340683, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-298 lr=['0.0100000'], tr/val_loss:  0.001079/  3.331642, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-299 lr=['0.0100000'], tr/val_loss:  0.001039/  3.324069, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-300 lr=['0.0100000'], tr/val_loss:  0.001125/  3.311405, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-301 lr=['0.0100000'], tr/val_loss:  0.001087/  3.316528, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-302 lr=['0.0100000'], tr/val_loss:  0.001140/  3.300061, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-303 lr=['0.0100000'], tr/val_loss:  0.001033/  3.305815, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-304 lr=['0.0100000'], tr/val_loss:  0.001249/  3.303960, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-305 lr=['0.0100000'], tr/val_loss:  0.001117/  3.312697, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-306 lr=['0.0100000'], tr/val_loss:  0.001095/  3.335403, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-307 lr=['0.0100000'], tr/val_loss:  0.001102/  3.350041, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-308 lr=['0.0100000'], tr/val_loss:  0.001075/  3.340469, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-309 lr=['0.0100000'], tr/val_loss:  0.001246/  3.340629, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-310 lr=['0.0100000'], tr/val_loss:  0.001123/  3.325033, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-311 lr=['0.0100000'], tr/val_loss:  0.001053/  3.331451, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-312 lr=['0.0100000'], tr/val_loss:  0.001051/  3.360653, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-313 lr=['0.0100000'], tr/val_loss:  0.001042/  3.345824, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-314 lr=['0.0100000'], tr/val_loss:  0.001041/  3.337082, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-315 lr=['0.0100000'], tr/val_loss:  0.001037/  3.341205, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-316 lr=['0.0100000'], tr/val_loss:  0.001044/  3.346864, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-317 lr=['0.0100000'], tr/val_loss:  0.001095/  3.341732, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-318 lr=['0.0100000'], tr/val_loss:  0.001098/  3.357025, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-319 lr=['0.0100000'], tr/val_loss:  0.001145/  3.351153, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-320 lr=['0.0100000'], tr/val_loss:  0.001148/  3.336168, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-321 lr=['0.0100000'], tr/val_loss:  0.001117/  3.366249, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-322 lr=['0.0100000'], tr/val_loss:  0.001066/  3.350161, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-323 lr=['0.0100000'], tr/val_loss:  0.001071/  3.328775, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-324 lr=['0.0100000'], tr/val_loss:  0.001095/  3.352114, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-325 lr=['0.0100000'], tr/val_loss:  0.001101/  3.356582, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-326 lr=['0.0100000'], tr/val_loss:  0.001031/  3.372466, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-327 lr=['0.0100000'], tr/val_loss:  0.001021/  3.355010, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-328 lr=['0.0100000'], tr/val_loss:  0.001005/  3.370951, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-329 lr=['0.0100000'], tr/val_loss:  0.001035/  3.359188, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-330 lr=['0.0100000'], tr/val_loss:  0.001032/  3.349499, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-331 lr=['0.0100000'], tr/val_loss:  0.001060/  3.369388, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-332 lr=['0.0100000'], tr/val_loss:  0.001070/  3.372562, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-333 lr=['0.0100000'], tr/val_loss:  0.001032/  3.373548, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-334 lr=['0.0100000'], tr/val_loss:  0.001052/  3.342111, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-335 lr=['0.0100000'], tr/val_loss:  0.001114/  3.349040, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-336 lr=['0.0100000'], tr/val_loss:  0.001061/  3.342617, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-337 lr=['0.0100000'], tr/val_loss:  0.001361/  3.352784, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-338 lr=['0.0100000'], tr/val_loss:  0.001148/  3.346697, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-339 lr=['0.0100000'], tr/val_loss:  0.001119/  3.339518, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-340 lr=['0.0100000'], tr/val_loss:  0.001011/  3.363520, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-341 lr=['0.0100000'], tr/val_loss:  0.001057/  3.354708, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-342 lr=['0.0100000'], tr/val_loss:  0.001507/  3.343611, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-343 lr=['0.0100000'], tr/val_loss:  0.001874/  3.351719, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-344 lr=['0.0100000'], tr/val_loss:  0.001845/  3.354340, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-345 lr=['0.0100000'], tr/val_loss:  0.001945/  3.327870, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-346 lr=['0.0100000'], tr/val_loss:  0.002460/  3.315770, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-347 lr=['0.0100000'], tr/val_loss:  0.003173/  3.354815, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-348 lr=['0.0100000'], tr/val_loss:  0.002370/  3.407287, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-349 lr=['0.0100000'], tr/val_loss:  0.003075/  3.332187, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-350 lr=['0.0100000'], tr/val_loss:  0.002824/  3.313638, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-351 lr=['0.0100000'], tr/val_loss:  0.003866/  3.279012, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-352 lr=['0.0100000'], tr/val_loss:  0.003240/  3.364590, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-353 lr=['0.0100000'], tr/val_loss:  0.002311/  3.384570, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-354 lr=['0.0100000'], tr/val_loss:  0.001913/  3.471926, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-355 lr=['0.0100000'], tr/val_loss:  0.001991/  3.401116, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-356 lr=['0.0100000'], tr/val_loss:  0.002464/  3.388907, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-357 lr=['0.0100000'], tr/val_loss:  0.002759/  3.368800, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-358 lr=['0.0100000'], tr/val_loss:  0.002623/  3.378294, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-359 lr=['0.0100000'], tr/val_loss:  0.002368/  3.400198, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-360 lr=['0.0100000'], tr/val_loss:  0.002960/  3.362010, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-361 lr=['0.0100000'], tr/val_loss:  0.004016/  3.319142, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-362 lr=['0.0100000'], tr/val_loss:  0.003892/  3.308181, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-363 lr=['0.0100000'], tr/val_loss:  0.005079/  3.537681, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-364 lr=['0.0100000'], tr/val_loss:  0.002917/  3.383402, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-365 lr=['0.0100000'], tr/val_loss:  0.002308/  3.371692, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-366 lr=['0.0100000'], tr/val_loss:  0.002976/  3.383718, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-367 lr=['0.0100000'], tr/val_loss:  0.003040/  3.291347, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-368 lr=['0.0100000'], tr/val_loss:  0.003132/  3.360569, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-369 lr=['0.0100000'], tr/val_loss:  0.002802/  3.406309, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-370 lr=['0.0100000'], tr/val_loss:  0.003821/  3.375593, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-371 lr=['0.0100000'], tr/val_loss:  0.004249/  3.350211, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-372 lr=['0.0100000'], tr/val_loss:  0.002939/  3.510678, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-373 lr=['0.0100000'], tr/val_loss:  0.003383/  3.364320, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-374 lr=['0.0100000'], tr/val_loss:  0.002430/  3.386803, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-375 lr=['0.0100000'], tr/val_loss:  0.002829/  3.388651, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-376 lr=['0.0100000'], tr/val_loss:  0.002497/  3.475101, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-377 lr=['0.0100000'], tr/val_loss:  0.002084/  3.407875, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-378 lr=['0.0100000'], tr/val_loss:  0.002055/  3.424675, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-379 lr=['0.0100000'], tr/val_loss:  0.002605/  3.456452, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-380 lr=['0.0100000'], tr/val_loss:  0.002737/  3.488072, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-381 lr=['0.0100000'], tr/val_loss:  0.002516/  3.478496, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-382 lr=['0.0100000'], tr/val_loss:  0.002545/  3.425472, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-383 lr=['0.0100000'], tr/val_loss:  0.002699/  3.447871, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-384 lr=['0.0100000'], tr/val_loss:  0.002056/  3.361186, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-385 lr=['0.0100000'], tr/val_loss:  0.001765/  3.403891, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-386 lr=['0.0100000'], tr/val_loss:  0.001753/  3.457721, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-387 lr=['0.0100000'], tr/val_loss:  0.003692/  3.379155, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-388 lr=['0.0100000'], tr/val_loss:  0.003232/  3.332009, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-389 lr=['0.0100000'], tr/val_loss:  0.003410/  3.422924, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-390 lr=['0.0100000'], tr/val_loss:  0.003970/  3.511458, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-391 lr=['0.0100000'], tr/val_loss:  0.003298/  3.471781, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-392 lr=['0.0100000'], tr/val_loss:  0.002664/  3.489122, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-393 lr=['0.0100000'], tr/val_loss:  0.003198/  3.404137, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-394 lr=['0.0100000'], tr/val_loss:  0.003855/  3.439977, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-395 lr=['0.0100000'], tr/val_loss:  0.003432/  3.374889, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-396 lr=['0.0100000'], tr/val_loss:  0.003868/  3.359189, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-397 lr=['0.0100000'], tr/val_loss:  0.004281/  3.415666, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-398 lr=['0.0100000'], tr/val_loss:  0.006522/  3.484824, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-399 lr=['0.0100000'], tr/val_loss:  0.005259/  3.376297, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-400 lr=['0.0100000'], tr/val_loss:  0.004185/  3.498510, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-401 lr=['0.0100000'], tr/val_loss:  0.003886/  3.372811, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-402 lr=['0.0100000'], tr/val_loss:  0.003162/  3.429302, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-403 lr=['0.0100000'], tr/val_loss:  0.003792/  3.394119, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-404 lr=['0.0100000'], tr/val_loss:  0.003958/  3.391780, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-405 lr=['0.0100000'], tr/val_loss:  0.003219/  3.458403, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-406 lr=['0.0100000'], tr/val_loss:  0.003086/  3.482101, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-407 lr=['0.0100000'], tr/val_loss:  0.002495/  3.484020, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-408 lr=['0.0100000'], tr/val_loss:  0.002424/  3.475083, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-409 lr=['0.0100000'], tr/val_loss:  0.002126/  3.469519, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-410 lr=['0.0100000'], tr/val_loss:  0.002157/  3.451045, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-411 lr=['0.0100000'], tr/val_loss:  0.002091/  3.465468, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-412 lr=['0.0100000'], tr/val_loss:  0.001964/  3.492821, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-413 lr=['0.0100000'], tr/val_loss:  0.001679/  3.488349, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-414 lr=['0.0100000'], tr/val_loss:  0.001895/  3.457857, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-415 lr=['0.0100000'], tr/val_loss:  0.001722/  3.439010, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-416 lr=['0.0100000'], tr/val_loss:  0.001995/  3.415972, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-417 lr=['0.0100000'], tr/val_loss:  0.003454/  3.528912, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-418 lr=['0.0100000'], tr/val_loss:  0.003080/  3.550671, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-419 lr=['0.0100000'], tr/val_loss:  0.002592/  3.512015, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-420 lr=['0.0100000'], tr/val_loss:  0.002766/  3.521509, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-421 lr=['0.0100000'], tr/val_loss:  0.002725/  3.511456, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-422 lr=['0.0100000'], tr/val_loss:  0.003089/  3.510947, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-423 lr=['0.0100000'], tr/val_loss:  0.002352/  3.450952, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-424 lr=['0.0100000'], tr/val_loss:  0.002430/  3.518153, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-425 lr=['0.0100000'], tr/val_loss:  0.002871/  3.494174, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-426 lr=['0.0100000'], tr/val_loss:  0.002770/  3.517752, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-427 lr=['0.0100000'], tr/val_loss:  0.002806/  3.496982, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-428 lr=['0.0100000'], tr/val_loss:  0.001936/  3.522703, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-429 lr=['0.0100000'], tr/val_loss:  0.001338/  3.541215, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-430 lr=['0.0100000'], tr/val_loss:  0.001278/  3.530645, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-431 lr=['0.0100000'], tr/val_loss:  0.001274/  3.528367, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-432 lr=['0.0100000'], tr/val_loss:  0.001772/  3.539690, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-433 lr=['0.0100000'], tr/val_loss:  0.002090/  3.543100, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-434 lr=['0.0100000'], tr/val_loss:  0.002370/  3.588457, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-435 lr=['0.0100000'], tr/val_loss:  0.001860/  3.577200, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-436 lr=['0.0100000'], tr/val_loss:  0.001512/  3.515908, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-437 lr=['0.0100000'], tr/val_loss:  0.001321/  3.579841, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-438 lr=['0.0100000'], tr/val_loss:  0.001509/  3.643361, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-439 lr=['0.0100000'], tr/val_loss:  0.001515/  3.626436, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-440 lr=['0.0100000'], tr/val_loss:  0.001520/  3.570937, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-441 lr=['0.0100000'], tr/val_loss:  0.001539/  3.554683, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-442 lr=['0.0100000'], tr/val_loss:  0.001308/  3.541233, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-443 lr=['0.0100000'], tr/val_loss:  0.001418/  3.566374, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-444 lr=['0.0100000'], tr/val_loss:  0.001178/  3.562097, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-445 lr=['0.0100000'], tr/val_loss:  0.001177/  3.579495, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-446 lr=['0.0100000'], tr/val_loss:  0.001334/  3.523469, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-447 lr=['0.0100000'], tr/val_loss:  0.001481/  3.540074, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-448 lr=['0.0100000'], tr/val_loss:  0.001628/  3.575463, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-449 lr=['0.0100000'], tr/val_loss:  0.002346/  3.519738, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-450 lr=['0.0100000'], tr/val_loss:  0.002038/  3.517075, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-451 lr=['0.0100000'], tr/val_loss:  0.001450/  3.526516, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-452 lr=['0.0100000'], tr/val_loss:  0.001509/  3.540975, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-453 lr=['0.0100000'], tr/val_loss:  0.001159/  3.591458, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-454 lr=['0.0100000'], tr/val_loss:  0.001214/  3.571563, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-455 lr=['0.0100000'], tr/val_loss:  0.001081/  3.563039, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-456 lr=['0.0100000'], tr/val_loss:  0.001064/  3.560886, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-457 lr=['0.0100000'], tr/val_loss:  0.001166/  3.560597, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-458 lr=['0.0100000'], tr/val_loss:  0.000978/  3.575137, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-459 lr=['0.0100000'], tr/val_loss:  0.001095/  3.597400, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-460 lr=['0.0100000'], tr/val_loss:  0.001092/  3.570526, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-461 lr=['0.0100000'], tr/val_loss:  0.001070/  3.558254, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-462 lr=['0.0100000'], tr/val_loss:  0.001165/  3.575181, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-463 lr=['0.0100000'], tr/val_loss:  0.001093/  3.570988, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-464 lr=['0.0100000'], tr/val_loss:  0.001021/  3.578401, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-465 lr=['0.0100000'], tr/val_loss:  0.001032/  3.568312, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-466 lr=['0.0100000'], tr/val_loss:  0.000995/  3.562452, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-467 lr=['0.0100000'], tr/val_loss:  0.001001/  3.570287, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-468 lr=['0.0100000'], tr/val_loss:  0.000989/  3.570586, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-469 lr=['0.0100000'], tr/val_loss:  0.001037/  3.568467, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-470 lr=['0.0100000'], tr/val_loss:  0.000887/  3.577072, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-471 lr=['0.0100000'], tr/val_loss:  0.001017/  3.562798, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-472 lr=['0.0100000'], tr/val_loss:  0.001090/  3.580635, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-473 lr=['0.0100000'], tr/val_loss:  0.001011/  3.597382, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-474 lr=['0.0100000'], tr/val_loss:  0.001041/  3.565668, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-475 lr=['0.0100000'], tr/val_loss:  0.000977/  3.575021, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-476 lr=['0.0100000'], tr/val_loss:  0.000972/  3.554368, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-477 lr=['0.0100000'], tr/val_loss:  0.000966/  3.567456, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-478 lr=['0.0100000'], tr/val_loss:  0.000924/  3.579243, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-479 lr=['0.0100000'], tr/val_loss:  0.000905/  3.583025, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-480 lr=['0.0100000'], tr/val_loss:  0.000952/  3.582933, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-481 lr=['0.0100000'], tr/val_loss:  0.000898/  3.569859, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-482 lr=['0.0100000'], tr/val_loss:  0.000980/  3.564679, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-483 lr=['0.0100000'], tr/val_loss:  0.000983/  3.578707, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-484 lr=['0.0100000'], tr/val_loss:  0.000901/  3.573651, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-485 lr=['0.0100000'], tr/val_loss:  0.000962/  3.566119, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-486 lr=['0.0100000'], tr/val_loss:  0.000912/  3.572340, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-487 lr=['0.0100000'], tr/val_loss:  0.000898/  3.574495, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-488 lr=['0.0100000'], tr/val_loss:  0.000956/  3.565786, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-489 lr=['0.0100000'], tr/val_loss:  0.001010/  3.570802, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-490 lr=['0.0100000'], tr/val_loss:  0.000921/  3.551803, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-491 lr=['0.0100000'], tr/val_loss:  0.000935/  3.568724, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-492 lr=['0.0100000'], tr/val_loss:  0.000934/  3.543086, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-493 lr=['0.0100000'], tr/val_loss:  0.001115/  3.545921, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-494 lr=['0.0100000'], tr/val_loss:  0.001319/  3.578966, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-495 lr=['0.0100000'], tr/val_loss:  0.001320/  3.551728, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-496 lr=['0.0100000'], tr/val_loss:  0.001135/  3.540746, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-497 lr=['0.0100000'], tr/val_loss:  0.001035/  3.550389, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-498 lr=['0.0100000'], tr/val_loss:  0.000968/  3.525105, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-499 lr=['0.0100000'], tr/val_loss:  0.001127/  3.520894, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-500 lr=['0.0100000'], tr/val_loss:  0.001081/  3.557647, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-501 lr=['0.0100000'], tr/val_loss:  0.001057/  3.604050, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-502 lr=['0.0100000'], tr/val_loss:  0.001072/  3.550650, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-503 lr=['0.0100000'], tr/val_loss:  0.001333/  3.537418, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-504 lr=['0.0100000'], tr/val_loss:  0.001027/  3.553169, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-505 lr=['0.0100000'], tr/val_loss:  0.001149/  3.522316, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-506 lr=['0.0100000'], tr/val_loss:  0.001152/  3.539114, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-507 lr=['0.0100000'], tr/val_loss:  0.001235/  3.589262, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-508 lr=['0.0100000'], tr/val_loss:  0.001216/  3.519673, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.0 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"1\",\n",
    "                single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = decay, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 6, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 9, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 5, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "#         \"epoch_num\": {\"values\": [100]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [5]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [100_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [True]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [1]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"0\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  unique_name_hyper,\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#                         ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = '6pj3lh8j'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
