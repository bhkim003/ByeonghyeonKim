{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12673/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAIhCAYAAACfVbSSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA72ElEQVR4nO3de1iUdf7/8deAAh4AjyAqIp020gwDK09dZknrqtl20LXykNpqeEhxS/naZmlJWpm7mZR5yjxEpqaVWWxuaZsmkWlnK02wJNJM1BRk5v794cpvR9BkmvnczszzcV33dS0f7vnc75m03vu6P/O5HZZlWQIAAIDPhdhdAAAAQLCg8QIAADCExgsAAMAQGi8AAABDaLwAAAAMofECAAAwhMYLAADAEBovAAAAQ2i8AAAADKHxAjywcOFCORyOiqNGjRqKi4vTX/7yF3399de21fXggw/K4XDYdv1T5efna8SIEbr00ksVGRmp2NhYXXfddVq/fn2lcwcNGuT2mdapU0ctW7bUDTfcoAULFqi0tLTa18/IyJDD4VDPnj298XYA4Hej8QJ+hwULFmjTpk3617/+pZEjR2rNmjXq1KmTDhw4YHdp54Rly5Zpy5YtGjx4sFavXq25c+cqPDxc1157rRYtWlTp/Fq1amnTpk3atGmTXnvtNU2ePFl16tTRXXfdpZSUFO3Zs+esr338+HEtXrxYkrRu3Tp9//33XntfAOAxC0C1LViwwJJk5eXluY0/9NBDliRr/vz5ttQ1adIk61z6a/3jjz9WGisvL7fatGljnX/++W7jAwcOtOrUqVPlPG+++aZVs2ZN68orrzzray9fvtySZPXo0cOSZD3yyCNn9bqysjLr+PHjVf7uyJEjZ319AKgKiRfgRampqZKkH3/8sWLs2LFjGjdunJKTkxUdHa0GDRqoffv2Wr16daXXOxwOjRw5Ui+88IKSkpJUu3ZtXXbZZXrttdcqnfv6668rOTlZ4eHhSkxM1OOPP15lTceOHVNmZqYSExMVFhamZs2aacSIEfrll1/czmvZsqV69uyp1157TW3btlWtWrWUlJRUce2FCxcqKSlJderU0RVXXKEPP/zwNz+PmJiYSmOhoaFKSUlRYWHhb77+pLS0NN1111364IMPtGHDhrN6zbx58xQWFqYFCxYoPj5eCxYskGVZbue88847cjgceuGFFzRu3Dg1a9ZM4eHh+uabbzRo0CDVrVtXn3zyidLS0hQZGalrr71WkpSbm6vevXurefPmioiI0AUXXKBhw4Zp3759FXNv3LhRDodDy5Ytq1TbokWL5HA4lJeXd9afAYDAQOMFeNGuXbskSRdddFHFWGlpqX7++Wf97W9/0yuvvKJly5apU6dOuummm6q83fb6669r1qxZmjx5slasWKEGDRroz3/+s3bu3Flxzttvv63evXsrMjJSL774oh577DG99NJLWrBggdtclmXpxhtv1OOPP67+/fvr9ddfV0ZGhp5//nl17dq10rqpbdu2KTMzU+PHj9fKlSsVHR2tm266SZMmTdLcuXM1depULVmyRAcPHlTPnj119OjRan9G5eXl2rhxo1q1alWt191www2SdFaN1549e/TWW2+pd+/eaty4sQYOHKhvvvnmtK/NzMxUQUGBnnnmGb366qsVDWNZWZluuOEGde3aVatXr9ZDDz0kSfr222/Vvn17ZWdn66233tIDDzygDz74QJ06ddLx48clSZ07d1bbtm319NNPV7rerFmz1K5dO7Vr165anwGAAGB35Ab4o5O3Gjdv3mwdP37cOnTokLVu3TqrSZMm1tVXX33aW1WWdeJW2/Hjx60hQ4ZYbdu2dfudJCs2NtYqKSmpGCsqKrJCQkKsrKysirErr7zSatq0qXX06NGKsZKSEqtBgwZutxrXrVtnSbKmT5/udp2cnBxLkjVnzpyKsYSEBKtWrVrWnj17KsY+/vhjS5IVFxfndpvtlVdesSRZa9asOZuPy83EiRMtSdYrr7ziNn6mW42WZVlffPGFJcm6++67f/MakydPtiRZ69atsyzLsnbu3Gk5HA6rf//+buf9+9//tiRZV199daU5Bg4ceFa3jV0ul3X8+HFr9+7dliRr9erVFb87+edk69atFWNbtmyxJFnPP//8b74PAIGHxAv4Ha666irVrFlTkZGR+uMf/6j69etr9erVqlGjhtt5y5cvV8eOHVW3bl3VqFFDNWvW1Lx58/TFF19UmvOaa65RZGRkxc+xsbGKiYnR7t27JUlHjhxRXl6ebrrpJkVERFScFxkZqV69ernNdfLbg4MGDXIbv/XWW1WnTh29/fbbbuPJyclq1qxZxc9JSUmSpC5duqh27dqVxk/WdLbmzp2rRx55ROPGjVPv3r2r9VrrlNuEZzrv5O3Fbt26SZISExPVpUsXrVixQiUlJZVec/PNN592vqp+V1xcrOHDhys+Pr7in2dCQoIkuf0z7devn2JiYtxSr6eeekqNGzdW3759z+r9AAgsNF7A77Bo0SLl5eVp/fr1GjZsmL744gv169fP7ZyVK1eqT58+atasmRYvXqxNmzYpLy9PgwcP1rFjxyrN2bBhw0pj4eHhFbf1Dhw4IJfLpSZNmlQ679Sx/fv3q0aNGmrcuLHbuMPhUJMmTbR//3638QYNGrj9HBYWdsbxquo/nQULFmjYsGH661//qscee+ysX3fSySavadOmZzxv/fr12rVrl2699VaVlJTol19+0S+//KI+ffro119/rXLNVVxcXJVz1a5dW1FRUW5jLpdLaWlpWrlype677z69/fbb2rJlizZv3ixJbrdfw8PDNWzYMC1dulS//PKLfvrpJ7300ksaOnSowsPDq/X+AQSGGr99CoDTSUpKqlhQf80118jpdGru3Ll6+eWXdcstt0iSFi9erMTEROXk5LjtseXJvlSSVL9+fTkcDhUVFVX63aljDRs2VHl5uX766Se35suyLBUVFRlbY7RgwQINHTpUAwcO1DPPPOPRXmNr1qyRdCJ9O5N58+ZJkmbMmKEZM2ZU+fthw4a5jZ2unqrGP/30U23btk0LFy7UwIEDK8a/+eabKue4++679eijj2r+/Pk6duyYysvLNXz48DO+BwCBi8QL8KLp06erfv36euCBB+RyuSSd+I93WFiY23/Ei4qKqvxW49k4+a3ClStXuiVOhw4d0quvvup27slv4Z3cz+qkFStW6MiRIxW/96WFCxdq6NChuuOOOzR37lyPmq7c3FzNnTtXHTp0UKdOnU573oEDB7Rq1Sp17NhR//73vysdt99+u/Ly8vTpp596/H5O1n9qYvXss89WeX5cXJxuvfVWzZ49W88884x69eqlFi1aeHx9AP6NxAvwovr16yszM1P33Xefli5dqjvuuEM9e/bUypUrlZ6erltuuUWFhYWaMmWK4uLiPN7lfsqUKfrjH/+obt26ady4cXI6nZo2bZrq1Kmjn3/+ueK8bt266frrr9f48eNVUlKijh07avv27Zo0aZLatm2r/v37e+utV2n58uUaMmSIkpOTNWzYMG3ZssXt923btnVrYFwuV8Utu9LSUhUUFOiNN97QSy+9pKSkJL300ktnvN6SJUt07NgxjR49uspkrGHDhlqyZInmzZunJ5980qP3dPHFF+v888/XhAkTZFmWGjRooFdffVW5ubmnfc0999yjK6+8UpIqffMUQJCxd20/4J9Ot4GqZVnW0aNHrRYtWlgXXnihVV5eblmWZT366KNWy5YtrfDwcCspKcl67rnnqtzsVJI1YsSISnMmJCRYAwcOdBtbs2aN1aZNGyssLMxq0aKF9eijj1Y559GjR63x48dbCQkJVs2aNa24uDjr7rvvtg4cOFDpGj169Kh07apq2rVrlyXJeuyxx077GVnW//9m4OmOXbt2nfbcWrVqWS1atLB69eplzZ8/3yotLT3jtSzLspKTk62YmJgznnvVVVdZjRo1skpLSyu+1bh8+fIqaz/dtyw///xzq1u3blZkZKRVv35969Zbb7UKCgosSdakSZOqfE3Lli2tpKSk33wPAAKbw7LO8qtCAACPbN++XZdddpmefvpppaen210OABvReAGAj3z77bfavXu3/u///k8FBQX65ptv3LblABB8WFwPAD4yZcoUdevWTYcPH9by5ctpugCQeAEAAJhC4gUAAGAIjRcAAIAhNF4AAACG+PUGqi6XSz/88IMiIyM92g0bAIBgYlmWDh06pKZNmyokxHz2cuzYMZWVlflk7rCwMEVERPhkbm/y68brhx9+UHx8vN1lAADgVwoLC9W8eXOj1zx27JgSE+qqqNjpk/mbNGmiXbt2nfPNl183XpGRkZKktj0nKrTmuf1Bn6ruK/l2l+CR0lX+2+jWHvSL3SV4JHaFb/4l5WufvtDK7hI89saEbLtL8MjArj3sLsEjX/69id0leOzjrs/bXUK1HDrsUmJKQcV/P00qKytTUbFTu/NbKirSu2lbySGXElK+U1lZGY2XL528vRhaM0I1/KzxquGoaXcJHnHWCf/tk85RNRxhdpfgkbC6/tl4hYb519/J/+Xt/yiYUiPEP/9+htTiz4ppdi7PqRvpUN1I717fJf9ZbuTXjRcAAPAvTsslp5d3EHVaLu9O6EP+2aoDAAD4IRIvAABgjEuWXPJu5OXt+XyJxAsAAMAQEi8AAGCMSy55e0WW92f0HRIvAAAAQ0i8AACAMU7LktPy7posb8/nSyReAAAAhpB4AQAAY4L9W400XgAAwBiXLDmDuPHiViMAAIAhJF4AAMCYYL/VSOIFAABgCIkXAAAwhu0kAAAAYASJFwAAMMb138Pbc/oL2xOv2bNnKzExUREREUpJSdHGjRvtLgkAAMAnbG28cnJyNGbMGE2cOFFbt25V586d1b17dxUUFNhZFgAA8BHnf/fx8vbhL2xtvGbMmKEhQ4Zo6NChSkpK0syZMxUfH6/s7Gw7ywIAAD7itHxz+AvbGq+ysjLl5+crLS3NbTwtLU3vv/9+la8pLS1VSUmJ2wEAAOAvbGu89u3bJ6fTqdjYWLfx2NhYFRUVVfmarKwsRUdHVxzx8fEmSgUAAF7i8tHhL2xfXO9wONx+tiyr0thJmZmZOnjwYMVRWFhookQAAACvsG07iUaNGik0NLRSulVcXFwpBTspPDxc4eHhJsoDAAA+4JJDTlUdsPyeOf2FbYlXWFiYUlJSlJub6zaem5urDh062FQVAACA79i6gWpGRob69++v1NRUtW/fXnPmzFFBQYGGDx9uZ1kAAMBHXNaJw9tz+gtbG6++fftq//79mjx5svbu3avWrVtr7dq1SkhIsLMsAAAAn7D9kUHp6elKT0+3uwwAAGCA0wdrvLw9ny/Z3ngBAIDgEeyNl+3bSQAAAAQLEi8AAGCMy3LIZXl5Owkvz+dLJF4AAACGkHgBAABjWOMFAAAAI0i8AACAMU6FyOnl3Mfp1dl8i8QLAADAEBIvAABgjOWDbzVafvStRhovAABgDIvrAQAAYASJFwAAMMZphchpeXlxveXV6XyKxAsAAMAQEi8AAGCMSw65vJz7uOQ/kReJFwAAgCEBkXgdaxii0DD/6iH3TbnK7hI8ErXM7go8d2BhLbtL8MjOt+rZXYJHSlP8aUtDd0N3X293CR65cE2x3SV45PMtze0uwWMP72ttdwnVUnr4uKTvbK2BbzUCAADAiIBIvAAAgH/wzbca/WeNF40XAAAw5sTieu/eGvT2fL7ErUYAAABDSLwAAIAxLoXIyXYSAAAA8DUSLwAAYEywL64n8QIAADCExAsAABjjUgiPDAIAAIDvkXgBAABjnJZDTsvLjwzy8ny+ROMFAACMcfpgOwkntxoBAABwKhIvAABgjMsKkcvL20m42E4CAAAApyLxAgAAxrDGCwAAAEaQeAEAAGNc8v72Dy6vzuZbJF4AAACGkHgBAABjfPPIIP/JkWi8AACAMU4rRE4vbyfh7fl8yX8qBQAA8HMkXgAAwBiXHHLJ24vr/edZjSReAAAAhpB4AQAAY1jjBQAAACNIvAAAgDG+eWSQ/+RI/lMpAACAnyPxAgAAxrgsh1zefmSQl+fzJRIvAAAAQ0i8AACAMS4frPHikUEAAABVcFkhcnl5+wdvz+dL/lMpAACAF82ePVuJiYmKiIhQSkqKNm7ceMbzlyxZossuu0y1a9dWXFyc7rzzTu3fv79a16TxAgAAxjjl8MlRXTk5ORozZowmTpyorVu3qnPnzurevbsKCgqqPP+9997TgAEDNGTIEH322Wdavny58vLyNHTo0Gpdl8YLAAAEhJKSErejtLT0tOfOmDFDQ4YM0dChQ5WUlKSZM2cqPj5e2dnZVZ6/efNmtWzZUqNHj1ZiYqI6deqkYcOG6cMPP6xWjTReAADAmJNrvLx9SFJ8fLyio6MrjqysrCprKCsrU35+vtLS0tzG09LS9P7771f5mg4dOmjPnj1au3atLMvSjz/+qJdfflk9evSo1vtncT0AAAgIhYWFioqKqvg5PDy8yvP27dsnp9Op2NhYt/HY2FgVFRVV+ZoOHTpoyZIl6tu3r44dO6by8nLdcMMNeuqpp6pVI4kXAAAwxilfrPM6ISoqyu04XeN1ksPhvjbMsqxKYyd9/vnnGj16tB544AHl5+dr3bp12rVrl4YPH16t90/iBQAAgkqjRo0UGhpaKd0qLi6ulIKdlJWVpY4dO+ree++VJLVp00Z16tRR586d9fDDDysuLu6srk3iBQAAjPHlGq+zFRYWppSUFOXm5rqN5+bmqkOHDlW+5tdff1VIiPt1QkNDJZ1Iys4WiRcAADDGaYXI6eUNTz2ZLyMjQ/3791dqaqrat2+vOXPmqKCgoOLWYWZmpr7//nstWrRIktSrVy/dddddys7O1vXXX6+9e/dqzJgxuuKKK9S0adOzvi6NFwAACDp9+/bV/v37NXnyZO3du1etW7fW2rVrlZCQIEnau3ev255egwYN0qFDhzRr1iyNGzdO9erVU9euXTVt2rRqXZfGCwAAGGPJIZcHG57+1pyeSE9PV3p6epW/W7hwYaWxUaNGadSoUR5d6yTWeAEAABhC4gUAAIw5V9Z42cV/KgUAAPBzAZF4Lc/4hyIj/auH7DXlXrtL8Mi+TsftLsFjSdEH7S7BIz/uqWd3CR7p3WuL3SV4bMXbV9ldgkdqHPGvfw+eNKnvy3aX4LG533Wyu4RqKT9SKukNW2twWQ65LO+u8fL2fL7kn39LAQAA/FBAJF4AAMA/OBUip5dzH2/P50s0XgAAwBhuNQIAAMAIEi8AAGCMSyFyeTn38fZ8vuQ/lQIAAPg5Ei8AAGCM03LI6eU1Wd6ez5dIvAAAAAwh8QIAAMbwrUYAAAAYQeIFAACMsawQubz8UGvLjx6STeMFAACMccohp7y8uN7L8/mS/7SIAAAAfo7ECwAAGOOyvL8Y3mV5dTqfIvECAAAwhMQLAAAY4/LB4npvz+dL/lMpAACAnyPxAgAAxrjkkMvL30L09ny+ZGvilZWVpXbt2ikyMlIxMTG68cYb9dVXX9lZEgAAgM/Y2ni9++67GjFihDZv3qzc3FyVl5crLS1NR44csbMsAADgIycfku3tw1/Yeqtx3bp1bj8vWLBAMTExys/P19VXX21TVQAAwFeCfXH9ObXG6+DBg5KkBg0aVPn70tJSlZaWVvxcUlJipC4AAABvOGdaRMuylJGRoU6dOql169ZVnpOVlaXo6OiKIz4+3nCVAADg93DJIZfl5YPF9dU3cuRIbd++XcuWLTvtOZmZmTp48GDFUVhYaLBCAACA3+ecuNU4atQorVmzRhs2bFDz5s1Pe154eLjCw8MNVgYAALzJ8sF2EpYfJV62Nl6WZWnUqFFatWqV3nnnHSUmJtpZDgAAgE/Z2niNGDFCS5cu1erVqxUZGamioiJJUnR0tGrVqmVnaQAAwAdOrsvy9pz+wtY1XtnZ2Tp48KC6dOmiuLi4iiMnJ8fOsgAAAHzC9luNAAAgeLCPFwAAgCHcagQAAIARJF4AAMAYlw+2k2ADVQAAAFRC4gUAAIxhjRcAAACMIPECAADGkHgBAADACBIvAABgTLAnXjReAADAmGBvvLjVCAAAYAiJFwAAMMaS9zc89acnP5N4AQAAGELiBQAAjGGNFwAAAIwg8QIAAMYEe+IVEI3Xn5bfo5CICLvLqJYdD2XbXYJHLv3gNrtL8Nh3+xvYXYJHys+zuwLPfDQq2e4SPOYc6LS7BI/E/8s/63406Xq7S/DYFfG77S6hWo7XKFOe3UUEuYBovAAAgH8g8QIAADAk2BsvFtcDAAAYQuIFAACMsSyHLC8nVN6ez5dIvAAAAAwh8QIAAMa45PD6I4O8PZ8vkXgBAAAYQuIFAACM4VuNAAAAMILECwAAGMO3GgEAAGAEiRcAADAm2Nd40XgBAABjuNUIAAAAI0i8AACAMZYPbjWSeAEAAKASEi8AAGCMJcmyvD+nvyDxAgAAMITECwAAGOOSQw4ekg0AAABfI/ECAADGBPs+XjReAADAGJflkCOId67nViMAAIAhJF4AAMAYy/LBdhJ+tJ8EiRcAAIAhJF4AAMCYYF9cT+IFAABgCIkXAAAwhsQLAAAARpB4AQAAY4J9Hy8aLwAAYAzbSQAAAMAIEi8AAGDMicTL24vrvTqdT5F4AQAAGELiBQAAjGE7CQAAABhB4gUAAIyx/nt4e05/QeIFAABgCIkXAAAwhjVeAAAAplg+Ojwwe/ZsJSYmKiIiQikpKdq4ceMZzy8tLdXEiROVkJCg8PBwnX/++Zo/f361rkniBQAAgk5OTo7GjBmj2bNnq2PHjnr22WfVvXt3ff7552rRokWVr+nTp49+/PFHzZs3TxdccIGKi4tVXl5erevSeAEAAHN8cKtRHsw3Y8YMDRkyREOHDpUkzZw5U2+++aays7OVlZVV6fx169bp3Xff1c6dO9WgQQNJUsuWLat9XW41AgCAgFBSUuJ2lJaWVnleWVmZ8vPzlZaW5jaelpam999/v8rXrFmzRqmpqZo+fbqaNWumiy66SH/729909OjRatVI4gUAAIzx5UOy4+Pj3cYnTZqkBx98sNL5+/btk9PpVGxsrNt4bGysioqKqrzGzp079d577ykiIkKrVq3Svn37lJ6erp9//rla67xovAAAQEAoLCxUVFRUxc/h4eFnPN/hcL9FaVlWpbGTXC6XHA6HlixZoujoaEknblfecsstevrpp1WrVq2zqjEgGq8rO32usLphdpdRLd0v6GB3CR5p9tpBu0vw2LEnmtpdgkdqlhyzuwSPhB49bncJHvvDbP+s/UjLunaX4JGQTyLtLsFjXdt8aXcJ1XK0ZrlesrkGX24nERUV5dZ4nU6jRo0UGhpaKd0qLi6ulIKdFBcXp2bNmlU0XZKUlJQky7K0Z88eXXjhhWdVK2u8AABAUAkLC1NKSopyc3PdxnNzc9WhQ9XBSMeOHfXDDz/o8OHDFWM7duxQSEiImjdvftbXpvECAADmWA7fHNWUkZGhuXPnav78+friiy80duxYFRQUaPjw4ZKkzMxMDRgwoOL82267TQ0bNtSdd96pzz//XBs2bNC9996rwYMHn/VtRilAbjUCAAD/4MvF9dXRt29f7d+/X5MnT9bevXvVunVrrV27VgkJCZKkvXv3qqCgoOL8unXrKjc3V6NGjVJqaqoaNmyoPn366OGHH67WdWm8AABAUEpPT1d6enqVv1u4cGGlsYsvvrjS7cnqovECAADm/I5H/JxxTj/BGi8AAABDSLwAAIAxvtxOwh+QeAEAABhC4gUAAMzyozVZ3kbiBQAAYAiJFwAAMCbY13jReAEAAHPYTgIAAAAmkHgBAACDHP89vD2nfyDxAgAAMITECwAAmMMaLwAAAJhA4gUAAMwh8QIAAIAJ50zjlZWVJYfDoTFjxthdCgAA8BXL4ZvDT5wTtxrz8vI0Z84ctWnTxu5SAACAD1nWicPbc/oL2xOvw4cP6/bbb9dzzz2n+vXr210OAACAz9jeeI0YMUI9evTQdddd95vnlpaWqqSkxO0AAAB+xPLR4SdsvdX44osv6qOPPlJeXt5ZnZ+VlaWHHnrIx1UBAAD4hm2JV2Fhoe655x4tXrxYERERZ/WazMxMHTx4sOIoLCz0cZUAAMCrWFxvj/z8fBUXFyslJaVizOl0asOGDZo1a5ZKS0sVGhrq9prw8HCFh4ebLhUAAMArbGu8rr32Wn3yySduY3feeacuvvhijR8/vlLTBQAA/J/DOnF4e05/YVvjFRkZqdatW7uN1alTRw0bNqw0DgAAEAiqvcbr+eef1+uvv17x83333ad69eqpQ4cO2r17t1eLAwAAASbIv9VY7cZr6tSpqlWrliRp06ZNmjVrlqZPn65GjRpp7Nixv6uYd955RzNnzvxdcwAAgHMYi+urp7CwUBdccIEk6ZVXXtEtt9yiv/71r+rYsaO6dOni7foAAAACRrUTr7p162r//v2SpLfeeqti49OIiAgdPXrUu9UBAIDAEuS3GqudeHXr1k1Dhw5V27ZttWPHDvXo0UOS9Nlnn6lly5berg8AACBgVDvxevrpp9W+fXv99NNPWrFihRo2bCjpxL5c/fr183qBAAAggJB4VU+9evU0a9asSuM8ygcAAODMzqrx2r59u1q3bq2QkBBt3779jOe2adPGK4UBAIAA5IuEKtASr+TkZBUVFSkmJkbJyclyOByyrP//Lk/+7HA45HQ6fVYsAACAPzurxmvXrl1q3Lhxxf8GAADwiC/23Qq0fbwSEhKq/N+n+t8UDAAAAO6q/a3G/v376/Dhw5XGv/vuO1199dVeKQoAAASmkw/J9vbhL6rdeH3++ee69NJL9Z///Kdi7Pnnn9dll12m2NhYrxYHAAACDNtJVM8HH3yg+++/X127dtW4ceP09ddfa926dfrHP/6hwYMH+6JGAACAgFDtxqtGjRp69NFHFR4erilTpqhGjRp699131b59e1/UBwAAEDCqfavx+PHjGjdunKZNm6bMzEy1b99ef/7zn7V27Vpf1AcAABAwqp14paam6tdff9U777yjq666SpZlafr06brppps0ePBgzZ492xd1AgCAAOCQ9xfD+89mEh42Xv/85z9Vp04dSSc2Tx0/fryuv/563XHHHV4v8Gz8XFZHNUvDbLm2p8Zuf9vuEjyS/soQu0vwWP27f7a7BI80uqXA7hI8EhLb2O4SPOYsKra7BI/U3lpqdwkeqftmbbtL8Ni0NtfbXUK1OH89JmmL3WUEtWo3XvPmzatyPDk5Wfn5+b+7IAAAEMDYQNVzR48e1fHjx93GwsPDf1dBAAAAgarai+uPHDmikSNHKiYmRnXr1lX9+vXdDgAAgNMK8n28qt143XfffVq/fr1mz56t8PBwzZ07Vw899JCaNm2qRYsW+aJGAAAQKIK88ar2rcZXX31VixYtUpcuXTR48GB17txZF1xwgRISErRkyRLdfvvtvqgTAADA71U78fr555+VmJgoSYqKitLPP5/4plinTp20YcMG71YHAAACCs9qrKbzzjtP3333nSTpkksu0UsvvSTpRBJWr149b9YGAAAQUKrdeN15553atm2bJCkzM7NirdfYsWN17733er1AAAAQQFjjVT1jx46t+N/XXHONvvzyS3344Yc6//zzddlll3m1OAAAgEDyu/bxkqQWLVqoRYsW3qgFAAAEOl8kVH6UeFX7ViMAAAA887sTLwAAgLPli28hBuS3Gvfs2ePLOgAAQDA4+axGbx9+4qwbr9atW+uFF17wZS0AAAAB7awbr6lTp2rEiBG6+eabtX//fl/WBAAAAlWQbydx1o1Xenq6tm3bpgMHDqhVq1Zas2aNL+sCAAAIONVaXJ+YmKj169dr1qxZuvnmm5WUlKQaNdyn+Oijj7xaIAAACBzBvri+2t9q3L17t1asWKEGDRqod+/elRovAAAAVK1aXdNzzz2ncePG6brrrtOnn36qxo0b+6ouAAAQiIJ8A9Wzbrz++Mc/asuWLZo1a5YGDBjgy5oAAAAC0lk3Xk6nU9u3b1fz5s19WQ8AAAhkPljjFZCJV25uri/rAAAAwSDIbzXyrEYAAABD+EoiAAAwh8QLAAAAJpB4AQAAY4J9A1USLwAAAENovAAAAAyh8QIAADCENV4AAMCcIP9WI40XAAAwhsX1AAAAMILECwAAmOVHCZW3kXgBAAAYQuIFAADMCfLF9SReAAAAhpB4AQAAY/hWIwAAAIwg8QIAAOYE+RovGi8AAGAMtxoBAABgBIkXAAAwJ8hvNZJ4AQAAGELiBQAAzCHxAgAAgAkkXgAAwJhg/1ZjQDRe3x+MVujxcLvLqJaMtXfZXYJnGrvsrsBjtZ+tZ3cJHrlj29t2l+CRf3wTb3cJHvvp+zZ2l+CRmlGldpfgkcdTXra7BI+NeetSu0uoFtdRbnTZjX8CAADAHMtHhwdmz56txMRERUREKCUlRRs3bjyr1/3nP/9RjRo1lJycXO1r0ngBAABzzpHGKycnR2PGjNHEiRO1detWde7cWd27d1dBQcEZX3fw4EENGDBA1157bfUvKhovAAAQhGbMmKEhQ4Zo6NChSkpK0syZMxUfH6/s7Owzvm7YsGG67bbb1L59e4+uS+MFAACMObm43tuHJJWUlLgdpaVVr3ssKytTfn6+0tLS3MbT0tL0/vvvn7b2BQsW6Ntvv9WkSZM8fv80XgAAICDEx8crOjq64sjKyqryvH379snpdCo2NtZtPDY2VkVFRVW+5uuvv9aECRO0ZMkS1ajh+XcTA+JbjQAAwE/4cAPVwsJCRUVFVQyHh595xwOHw+E+jWVVGpMkp9Op2267TQ899JAuuuii31UqjRcAAAgIUVFRbo3X6TRq1EihoaGV0q3i4uJKKZgkHTp0SB9++KG2bt2qkSNHSpJcLpcsy1KNGjX01ltvqWvXrmdVI40XAAAw5lzYQDUsLEwpKSnKzc3Vn//854rx3Nxc9e7du9L5UVFR+uSTT9zGZs+erfXr1+vll19WYmLiWV+bxgsAAASdjIwM9e/fX6mpqWrfvr3mzJmjgoICDR8+XJKUmZmp77//XosWLVJISIhat27t9vqYmBhFRERUGv8tNF4AAMCcc+Qh2X379tX+/fs1efJk7d27V61bt9batWuVkJAgSdq7d+9v7unlCRovAABgzjnSeElSenq60tPTq/zdwoULz/jaBx98UA8++GC1r8l2EgAAAIaQeAEAAGMc/z28Pae/IPECAAAwhMQLAACYcw6t8bIDiRcAAIAhJF4AAMCYc2EDVTuReAEAABhie+P1/fff64477lDDhg1Vu3ZtJScnKz8/3+6yAACAL1g+OvyErbcaDxw4oI4dO+qaa67RG2+8oZiYGH377beqV6+enWUBAABf8qNGydtsbbymTZum+Ph4LViwoGKsZcuW9hUEAADgQ7bealyzZo1SU1N16623KiYmRm3bttVzzz132vNLS0tVUlLidgAAAP9xcnG9tw9/YWvjtXPnTmVnZ+vCCy/Um2++qeHDh2v06NFatGhRlednZWUpOjq64oiPjzdcMQAAgOdsbbxcLpcuv/xyTZ06VW3bttWwYcN01113KTs7u8rzMzMzdfDgwYqjsLDQcMUAAOB3CfLF9bY2XnFxcbrkkkvcxpKSklRQUFDl+eHh4YqKinI7AAAA/IWti+s7duyor776ym1sx44dSkhIsKkiAADgS2ygaqOxY8dq8+bNmjp1qr755hstXbpUc+bM0YgRI+wsCwAAwCdsbbzatWunVatWadmyZWrdurWmTJmimTNn6vbbb7ezLAAA4CtBvsbL9mc19uzZUz179rS7DAAAAJ+zvfECAADBI9jXeNF4AQAAc3xxa9CPGi/bH5INAAAQLEi8AACAOSReAAAAMIHECwAAGBPsi+tJvAAAAAwh8QIAAOawxgsAAAAmkHgBAABjHJYlh+XdiMrb8/kSjRcAADCHW40AAAAwgcQLAAAYw3YSAAAAMILECwAAmMMaLwAAAJgQEIlX3H1HVCOk3O4yquXXuUftLsEj4X/aa3cJHnNeeYndJXjk77m32F2CR27usMXuEjx28Xn++ef83V8usrsEj3xT2sTuEjwWeti/8gvHMfvrZY0XAAAAjAiIxAsAAPiJIF/jReMFAACM4VYjAAAAjCDxAgAA5gT5rUYSLwAAAENIvAAAgFH+tCbL20i8AAAADCHxAgAA5ljWicPbc/oJEi8AAABDSLwAAIAxwb6PF40XAAAwh+0kAAAAYAKJFwAAMMbhOnF4e05/QeIFAABgCIkXAAAwhzVeAAAAMIHECwAAGBPs20mQeAEAABhC4gUAAMwJ8kcG0XgBAABjuNUIAAAAI0i8AACAOWwnAQAAABNIvAAAgDGs8QIAAIARJF4AAMCcIN9OgsQLAADAEBIvAABgTLCv8aLxAgAA5rCdBAAAAEwg8QIAAMYE+61GEi8AAABDSLwAAIA5LuvE4e05/QSJFwAAgCEkXgAAwBy+1QgAAAATSLwAAIAxDvngW43enc6naLwAAIA5PKsRAAAAJpB4AQAAY9hAFQAAAEaQeAEAAHPYTgIAAAAmkHgBAABjHJYlh5e/hejt+XwpIBqv4i7NFBoWYXcZ1bL/a6fdJXgkengzu0vwmMM/P3KlJn9ldwke2X7Af/+s/Pvpq+wuwSOvTnrM7hI80vvv99pdgsfO/+yQ3SVUS7nzmL6zu4ggFxCNFwAA8BOu/x7entNP0HgBAABjgv1WI4vrAQAADCHxAgAA5rCdBAAAAEwg8QIAAObwkGwAAACYQOIFAACM4SHZAAAAQWj27NlKTExURESEUlJStHHjxtOeu3LlSnXr1k2NGzdWVFSU2rdvrzfffLPa16TxAgAA5pxc4+Xto5pycnI0ZswYTZw4UVu3blXnzp3VvXt3FRQUVHn+hg0b1K1bN61du1b5+fm65ppr1KtXL23durVa16XxAgAAQWfGjBkaMmSIhg4dqqSkJM2cOVPx8fHKzs6u8vyZM2fqvvvuU7t27XThhRdq6tSpuvDCC/Xqq69W67qs8QIAAMY4XCcOb88pSSUlJW7j4eHhCg8Pr3R+WVmZ8vPzNWHCBLfxtLQ0vf/++2d1TZfLpUOHDqlBgwbVqpXECwAAmOPDW43x8fGKjo6uOLKysqosYd++fXI6nYqNjXUbj42NVVFR0Vm9jSeeeEJHjhxRnz59qvX2SbwAAEBAKCwsVFRUVMXPVaVd/8vhcLj9bFlWpbGqLFu2TA8++KBWr16tmJiYatVI4wUAAMzx4SODoqKi3Bqv02nUqJFCQ0MrpVvFxcWVUrBT5eTkaMiQIVq+fLmuu+66apfKrUYAABBUwsLClJKSotzcXLfx3NxcdejQ4bSvW7ZsmQYNGqSlS5eqR48eHl2bxAsAABjjsCw5vPyIH0/my8jIUP/+/ZWamqr27dtrzpw5Kigo0PDhwyVJmZmZ+v7777Vo0SJJJ5quAQMG6B//+IeuuuqqirSsVq1aio6OPuvr0ngBAICg07dvX+3fv1+TJ0/W3r171bp1a61du1YJCQmSpL1797rt6fXss8+qvLxcI0aM0IgRIyrGBw4cqIULF571dWm8AACAOefQQ7LT09OVnp5e5e9Obabeeecdj65xKlvXeJWXl+v+++9XYmKiatWqpfPOO0+TJ0+Wy+XlDT4AAADOAbYmXtOmTdMzzzyj559/Xq1atdKHH36oO++8U9HR0brnnnvsLA0AAPiCJcnb+YofPSTb1sZr06ZN6t27d8U3A1q2bKlly5bpww8/rPL80tJSlZaWVvx86g61AADg3HauLK63i623Gjt16qS3335bO3bskCRt27ZN7733nv70pz9VeX5WVpbbjrTx8fEmywUAAPhdbE28xo8fr4MHD+riiy9WaGionE6nHnnkEfXr16/K8zMzM5WRkVHxc0lJCc0XAAD+xJIPFtd7dzpfsrXxysnJ0eLFi7V06VK1atVKH3/8scaMGaOmTZtq4MCBlc4/3cMuAQAA/IGtjde9996rCRMm6C9/+Ysk6dJLL9Xu3buVlZVVZeMFAAD83Dm0nYQdbF3j9euvvyokxL2E0NBQtpMAAAABydbEq1evXnrkkUfUokULtWrVSlu3btWMGTM0ePBgO8sCAAC+4pLk8MGcfsLWxuupp57S3//+d6Wnp6u4uFhNmzbVsGHD9MADD9hZFgAAgE/Y2nhFRkZq5syZmjlzpp1lAAAAQ4J9Hy+e1QgAAMxhcT0AAABMIPECAADmkHgBAADABBIvAABgDokXAAAATCDxAgAA5gT5BqokXgAAAIaQeAEAAGPYQBUAAMAUFtcDAADABBIvAABgjsuSHF5OqFwkXgAAADgFiRcAADCHNV4AAAAwgcQLAAAY5IPES/6TeAVE4+WMcEhh3t4G17cSXvefPyT/a/ctpXaX4LE+yfl2l+CR7Z3q2F2CR1xt4+0uwWNLlzxudwke6fDGWLtL8Ei9uv717+//tXjVs3aXUC2HDrl0QZLdVQS3gGi8AACAnwjyNV40XgAAwByXJa/fGmQ7CQAAAJyKxAsAAJhjuU4c3p7TT5B4AQAAGELiBQAAzAnyxfUkXgAAAIaQeAEAAHP4ViMAAABMIPECAADmBPkaLxovAABgjiUfNF7enc6XuNUIAABgCIkXAAAwJ8hvNZJ4AQAAGELiBQAAzHG5JHn5ET8uHhkEAACAU5B4AQAAc1jjBQAAABNIvAAAgDlBnnjReAEAAHN4ViMAAABMIPECAADGWJZLluXd7R+8PZ8vkXgBAAAYQuIFAADMsSzvr8nyo8X1JF4AAACGkHgBAABzLB98q5HECwAAAKci8QIAAOa4XJLDy99C9KNvNdJ4AQAAc7jVCAAAABNIvAAAgDGWyyXLy7ca2UAVAAAAlZB4AQAAc1jjBQAAABNIvAAAgDkuS3KQeAEAAMDHSLwAAIA5liXJ2xuokngBAADgFCReAADAGMtlyfLyGi/LjxIvGi8AAGCO5ZL3bzWygSoAAABOQeIFAACMCfZbjSReAAAAhpB4AQAAc4J8jZdfN14no0Vn2TGbK6m+8uPH7S7BI66j/vOH+1Slh/3zMy+3yuwuwSPl5f739/Kkw4f888+566h/fubOMqfdJXjskJ/9WTl0+ES9dt6aK9dxrz+qsVz+8+93h+VPN0ZPsWfPHsXHx9tdBgAAfqWwsFDNmzc3es1jx44pMTFRRUVFPpm/SZMm2rVrlyIiInwyv7f4dePlcrn0ww8/KDIyUg6Hw6tzl5SUKD4+XoWFhYqKivLq3Kgan7lZfN5m8Xmbx2demWVZOnTokJo2baqQEPPLvI8dO6ayMt+k+GFhYed80yX5+a3GkJAQn3fsUVFR/IU1jM/cLD5vs/i8zeMzdxcdHW3btSMiIvyiOfIlvtUIAABgCI0XAACAITRepxEeHq5JkyYpPDzc7lKCBp+5WXzeZvF5m8dnjnORXy+uBwAA8CckXgAAAIbQeAEAABhC4wUAAGAIjRcAAIAhNF6nMXv2bCUmJioiIkIpKSnauHGj3SUFpKysLLVr106RkZGKiYnRjTfeqK+++srusoJGVlaWHA6HxowZY3cpAe3777/XHXfcoYYNG6p27dpKTk5Wfn6+3WUFpPLyct1///1KTExUrVq1dN5552ny5MlyufzrmYoIXDReVcjJydGYMWM0ceJEbd26VZ07d1b37t1VUFBgd2kB591339WIESO0efNm5ebmqry8XGlpaTpy5IjdpQW8vLw8zZkzR23atLG7lIB24MABdezYUTVr1tQbb7yhzz//XE888YTq1atnd2kBadq0aXrmmWc0a9YsffHFF5o+fboee+wxPfXUU3aXBkhiO4kqXXnllbr88suVnZ1dMZaUlKQbb7xRWVlZNlYW+H766SfFxMTo3Xff1dVXX213OQHr8OHDuvzyyzV79mw9/PDDSk5O1syZM+0uKyBNmDBB//nPf0jNDenZs6diY2M1b968irGbb75ZtWvX1gsvvGBjZcAJJF6nKCsrU35+vtLS0tzG09LS9P7779tUVfA4ePCgJKlBgwY2VxLYRowYoR49eui6666zu5SAt2bNGqWmpurWW29VTEyM2rZtq+eee87usgJWp06d9Pbbb2vHjh2SpG3btum9997Tn/70J5srA07w64dk+8K+ffvkdDoVGxvrNh4bG6uioiKbqgoOlmUpIyNDnTp1UuvWre0uJ2C9+OKL+uijj5SXl2d3KUFh586dys7OVkZGhv7v//5PW7Zs0ejRoxUeHq4BAwbYXV7AGT9+vA4ePKiLL75YoaGhcjqdeuSRR9SvXz+7SwMk0XidlsPhcPvZsqxKY/CukSNHavv27XrvvffsLiVgFRYW6p577tFbb72liIgIu8sJCi6XS6mpqZo6daokqW3btvrss8+UnZ1N4+UDOTk5Wrx4sZYuXapWrVrp448/1pgxY9S0aVMNHDjQ7vIAGq9TNWrUSKGhoZXSreLi4kopGLxn1KhRWrNmjTZs2KDmzZvbXU7Ays/PV3FxsVJSUirGnE6nNmzYoFmzZqm0tFShoaE2Vhh44uLidMkll7iNJSUlacWKFTZVFNjuvfdeTZgwQX/5y18kSZdeeql2796trKwsGi+cE1jjdYqwsDClpKQoNzfXbTw3N1cdOnSwqarAZVmWRo4cqZUrV2r9+vVKTEy0u6SAdu211+qTTz7Rxx9/XHGkpqbq9ttv18cff0zT5QMdO3astEXKjh07lJCQYFNFge3XX39VSIj7f9pCQ0PZTgLnDBKvKmRkZKh///5KTU1V+/btNWfOHBUUFGj48OF2lxZwRowYoaVLl2r16tWKjIysSBqjo6NVq1Ytm6sLPJGRkZXWz9WpU0cNGzZkXZ2PjB07Vh06dNDUqVPVp08fbdmyRXPmzNGcOXPsLi0g9erVS4888ohatGihVq1aaevWrZoxY4YGDx5sd2mAJLaTOK3Zs2dr+vTp2rt3r1q3bq0nn3yS7Q184HTr5hYsWKBBgwaZLSZIdenShe0kfOy1115TZmamvv76ayUmJiojI0N33XWX3WUFpEOHDunvf/+7Vq1apeLiYjVt2lT9+vXTAw88oLCwMLvLA2i8AAAATGGNFwAAgCE0XgAAAIbQeAEAABhC4wUAAGAIjRcAAIAhNF4AAACG0HgBAAAYQuMFAABgCI0XANs5HA698sordpcBAD5H4wVATqdTHTp00M033+w2fvDgQcXHx+v+++/36fX37t2r7t27+/QaAHAu4JFBACRJX3/9tZKTkzVnzhzdfvvtkqQBAwZo27ZtysvL4zl3AOAFJF4AJEkXXnihsrKyNGrUKP3www9avXq1XnzxRT3//PNnbLoWL16s1NRURUZGqkmTJrrttttUXFxc8fvJkyeradOm2r9/f8XYDTfcoKuvvloul0uS+63GsrIyjRw5UnFxcYqIiFDLli2VlZXlmzcNAIaReAGoYFmWunbtqtDQUH3yyScaNWrUb95mnD9/vuLi4vSHP/xBxcXFGjt2rOrXr6+1a9dKOnEbs3PnzoqNjdWqVav0zDPPaMKECdq2bZsSEhIknWi8Vq1apRtvvFGPP/64/vnPf2rJkiVq0aKFCgsLVVhYqH79+vn8/QOAr9F4AXDz5ZdfKikpSZdeeqk++ugj1ahRo1qvz8vL0xVXXKFDhw6pbt26kqSdO3cqOTlZ6enpeuqpp9xuZ0rujdfo0aP12Wef6V//+pccDodX3xsA2I1bjQDczJ8/X7Vr19auXbu0Z8+e3zx/69at6t27txISEhQZGakuXbpIkgoKCirOOe+88/T4449r2rRp6tWrl1vTdapBgwbp448/1h/+8AeNHj1ab7311u9+TwBwrqDxAlBh06ZNevLJJ7V69Wq1b99eQ4YM0ZlC8SNHjigtLU1169bV4sWLlZeXp1WrVkk6sVbrf23YsEGhoaH67rvvVF5efto5L7/8cu3atUtTpkzR0aNH1adPH91yyy3eeYMAYDMaLwCSpKNHj2rgwIEaNmyYrrvuOs2dO1d5eXl69tlnT/uaL7/8Uvv27dOjjz6qzp076+KLL3ZbWH9STk6OVq5cqXfeeUeFhYWaMmXKGWuJiopS37599dxzzyknJ0crVqzQzz///LvfIwDYjcYLgCRpwoQJcrlcmjZtmiSpRYsWeuKJJ3Tvvffqu+++q/I1LVq0UFhYmJ566int3LlTa9asqdRU7dmzR3fffbemTZumTp06aeHChcrKytLmzZurnPPJJ5/Uiy++qC+//FI7duzQ8uXL1aRJE9WrV8+bbxcAbEHjBUDvvvuunn76aS1cuFB16tSpGL/rrrvUoUOH095ybNy4sRYuXKjly5frkksu0aOPPqrHH3+84veWZWnQoEG64oorNHLkSElSt27dNHLkSN1xxx06fPhwpTnr1q2radOmKTU1Ve3atdN3332ntWvXKiSEf10B8H98qxEAAMAQ/i8kAACAITReAAAAhtB4AQAAGELjBQAAYAiNFwAAgCE0XgAAAIbQeAEAABhC4wUAAGAIjRcAAIAhNF4AAACG0HgBAAAY8v8AALoUkND9srwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250507_192312-yqkxz5wa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yqkxz5wa' target=\"_blank\">tough-serenity-8267</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yqkxz5wa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yqkxz5wa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.01, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABOeElEQVR4nO3de1hVdd738c/mfAhJRdmQpmSeCjIPpWmBpmCamjmTlaammDaWSepjOc7c4dzdpPZ4aLTsMIqWgzoddJqaTJw8hk1q5qiVOWUeQUZDQEFA9u/5w5v9tAUUENi4eL+ui+tq/dZ3r/X77sXKD2vvtbfNGGMEAACAa56HuycAAACA6kGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwA64x7733nmw2m1avXl1qXYcOHWSz2fTpp5+WWteqVSt16tSpUvt6/PHH1bJlyyrNMzExUTabTadOnbpibVJSktauXXvFur/+9a+y2Wx6/fXXy61JTU2VzWbTvHnzKjzXq+nzarVs2VI2m002m00eHh4KDg5W+/btNXLkSK1fv77Mx9hsNiUmJlZqP3//+98r/Ziy9rVs2TLZbDbt3Lmz0tsqz4kTJ5SYmKivv/661LqS3yMAFUOwA64xPXv2lM1m08aNG13Gf/75Z+3du1eBgYGl1h07dkw//vijevXqVal9/f73v9eaNWuues5XUtFgd//998tut2vp0qXl1iQnJ8vb21sjRoyoxhnWrB49emj79u1KS0vT+++/r6efflqHDh1S37599etf/1pFRUUu9du3b9fYsWMrtY+///3vmjlzZqXnVpV9VdaJEyc0c+bMMoPd2LFjtX379hrdP2AlBDvgGhMSEqLIyEht2rTJZXzz5s3y8vJSfHx8qWBXslzZYNeqVSt17NjxquZbnby8vDRy5Ejt2LFD+/btK7X+zJkzWrNmjQYNGqQmTZq4YYZVc/3116tbt27q1q2b+vTpo6eeekpbt27VCy+8oPfff1+/+93vXOq7deumZs2a1dh8jDHKz8+vlX1dSbNmzdStWze37R+41hDsgGtQr169dODAAaWnpzvHNm3apDvuuEP9+/fXrl27lJub67LO09NT99xzj6SL/3C/9tpruv322+Xv76+GDRvq17/+tX788UeX/ZT1EuWZM2cUHx+vRo0a6brrrtP999+vH3/8sdyXB0+ePKlHH31UwcHBCg0N1ZgxY5Sdne1cb7PZdO7cOS1fvtz5kmTPnj3L7T0+Pl7SxStzl1q5cqXOnz+vMWPGSJJeffVVRUdHq2nTpgoMDFRUVJTmzJlT6grYpX766SfZbDYtW7as1Lqy+jx48KCGDRumpk2bytfXV+3bt9err7562X1URGJiom699VYtWrRI58+fL3cOeXl5mjp1qiIiIuTn56dGjRqpS5cuWrlypaSLx7FkPiXPsc1m008//eQce/rpp/X666+rffv28vX11fLly8vtV5KysrI0evRoNWrUSIGBgRo4cGCp35+WLVvq8ccfL/XYnj17Oo9xye+tJI0ePdo5t5J9lvVSrMPh0Jw5c9SuXTv5+vqqadOmGjlypI4dO1ZqP5GRkdqxY4fuueceBQQE6KabbtKsWbPkcDjKf+KBaxjBDrgGlVx5++VVu40bNyomJkY9evSQzWbT1q1bXdZ16tRJwcHBkqTx48crISFBffr00dq1a/Xaa69p//796t69u06ePFnufh0OhwYOHKiUlBQ999xzWrNmjbp27ar77ruv3Mf86le/Ups2bfT+++/r+eefV0pKip599lnn+u3bt8vf31/9+/fX9u3btX37dr322mvlbq9Nmza6++67tWLFilIBLTk5WTfccIP69u0rSfrhhx80bNgwvfPOO/roo48UHx+vl19+WePHjy93+5X1zTff6I477tC+ffs0d+5cffTRR7r//vv1zDPPVOmlz0sNHDhQeXl5l31P2+TJk7V48WI988wzWrdund555x099NBDOn36tKSLL6n/+te/liTnc7x9+3aFhYU5t7F27VotXrxY//Vf/6VPP/3U+UdAeeLj4+Xh4aGUlBQtWLBAX375pXr27KkzZ85Uqr9OnTo5Q/rvfvc759wu9/Lvb37zGz333HOKjY3Vhx9+qP/+7//WunXr1L1791Lv6czIyNDw4cP12GOP6cMPP1S/fv00ffp0rVixolLzBK4ZBsA15+effzYeHh5m3LhxxhhjTp06ZWw2m1m3bp0xxpg777zTTJ061RhjzJEjR4wkM23aNGOMMdu3bzeSzNy5c122efToUePv7++sM8aYUaNGmRYtWjiXP/74YyPJLF682OWxL730kpFkXnjhBefYCy+8YCSZOXPmuNROmDDB+Pn5GYfD4RwLDAw0o0aNqnD/ycnJRpL54IMPnGP79u0zksyMGTPKfExxcbEpKioyb7/9tvH09DQ///xzuX0eOnTISDLJycmltnNpn3379jXNmjUz2dnZLnVPP/208fPzc9lPWVq0aGHuv//+ctcvXrzYSDKrV68udw6RkZFm8ODBl93PU089Zcr7X74kExwcXOZcL91XyXP/4IMPutR9/vnnRpJ58cUXXXor67jGxMSYmJgY5/KOHTvKfb5Lfo9KfPvtt0aSmTBhgkvdP//5TyPJ/Pa3v3XZjyTzz3/+06X2lltuMX379i21L8AKuGIHXIMaNmyoDh06OK/Ybd68WZ6enurRo4ckKSYmxvm+ukvfX/fRRx/JZrPpscce04ULF5w/drvdZZtl2bx5syRp6NChLuOPPvpouY8ZNGiQy/Jtt92m8+fPKzMzs+INX2Lo0KEKCgpyuYli6dKlstlsGj16tHNs9+7dGjRokBo3bixPT095e3tr5MiRKi4u1vfff1/l/Zc4f/68/vGPf+jBBx9UQECAy/PZv39/nT9/Xl988cVV7cMYc8WaO++8U5988omef/55bdq0yfn+uMq499571bBhwwrXDx8+3GW5e/fuatGiRan3d1a3ku1f+hLvnXfeqfbt2+sf//iHy7jdbtedd97pMnbbbbfp8OHDNTpPwF0IdsA1qlevXvr+++914sQJbdy4UZ07d9Z1110n6WKw2717t7Kzs7Vx40Z5eXnp7rvvlnTxPW/GGIWGhsrb29vl54svvrjsx5OcPn1aXl5eatSokct4aGhouY9p3Lixy7Kvr68kVSl8lAgICNAjjzyidevWKSMjQxcuXNCKFSsUExOjVq1aSZKOHDmie+65R8ePH9crr7yirVu3aseOHc73ml3N/kucPn1aFy5c0MKFC0s9l/3795ekCn3cy+WUBJDw8PBya/74xz/queee09q1a9WrVy81atRIgwcP1sGDByu8n1++LFsRdru9zLGSl39rSsn2y5pveHh4qf1f+vsnXfwdrI7jD9RFXu6eAICq6dWrl+bNm6dNmzZp06ZNziAhyRnitmzZ4nxzeknoCwkJcb4HryRk/VJZYyUaN26sCxcu6Oeff3YJdxkZGdXVVoXFx8frrbfe0ttvv602bdooMzNTc+fOda5fu3atzp07pw8++EAtWrRwjpf1kRqX8vPzkyQVFBS4jF8aGho2bChPT0+NGDFCTz31VJnbioiIqGhLpRhj9Le//U2BgYHq0qVLuXWBgYGaOXOmZs6cqZMnTzqv3g0cOFDfffddhfZV2c+KK+uYZ2Rk6Oabb3Yu+/n5lXoOpYthNyQkpFL7K1ES1NLT00vdrXvixIkqbxewCq7YAdeo6OhoeXp66r333tP+/ftd7iQNDg7W7bffruXLl+unn35y+ZiTAQMGyBij48ePq0uXLqV+oqKiyt1nTEyMJJX6cORVq1ZdVS9VuYLStWtXRUZGKjk5WcnJyQoODtavfvUr5/qSoPLLoGqM0VtvvXXFbYeGhsrPz0//+te/XMb/+te/uiwHBASoV69e2r17t2677bYyn8+yrhhV1MyZM/XNN99o0qRJzrBZkbk//vjjevTRR3XgwAHl5eVJqp4rpb/05z//2WU5LS1Nhw8fdvk9bNmyZann8Pvvv9eBAwdcxiozt3vvvVeSSt38sGPHDn377bfq3bt3hXsArIgrdsA1qkGDBurUqZPWrl0rDw8P5/vrSsTExGjBggWSXD+/rkePHho3bpxGjx6tnTt3Kjo6WoGBgUpPT9e2bdsUFRWl3/zmN2Xu87777lOPHj00ZcoU5eTkqHPnztq+fbvefvttSZKHR9X+VoyKitKmTZv0t7/9TWFhYQoKClLbtm2v+LgxY8Zo8uTJOnDggMaPHy9/f3/nutjYWPn4+OjRRx/VtGnTdP78eS1evFhZWVlX3G7JexCXLl2qVq1aqUOHDvryyy+VkpJSqvaVV17R3XffrXvuuUe/+c1v1LJlS+Xm5urf//63/va3v+mzzz674v7OnDnjfC/euXPndODAAa1atUpbt27V0KFDr3h3bdeuXTVgwADddtttatiwob799lu98847uuuuuxQQECBJzsA+e/Zs9evXT56enrrtttvk4+NzxfmVZefOnRo7dqweeughHT16VDNmzNANN9ygCRMmOGtGjBihxx57TBMmTNCvfvUrHT58WHPmzCn1GYOtWrWSv7+//vznP6t9+/a67rrrFB4eXubLz23bttW4ceO0cOFCeXh4qF+/fvrpp5/0+9//Xs2bN3e54xqol9x66waAqzJt2jQjyXTp0qXUurVr1xpJxsfHx5w7d67U+qVLl5quXbuawMBA4+/vb1q1amVGjhxpdu7c6ay59G5RYy7ekTt69Ghz/fXXm4CAABMbG2u++OILI8m88sorzrqSuxn/85//uDy+5K7KQ4cOOce+/vpr06NHDxMQEGAkudwxeTn/+c9/jI+Pj5Fkvvzyy1Lr//a3v5kOHToYPz8/c8MNN5j/83/+j/nkk0+MJLNx48bL9pmdnW3Gjh1rQkNDTWBgoBk4cKD56aefSt0laszFu2jHjBljbrjhBuPt7W2aNGliunfv7nKHaHlatGhhJBlJxmazmeuuu860bdvWjBgxwnz66adlPubSOTz//POmS5cupmHDhsbX19fcdNNN5tlnnzWnTp1y1hQUFJixY8eaJk2aGJvN5nIMJJmnnnqqQvsqOX7r1683I0aMMNdff73x9/c3/fv3NwcPHnR5rMPhMHPmzDE33XST8fPzM126dDGfffZZqbtijTFm5cqVpl27dsbb29tln5feFWvMxTucZ8+ebdq0aWO8vb1NSEiIeeyxx8zRo0dd6mJiYsytt95aqqeyjjdgFTZjKnDLFQBcRkpKioYPH67PP/9c3bt3d/d0AKDeItgBqJSVK1fq+PHjioqKkoeHh7744gu9/PLL6tixo/PjUAAA7sF77ABUSlBQkFatWqUXX3xR586dU1hYmB5//HG9+OKL7p4aANR7XLEDAACwCD7uBAAAwCIIdgAAABZBsAMAALAIbp6Q5HA4dOLECQUFBVX6a3UAAABqkjFGubm5Cg8Pv+IHwRPsdPH7BZs3b+7uaQAAAJTr6NGjpb4j+VIEO138+Abp4hPWoEGDGtlHUVGR1q9fr7i4OHl7e9fIPuoqeqd3eq8/6J3e6b365eTkqHnz5s68cjkEO/3/Lwtv0KBBjQa7gIAANWjQoF7+0tM7vdcn9E7v9F5/1GbvFXm7GDdPAAAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFuDXYtWzZUjabrdTPU089JeniB/IlJiYqPDxc/v7+6tmzp/bv3++yjYKCAk2cOFEhISEKDAzUoEGDdOzYMXe0AwAA4FZuDXY7duxQenq68yc1NVWS9NBDD0mS5syZo3nz5mnRokXasWOH7Ha7YmNjlZub69xGQkKC1qxZo1WrVmnbtm06e/asBgwYoOLiYrf0BAAA4C5uDXZNmjSR3W53/nz00Udq1aqVYmJiZIzRggULNGPGDA0ZMkSRkZFavny58vLylJKSIknKzs7WkiVLNHfuXPXp00cdO3bUihUrtHfvXm3YsMGdrQEAANS6OvMBxYWFhVqxYoUmT54sm82mH3/8URkZGYqLi3PW+Pr6KiYmRmlpaRo/frx27dqloqIil5rw8HBFRkYqLS1Nffv2LXNfBQUFKigocC7n5ORIuvghg0VFRTXSX8l2a2r7dRm903t9Q+/0Xt/Qe832Xplt15lgt3btWp05c0aPP/64JCkjI0OSFBoa6lIXGhqqw4cPO2t8fHzUsGHDUjUljy/LSy+9pJkzZ5YaX79+vQICAq6mjSsqebm5PqL3+one6yd6r5/ovWbk5eVVuLbOBLslS5aoX79+Cg8Pdxm/9OszjDFX/EqNK9VMnz5dkydPdi6XfAdbXFxcjX6lWGpqqmJjY+vl163QO73XJ/RO7/Ref9RG7yWvLFZEnQh2hw8f1oYNG/TBBx84x+x2u6SLV+XCwsKc45mZmc6reHa7XYWFhcrKynK5apeZmanu3buXuz9fX1/5+vqWGvf29q7xX8ja2EddRe/0Xt/QO73XN/ReM71XZrt14nPskpOT1bRpU91///3OsYiICNntdpdLm4WFhdq8ebMztHXu3Fne3t4uNenp6dq3b99lgx0AAIAVuf2KncPhUHJyskaNGiUvr/8/HZvNpoSEBCUlJal169Zq3bq1kpKSFBAQoGHDhkmSgoODFR8frylTpqhx48Zq1KiRpk6dqqioKPXp08ddLQEAALiF24Pdhg0bdOTIEY0ZM6bUumnTpik/P18TJkxQVlaWunbtqvXr1ysoKMhZM3/+fHl5eWno0KHKz89X7969tWzZMnl6etZmGwAAAG7n9mAXFxcnY0yZ62w2mxITE5WYmFju4/38/LRw4UItXLiwhmZYvfbs2SMPj8u/Ah4SEqIbb7yxlmYEAACswu3Brr4o+Zqz6Oho5efnX7bWPyBA3337LeEOAABUCsGulpw+fVqS9ODv56tRi5vLrcs8dFB/+d1vdOrUKYIdAACoFIJdLWvSopXs7Tu4exoAAMCC6sTHnQAAAODqEewAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBF8M0TAFBD9uzZIw+Py//9HBISwtcHAqg2BDsAqGbHjh2TJEVHRys/P/+ytf4BAfru228JdwCqBcEOAKrZ6dOnJUkP/n6+GrW4udy6zEMH9Zff/UanTp0i2AGoFgQ7AKghTVq0kr19B3dPA0A9ws0TAAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCmycAAADKcOTIEZ06deqyNQ6Ho5ZmUzEEOwAAgEscOXJE7dq3V35e3mXr/P39tXLlSh07dkwRERG1NLvyEewAAAAucerUKeXn5Wnoi4vVNKJ1uXU/H/63pIufX0mwAwAAqMOaRrTWDZf5PEpPGUnnam9CV8DNEwAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAItwe748eP67HHHlPjxo0VEBCg22+/Xbt27XKuN8YoMTFR4eHh8vf3V8+ePbV//36XbRQUFGjixIkKCQlRYGCgBg0apGPHjtV2KwAAAG7l1mCXlZWlHj16yNvbW5988om++eYbzZ07V9dff72zZs6cOZo3b54WLVqkHTt2yG63KzY2Vrm5uc6ahIQErVmzRqtWrdK2bdt09uxZDRgwQMXFxW7oCgAAwD283Lnz2bNnq3nz5kpOTnaOtWzZ0vnfxhgtWLBAM2bM0JAhQyRJy5cvV2hoqFJSUjR+/HhlZ2dryZIleuedd9SnTx9J0ooVK9S8eXNt2LBBffv2rdWeAAAA3MWtV+w+/PBDdenSRQ899JCaNm2qjh076q233nKuP3TokDIyMhQXF+cc8/X1VUxMjNLS0iRJu3btUlFRkUtNeHi4IiMjnTUAAAD1gVuv2P34449avHixJk+erN/+9rf68ssv9cwzz8jX11cjR45URkaGJCk0NNTlcaGhoTp8+LAkKSMjQz4+PmrYsGGpmpLHX6qgoEAFBQXO5ZycHElSUVGRioqKqq2/X3I4HJIkTxl5OC6UW+cpI39/fzkcjhqbS20r6cMq/VQGvdfP3jnf6+dxp3dr9e5wOOTv71+h87ikvqb6r8x2bcYYUyOzqAAfHx916dLF5craM888ox07dmj79u1KS0tTjx49dOLECYWFhTlrnnjiCR09elTr1q1TSkqKRo8e7RLUJCk2NlatWrXS66+/Xmq/iYmJmjlzZqnxlJQUBQQEVGOHAAAAVycvL0/Dhg1Tdna2GjRocNlat16xCwsL0y233OIy1r59e73//vuSJLvdLuniVblfBrvMzEznVTy73a7CwkJlZWW5XLXLzMxU9+7dy9zv9OnTNXnyZOdyTk6Omjdvrri4uCs+YVW1e/dupaena8u5AIW2jSq37sSBfXpz7CBt2bJFHTp0qJG51LaioiKlpqYqNjZW3t7e7p5OraL3+tk753v9PO70bq3e9+zZo+joaI3704cKbxtZbt3JA3sVHZinsLAwdezYsUbmUvLKYkW4Ndj16NFDBw4ccBn7/vvv1aJFC0lSRESE7Ha7UlNTnU9WYWGhNm/erNmzZ0uSOnfuLG9vb6Wmpmro0KGSpPT0dO3bt09z5swpc7++vr7y9fUtNe7t7V1jv5AeHhffzlgsmxwe5T/txbIpPz9fHh4eljk5StTk81vX0Xv96p3zvX4e9xL0bo3ePTw8lJ+fX6HzuKS+pnqvzHbdGuyeffZZde/eXUlJSRo6dKi+/PJLvfnmm3rzzTclSTabTQkJCUpKSlLr1q3VunVrJSUlKSAgQMOGDZMkBQcHKz4+XlOmTFHjxo3VqFEjTZ06VVFRUc67ZAEAAOoDtwa7O+64Q2vWrNH06dP1hz/8QREREVqwYIGGDx/urJk2bZry8/M1YcIEZWVlqWvXrlq/fr2CgoKcNfPnz5eXl5eGDh2q/Px89e7dW8uWLZOnp6c72gIAAHALtwY7SRowYIAGDBhQ7nqbzabExEQlJiaWW+Pn56eFCxdq4cKFNTBDAACAa4Pbv1IMAAAA1YNgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAItwa7BLTEyUzWZz+bHb7c71xhglJiYqPDxc/v7+6tmzp/bv3++yjYKCAk2cOFEhISEKDAzUoEGDdOzYsdpuBQAAwO3cfsXu1ltvVXp6uvNn7969znVz5szRvHnztGjRIu3YsUN2u12xsbHKzc111iQkJGjNmjVatWqVtm3bprNnz2rAgAEqLi52RzsAAABu4+X2CXh5uVylK2GM0YIFCzRjxgwNGTJEkrR8+XKFhoYqJSVF48ePV3Z2tpYsWaJ33nlHffr0kSStWLFCzZs314YNG9S3b99a7QUAAMCd3B7sDh48qPDwcPn6+qpr165KSkrSTTfdpEOHDikjI0NxcXHOWl9fX8XExCgtLU3jx4/Xrl27VFRU5FITHh6uyMhIpaWllRvsCgoKVFBQ4FzOycmRJBUVFamoqKhG+nQ4HJIkTxl5OC6UW+cpI39/fzkcjhqbS20r6cMq/VQGvdfP3jnf6+dxp3dr9e5wOOTv71+h87ikvqb6r8x2bcYYUyOzqIBPPvlEeXl5atOmjU6ePKkXX3xR3333nfbv368DBw6oR48eOn78uMLDw52PGTdunA4fPqxPP/1UKSkpGj16tEtIk6S4uDhFRETojTfeKHO/iYmJmjlzZqnxlJQUBQQEVG+TAAAAVyEvL0/Dhg1Tdna2GjRocNlat16x69evn/O/o6KidNddd6lVq1Zavny5unXrJkmy2WwujzHGlBq71JVqpk+frsmTJzuXc3Jy1Lx5c8XFxV3xCauq3bt3Kz09XVvOBSi0bVS5dScO7NObYwdpy5Yt6tChQ43MpbYVFRUpNTVVsbGx8vb2dvd0ahW918/eOd/r53Gnd2v1vmfPHkVHR2vcnz5UeNvIcutOHtir6MA8hYWFqWPHjjUyl5JXFivC7S/F/lJgYKCioqJ08OBBDR48WJKUkZGhsLAwZ01mZqZCQ0MlSXa7XYWFhcrKylLDhg1darp3717ufnx9feXr61tq3Nvbu8Z+IT08Lt6nUiybHB7lP+3Fsik/P18eHh6WOTlK1OTzW9fRe/3qnfO9fh73EvRujd49PDyUn59fofO4pL6meq/Mdt1+V+wvFRQU6Ntvv1VYWJgiIiJkt9uVmprqXF9YWKjNmzc7Q1vnzp3l7e3tUpOenq59+/ZdNtgBAABYkVuv2E2dOlUDBw7UjTfeqMzMTL344ovKycnRqFGjZLPZlJCQoKSkJLVu3VqtW7dWUlKSAgICNGzYMElScHCw4uPjNWXKFDVu3FiNGjXS1KlTFRUV5bxLFgAAoL5wa7A7duyYHn30UZ06dUpNmjRRt27d9MUXX6hFixaSpGnTpik/P18TJkxQVlaWunbtqvXr1ysoKMi5jfnz58vLy0tDhw5Vfn6+evfurWXLlsnT09NdbQEAALiFW4PdqlWrLrveZrMpMTFRiYmJ5db4+flp4cKFWrhwYTXPDgAA4NpSp95jBwAAgKoj2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFhElYLdoUOHqnseAAAAuEpVCnY333yzevXqpRUrVuj8+fPVPScAAABUQZWC3Z49e9SxY0dNmTJFdrtd48eP15dfflndcwMAAEAlVCnYRUZGat68eTp+/LiSk5OVkZGhu+++W7feeqvmzZun//znP9U9TwAAAFzBVd084eXlpQcffFB/+ctfNHv2bP3www+aOnWqmjVrppEjRyo9Pb265gkAAIAruKpgt3PnTk2YMEFhYWGaN2+epk6dqh9++EGfffaZjh8/rgceeKC65gkAAIAr8KrKg+bNm6fk5GQdOHBA/fv319tvv63+/fvLw+NiToyIiNAbb7yhdu3aVetkAQAAUL4qBbvFixdrzJgxGj16tOx2e5k1N954o5YsWXJVkwMAAEDFVeml2IMHD2r69OnlhjpJ8vHx0ahRoyq8zZdeekk2m00JCQnOMWOMEhMTFR4eLn9/f/Xs2VP79+93eVxBQYEmTpyokJAQBQYGatCgQTp27FilewIAALjWVSnYJScn69133y01/u6772r58uWV3t6OHTv05ptv6rbbbnMZnzNnjubNm6dFixZpx44dstvtio2NVW5urrMmISFBa9as0apVq7Rt2zadPXtWAwYMUHFxceUbAwAAuIZVKdjNmjVLISEhpcabNm2qpKSkSm3r7NmzGj58uN566y01bNjQOW6M0YIFCzRjxgwNGTJEkZGRWr58ufLy8pSSkiJJys7O1pIlSzR37lz16dNHHTt21IoVK7R3715t2LChKq0BAABcs6oU7A4fPqyIiIhS4y1atNCRI0cqta2nnnpK999/v/r06eMyfujQIWVkZCguLs455uvrq5iYGKWlpUmSdu3apaKiIpea8PBwRUZGOmsAAADqiyrdPNG0aVP961//UsuWLV3G9+zZo8aNG1d4O6tWrdJXX32lHTt2lFqXkZEhSQoNDXUZDw0N1eHDh501Pj4+Llf6SmpKHl+WgoICFRQUOJdzcnIkSUVFRSoqKqrw/CvD4XBIkjxl5OG4UG6dp4z8/f3lcDhqbC61raQPq/RTGfReP3vnfK+fx53erdW7w+GQv79/hc7jkvqa6r8y261SsHvkkUf0zDPPKCgoSNHR0ZKkzZs3a9KkSXrkkUcqtI2jR49q0qRJWr9+vfz8/Mqts9lsLsvGmFJjl7pSzUsvvaSZM2eWGl+/fr0CAgKuMPOrEx2YJx37Z7nr2wZKvVau1PHjx3X8+PEanUttS01NdfcU3Ibe6yfO9/qJ3q1j5cqVks5d8TyWpPT09Br7Yoa8vLwK11Yp2L344os6fPiwevfuLS+vi5twOBwaOXJkhd9jt2vXLmVmZqpz587OseLiYm3ZskWLFi3SgQMHJF28KhcWFuasyczMdF7Fs9vtKiwsVFZWlstVu8zMTHXv3r3cfU+fPl2TJ092Lufk5Kh58+aKi4tTgwYNKjT/ytq9e7fS09O15VyAQttGlVt34sA+vTl2kLZs2aIOHTrUyFxqW1FRkVJTUxUbGytvb293T6dW0Xv97J3zvX4ed3q3Vu979uxRdHS0xv3pQ4W3jSy37uSBvYoOzFNYWJg6duxYI3MpeWWxIqoU7Hx8fLR69Wr993//t/bs2SN/f39FRUWpRYsWFd5G7969tXfvXpex0aNHq127dnruued00003yW63KzU11flEFRYWavPmzZo9e7YkqXPnzvL29lZqaqqGDh0q6WJi3rdvn+bMmVPuvn19feXr61tq3Nvbu8Z+IUs+vLlYNjk8yn/ai2VTfn6+PDw8LHNylKjJ57euo/f61Tvne/087iXo3Rq9e3h4KD8/v0LncUl9TfVeme1WKdiVaNOmjdq0aVOlxwYFBSky0jUBBwYGqnHjxs7xhIQEJSUlqXXr1mrdurWSkpIUEBCgYcOGSZKCg4MVHx+vKVOmqHHjxmrUqJGmTp2qqKioUjdjAAAAWF2Vgl1xcbGWLVumf/zjH8rMzHS+UbjEZ599Vi2TmzZtmvLz8zVhwgRlZWWpa9euWr9+vYKCgpw18+fPl5eXl4YOHar8/Hz17t1by5Ytk6enZ7XMAQAA4FpRpWA3adIkLVu2TPfff78iIyOveDNDRW3atMll2WazKTExUYmJieU+xs/PTwsXLtTChQurZQ4AAADXqioFu1WrVukvf/mL+vfvX93zAQAAQBVV6QOKfXx8dPPNN1f3XAAAAHAVqhTspkyZoldeeUXGmOqeDwAAAKqoSi/Fbtu2TRs3btQnn3yiW2+9tdRtuB988EG1TA4AAAAVV6Vgd/311+vBBx+s7rkAAADgKlQp2CUnJ1f3PAAAAHCVqvQeO0m6cOGCNmzYoDfeeEO5ubmSpBMnTujs2bPVNjkAAABUXJWu2B0+fFj33Xefjhw5ooKCAsXGxiooKEhz5szR+fPn9frrr1f3PAEAAHAFVbpiN2nSJHXp0kVZWVny9/d3jj/44IP6xz/+UW2TAwAAQMVV+a7Yzz//XD4+Pi7jLVq00PHjx6tlYgAAAKicKl2xczgcKi4uLjV+7Ngxl+9xBQAAQO2pUrCLjY3VggULnMs2m01nz57VCy+8wNeMAQAAuEmVXoqdP3++evXqpVtuuUXnz5/XsGHDdPDgQYWEhGjlypXVPUcAAABUQJWCXXh4uL7++mutXLlSX331lRwOh+Lj4zV8+HCXmykAAABQe6oU7CTJ399fY8aM0ZgxY6pzPgAAAKiiKgW7t99++7LrR44cWaXJAAAAoOqqFOwmTZrkslxUVKS8vDz5+PgoICCAYAcAAOAGVborNisry+Xn7NmzOnDggO6++25ungAAAHCTKn9X7KVat26tWbNmlbqaBwAAgNpRbcFOkjw9PXXixInq3CQAAAAqqErvsfvwww9dlo0xSk9P16JFi9SjR49qmRgAAAAqp0rBbvDgwS7LNptNTZo00b333qu5c+dWx7wAAABQSVUKdg6Ho7rnAQAAgKtUre+xAwAAgPtU6Yrd5MmTK1w7b968quwCAAAAlVSlYLd792599dVXunDhgtq2bStJ+v777+Xp6alOnTo562w2W/XMEgAAAFdUpWA3cOBABQUFafny5WrYsKGkix9aPHr0aN1zzz2aMmVKtU4SAAAAV1al99jNnTtXL730kjPUSVLDhg314osvclcsAACAm1Qp2OXk5OjkyZOlxjMzM5Wbm3vVkwIAAEDlVSnYPfjggxo9erTee+89HTt2TMeOHdN7772n+Ph4DRkypLrnCAAAgAqo0nvsXn/9dU2dOlWPPfaYioqKLm7Iy0vx8fF6+eWXq3WCAAAAqJgqBbuAgAC99tprevnll/XDDz/IGKObb75ZgYGB1T0/AAAAVNBVfUBxenq60tPT1aZNGwUGBsoYU13zAgAAQCVVKdidPn1avXv3Vps2bdS/f3+lp6dLksaOHctHnQAAALhJlYLds88+K29vbx05ckQBAQHO8Ycffljr1q2rtskBAACg4qr0Hrv169fr008/VbNmzVzGW7durcOHD1fLxAAAAFA5Vbpid+7cOZcrdSVOnTolX1/fq54UAAAAKq9KwS46Olpvv/22c9lms8nhcOjll19Wr169qm1yAAAAqLgqvRT78ssvq2fPntq5c6cKCws1bdo07d+/Xz///LM+//zz6p4jAAAAKqBKV+xuueUW/etf/9Kdd96p2NhYnTt3TkOGDNHu3bvVqlWr6p4jAAAAKqDSV+yKiooUFxenN954QzNnzqyJOQEAAKAKKn3FztvbW/v27ZPNZquJ+QAAAKCKqvRS7MiRI7VkyZLqngsAAACuQpWCXWFhoRYvXqzOnTtr/Pjxmjx5sstPRS1evFi33XabGjRooAYNGuiuu+7SJ5984lxvjFFiYqLCw8Pl7++vnj17av/+/S7bKCgo0MSJExUSEqLAwEANGjRIx44dq0pbAAAA17RKBbsff/xRDodD+/btU6dOndSgQQN9//332r17t/Pn66+/rvD2mjVrplmzZmnnzp3auXOn7r33Xj3wwAPO8DZnzhzNmzdPixYt0o4dO2S32xUbG6vc3FznNhISErRmzRqtWrVK27Zt09mzZzVgwAAVFxdXpjUAAIBrXqVunmjdurXS09O1ceNGSRe/QuyPf/yjQkNDq7TzgQMHuiz/z//8jxYvXqwvvvhCt9xyixYsWKAZM2ZoyJAhkqTly5crNDRUKSkpGj9+vLKzs7VkyRK988476tOnjyRpxYoVat68uTZs2KC+fftWaV4AAADXokoFO2OMy/Inn3yic+fOVctEiouL9e677+rcuXO66667dOjQIWVkZCguLs5Z4+vrq5iYGKWlpWn8+PHatWuX8y7dEuHh4YqMjFRaWlq5wa6goEAFBQXO5ZycHEkX7/gtKiqqln4u5XA4JEmeMvJwXCi3zlNG/v7+cjgcNTaX2lbSh1X6qQx6r5+9c77Xz+NO79bq3eFwyN/fv0LncUl9TfVfme3azKVp7TI8PDyUkZGhpk2bSpKCgoK0Z88e3XTTTZWf5f/au3ev7rrrLp0/f17XXXedUlJS1L9/f6WlpalHjx46fvy4wsPDnfXjxo3T4cOH9emnnyolJUWjR492CWmSFBcXp4iICL3xxhtl7jMxMbHMj2pJSUkp86vSAAAA3CUvL0/Dhg1Tdna2GjRocNnaSl2xs9lspT7m5Go/9qRt27b6+uuvdebMGb3//vsaNWqUNm/eXO72jTFX3OeVaqZPn+5yk0dOTo6aN2+uuLi4Kz5hVbV7926lp6dry7kAhbaNKrfuxIF9enPsIG3ZskUdOnSokbnUtqKiIqWmpio2Nlbe3t7unk6tovf62Tvne/087vRurd737Nmj6OhojfvThwpvG1lu3ckDexUdmKewsDB17NixRuZS8spiRVT6pdjHH39cvr6+kqTz58/rySefVGBgoEvdBx98UOFt+vj46Oabb5YkdenSRTt27NArr7yi5557TpKUkZGhsLAwZ31mZqbzPX12u12FhYXKyspSw4YNXWq6d+9e7j59fX2dPfySt7d3jf1CenhcvE+lWDY5PMp/2otlU35+vjw8PCxzcpSoyee3rqP3+tU753v9PO4l6N0avXt4eCg/P79C53FJfU31XpntVuqu2FGjRqlp06YKDg5WcHCwHnvsMYWHhzuXS36uhjFGBQUFioiIkN1uV2pqqnNdYWGhNm/e7AxtnTt3lre3t0tNenq69u3bd9lgBwAAYEWVumKXnJxcrTv/7W9/q379+ql58+bKzc3VqlWrtGnTJq1bt042m00JCQlKSkpS69at1bp1ayUlJSkgIEDDhg2TJAUHBys+Pl5TpkxR48aN1ahRI02dOlVRUVHOu2QBAADqi0p/V2x1OnnypEaMGKH09HQFBwfrtttu07p16xQbGytJmjZtmvLz8zVhwgRlZWWpa9euWr9+vYKCgpzbmD9/vry8vDR06FDl5+erd+/eWrZsmTw9Pd3VFgAAgFu4Ndhd6WvJbDabEhMTlZiYWG6Nn5+fFi5cqIULF1bz7AAAAK4tVfpKMQAAANQ9BDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLcGuwe+mll3THHXcoKChITZs21eDBg3XgwAGXGmOMEhMTFR4eLn9/f/Xs2VP79+93qSkoKNDEiRMVEhKiwMBADRo0SMeOHavNVgAAANzOrcFu8+bNeuqpp/TFF18oNTVVFy5cUFxcnM6dO+esmTNnjubNm6dFixZpx44dstvtio2NVW5urrMmISFBa9as0apVq7Rt2zadPXtWAwYMUHFxsTvaAgAAcAsvd+583bp1LsvJyclq2rSpdu3apejoaBljtGDBAs2YMUNDhgyRJC1fvlyhoaFKSUnR+PHjlZ2drSVLluidd95Rnz59JEkrVqxQ8+bNtWHDBvXt27fW+wIAAHCHOvUeu+zsbElSo0aNJEmHDh1SRkaG4uLinDW+vr6KiYlRWlqaJGnXrl0qKipyqQkPD1dkZKSzBgAAoD5w6xW7XzLGaPLkybr77rsVGRkpScrIyJAkhYaGutSGhobq8OHDzhofHx81bNiwVE3J4y9VUFCggoIC53JOTo4kqaioSEVFRdXT0CUcDockyVNGHo4L5dZ5ysjf318Oh6PG5lLbSvqwSj+VQe/1s3fO9/p53OndWr07HA75+/tX6Dwuqa+p/iuzXZsxxtTILCrpqaee0scff6xt27apWbNmkqS0tDT16NFDJ06cUFhYmLP2iSee0NGjR7Vu3TqlpKRo9OjRLkFNkmJjY9WqVSu9/vrrpfaVmJiomTNnlhpPSUlRQEBANXcGAABQdXl5eRo2bJiys7PVoEGDy9bWiSt2EydO1IcffqgtW7Y4Q50k2e12SRevyv0y2GVmZjqv4tntdhUWFiorK8vlql1mZqa6d+9e5v6mT5+uyZMnO5dzcnLUvHlzxcXFXfEJq6rdu3crPT1dW84FKLRtVLl1Jw7s05tjB2nLli3q0KFDjcylthUVFSk1NVWxsbHy9vZ293RqFb3Xz9453+vncad3a/W+Z88eRUdHa9yfPlR428hy604e2KvowDyFhYWpY8eONTKXklcWK8Ktwc4Yo4kTJ2rNmjXatGmTIiIiXNZHRETIbrcrNTXV+WQVFhZq8+bNmj17tiSpc+fO8vb2VmpqqoYOHSpJSk9P1759+zRnzpwy9+vr6ytfX99S497e3jX2C+nhcfHtjMWyyeFR/tNeLJvy8/Pl4eFhmZOjRE0+v3Udvdev3jnf6+dxL0Hv1ujdw8ND+fn5FTqPS+prqvfKbNetwe6pp55SSkqK/vrXvyooKMj5nrjg4GD5+/vLZrMpISFBSUlJat26tVq3bq2kpCQFBARo2LBhztr4+HhNmTJFjRs3VqNGjTR16lRFRUU575IFAACoD9wa7BYvXixJ6tmzp8t4cnKyHn/8cUnStGnTlJ+frwkTJigrK0tdu3bV+vXrFRQU5KyfP3++vLy8NHToUOXn56t3795atmyZPD09a6sVAAAAt3P7S7FXYrPZlJiYqMTExHJr/Pz8tHDhQi1cuLAaZwcAAHBtqVOfYwcAAICqI9gBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAItwa7LZs2aKBAwcqPDxcNptNa9eudVlvjFFiYqLCw8Pl7++vnj17av/+/S41BQUFmjhxokJCQhQYGKhBgwbp2LFjtdgFAABA3eDWYHfu3Dl16NBBixYtKnP9nDlzNG/ePC1atEg7duyQ3W5XbGyscnNznTUJCQlas2aNVq1apW3btuns2bMaMGCAiouLa6sNAACAOsHLnTvv16+f+vXrV+Y6Y4wWLFigGTNmaMiQIZKk5cuXKzQ0VCkpKRo/fryys7O1ZMkSvfPOO+rTp48kacWKFWrevLk2bNigvn371lovAAAA7ubWYHc5hw4dUkZGhuLi4pxjvr6+iomJUVpamsaPH69du3apqKjIpSY8PFyRkZFKS0srN9gVFBSooKDAuZyTkyNJKioqUlFRUY3043A4JEmeMvJwXCi3zlNG/v7+cjgcNTaX2lbSh1X6qQx6r5+9c77Xz+NO79bq3eFwyN/fv0LncUl9TfVfme3ajDGmRmZRSTabTWvWrNHgwYMlSWlpaerRo4eOHz+u8PBwZ924ceN0+PBhffrpp0pJSdHo0aNdQpokxcXFKSIiQm+88UaZ+0pMTNTMmTNLjaekpCggIKD6mgIAALhKeXl5GjZsmLKzs9WgQYPL1tbZK3YlbDaby7IxptTYpa5UM336dE2ePNm5nJOTo+bNmysuLu6KT1hV7d69W+np6dpyLkChbaPKrTtxYJ/eHDtIW7ZsUYcOHWpkLrWtqKhIqampio2Nlbe3t7unU6vovX72zvleP487vVur9z179ig6Olrj/vShwttGllt38sBeRQfmKSwsTB07dqyRuZS8slgRdTbY2e12SVJGRobCwsKc45mZmQoNDXXWFBYWKisrSw0bNnSp6d69e7nb9vX1la+vb6lxb2/vGvuF9PC4eJ9KsWxyeJT/tBfLpvz8fHl4eFjm5ChRk89vXUfv9at3zvf6edxL0Ls1evfw8FB+fn6FzuOS+prqvTLbrbOfYxcRESG73a7U1FTnWGFhoTZv3uwMbZ07d5a3t7dLTXp6uvbt23fZYAcAAGBFbr1id/bsWf373/92Lh86dEhff/21GjVqpBtvvFEJCQlKSkpS69at1bp1ayUlJSkgIEDDhg2TJAUHBys+Pl5TpkxR48aN1ahRI02dOlVRUVHOu2QBAADqC7cGu507d6pXr17O5ZL3vY0aNUrLli3TtGnTlJ+frwkTJigrK0tdu3bV+vXrFRQU5HzM/Pnz5eXlpaFDhyo/P1+9e/fWsmXL5OnpWev9AAAAuJNbg13Pnj11uZtybTabEhMTlZiYWG6Nn5+fFi5cqIULF9bADAEAAK4ddfY9dgAAAKgcgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFWCbYvfbaa4qIiJCfn586d+6srVu3untKAAAAtcoSwW716tVKSEjQjBkztHv3bt1zzz3q16+fjhw54u6pAQAA1BpLBLt58+YpPj5eY8eOVfv27bVgwQI1b95cixcvdvfUAAAAas01H+wKCwu1a9cuxcXFuYzHxcUpLS3NTbMCAACofV7unsDVOnXqlIqLixUaGuoyHhoaqoyMjDIfU1BQoIKCAudydna2JOnnn39WUVFRjcwzJydHeXl5OnnwJxXknSu37vTRQ/Lz89OuXbuUk5Nz2W16eHjI4XBccd/uqiupvXDhgvLy8rR161Z5eJT9t0Rd76WqdQ6Ho8ze68r8arLu0t7r4hxrqu7gwYO67rrr6tX5XlJX3u98ZbdXk3OsqTrO94u9e3l51ci/Je44j/38/HTywF5dyDtbbt2Z4z8pr01T5eTk6PTp01fcd1Xk5uZKkowxV6y95oNdCZvN5rJsjCk1VuKll17SzJkzS41HRETUyNyqYty4ce6eAoBawvkO1F3v/uHZK9asqoV5SBcDXnBw8GVrrvlgFxISIk9Pz1JX5zIzM0tdxSsxffp0TZ482bnscDj0888/q3HjxuWGwauVk5Oj5s2b6+jRo2rQoEGN7KOuond6p/f6g97pnd6rnzFGubm5Cg8Pv2LtNR/sfHx81LlzZ6WmpurBBx90jqempuqBBx4o8zG+vr7y9fV1Gbv++utrcppODRo0qHe/9CXond7rG3qn9/qG3muu9ytdqStxzQc7SZo8ebJGjBihLl266K677tKbb76pI0eO6Mknn3T31AAAAGqNJYLdww8/rNOnT+sPf/iD0tPTFRkZqb///e9q0aKFu6cGAABQaywR7CRpwoQJmjBhgrunUS5fX1+98MILpV4Crg/ond7rG3qn9/qG3utO7zZTkXtnAQAAUOdd8x9QDAAAgIsIdgAAABZBsAMAALAIgl01+Z//+R91795dAQEBFf5MPGOMEhMTFR4eLn9/f/Xs2VP79+93qSkoKNDEiRMVEhKiwMBADRo0SMeOHauBDqouKytLI0aMUHBwsIKDgzVixAidOXPmso+x2Wxl/rz88svOmp49e5Za/8gjj9RwN5VTld4ff/zxUn1169bNpcaKx72oqEjPPfecoqKiFBgYqPDwcI0cOVInTpxwqauLx/21115TRESE/Pz81LlzZ23duvWy9Zs3b1bnzp3l5+enm266Sa+//nqpmvfff1+33HKLfH19dcstt2jNmjU1Nf2rUpneP/jgA8XGxqpJkyZq0KCB7rrrLn366acuNcuWLSvz3D9//nxNt1Jplel906ZNZfb13XffudRZ8biX9f80m82mW2+91VlzrRz3LVu2aODAgQoPD5fNZtPatWuv+Jg6d74bVIv/+q//MvPmzTOTJ082wcHBFXrMrFmzTFBQkHn//ffN3r17zcMPP2zCwsJMTk6Os+bJJ580N9xwg0lNTTVfffWV6dWrl+nQoYO5cOFCDXVSeffdd5+JjIw0aWlpJi0tzURGRpoBAwZc9jHp6ekuP0uXLjU2m8388MMPzpqYmBjzxBNPuNSdOXOmptuplKr0PmrUKHPfffe59HX69GmXGise9zNnzpg+ffqY1atXm++++85s377ddO3a1XTu3Nmlrq4d91WrVhlvb2/z1ltvmW+++cZMmjTJBAYGmsOHD5dZ/+OPP5qAgAAzadIk880335i33nrLeHt7m/fee89Zk5aWZjw9PU1SUpL59ttvTVJSkvHy8jJffPFFbbVVIZXtfdKkSWb27Nnmyy+/NN9//72ZPn268fb2Nl999ZWzJjk52TRo0KDU/wPqmsr2vnHjRiPJHDhwwKWvX56zVj3uZ86ccen56NGjplGjRuaFF15w1lwrx/3vf/+7mTFjhnn//feNJLNmzZrL1tfF851gV82Sk5MrFOwcDoex2+1m1qxZzrHz58+b4OBg8/rrrxtjLp4s3t7eZtWqVc6a48ePGw8PD7Nu3bpqn3tVfPPNN0aSyy/o9u3bjSTz3XffVXg7DzzwgLn33ntdxmJiYsykSZOqa6rVrqq9jxo1yjzwwAPlrq9Px/3LL780klz+wahrx/3OO+80Tz75pMtYu3btzPPPP19m/bRp00y7du1cxsaPH2+6devmXB46dKi57777XGr69u1rHnnkkWqadfWobO9lueWWW8zMmTOdyxX9f6S7Vbb3kmCXlZVV7jbry3Ffs2aNsdls5qeffnKOXSvH/ZcqEuzq4vnOS7FucujQIWVkZCguLs455uvrq5iYGKWlpUmSdu3apaKiIpea8PBwRUZGOmvcbfv27QoODlbXrl2dY926dVNwcHCF53jy5El9/PHHio+PL7Xuz3/+s0JCQnTrrbdq6tSpys3Nrba5X62r6X3Tpk1q2rSp2rRpoyeeeEKZmZnOdfXluEtSdna2bDZbqbcv1JXjXlhYqF27drkcC0mKi4srt8/t27eXqu/bt6927typoqKiy9bUleMrVa33SzkcDuXm5qpRo0Yu42fPnlWLFi3UrFkzDRgwQLt37662eVeHq+m9Y8eOCgsLU+/evbVx40aXdfXluC9ZskR9+vQp9SUBdf24V0VdPN8t8wHF15qMjAxJUmhoqMt4aGioDh8+7Kzx8fFRw4YNS9WUPN7dMjIy1LRp01LjTZs2rfAcly9frqCgIA0ZMsRlfPjw4YqIiJDdbte+ffs0ffp07dmzR6mpqdUy96tV1d779eunhx56SC1atNChQ4f0+9//Xvfee6927dolX1/fenPcz58/r+eff17Dhg1z+X7FunTcT506peLi4jLP0/L6zMjIKLP+woULOnXqlMLCwsqtqSvHV6pa75eaO3euzp07p6FDhzrH2rVrp2XLlikqKko5OTl65ZVX1KNHD+3Zs0etW7eu1h6qqiq9h4WF6c0331Tnzp1VUFCgd955R71799amTZsUHR0tqfzfDSsd9/T0dH3yySdKSUlxGb8WjntV1MXznWB3GYmJiZo5c+Zla3bs2KEuXbpUeR82m81l2RhTauxSFam5WhXtXSrdg1S5OS5dulTDhw+Xn5+fy/gTTzzh/O/IyEi1bt1aXbp00VdffaVOnTpVaNtVUdO9P/zww87/joyMVJcuXdSiRQt9/PHHpcJtZbZbHWrruBcVFemRRx6Rw+HQa6+95rLOXcf9cip7npZVf+l4Vc59d6jqPFeuXKnExET99a9/dfkjoFu3bi43C/Xo0UOdOnXSwoUL9cc//rH6Jl4NKtN727Zt1bZtW+fyXXfdpaNHj+r//t//6wx2ld2mO1V1nsuWLdP111+vwYMHu4xfS8e9sura+U6wu4ynn376infjtWzZskrbttvtki6m/bCwMOd4ZmamM9nb7XYVFhYqKyvL5epNZmamunfvXqX9VlRFe//Xv/6lkydPllr3n//8p9RfKGXZunWrDhw4oNWrV1+xtlOnTvL29tbBgwdr9B/42uq9RFhYmFq0aKGDBw9Ksv5xLyoq0tChQ3Xo0CF99tlnLlfrylJbx70sISEh8vT0LPWX9S/P00vZ7fYy6728vNS4cePL1lTm96amVaX3EqtXr1Z8fLzeffdd9enT57K1Hh4euuOOO5y//3XB1fT+S926ddOKFSucy1Y/7sYYLV26VCNGjJCPj89la+vica+KOnm+18g79+qxyt48MXv2bOdYQUFBmTdPrF692llz4sSJOvkm+n/+85/OsS+++KLCb6IfNWpUqbsiy7N3714jyWzevLnK861OV9t7iVOnThlfX1+zfPlyY4y1j3thYaEZPHiwufXWW01mZmaF9uXu437nnXea3/zmNy5j7du3v+zNE+3bt3cZe/LJJ0u9mbpfv34uNffdd1+dfBN9ZXo3xpiUlBTj5+d3xTedl3A4HKZLly5m9OjRVzPValeV3i/1q1/9yvTq1cu5bOXjbsz/v4Fk7969V9xHXT3uv6QK3jxR1853gl01OXz4sNm9e7eZOXOmue6668zu3bvN7t27TW5urrOmbdu25oMPPnAuz5o1ywQHB5sPPvjA7N271zz66KNlftxJs2bNzIYNG8xXX31l7r333jr5sRe33Xab2b59u9m+fbuJiooq9bEXl/ZujDHZ2dkmICDALF68uNQ2//3vf5uZM2eaHTt2mEOHDpmPP/7YtGvXznTs2PGa7j03N9dMmTLFpKWlmUOHDpmNGzeau+66y9xwww2WP+5FRUVm0KBBplmzZubrr792+ciDgoICY0zdPO4lH/2wZMkS880335iEhAQTGBjovOPv+eefNyNGjHDWl3z8wbPPPmu++eYbs2TJklIff/D5558bT09PM2vWLPPtt9+aWbNm1emPvaho7ykpKcbLy8u8+uqr5X5cTWJiolm3bp354YcfzO7du83o0aONl5eXyx8JdUFle58/f75Zs2aN+f77782+ffvM888/bySZ999/31lj1eNe4rHHHjNdu3Ytc5vXynHPzc11/vstycybN8/s3r3beef+tXC+E+yqyahRo4ykUj8bN2501kgyycnJzmWHw2FeeOEFY7fbja+vr4mOji71l05+fr55+umnTaNGjYy/v78ZMGCAOXLkSC11VTGnT582w4cPN0FBQSYoKMgMHz681C3/l/ZujDFvvPGG8ff3L/Mzyo4cOWKio6NNo0aNjI+Pj2nVqpV55plnSn3em7tVtve8vDwTFxdnmjRpYry9vc2NN95oRo0aVeqYWvG4Hzp0qMxz5JfnSV097q+++qpp0aKF8fHxMZ06dXK5ejhq1CgTExPjUr9p0ybTsWNH4+PjY1q2bFnmHy/vvvuuadu2rfH29jbt2rVzCQB1SWV6j4mJKfP4jho1ylmTkJBgbrzxRuPj42OaNGli4uLiTFpaWi12VHGV6X327NmmVatWxs/PzzRs2NDcfffd5uOPPy61TSsed2MuvtLg7+9v3nzzzTK3d60c95KrjuX9Dl8L57vNmP99lx8AAACuaXyOHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQBUo549eyohIcHd0wBQTxHsAOB/DRw4UH369Clz3fbt22Wz2fTVV1/V8qwAoOIIdgDwv+Lj4/XZZ5/p8OHDpdYtXbpUt99+uzp16uSGmQFAxRDsAOB/DRgwQE2bNtWyZctcxvPy8rR69WoNHjxYjz76qJo1a6aAgABFRUVp5cqVl92mzWbT2rVrXcauv/56l30cP35cDz/8sBo2bKjGjRvrgQce0E8//VQ9TQGoVwh2APC/vLy8NHLkSC1btkzGGOf4u+++q8LCQo0dO1adO3fWRx99pH379mncuHEaMWKE/vnPf1Z5n3l5eerVq5euu+46bdmyRdu2bdN1112n++67T4WFhdXRFoB6hGAHAL8wZswY/fTTT9q0aZNzbOnSpRoyZIhuuOEGTZ06VbfffrtuuukmTZw4UX379tW7775b5f2tWrVKHh4e+tOf/qSoqCi1b99eycnJOnLkiMscAKAivNw9AQCoS9q1a6fu3btr6dKl6tWrl3744Qdt3bpV69evV3FxsWbNmqXVq1fr+PHjKigoUEFBgQIDA6u8v127dunf//63goKCXMbPnz+vH3744WrbAVDPEOwA4BLx8fF6+umn9eqrryo5OVktWrRQ79699fLLL2v+/PlasGCBoqKiFBgYqISEhMu+ZGqz2Vxe1pWkoqIi5387HA517txZf/7zn0s9tkmTJtXXFIB6gWAHAJcYOnSoJk2apJSUFC1fvlxPPPGEbDabtm7dqgceeECPPfaYpIuh7ODBg2rfvn2522rSpInS09OdywcPHlReXp5zuVOnTlq9erWaNm2qBg0a1FxTAOoF3mMHAJe47rrr9PDDD+u3v/2tTpw4occff1ySdPPNNys1NVVpaWn69ttvNX78eGVkZFx2W/fee68WLVqkr776Sjt37tSTTz4pb29v5/rhw4crJCREDzzwgLZu3apDhw5p8+bNmjRpko4dO1aTbQKwIIIdAJQhPj5eWVlZ6tOnj2688UZJ0u9//3t16tRJffv2Vc+ePWW32zV48ODLbmfu3Llq3ry5oqOjNWzYME2dOlUBAQHO9QEBAdqyZYtuvPFGDRkyRO3bt9eYMWOUn5/PFTwAlWYzl775AwAAANckrtgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsIj/B4D13hkeL8qpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.067946/  3.362884, val:  46.67%, val_best:  46.67%, tr:  47.97%, tr_best:  47.97%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.767926/  1.828800, val:  53.33%, val_best:  53.33%, tr:  58.48%, tr_best:  58.48%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.557907/  1.826442, val:  58.75%, val_best:  58.75%, tr:  61.54%, tr_best:  61.54%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.545257/  1.554740, val:  64.58%, val_best:  64.58%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.325836/  2.230688, val:  60.00%, val_best:  64.58%, tr:  70.06%, tr_best:  70.06%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.285393/  1.382507, val:  69.58%, val_best:  69.58%, tr:  72.63%, tr_best:  72.63%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.084371/  1.434148, val:  74.17%, val_best:  74.17%, tr:  79.76%, tr_best:  79.76%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.954193/  1.585408, val:  74.58%, val_best:  74.58%, tr:  82.06%, tr_best:  82.06%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.928799/  1.775933, val:  69.58%, val_best:  74.58%, tr:  84.49%, tr_best:  84.49%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.890854/  1.490066, val:  83.75%, val_best:  83.75%, tr:  86.29%, tr_best:  86.29%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.726478/  1.203175, val:  79.58%, val_best:  83.75%, tr:  90.06%, tr_best:  90.06%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.713288/  1.201030, val:  86.25%, val_best:  86.25%, tr:  89.38%, tr_best:  90.06%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.632138/  1.155620, val:  86.25%, val_best:  86.25%, tr:  91.97%, tr_best:  91.97%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.581806/  1.179153, val:  87.50%, val_best:  87.50%, tr:  93.37%, tr_best:  93.37%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.613621/  1.498377, val:  75.83%, val_best:  87.50%, tr:  92.81%, tr_best:  93.37%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.543067/  1.321320, val:  81.25%, val_best:  87.50%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.477602/  1.241275, val:  85.00%, val_best:  87.50%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.434452/  1.355758, val:  82.92%, val_best:  87.50%, tr:  96.46%, tr_best:  96.46%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.444748/  1.474077, val:  80.42%, val_best:  87.50%, tr:  96.60%, tr_best:  96.60%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.391013/  1.397815, val:  82.92%, val_best:  87.50%, tr:  97.09%, tr_best:  97.09%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.368361/  1.716760, val:  78.33%, val_best:  87.50%, tr:  97.93%, tr_best:  97.93%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.327564/  1.431870, val:  85.00%, val_best:  87.50%, tr:  98.56%, tr_best:  98.56%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.312217/  1.336238, val:  89.17%, val_best:  89.17%, tr:  98.56%, tr_best:  98.56%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.314708/  1.333107, val:  85.42%, val_best:  89.17%, tr:  98.26%, tr_best:  98.56%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.297263/  1.495274, val:  85.00%, val_best:  89.17%, tr:  98.87%, tr_best:  98.87%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.321141/  1.355759, val:  88.33%, val_best:  89.17%, tr:  98.24%, tr_best:  98.87%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.265612/  1.427722, val:  86.25%, val_best:  89.17%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.240059/  1.531972, val:  83.33%, val_best:  89.17%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.227622/  1.476081, val:  86.25%, val_best:  89.17%, tr:  99.39%, tr_best:  99.44%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.200869/  1.528453, val:  85.83%, val_best:  89.17%, tr:  99.64%, tr_best:  99.64%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.200061/  1.510420, val:  87.92%, val_best:  89.17%, tr:  99.59%, tr_best:  99.64%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.182870/  1.504685, val:  88.33%, val_best:  89.17%, tr:  99.41%, tr_best:  99.64%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.201392/  1.612056, val:  83.33%, val_best:  89.17%, tr:  99.19%, tr_best:  99.64%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.170656/  1.588673, val:  87.08%, val_best:  89.17%, tr:  99.68%, tr_best:  99.68%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.149068/  1.509830, val:  90.00%, val_best:  90.00%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.134952/  1.577342, val:  87.92%, val_best:  90.00%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.128952/  1.602040, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.130990/  1.674015, val:  87.08%, val_best:  90.00%, tr:  99.89%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.122871/  1.647588, val:  88.75%, val_best:  90.00%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.114339/  1.748160, val:  87.50%, val_best:  90.00%, tr:  99.91%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.112564/  1.695195, val:  88.75%, val_best:  90.00%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.104217/  1.659392, val:  90.00%, val_best:  90.00%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.104342/  1.614803, val:  89.58%, val_best:  90.00%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.091240/  1.697941, val:  89.17%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.084292/  1.760265, val:  89.17%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.070301/  1.784512, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.076864/  1.806266, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.070158/  1.780534, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.063132/  1.816226, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.059352/  1.781457, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.057331/  1.833946, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.062310/  1.945005, val:  89.17%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.058726/  1.879940, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.053575/  1.847889, val:  89.58%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.047354/  1.909572, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.045560/  1.975203, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.048739/  2.009758, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.046714/  2.071005, val:  87.50%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.042481/  2.023240, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.039719/  2.031779, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.034935/  2.088445, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.032768/  1.976736, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.030136/  2.078460, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.029027/  2.074123, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.028582/  2.075763, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.026641/  2.164817, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.024568/  2.037627, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.028188/  2.161793, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.024403/  2.181430, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.024231/  2.109634, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.020379/  2.207982, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.021772/  2.148055, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.025072/  2.253972, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.022115/  2.207039, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.022792/  2.244877, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.018507/  2.252358, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.019907/  2.290913, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.023253/  2.257904, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.018427/  2.219831, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.017886/  2.215021, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.016469/  2.291522, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.016958/  2.239254, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.015426/  2.301212, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.016492/  2.259010, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.015730/  2.226734, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.013427/  2.236676, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.011614/  2.376139, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.011806/  2.348324, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.013476/  2.402199, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.013050/  2.399274, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.014311/  2.456926, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.011375/  2.364189, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.013018/  2.330353, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.011681/  2.508019, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.010581/  2.459913, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.009287/  2.500555, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009182/  2.463724, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.008715/  2.509110, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.008865/  2.388515, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.009346/  2.459973, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.008777/  2.499037, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.007481/  2.583629, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.008134/  2.539686, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.006569/  2.606579, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.007010/  2.560155, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.008649/  2.667645, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.008795/  2.538800, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.007633/  2.605994, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.007778/  2.561319, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.008893/  2.605730, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.006854/  2.613182, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.005591/  2.587218, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.007704/  2.524127, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.006291/  2.595799, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.006089/  2.596408, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.006670/  2.554045, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.007100/  2.652694, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.008121/  2.618380, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.009311/  2.621298, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.008396/  2.632817, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.008746/  2.609916, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.009212/  2.619775, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.006787/  2.619343, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.007869/  2.619900, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.006742/  2.614093, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.008983/  2.675704, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.010677/  2.615376, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.007393/  2.616216, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.008613/  2.655009, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.007323/  2.792901, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.007561/  2.685595, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.005593/  2.707511, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.006563/  2.709813, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.006353/  2.710068, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.006375/  2.793530, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.006994/  2.707422, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.004995/  2.788130, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.006479/  2.725322, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.008461/  2.709460, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.006958/  2.771474, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.005396/  2.689919, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.005558/  2.786802, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.008555/  2.733796, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.007303/  2.753664, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.006164/  2.829827, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.006474/  2.758401, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.006334/  2.755616, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.005714/  2.734602, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.006133/  2.833653, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.007534/  2.764591, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.007779/  2.854313, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.008176/  2.822042, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.006548/  2.805012, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.006470/  2.820237, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.006423/  2.868371, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.007220/  2.751562, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.005717/  2.875687, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.006419/  2.773239, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.004679/  2.789112, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.004570/  2.797856, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.005449/  2.739307, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.004354/  2.794695, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.004755/  2.810823, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.004200/  2.835806, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.004660/  2.832551, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.003567/  2.823478, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.003108/  2.846156, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.004462/  2.885005, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.003788/  2.867009, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.005271/  2.874645, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.004917/  2.869260, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.004174/  2.835926, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.003376/  2.873075, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.002923/  2.894309, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.002477/  2.893757, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.002459/  2.866132, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.002579/  2.823336, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.002966/  2.829304, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.003329/  2.858649, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.002257/  2.852141, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.002501/  2.916910, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.004203/  2.877995, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.002579/  2.865137, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.003900/  2.854363, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.004498/  2.858460, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.003792/  2.909814, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.004884/  2.865137, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.003430/  2.895937, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.003226/  2.871412, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.003850/  2.947196, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.003507/  3.004738, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.005075/  2.925866, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.004086/  2.861227, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.004102/  2.924607, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.002939/  2.899978, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.003290/  2.964427, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.003579/  2.913672, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.003639/  2.911765, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.002748/  2.933931, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.003749/  2.844859, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-200 lr=['0.0100000'], tr/val_loss:  0.004190/  2.928385, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-201 lr=['0.0100000'], tr/val_loss:  0.003070/  2.961777, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-202 lr=['0.0100000'], tr/val_loss:  0.003451/  2.876293, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-203 lr=['0.0100000'], tr/val_loss:  0.003120/  2.956712, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-204 lr=['0.0100000'], tr/val_loss:  0.002719/  2.916649, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-205 lr=['0.0100000'], tr/val_loss:  0.003098/  2.962889, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-206 lr=['0.0100000'], tr/val_loss:  0.004127/  2.936752, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-207 lr=['0.0100000'], tr/val_loss:  0.003708/  2.942680, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-208 lr=['0.0100000'], tr/val_loss:  0.002981/  2.936314, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-209 lr=['0.0100000'], tr/val_loss:  0.003564/  2.956748, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-210 lr=['0.0100000'], tr/val_loss:  0.003598/  2.934822, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-211 lr=['0.0100000'], tr/val_loss:  0.003751/  2.931806, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-212 lr=['0.0100000'], tr/val_loss:  0.003351/  2.989607, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-213 lr=['0.0100000'], tr/val_loss:  0.004112/  2.985626, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-214 lr=['0.0100000'], tr/val_loss:  0.003466/  3.021178, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-215 lr=['0.0100000'], tr/val_loss:  0.002398/  2.997670, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-216 lr=['0.0100000'], tr/val_loss:  0.002823/  3.016198, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-217 lr=['0.0100000'], tr/val_loss:  0.002368/  3.044117, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-218 lr=['0.0100000'], tr/val_loss:  0.001820/  3.032926, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-219 lr=['0.0100000'], tr/val_loss:  0.001894/  3.087964, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-220 lr=['0.0100000'], tr/val_loss:  0.003437/  3.103790, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-221 lr=['0.0100000'], tr/val_loss:  0.003048/  3.018403, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-222 lr=['0.0100000'], tr/val_loss:  0.002797/  2.993464, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-223 lr=['0.0100000'], tr/val_loss:  0.002610/  3.027706, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-224 lr=['0.0100000'], tr/val_loss:  0.002481/  3.040130, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-225 lr=['0.0100000'], tr/val_loss:  0.001662/  3.072897, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-226 lr=['0.0100000'], tr/val_loss:  0.001581/  3.048791, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-227 lr=['0.0100000'], tr/val_loss:  0.001759/  3.015519, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-228 lr=['0.0100000'], tr/val_loss:  0.002500/  3.091322, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-229 lr=['0.0100000'], tr/val_loss:  0.002153/  3.053387, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-230 lr=['0.0100000'], tr/val_loss:  0.001668/  3.041918, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-231 lr=['0.0100000'], tr/val_loss:  0.001680/  3.022828, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-232 lr=['0.0100000'], tr/val_loss:  0.001367/  3.049808, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-233 lr=['0.0100000'], tr/val_loss:  0.001289/  3.006362, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-234 lr=['0.0100000'], tr/val_loss:  0.001774/  3.039422, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-235 lr=['0.0100000'], tr/val_loss:  0.001718/  3.034570, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-236 lr=['0.0100000'], tr/val_loss:  0.001607/  3.051589, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-237 lr=['0.0100000'], tr/val_loss:  0.001657/  3.052766, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-238 lr=['0.0100000'], tr/val_loss:  0.001977/  3.080875, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-239 lr=['0.0100000'], tr/val_loss:  0.001617/  3.045549, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-240 lr=['0.0100000'], tr/val_loss:  0.001419/  3.088318, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-241 lr=['0.0100000'], tr/val_loss:  0.001404/  3.052097, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-242 lr=['0.0100000'], tr/val_loss:  0.001748/  3.108348, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-243 lr=['0.0100000'], tr/val_loss:  0.001608/  3.060405, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-244 lr=['0.0100000'], tr/val_loss:  0.001483/  3.084896, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-245 lr=['0.0100000'], tr/val_loss:  0.001596/  3.105344, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-246 lr=['0.0100000'], tr/val_loss:  0.001397/  3.087971, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-247 lr=['0.0100000'], tr/val_loss:  0.001426/  3.093276, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-248 lr=['0.0100000'], tr/val_loss:  0.001427/  3.108690, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-249 lr=['0.0100000'], tr/val_loss:  0.001423/  3.069838, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-250 lr=['0.0100000'], tr/val_loss:  0.001529/  3.098588, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-251 lr=['0.0100000'], tr/val_loss:  0.001313/  3.096683, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-252 lr=['0.0100000'], tr/val_loss:  0.001244/  3.058512, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-253 lr=['0.0100000'], tr/val_loss:  0.001302/  3.118196, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-254 lr=['0.0100000'], tr/val_loss:  0.001520/  3.086379, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-255 lr=['0.0100000'], tr/val_loss:  0.001709/  3.087867, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-256 lr=['0.0100000'], tr/val_loss:  0.001352/  3.046274, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-257 lr=['0.0100000'], tr/val_loss:  0.001508/  3.112595, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-258 lr=['0.0100000'], tr/val_loss:  0.001615/  3.110972, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-259 lr=['0.0100000'], tr/val_loss:  0.001969/  3.113810, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-260 lr=['0.0100000'], tr/val_loss:  0.002002/  3.028224, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-261 lr=['0.0100000'], tr/val_loss:  0.001893/  3.085696, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-262 lr=['0.0100000'], tr/val_loss:  0.001335/  3.088238, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-263 lr=['0.0100000'], tr/val_loss:  0.001498/  3.081232, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-264 lr=['0.0100000'], tr/val_loss:  0.001327/  3.076419, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-265 lr=['0.0100000'], tr/val_loss:  0.001199/  3.069447, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-266 lr=['0.0100000'], tr/val_loss:  0.001245/  3.029284, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-267 lr=['0.0100000'], tr/val_loss:  0.001126/  3.071360, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-268 lr=['0.0100000'], tr/val_loss:  0.001312/  3.100102, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-269 lr=['0.0100000'], tr/val_loss:  0.001308/  3.075129, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-270 lr=['0.0100000'], tr/val_loss:  0.001173/  3.081841, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-271 lr=['0.0100000'], tr/val_loss:  0.001143/  3.073836, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-272 lr=['0.0100000'], tr/val_loss:  0.001195/  3.103192, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-273 lr=['0.0100000'], tr/val_loss:  0.001223/  3.100762, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-274 lr=['0.0100000'], tr/val_loss:  0.001202/  3.115342, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-275 lr=['0.0100000'], tr/val_loss:  0.001054/  3.119491, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-276 lr=['0.0100000'], tr/val_loss:  0.001067/  3.138570, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-277 lr=['0.0100000'], tr/val_loss:  0.001056/  3.114586, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-278 lr=['0.0100000'], tr/val_loss:  0.001044/  3.121019, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-279 lr=['0.0100000'], tr/val_loss:  0.001143/  3.115650, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-280 lr=['0.0100000'], tr/val_loss:  0.001099/  3.129624, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-281 lr=['0.0100000'], tr/val_loss:  0.001025/  3.117361, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-282 lr=['0.0100000'], tr/val_loss:  0.001169/  3.139441, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-283 lr=['0.0100000'], tr/val_loss:  0.001062/  3.140804, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-284 lr=['0.0100000'], tr/val_loss:  0.001479/  3.161208, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-285 lr=['0.0100000'], tr/val_loss:  0.001148/  3.175511, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-286 lr=['0.0100000'], tr/val_loss:  0.001129/  3.112970, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-287 lr=['0.0100000'], tr/val_loss:  0.001040/  3.125004, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-288 lr=['0.0100000'], tr/val_loss:  0.001112/  3.110703, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-289 lr=['0.0100000'], tr/val_loss:  0.001065/  3.098318, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-290 lr=['0.0100000'], tr/val_loss:  0.001120/  3.091527, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-291 lr=['0.0100000'], tr/val_loss:  0.001011/  3.102258, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-292 lr=['0.0100000'], tr/val_loss:  0.001040/  3.107948, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-293 lr=['0.0100000'], tr/val_loss:  0.001064/  3.105888, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-294 lr=['0.0100000'], tr/val_loss:  0.001018/  3.108344, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-295 lr=['0.0100000'], tr/val_loss:  0.000985/  3.107862, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-296 lr=['0.0100000'], tr/val_loss:  0.001458/  3.181077, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-297 lr=['0.0100000'], tr/val_loss:  0.001583/  3.148324, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-298 lr=['0.0100000'], tr/val_loss:  0.001313/  3.075609, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-299 lr=['0.0100000'], tr/val_loss:  0.001326/  3.155684, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-300 lr=['0.0100000'], tr/val_loss:  0.001725/  3.103983, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-301 lr=['0.0100000'], tr/val_loss:  0.002181/  3.145965, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-302 lr=['0.0100000'], tr/val_loss:  0.002112/  3.121867, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-303 lr=['0.0100000'], tr/val_loss:  0.002109/  3.081282, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-304 lr=['0.0100000'], tr/val_loss:  0.002725/  3.118645, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-305 lr=['0.0100000'], tr/val_loss:  0.002888/  3.095040, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-306 lr=['0.0100000'], tr/val_loss:  0.002967/  3.126162, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-307 lr=['0.0100000'], tr/val_loss:  0.003328/  3.134198, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-308 lr=['0.0100000'], tr/val_loss:  0.004312/  3.096119, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-309 lr=['0.0100000'], tr/val_loss:  0.003374/  3.137818, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-310 lr=['0.0100000'], tr/val_loss:  0.004392/  3.089263, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-311 lr=['0.0100000'], tr/val_loss:  0.002976/  3.081211, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-312 lr=['0.0100000'], tr/val_loss:  0.002115/  3.133629, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-313 lr=['0.0100000'], tr/val_loss:  0.001846/  3.135224, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-314 lr=['0.0100000'], tr/val_loss:  0.001412/  3.184973, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-315 lr=['0.0100000'], tr/val_loss:  0.001386/  3.198681, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-316 lr=['0.0100000'], tr/val_loss:  0.001691/  3.149868, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-317 lr=['0.0100000'], tr/val_loss:  0.002259/  3.187317, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-318 lr=['0.0100000'], tr/val_loss:  0.002156/  3.197680, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-319 lr=['0.0100000'], tr/val_loss:  0.002297/  3.146353, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-320 lr=['0.0100000'], tr/val_loss:  0.001658/  3.143334, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-321 lr=['0.0100000'], tr/val_loss:  0.002212/  3.165318, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-322 lr=['0.0100000'], tr/val_loss:  0.001644/  3.199516, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-323 lr=['0.0100000'], tr/val_loss:  0.001313/  3.212698, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-324 lr=['0.0100000'], tr/val_loss:  0.001613/  3.216071, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-325 lr=['0.0100000'], tr/val_loss:  0.001689/  3.194223, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-326 lr=['0.0100000'], tr/val_loss:  0.002106/  3.215285, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-327 lr=['0.0100000'], tr/val_loss:  0.002289/  3.215456, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-328 lr=['0.0100000'], tr/val_loss:  0.002489/  3.210984, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-329 lr=['0.0100000'], tr/val_loss:  0.001606/  3.197296, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-330 lr=['0.0100000'], tr/val_loss:  0.001540/  3.254769, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-331 lr=['0.0100000'], tr/val_loss:  0.001481/  3.218468, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-332 lr=['0.0100000'], tr/val_loss:  0.001559/  3.222130, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-333 lr=['0.0100000'], tr/val_loss:  0.001493/  3.244853, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-334 lr=['0.0100000'], tr/val_loss:  0.002036/  3.243876, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-335 lr=['0.0100000'], tr/val_loss:  0.002334/  3.229545, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-336 lr=['0.0100000'], tr/val_loss:  0.001770/  3.206661, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-337 lr=['0.0100000'], tr/val_loss:  0.001587/  3.265618, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-338 lr=['0.0100000'], tr/val_loss:  0.002142/  3.211576, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-339 lr=['0.0100000'], tr/val_loss:  0.002276/  3.229341, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-340 lr=['0.0100000'], tr/val_loss:  0.002443/  3.243695, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-341 lr=['0.0100000'], tr/val_loss:  0.001815/  3.176932, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-342 lr=['0.0100000'], tr/val_loss:  0.001693/  3.202057, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-343 lr=['0.0100000'], tr/val_loss:  0.001549/  3.196405, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-344 lr=['0.0100000'], tr/val_loss:  0.001483/  3.248347, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-345 lr=['0.0100000'], tr/val_loss:  0.001492/  3.183923, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-346 lr=['0.0100000'], tr/val_loss:  0.002030/  3.187250, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-347 lr=['0.0100000'], tr/val_loss:  0.002452/  3.253713, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-348 lr=['0.0100000'], tr/val_loss:  0.002195/  3.251617, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-349 lr=['0.0100000'], tr/val_loss:  0.002917/  3.178664, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-350 lr=['0.0100000'], tr/val_loss:  0.003098/  3.179611, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-351 lr=['0.0100000'], tr/val_loss:  0.002689/  3.189217, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-352 lr=['0.0100000'], tr/val_loss:  0.002419/  3.227343, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-353 lr=['0.0100000'], tr/val_loss:  0.002477/  3.249247, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-354 lr=['0.0100000'], tr/val_loss:  0.002322/  3.197145, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-355 lr=['0.0100000'], tr/val_loss:  0.003690/  3.290119, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-356 lr=['0.0100000'], tr/val_loss:  0.006042/  3.235330, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-357 lr=['0.0100000'], tr/val_loss:  0.003009/  3.195910, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-358 lr=['0.0100000'], tr/val_loss:  0.002977/  3.245420, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-359 lr=['0.0100000'], tr/val_loss:  0.003138/  3.258737, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-360 lr=['0.0100000'], tr/val_loss:  0.003170/  3.239357, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-361 lr=['0.0100000'], tr/val_loss:  0.004257/  3.294995, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-362 lr=['0.0100000'], tr/val_loss:  0.003695/  3.318985, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-363 lr=['0.0100000'], tr/val_loss:  0.003095/  3.302844, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-364 lr=['0.0100000'], tr/val_loss:  0.002710/  3.312847, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-365 lr=['0.0100000'], tr/val_loss:  0.005847/  3.189168, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-366 lr=['0.0100000'], tr/val_loss:  0.009436/  3.228199, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-367 lr=['0.0100000'], tr/val_loss:  0.008289/  3.141749, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.0 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"3\",\n",
    "                single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = decay, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 6, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 9, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 5, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "#         \"epoch_num\": {\"values\": [100]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [5]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [100_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [True]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [1]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"0\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  unique_name_hyper,\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#                         ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = '6pj3lh8j'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
