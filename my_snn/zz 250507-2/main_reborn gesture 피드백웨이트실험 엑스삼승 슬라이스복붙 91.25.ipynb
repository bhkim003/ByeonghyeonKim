{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37077/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250507_213113-hylbe5qf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hylbe5qf' target=\"_blank\">frosty-surf-8289</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hylbe5qf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hylbe5qf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.01, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.969671/  3.120895, val:  43.75%, val_best:  43.75%, tr:  50.16%, tr_best:  50.16%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.794953/  2.303602, val:  50.42%, val_best:  50.42%, tr:  59.58%, tr_best:  59.58%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.572712/  2.130874, val:  50.00%, val_best:  50.42%, tr:  61.63%, tr_best:  61.63%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.694742/  1.714871, val:  66.67%, val_best:  66.67%, tr:  65.62%, tr_best:  65.62%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.587573/  2.883298, val:  56.67%, val_best:  66.67%, tr:  67.45%, tr_best:  67.45%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.490014/  1.725927, val:  61.67%, val_best:  66.67%, tr:  70.31%, tr_best:  70.31%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.228183/  1.915246, val:  59.17%, val_best:  66.67%, tr:  76.69%, tr_best:  76.69%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.089684/  1.785573, val:  65.42%, val_best:  66.67%, tr:  80.03%, tr_best:  80.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.036187/  2.192775, val:  68.75%, val_best:  68.75%, tr:  82.80%, tr_best:  82.80%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.974397/  1.424375, val:  85.00%, val_best:  85.00%, tr:  85.26%, tr_best:  85.26%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.792482/  1.331049, val:  84.17%, val_best:  85.00%, tr:  88.82%, tr_best:  88.82%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.775148/  1.870154, val:  66.67%, val_best:  85.00%, tr:  87.76%, tr_best:  88.82%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.774228/  1.490665, val:  80.42%, val_best:  85.00%, tr:  88.07%, tr_best:  88.82%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.622408/  1.406648, val:  85.83%, val_best:  85.83%, tr:  92.22%, tr_best:  92.22%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.739528/  1.731890, val:  80.42%, val_best:  85.83%, tr:  89.47%, tr_best:  92.22%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.602000/  1.802494, val:  76.25%, val_best:  85.83%, tr:  93.03%, tr_best:  93.03%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.539599/  1.503006, val:  84.58%, val_best:  85.83%, tr:  93.94%, tr_best:  93.94%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.489838/  1.727641, val:  71.25%, val_best:  85.83%, tr:  94.70%, tr_best:  94.70%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.502483/  1.742179, val:  75.83%, val_best:  85.83%, tr:  94.61%, tr_best:  94.70%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.451778/  1.448710, val:  85.00%, val_best:  85.83%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.444689/  2.095919, val:  74.58%, val_best:  85.83%, tr:  95.76%, tr_best:  95.76%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.369705/  1.738045, val:  81.25%, val_best:  85.83%, tr:  96.82%, tr_best:  96.82%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.346287/  1.434508, val:  88.33%, val_best:  88.33%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.296566/  1.439596, val:  87.08%, val_best:  88.33%, tr:  97.79%, tr_best:  97.79%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.347798/  1.614954, val:  86.25%, val_best:  88.33%, tr:  96.55%, tr_best:  97.79%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.412984/  1.577427, val:  86.25%, val_best:  88.33%, tr:  96.10%, tr_best:  97.79%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.311537/  1.570103, val:  86.25%, val_best:  88.33%, tr:  97.95%, tr_best:  97.95%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.256471/  1.702137, val:  81.25%, val_best:  88.33%, tr:  98.65%, tr_best:  98.65%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.252985/  1.587283, val:  88.75%, val_best:  88.75%, tr:  98.44%, tr_best:  98.65%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.240690/  1.666071, val:  85.83%, val_best:  88.75%, tr:  98.85%, tr_best:  98.85%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.239414/  1.665451, val:  84.58%, val_best:  88.75%, tr:  98.67%, tr_best:  98.85%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.202882/  1.598449, val:  88.33%, val_best:  88.75%, tr:  99.01%, tr_best:  99.01%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.213406/  1.651124, val:  87.08%, val_best:  88.75%, tr:  98.49%, tr_best:  99.01%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.199311/  1.646056, val:  88.75%, val_best:  88.75%, tr:  98.96%, tr_best:  99.01%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.189609/  1.709803, val:  88.33%, val_best:  88.75%, tr:  98.99%, tr_best:  99.01%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.194082/  1.749303, val:  87.92%, val_best:  88.75%, tr:  98.69%, tr_best:  99.01%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.177938/  1.720287, val:  89.58%, val_best:  89.58%, tr:  99.32%, tr_best:  99.32%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.178284/  1.732400, val:  88.33%, val_best:  89.58%, tr:  99.03%, tr_best:  99.32%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.162583/  1.814200, val:  88.33%, val_best:  89.58%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.135649/  1.900564, val:  86.25%, val_best:  89.58%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.136440/  1.762186, val:  88.33%, val_best:  89.58%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.142745/  1.830118, val:  87.92%, val_best:  89.58%, tr:  99.39%, tr_best:  99.57%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.140598/  1.765974, val:  90.00%, val_best:  90.00%, tr:  99.32%, tr_best:  99.57%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.118410/  1.801983, val:  87.92%, val_best:  90.00%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.103703/  1.839431, val:  89.58%, val_best:  90.00%, tr:  99.64%, tr_best:  99.73%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.127136/  1.825410, val:  89.17%, val_best:  90.00%, tr:  99.35%, tr_best:  99.73%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.103218/  1.934881, val:  87.50%, val_best:  90.00%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.100898/  1.889129, val:  90.42%, val_best:  90.42%, tr:  99.75%, tr_best:  99.77%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.099430/  1.943465, val:  90.00%, val_best:  90.42%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.085818/  2.011366, val:  88.75%, val_best:  90.42%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.094776/  1.958806, val:  89.17%, val_best:  90.42%, tr:  99.80%, tr_best:  99.89%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.072219/  1.936539, val:  91.25%, val_best:  91.25%, tr:  99.86%, tr_best:  99.89%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.074324/  2.024604, val:  89.58%, val_best:  91.25%, tr:  99.82%, tr_best:  99.89%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.078637/  2.016870, val:  87.92%, val_best:  91.25%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.077832/  2.005900, val:  90.00%, val_best:  91.25%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.066566/  2.049750, val:  90.00%, val_best:  91.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.061089/  2.083563, val:  89.17%, val_best:  91.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.066208/  2.166108, val:  86.67%, val_best:  91.25%, tr:  99.84%, tr_best:  99.95%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.060900/  1.995120, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.060004/  2.131639, val:  89.17%, val_best:  91.25%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.058798/  2.185396, val:  87.50%, val_best:  91.25%, tr:  99.89%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.052568/  2.163355, val:  90.00%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.046672/  2.167722, val:  87.08%, val_best:  91.25%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.046436/  2.197771, val:  90.83%, val_best:  91.25%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.055072/  2.210685, val:  89.17%, val_best:  91.25%, tr:  99.89%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.045772/  2.220433, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.044767/  2.272178, val:  87.92%, val_best:  91.25%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.041824/  2.291831, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.054974/  2.218596, val:  88.33%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.043301/  2.228745, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.037206/  2.296403, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.037552/  2.406052, val:  88.75%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.037138/  2.299267, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.034711/  2.304207, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.028825/  2.360333, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.033067/  2.411486, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.035062/  2.358191, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.030805/  2.342535, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.030649/  2.326581, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.030330/  2.371750, val:  89.58%, val_best:  91.25%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.028111/  2.451001, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.027261/  2.467454, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.026530/  2.485339, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.026562/  2.490440, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.031912/  2.427213, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.024975/  2.425734, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.022350/  2.388472, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.022550/  2.419506, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.026086/  2.465930, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.025933/  2.528538, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.027076/  2.522504, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.022744/  2.520222, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.022990/  2.492154, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.022859/  2.584463, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.020205/  2.543302, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.018830/  2.563689, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.022616/  2.546077, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.018214/  2.623041, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.019376/  2.599510, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.020514/  2.576324, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.017008/  2.579941, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.016840/  2.559489, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.016362/  2.649876, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.016645/  2.585836, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.018020/  2.689267, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.016343/  2.651022, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.014132/  2.634349, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.019874/  2.665658, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.016044/  2.623490, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.011383/  2.640436, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.014599/  2.668613, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.013244/  2.674848, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.012708/  2.728868, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.010956/  2.704250, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.011887/  2.654139, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.012635/  2.695984, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.014732/  2.784639, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.011921/  2.675640, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.010418/  2.781862, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.010362/  2.739149, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.012045/  2.697916, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.011133/  2.766060, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.009657/  2.722617, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.009625/  2.722065, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.009259/  2.793433, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.007655/  2.758296, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.009039/  2.740424, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.007285/  2.788980, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.009189/  2.773225, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.007646/  2.899773, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.007651/  2.831165, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.008724/  2.899493, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.008553/  2.910704, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.007482/  2.866697, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.009966/  2.889146, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.008327/  2.894704, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.008666/  2.878087, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.007618/  2.864696, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.009250/  2.945350, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.009766/  2.872163, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.007696/  2.885484, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.006708/  2.874642, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.011554/  2.965488, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.009962/  2.915449, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.009421/  2.889753, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.008021/  2.858421, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.010877/  2.875306, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.010838/  2.939363, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.009307/  2.952904, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.009589/  2.866676, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.008099/  2.913512, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.010529/  2.918811, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.007667/  2.890751, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.005801/  2.975148, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.006116/  2.873044, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.005823/  2.935839, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.006390/  2.910621, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.005338/  2.915017, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.005968/  2.931607, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.007192/  2.982858, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.006750/  2.962804, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.003831/  2.961179, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.004410/  2.998042, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.005759/  3.008770, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.006857/  3.053023, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.007126/  2.973243, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.006171/  3.032871, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.007440/  3.029358, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.006794/  2.999912, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.006472/  3.039607, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.009253/  3.029460, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.010532/  2.972119, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.009631/  3.052889, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.005256/  2.974101, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.004717/  2.973407, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.005723/  2.942004, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.004597/  3.039860, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.006077/  3.078515, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.005297/  3.060000, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.003222/  3.076360, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.005093/  3.091039, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.003860/  3.087540, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.004090/  3.018450, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.004828/  3.095879, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.004831/  3.146105, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.005975/  3.078660, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.004769/  3.078818, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.003760/  3.148940, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.004685/  3.099487, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.003515/  3.072935, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.004577/  3.082003, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.005448/  3.039602, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.004686/  3.002499, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.004230/  3.052382, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.005802/  3.050024, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.004065/  3.040608, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.006174/  3.060343, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.004889/  3.093337, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.006590/  3.092612, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.006188/  3.127414, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-200 lr=['0.0100000'], tr/val_loss:  0.005026/  3.104335, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-201 lr=['0.0100000'], tr/val_loss:  0.005916/  3.153003, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-202 lr=['0.0100000'], tr/val_loss:  0.005248/  3.131211, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-203 lr=['0.0100000'], tr/val_loss:  0.004441/  3.192024, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-204 lr=['0.0100000'], tr/val_loss:  0.004571/  3.196502, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-205 lr=['0.0100000'], tr/val_loss:  0.004605/  3.190178, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-206 lr=['0.0100000'], tr/val_loss:  0.005961/  3.177764, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-207 lr=['0.0100000'], tr/val_loss:  0.007701/  3.169390, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-208 lr=['0.0100000'], tr/val_loss:  0.006194/  3.128455, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-209 lr=['0.0100000'], tr/val_loss:  0.005915/  3.169671, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-210 lr=['0.0100000'], tr/val_loss:  0.004329/  3.145909, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-211 lr=['0.0100000'], tr/val_loss:  0.004016/  3.100607, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-212 lr=['0.0100000'], tr/val_loss:  0.004538/  3.137065, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-213 lr=['0.0100000'], tr/val_loss:  0.004948/  3.127151, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-214 lr=['0.0100000'], tr/val_loss:  0.006315/  3.094253, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-215 lr=['0.0100000'], tr/val_loss:  0.005012/  3.137550, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-216 lr=['0.0100000'], tr/val_loss:  0.004117/  3.090197, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-217 lr=['0.0100000'], tr/val_loss:  0.005018/  3.193635, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-218 lr=['0.0100000'], tr/val_loss:  0.005852/  3.162363, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-219 lr=['0.0100000'], tr/val_loss:  0.005846/  3.172544, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-220 lr=['0.0100000'], tr/val_loss:  0.005299/  3.141947, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-221 lr=['0.0100000'], tr/val_loss:  0.004457/  3.185298, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-222 lr=['0.0100000'], tr/val_loss:  0.004321/  3.163537, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-223 lr=['0.0100000'], tr/val_loss:  0.004473/  3.172309, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-224 lr=['0.0100000'], tr/val_loss:  0.006631/  3.093950, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-225 lr=['0.0100000'], tr/val_loss:  0.006343/  3.105032, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-226 lr=['0.0100000'], tr/val_loss:  0.004829/  3.105695, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-227 lr=['0.0100000'], tr/val_loss:  0.005538/  3.225157, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-228 lr=['0.0100000'], tr/val_loss:  0.006472/  3.163358, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-229 lr=['0.0100000'], tr/val_loss:  0.005289/  3.186381, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-230 lr=['0.0100000'], tr/val_loss:  0.006017/  3.152875, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-231 lr=['0.0100000'], tr/val_loss:  0.005769/  3.163717, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-232 lr=['0.0100000'], tr/val_loss:  0.005999/  3.181393, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-233 lr=['0.0100000'], tr/val_loss:  0.005237/  3.198984, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-234 lr=['0.0100000'], tr/val_loss:  0.004962/  3.184518, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-235 lr=['0.0100000'], tr/val_loss:  0.004204/  3.166245, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-236 lr=['0.0100000'], tr/val_loss:  0.004973/  3.147977, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-237 lr=['0.0100000'], tr/val_loss:  0.005977/  3.166330, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-238 lr=['0.0100000'], tr/val_loss:  0.004953/  3.108556, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-239 lr=['0.0100000'], tr/val_loss:  0.006242/  3.214320, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-240 lr=['0.0100000'], tr/val_loss:  0.006010/  3.200567, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-241 lr=['0.0100000'], tr/val_loss:  0.005957/  3.136272, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-242 lr=['0.0100000'], tr/val_loss:  0.004825/  3.209616, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-243 lr=['0.0100000'], tr/val_loss:  0.003685/  3.212564, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-244 lr=['0.0100000'], tr/val_loss:  0.003225/  3.190162, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-245 lr=['0.0100000'], tr/val_loss:  0.003420/  3.184891, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-246 lr=['0.0100000'], tr/val_loss:  0.003250/  3.191146, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-247 lr=['0.0100000'], tr/val_loss:  0.003534/  3.214149, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-248 lr=['0.0100000'], tr/val_loss:  0.004891/  3.157086, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-249 lr=['0.0100000'], tr/val_loss:  0.004048/  3.124531, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-250 lr=['0.0100000'], tr/val_loss:  0.003752/  3.112838, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-251 lr=['0.0100000'], tr/val_loss:  0.005072/  3.156411, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-252 lr=['0.0100000'], tr/val_loss:  0.005611/  3.125633, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-253 lr=['0.0100000'], tr/val_loss:  0.004411/  3.133909, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-254 lr=['0.0100000'], tr/val_loss:  0.005414/  3.226047, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-255 lr=['0.0100000'], tr/val_loss:  0.004967/  3.193453, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-256 lr=['0.0100000'], tr/val_loss:  0.006423/  3.185496, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-257 lr=['0.0100000'], tr/val_loss:  0.004393/  3.247936, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-258 lr=['0.0100000'], tr/val_loss:  0.003731/  3.201672, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-259 lr=['0.0100000'], tr/val_loss:  0.004136/  3.227833, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-260 lr=['0.0100000'], tr/val_loss:  0.004036/  3.265522, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-261 lr=['0.0100000'], tr/val_loss:  0.006493/  3.237387, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-262 lr=['0.0100000'], tr/val_loss:  0.006298/  3.241589, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-263 lr=['0.0100000'], tr/val_loss:  0.004220/  3.224095, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-264 lr=['0.0100000'], tr/val_loss:  0.003529/  3.175786, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-265 lr=['0.0100000'], tr/val_loss:  0.003569/  3.247606, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-266 lr=['0.0100000'], tr/val_loss:  0.002903/  3.239275, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-267 lr=['0.0100000'], tr/val_loss:  0.004148/  3.175948, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-268 lr=['0.0100000'], tr/val_loss:  0.004857/  3.234470, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-269 lr=['0.0100000'], tr/val_loss:  0.004720/  3.200636, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-270 lr=['0.0100000'], tr/val_loss:  0.004418/  3.197425, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-271 lr=['0.0100000'], tr/val_loss:  0.004450/  3.171019, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-272 lr=['0.0100000'], tr/val_loss:  0.006038/  3.323589, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-273 lr=['0.0100000'], tr/val_loss:  0.005483/  3.287690, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-274 lr=['0.0100000'], tr/val_loss:  0.004367/  3.231823, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-275 lr=['0.0100000'], tr/val_loss:  0.003689/  3.240121, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-276 lr=['0.0100000'], tr/val_loss:  0.004055/  3.288206, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-277 lr=['0.0100000'], tr/val_loss:  0.003793/  3.257581, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-278 lr=['0.0100000'], tr/val_loss:  0.004402/  3.223059, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-279 lr=['0.0100000'], tr/val_loss:  0.005559/  3.177312, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-280 lr=['0.0100000'], tr/val_loss:  0.005187/  3.263870, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-281 lr=['0.0100000'], tr/val_loss:  0.002905/  3.257301, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-282 lr=['0.0100000'], tr/val_loss:  0.006102/  3.221730, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-283 lr=['0.0100000'], tr/val_loss:  0.004316/  3.202701, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-284 lr=['0.0100000'], tr/val_loss:  0.004453/  3.275856, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-285 lr=['0.0100000'], tr/val_loss:  0.005217/  3.291932, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-286 lr=['0.0100000'], tr/val_loss:  0.005159/  3.261753, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-287 lr=['0.0100000'], tr/val_loss:  0.003043/  3.283804, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-288 lr=['0.0100000'], tr/val_loss:  0.003543/  3.265381, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-289 lr=['0.0100000'], tr/val_loss:  0.002678/  3.296291, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-290 lr=['0.0100000'], tr/val_loss:  0.002482/  3.251886, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-291 lr=['0.0100000'], tr/val_loss:  0.002797/  3.258953, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-292 lr=['0.0100000'], tr/val_loss:  0.002970/  3.299025, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-293 lr=['0.0100000'], tr/val_loss:  0.002084/  3.295099, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-294 lr=['0.0100000'], tr/val_loss:  0.002363/  3.318200, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-295 lr=['0.0100000'], tr/val_loss:  0.002737/  3.366041, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-296 lr=['0.0100000'], tr/val_loss:  0.001577/  3.299473, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-297 lr=['0.0100000'], tr/val_loss:  0.001596/  3.308814, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-298 lr=['0.0100000'], tr/val_loss:  0.001804/  3.296429, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-299 lr=['0.0100000'], tr/val_loss:  0.001644/  3.322763, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-300 lr=['0.0100000'], tr/val_loss:  0.001575/  3.357819, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-301 lr=['0.0100000'], tr/val_loss:  0.002244/  3.386425, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-302 lr=['0.0100000'], tr/val_loss:  0.001772/  3.388736, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-303 lr=['0.0100000'], tr/val_loss:  0.002351/  3.364132, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-304 lr=['0.0100000'], tr/val_loss:  0.003224/  3.289209, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-305 lr=['0.0100000'], tr/val_loss:  0.003950/  3.267079, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-306 lr=['0.0100000'], tr/val_loss:  0.003195/  3.362826, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-307 lr=['0.0100000'], tr/val_loss:  0.002881/  3.350590, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-308 lr=['0.0100000'], tr/val_loss:  0.002450/  3.357931, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-309 lr=['0.0100000'], tr/val_loss:  0.002880/  3.361882, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-310 lr=['0.0100000'], tr/val_loss:  0.003243/  3.325337, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-311 lr=['0.0100000'], tr/val_loss:  0.002692/  3.345482, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-312 lr=['0.0100000'], tr/val_loss:  0.002115/  3.376052, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-313 lr=['0.0100000'], tr/val_loss:  0.002438/  3.379465, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-314 lr=['0.0100000'], tr/val_loss:  0.002858/  3.392166, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-315 lr=['0.0100000'], tr/val_loss:  0.004939/  3.345481, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-316 lr=['0.0100000'], tr/val_loss:  0.004505/  3.351032, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-317 lr=['0.0100000'], tr/val_loss:  0.003525/  3.404757, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-318 lr=['0.0100000'], tr/val_loss:  0.004718/  3.419981, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-319 lr=['0.0100000'], tr/val_loss:  0.003475/  3.442036, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-320 lr=['0.0100000'], tr/val_loss:  0.003526/  3.411958, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-321 lr=['0.0100000'], tr/val_loss:  0.001952/  3.384043, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-322 lr=['0.0100000'], tr/val_loss:  0.002765/  3.440903, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-323 lr=['0.0100000'], tr/val_loss:  0.002443/  3.421953, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-324 lr=['0.0100000'], tr/val_loss:  0.004038/  3.369574, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-325 lr=['0.0100000'], tr/val_loss:  0.003624/  3.395915, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-326 lr=['0.0100000'], tr/val_loss:  0.005260/  3.396968, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-327 lr=['0.0100000'], tr/val_loss:  0.003776/  3.410830, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-328 lr=['0.0100000'], tr/val_loss:  0.003430/  3.439251, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-329 lr=['0.0100000'], tr/val_loss:  0.003395/  3.363388, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-330 lr=['0.0100000'], tr/val_loss:  0.003335/  3.448501, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-331 lr=['0.0100000'], tr/val_loss:  0.002934/  3.401465, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-332 lr=['0.0100000'], tr/val_loss:  0.003688/  3.453267, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-333 lr=['0.0100000'], tr/val_loss:  0.003574/  3.431378, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-334 lr=['0.0100000'], tr/val_loss:  0.002672/  3.388777, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-335 lr=['0.0100000'], tr/val_loss:  0.002007/  3.390116, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-336 lr=['0.0100000'], tr/val_loss:  0.001832/  3.345919, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-337 lr=['0.0100000'], tr/val_loss:  0.002158/  3.356256, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-338 lr=['0.0100000'], tr/val_loss:  0.002064/  3.342428, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-339 lr=['0.0100000'], tr/val_loss:  0.002140/  3.399763, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-340 lr=['0.0100000'], tr/val_loss:  0.001772/  3.377892, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-341 lr=['0.0100000'], tr/val_loss:  0.001587/  3.341387, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-342 lr=['0.0100000'], tr/val_loss:  0.001390/  3.359226, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-343 lr=['0.0100000'], tr/val_loss:  0.001285/  3.385050, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-344 lr=['0.0100000'], tr/val_loss:  0.001459/  3.382572, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-345 lr=['0.0100000'], tr/val_loss:  0.002145/  3.434922, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-346 lr=['0.0100000'], tr/val_loss:  0.001675/  3.409641, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-347 lr=['0.0100000'], tr/val_loss:  0.001541/  3.407857, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-348 lr=['0.0100000'], tr/val_loss:  0.002010/  3.422932, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-349 lr=['0.0100000'], tr/val_loss:  0.001597/  3.414733, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-350 lr=['0.0100000'], tr/val_loss:  0.001281/  3.439623, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-351 lr=['0.0100000'], tr/val_loss:  0.001104/  3.458855, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-352 lr=['0.0100000'], tr/val_loss:  0.001543/  3.444114, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-353 lr=['0.0100000'], tr/val_loss:  0.001450/  3.423547, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-354 lr=['0.0100000'], tr/val_loss:  0.002268/  3.473871, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-355 lr=['0.0100000'], tr/val_loss:  0.002713/  3.451602, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-356 lr=['0.0100000'], tr/val_loss:  0.003452/  3.395193, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-357 lr=['0.0100000'], tr/val_loss:  0.002863/  3.475077, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-358 lr=['0.0100000'], tr/val_loss:  0.002749/  3.421469, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-359 lr=['0.0100000'], tr/val_loss:  0.001744/  3.431245, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-360 lr=['0.0100000'], tr/val_loss:  0.002239/  3.455920, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-361 lr=['0.0100000'], tr/val_loss:  0.002984/  3.440118, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-362 lr=['0.0100000'], tr/val_loss:  0.005081/  3.403970, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-363 lr=['0.0100000'], tr/val_loss:  0.005063/  3.445184, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-364 lr=['0.0100000'], tr/val_loss:  0.004461/  3.482316, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-365 lr=['0.0100000'], tr/val_loss:  0.002924/  3.372254, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-366 lr=['0.0100000'], tr/val_loss:  0.004049/  3.417288, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-367 lr=['0.0100000'], tr/val_loss:  0.007092/  3.400229, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-368 lr=['0.0100000'], tr/val_loss:  0.005970/  3.372795, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-369 lr=['0.0100000'], tr/val_loss:  0.004232/  3.458780, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-370 lr=['0.0100000'], tr/val_loss:  0.005696/  3.412635, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-371 lr=['0.0100000'], tr/val_loss:  0.004371/  3.463987, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-372 lr=['0.0100000'], tr/val_loss:  0.003005/  3.422417, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-373 lr=['0.0100000'], tr/val_loss:  0.003499/  3.421912, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-374 lr=['0.0100000'], tr/val_loss:  0.003073/  3.442049, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-375 lr=['0.0100000'], tr/val_loss:  0.002260/  3.453299, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-376 lr=['0.0100000'], tr/val_loss:  0.002800/  3.457023, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-377 lr=['0.0100000'], tr/val_loss:  0.002606/  3.405041, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-378 lr=['0.0100000'], tr/val_loss:  0.003173/  3.446099, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-379 lr=['0.0100000'], tr/val_loss:  0.002501/  3.394392, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-380 lr=['0.0100000'], tr/val_loss:  0.002477/  3.431700, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-381 lr=['0.0100000'], tr/val_loss:  0.002600/  3.452624, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-382 lr=['0.0100000'], tr/val_loss:  0.002259/  3.459822, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-383 lr=['0.0100000'], tr/val_loss:  0.002393/  3.397882, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-384 lr=['0.0100000'], tr/val_loss:  0.002250/  3.422572, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-385 lr=['0.0100000'], tr/val_loss:  0.001691/  3.476292, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-386 lr=['0.0100000'], tr/val_loss:  0.001539/  3.466883, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-387 lr=['0.0100000'], tr/val_loss:  0.001158/  3.442679, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-388 lr=['0.0100000'], tr/val_loss:  0.001879/  3.435397, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-389 lr=['0.0100000'], tr/val_loss:  0.002090/  3.494458, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-390 lr=['0.0100000'], tr/val_loss:  0.002007/  3.421861, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-391 lr=['0.0100000'], tr/val_loss:  0.001698/  3.442747, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-392 lr=['0.0100000'], tr/val_loss:  0.001421/  3.455113, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-393 lr=['0.0100000'], tr/val_loss:  0.001556/  3.481037, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-394 lr=['0.0100000'], tr/val_loss:  0.001243/  3.472933, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-395 lr=['0.0100000'], tr/val_loss:  0.001662/  3.405332, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-396 lr=['0.0100000'], tr/val_loss:  0.001217/  3.434043, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-397 lr=['0.0100000'], tr/val_loss:  0.001852/  3.442165, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-398 lr=['0.0100000'], tr/val_loss:  0.001646/  3.460166, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-399 lr=['0.0100000'], tr/val_loss:  0.002048/  3.435686, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-400 lr=['0.0100000'], tr/val_loss:  0.001878/  3.463128, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-401 lr=['0.0100000'], tr/val_loss:  0.001725/  3.485532, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-402 lr=['0.0100000'], tr/val_loss:  0.001171/  3.464171, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-403 lr=['0.0100000'], tr/val_loss:  0.001087/  3.471738, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-404 lr=['0.0100000'], tr/val_loss:  0.001280/  3.487375, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-405 lr=['0.0100000'], tr/val_loss:  0.001378/  3.477315, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-406 lr=['0.0100000'], tr/val_loss:  0.001127/  3.509240, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-407 lr=['0.0100000'], tr/val_loss:  0.001608/  3.519624, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-408 lr=['0.0100000'], tr/val_loss:  0.001160/  3.472304, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-409 lr=['0.0100000'], tr/val_loss:  0.001013/  3.506531, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-410 lr=['0.0100000'], tr/val_loss:  0.001018/  3.491830, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-411 lr=['0.0100000'], tr/val_loss:  0.000938/  3.469069, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-412 lr=['0.0100000'], tr/val_loss:  0.000940/  3.514219, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-413 lr=['0.0100000'], tr/val_loss:  0.000953/  3.507403, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-414 lr=['0.0100000'], tr/val_loss:  0.001047/  3.513762, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-415 lr=['0.0100000'], tr/val_loss:  0.001095/  3.494420, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-416 lr=['0.0100000'], tr/val_loss:  0.001176/  3.491185, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-417 lr=['0.0100000'], tr/val_loss:  0.001239/  3.483062, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-418 lr=['0.0100000'], tr/val_loss:  0.001534/  3.462253, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-419 lr=['0.0100000'], tr/val_loss:  0.001690/  3.458695, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-420 lr=['0.0100000'], tr/val_loss:  0.001544/  3.471939, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-421 lr=['0.0100000'], tr/val_loss:  0.001826/  3.551398, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-422 lr=['0.0100000'], tr/val_loss:  0.002226/  3.562735, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-423 lr=['0.0100000'], tr/val_loss:  0.002516/  3.438578, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-424 lr=['0.0100000'], tr/val_loss:  0.001822/  3.450363, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-425 lr=['0.0100000'], tr/val_loss:  0.001351/  3.440221, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-426 lr=['0.0100000'], tr/val_loss:  0.001366/  3.438069, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-427 lr=['0.0100000'], tr/val_loss:  0.001249/  3.447102, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-428 lr=['0.0100000'], tr/val_loss:  0.001093/  3.443117, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-429 lr=['0.0100000'], tr/val_loss:  0.001040/  3.469174, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-430 lr=['0.0100000'], tr/val_loss:  0.000983/  3.477502, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-431 lr=['0.0100000'], tr/val_loss:  0.000980/  3.481024, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-432 lr=['0.0100000'], tr/val_loss:  0.000957/  3.493745, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-433 lr=['0.0100000'], tr/val_loss:  0.000987/  3.504951, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-434 lr=['0.0100000'], tr/val_loss:  0.000980/  3.495337, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-435 lr=['0.0100000'], tr/val_loss:  0.000975/  3.493407, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-436 lr=['0.0100000'], tr/val_loss:  0.000937/  3.497015, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-437 lr=['0.0100000'], tr/val_loss:  0.000953/  3.489636, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-438 lr=['0.0100000'], tr/val_loss:  0.001069/  3.503228, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-439 lr=['0.0100000'], tr/val_loss:  0.000876/  3.503799, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-440 lr=['0.0100000'], tr/val_loss:  0.000975/  3.535295, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-441 lr=['0.0100000'], tr/val_loss:  0.000932/  3.522126, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-442 lr=['0.0100000'], tr/val_loss:  0.000982/  3.510440, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-443 lr=['0.0100000'], tr/val_loss:  0.000958/  3.539950, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-444 lr=['0.0100000'], tr/val_loss:  0.000890/  3.523039, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-445 lr=['0.0100000'], tr/val_loss:  0.001139/  3.534950, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-446 lr=['0.0100000'], tr/val_loss:  0.001036/  3.485519, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-447 lr=['0.0100000'], tr/val_loss:  0.000934/  3.514565, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-448 lr=['0.0100000'], tr/val_loss:  0.000987/  3.511993, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-449 lr=['0.0100000'], tr/val_loss:  0.001145/  3.493180, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-450 lr=['0.0100000'], tr/val_loss:  0.001047/  3.505646, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-451 lr=['0.0100000'], tr/val_loss:  0.001015/  3.477104, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-452 lr=['0.0100000'], tr/val_loss:  0.001030/  3.477421, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-453 lr=['0.0100000'], tr/val_loss:  0.000984/  3.469490, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-454 lr=['0.0100000'], tr/val_loss:  0.000942/  3.481925, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-455 lr=['0.0100000'], tr/val_loss:  0.000954/  3.486839, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-456 lr=['0.0100000'], tr/val_loss:  0.000998/  3.507755, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-457 lr=['0.0100000'], tr/val_loss:  0.001157/  3.489007, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-458 lr=['0.0100000'], tr/val_loss:  0.000908/  3.492303, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-459 lr=['0.0100000'], tr/val_loss:  0.000956/  3.494231, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-460 lr=['0.0100000'], tr/val_loss:  0.000955/  3.480793, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-461 lr=['0.0100000'], tr/val_loss:  0.000958/  3.498872, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-462 lr=['0.0100000'], tr/val_loss:  0.000946/  3.504075, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-463 lr=['0.0100000'], tr/val_loss:  0.000922/  3.500485, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-464 lr=['0.0100000'], tr/val_loss:  0.000882/  3.505315, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-465 lr=['0.0100000'], tr/val_loss:  0.000919/  3.507442, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.0 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"4\",\n",
    "                single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = decay, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 6, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 9, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 5, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "#         \"epoch_num\": {\"values\": [100]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [5]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [100_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [True]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [1]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"0\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  unique_name_hyper,\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#                         ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = '6pj3lh8j'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
