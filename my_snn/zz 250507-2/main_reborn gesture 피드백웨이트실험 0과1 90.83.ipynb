{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12086/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250507_192240-5fb3pu78</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5fb3pu78' target=\"_blank\">playful-eon-8266</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5fb3pu78' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5fb3pu78</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.01, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.0, v_threshold=0.5, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True, time_different_weight=False)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.039829/  3.041635, val:  53.75%, val_best:  53.75%, tr:  49.10%, tr_best:  49.10%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.842573/  2.948221, val:  51.25%, val_best:  53.75%, tr:  59.58%, tr_best:  59.58%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.628476/  2.117518, val:  56.67%, val_best:  56.67%, tr:  62.38%, tr_best:  62.38%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.697026/  2.024488, val:  64.58%, val_best:  64.58%, tr:  65.53%, tr_best:  65.53%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.400951/  2.148838, val:  58.75%, val_best:  64.58%, tr:  71.39%, tr_best:  71.39%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.288106/  1.573769, val:  65.83%, val_best:  65.83%, tr:  75.36%, tr_best:  75.36%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.059096/  1.658229, val:  67.50%, val_best:  67.50%, tr:  80.50%, tr_best:  80.50%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.982477/  2.013182, val:  60.83%, val_best:  67.50%, tr:  82.71%, tr_best:  82.71%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.935469/  1.844689, val:  76.25%, val_best:  76.25%, tr:  84.90%, tr_best:  84.90%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.754148/  1.414357, val:  85.42%, val_best:  85.42%, tr:  89.43%, tr_best:  89.43%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.711406/  1.326063, val:  80.42%, val_best:  85.42%, tr:  90.22%, tr_best:  90.22%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.674071/  1.693910, val:  70.00%, val_best:  85.42%, tr:  91.10%, tr_best:  91.10%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.588173/  1.335393, val:  85.42%, val_best:  85.42%, tr:  92.92%, tr_best:  92.92%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.543834/  1.268622, val:  85.00%, val_best:  85.42%, tr:  94.57%, tr_best:  94.57%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.547647/  1.568630, val:  77.50%, val_best:  85.42%, tr:  94.34%, tr_best:  94.57%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.547270/  1.520149, val:  77.08%, val_best:  85.42%, tr:  94.52%, tr_best:  94.57%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.458175/  1.290790, val:  86.25%, val_best:  86.25%, tr:  96.39%, tr_best:  96.39%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.418968/  1.349840, val:  85.42%, val_best:  86.25%, tr:  96.37%, tr_best:  96.39%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.403092/  1.532872, val:  77.92%, val_best:  86.25%, tr:  97.18%, tr_best:  97.18%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.351488/  1.365163, val:  85.42%, val_best:  86.25%, tr:  97.81%, tr_best:  97.81%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.341337/  1.786310, val:  80.00%, val_best:  86.25%, tr:  98.15%, tr_best:  98.15%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.298357/  1.473075, val:  85.42%, val_best:  86.25%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.272214/  1.368559, val:  88.33%, val_best:  88.33%, tr:  98.85%, tr_best:  98.85%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.267008/  1.381819, val:  88.75%, val_best:  88.75%, tr:  98.65%, tr_best:  98.85%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.276933/  1.387771, val:  89.58%, val_best:  89.58%, tr:  99.10%, tr_best:  99.10%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.267582/  1.518770, val:  87.08%, val_best:  89.58%, tr:  98.69%, tr_best:  99.10%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.226865/  1.382844, val:  90.42%, val_best:  90.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.204241/  1.632264, val:  87.08%, val_best:  90.42%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.174782/  1.550818, val:  86.67%, val_best:  90.42%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.179134/  1.601087, val:  87.08%, val_best:  90.42%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.174805/  1.601119, val:  88.33%, val_best:  90.42%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.154480/  1.581020, val:  87.50%, val_best:  90.42%, tr:  99.64%, tr_best:  99.73%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.155218/  1.730951, val:  83.75%, val_best:  90.42%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.138752/  1.731621, val:  86.25%, val_best:  90.42%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.139139/  1.639650, val:  86.67%, val_best:  90.42%, tr:  99.77%, tr_best:  99.82%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.120136/  1.579046, val:  89.17%, val_best:  90.42%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.125036/  1.652079, val:  90.00%, val_best:  90.42%, tr:  99.84%, tr_best:  99.91%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.105945/  1.687785, val:  90.83%, val_best:  90.83%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.095347/  1.714501, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.096455/  1.715066, val:  90.00%, val_best:  90.83%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.085650/  1.793183, val:  87.08%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.085573/  1.808516, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.075359/  1.872074, val:  87.92%, val_best:  90.83%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.075795/  1.816095, val:  88.33%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.066587/  1.832888, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.064176/  1.902565, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.058118/  1.945216, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.050771/  1.983003, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.050169/  1.968869, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.044933/  2.000760, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.046066/  1.987545, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.045869/  1.955345, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.042964/  2.027449, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.043541/  2.082717, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.041033/  2.091281, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.038557/  2.095590, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.033630/  2.118154, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.031293/  2.187805, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.028796/  2.158591, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.027873/  2.137725, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.024281/  2.296130, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.025907/  2.172318, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.024666/  2.235646, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.027275/  2.193694, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.028790/  2.228178, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.024921/  2.217252, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.022145/  2.276613, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.022182/  2.252505, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.021130/  2.297382, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.021430/  2.228156, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.019594/  2.310212, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.020569/  2.371282, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.019084/  2.361878, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.017442/  2.378502, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.017625/  2.426651, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.017070/  2.391964, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.015004/  2.372949, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.014425/  2.364213, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.013686/  2.413266, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.012879/  2.419436, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.016155/  2.435537, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.012817/  2.462840, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.015275/  2.533953, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.013927/  2.478146, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.014032/  2.443727, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.013847/  2.471190, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.013407/  2.570059, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.012141/  2.505622, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.017020/  2.518775, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.012650/  2.555794, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.011132/  2.618554, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.010712/  2.573786, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.010456/  2.538433, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.011011/  2.580673, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.010641/  2.548201, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.008821/  2.633374, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009221/  2.643441, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.008361/  2.580789, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.008094/  2.557523, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.009263/  2.573775, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.009443/  2.588910, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.010646/  2.621235, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.010120/  2.655904, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.009869/  2.690103, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.009132/  2.618920, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.009456/  2.619696, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.008448/  2.650231, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.008177/  2.717537, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.008621/  2.613902, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.008547/  2.703001, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.007879/  2.785235, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.009938/  2.626229, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.009159/  2.728677, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.009458/  2.673734, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.008709/  2.776666, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.009077/  2.742633, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.007793/  2.804433, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.007525/  2.769935, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.006889/  2.796240, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.007479/  2.795572, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.006637/  2.742870, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.005176/  2.782120, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.006082/  2.767181, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.004980/  2.834850, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.004534/  2.788407, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.006185/  2.852485, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.005635/  2.809936, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.005702/  2.770190, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.006259/  2.804235, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.005369/  2.807810, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.005567/  2.830730, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.005663/  2.812236, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.005058/  2.776963, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.005997/  2.808582, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.004861/  2.818862, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.005457/  2.838569, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.005699/  2.872507, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.004341/  2.872973, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.004147/  2.816456, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.007164/  2.838883, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.007931/  2.890664, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.007671/  2.858804, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.005896/  2.882212, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.005903/  2.957011, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.005134/  2.901587, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.005659/  2.864081, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.004255/  2.856583, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.004680/  2.887359, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.004597/  2.850310, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.004284/  2.885724, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.004439/  2.890160, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.004902/  2.931043, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.005458/  2.986426, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.006847/  2.948757, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.005964/  2.981207, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.002804/  2.985531, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.004776/  2.980411, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.004858/  2.969737, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.004011/  2.922050, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.004085/  2.999066, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.004288/  2.890854, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.004417/  2.908630, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.004373/  2.975003, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.002947/  2.926176, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.004214/  2.898963, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.004357/  2.955931, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.003936/  2.990810, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.003757/  2.974347, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.004843/  2.978524, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.003622/  2.933310, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.003458/  3.020815, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.002261/  2.993937, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.002657/  3.008466, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.002686/  3.009719, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.002759/  3.015115, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.003189/  2.966894, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.002510/  2.998744, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.002608/  3.016987, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.002912/  3.030864, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.003382/  2.977227, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.002742/  2.970861, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.003024/  2.999401, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.003008/  3.006574, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.003551/  3.044665, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.003319/  3.101305, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.003075/  3.042043, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.003072/  3.060747, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.002654/  3.093529, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.002747/  3.076076, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.003723/  3.109933, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.003731/  3.073740, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.003164/  3.059366, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.003768/  3.140226, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.003486/  3.054621, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.004737/  3.103946, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.003994/  3.072862, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.002657/  3.099705, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.004706/  3.007148, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.004997/  3.082709, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.003405/  3.019230, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-200 lr=['0.0100000'], tr/val_loss:  0.003720/  3.086752, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-201 lr=['0.0100000'], tr/val_loss:  0.003094/  3.089205, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-202 lr=['0.0100000'], tr/val_loss:  0.002761/  3.110226, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-203 lr=['0.0100000'], tr/val_loss:  0.002856/  3.074771, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-204 lr=['0.0100000'], tr/val_loss:  0.003100/  3.086883, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-205 lr=['0.0100000'], tr/val_loss:  0.003222/  3.076933, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-206 lr=['0.0100000'], tr/val_loss:  0.002082/  3.103385, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-207 lr=['0.0100000'], tr/val_loss:  0.001927/  3.084853, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-208 lr=['0.0100000'], tr/val_loss:  0.001533/  3.129177, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-209 lr=['0.0100000'], tr/val_loss:  0.002157/  3.101057, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-210 lr=['0.0100000'], tr/val_loss:  0.002334/  3.142279, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-211 lr=['0.0100000'], tr/val_loss:  0.002345/  3.057914, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-212 lr=['0.0100000'], tr/val_loss:  0.001909/  3.046494, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-213 lr=['0.0100000'], tr/val_loss:  0.002592/  3.112243, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-214 lr=['0.0100000'], tr/val_loss:  0.002294/  3.124746, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-215 lr=['0.0100000'], tr/val_loss:  0.006278/  3.088584, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-216 lr=['0.0100000'], tr/val_loss:  0.006349/  3.174495, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-217 lr=['0.0100000'], tr/val_loss:  0.005974/  3.164961, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-218 lr=['0.0100000'], tr/val_loss:  0.004741/  3.123910, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-219 lr=['0.0100000'], tr/val_loss:  0.004546/  3.136183, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-220 lr=['0.0100000'], tr/val_loss:  0.004070/  3.139825, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-221 lr=['0.0100000'], tr/val_loss:  0.003299/  3.193968, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-222 lr=['0.0100000'], tr/val_loss:  0.003006/  3.172299, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-223 lr=['0.0100000'], tr/val_loss:  0.002769/  3.144536, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-224 lr=['0.0100000'], tr/val_loss:  0.002287/  3.129120, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-225 lr=['0.0100000'], tr/val_loss:  0.002946/  3.152667, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-226 lr=['0.0100000'], tr/val_loss:  0.002708/  3.174230, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-227 lr=['0.0100000'], tr/val_loss:  0.003526/  3.104985, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-228 lr=['0.0100000'], tr/val_loss:  0.003034/  3.140702, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-229 lr=['0.0100000'], tr/val_loss:  0.003887/  3.146259, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-230 lr=['0.0100000'], tr/val_loss:  0.002849/  3.172276, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-231 lr=['0.0100000'], tr/val_loss:  0.003030/  3.155902, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-232 lr=['0.0100000'], tr/val_loss:  0.003436/  3.241179, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-233 lr=['0.0100000'], tr/val_loss:  0.004314/  3.151888, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-234 lr=['0.0100000'], tr/val_loss:  0.004170/  3.167185, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-235 lr=['0.0100000'], tr/val_loss:  0.003702/  3.216429, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-236 lr=['0.0100000'], tr/val_loss:  0.004461/  3.207056, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-237 lr=['0.0100000'], tr/val_loss:  0.004232/  3.192817, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-238 lr=['0.0100000'], tr/val_loss:  0.003984/  3.215557, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-239 lr=['0.0100000'], tr/val_loss:  0.004601/  3.140616, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-240 lr=['0.0100000'], tr/val_loss:  0.003053/  3.186125, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-241 lr=['0.0100000'], tr/val_loss:  0.003195/  3.189179, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-242 lr=['0.0100000'], tr/val_loss:  0.002873/  3.173772, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-243 lr=['0.0100000'], tr/val_loss:  0.002460/  3.162585, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-244 lr=['0.0100000'], tr/val_loss:  0.002553/  3.169333, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-245 lr=['0.0100000'], tr/val_loss:  0.002525/  3.192707, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-246 lr=['0.0100000'], tr/val_loss:  0.002853/  3.160064, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-247 lr=['0.0100000'], tr/val_loss:  0.002284/  3.171350, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-248 lr=['0.0100000'], tr/val_loss:  0.002587/  3.194345, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-249 lr=['0.0100000'], tr/val_loss:  0.003275/  3.163967, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-250 lr=['0.0100000'], tr/val_loss:  0.002633/  3.227034, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-251 lr=['0.0100000'], tr/val_loss:  0.002758/  3.219661, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-252 lr=['0.0100000'], tr/val_loss:  0.002208/  3.206737, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-253 lr=['0.0100000'], tr/val_loss:  0.001998/  3.242326, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-254 lr=['0.0100000'], tr/val_loss:  0.002780/  3.187532, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-255 lr=['0.0100000'], tr/val_loss:  0.002336/  3.182191, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-256 lr=['0.0100000'], tr/val_loss:  0.002550/  3.208607, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-257 lr=['0.0100000'], tr/val_loss:  0.002284/  3.169337, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-258 lr=['0.0100000'], tr/val_loss:  0.002433/  3.216266, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-259 lr=['0.0100000'], tr/val_loss:  0.001988/  3.206658, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-260 lr=['0.0100000'], tr/val_loss:  0.003052/  3.239661, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-261 lr=['0.0100000'], tr/val_loss:  0.002873/  3.234874, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-262 lr=['0.0100000'], tr/val_loss:  0.004428/  3.228125, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-263 lr=['0.0100000'], tr/val_loss:  0.003465/  3.259761, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-264 lr=['0.0100000'], tr/val_loss:  0.003451/  3.316170, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-265 lr=['0.0100000'], tr/val_loss:  0.002744/  3.306919, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-266 lr=['0.0100000'], tr/val_loss:  0.002699/  3.240743, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-267 lr=['0.0100000'], tr/val_loss:  0.002534/  3.275922, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-268 lr=['0.0100000'], tr/val_loss:  0.004476/  3.298777, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-269 lr=['0.0100000'], tr/val_loss:  0.002790/  3.331492, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-270 lr=['0.0100000'], tr/val_loss:  0.002338/  3.247240, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-271 lr=['0.0100000'], tr/val_loss:  0.002155/  3.205369, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-272 lr=['0.0100000'], tr/val_loss:  0.002724/  3.311784, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-273 lr=['0.0100000'], tr/val_loss:  0.002624/  3.344099, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-274 lr=['0.0100000'], tr/val_loss:  0.001977/  3.323856, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-275 lr=['0.0100000'], tr/val_loss:  0.002499/  3.269390, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-276 lr=['0.0100000'], tr/val_loss:  0.002048/  3.291817, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-277 lr=['0.0100000'], tr/val_loss:  0.001730/  3.298354, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-278 lr=['0.0100000'], tr/val_loss:  0.001590/  3.263193, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-279 lr=['0.0100000'], tr/val_loss:  0.001804/  3.299299, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-280 lr=['0.0100000'], tr/val_loss:  0.001840/  3.245020, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-281 lr=['0.0100000'], tr/val_loss:  0.001639/  3.266657, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-282 lr=['0.0100000'], tr/val_loss:  0.001532/  3.244918, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-283 lr=['0.0100000'], tr/val_loss:  0.001345/  3.237534, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-284 lr=['0.0100000'], tr/val_loss:  0.001413/  3.249379, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-285 lr=['0.0100000'], tr/val_loss:  0.001580/  3.246720, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-286 lr=['0.0100000'], tr/val_loss:  0.001352/  3.279187, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-287 lr=['0.0100000'], tr/val_loss:  0.001420/  3.316456, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-288 lr=['0.0100000'], tr/val_loss:  0.001592/  3.272629, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-289 lr=['0.0100000'], tr/val_loss:  0.001638/  3.250924, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-290 lr=['0.0100000'], tr/val_loss:  0.001698/  3.318384, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-291 lr=['0.0100000'], tr/val_loss:  0.001711/  3.279174, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-292 lr=['0.0100000'], tr/val_loss:  0.001467/  3.293129, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-293 lr=['0.0100000'], tr/val_loss:  0.001565/  3.308935, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-294 lr=['0.0100000'], tr/val_loss:  0.002293/  3.301953, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-295 lr=['0.0100000'], tr/val_loss:  0.002333/  3.322485, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-296 lr=['0.0100000'], tr/val_loss:  0.001715/  3.257227, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-297 lr=['0.0100000'], tr/val_loss:  0.001744/  3.267306, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-298 lr=['0.0100000'], tr/val_loss:  0.001951/  3.346991, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-299 lr=['0.0100000'], tr/val_loss:  0.001993/  3.337577, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-300 lr=['0.0100000'], tr/val_loss:  0.001996/  3.320410, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-301 lr=['0.0100000'], tr/val_loss:  0.002484/  3.303204, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-302 lr=['0.0100000'], tr/val_loss:  0.002333/  3.355499, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-303 lr=['0.0100000'], tr/val_loss:  0.002387/  3.246495, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-304 lr=['0.0100000'], tr/val_loss:  0.002305/  3.321295, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-305 lr=['0.0100000'], tr/val_loss:  0.003257/  3.237903, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-306 lr=['0.0100000'], tr/val_loss:  0.002122/  3.306497, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-307 lr=['0.0100000'], tr/val_loss:  0.002801/  3.193786, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-308 lr=['0.0100000'], tr/val_loss:  0.002232/  3.284705, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-309 lr=['0.0100000'], tr/val_loss:  0.001732/  3.306142, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-310 lr=['0.0100000'], tr/val_loss:  0.001693/  3.341947, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-311 lr=['0.0100000'], tr/val_loss:  0.001955/  3.287502, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-312 lr=['0.0100000'], tr/val_loss:  0.002005/  3.345212, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-313 lr=['0.0100000'], tr/val_loss:  0.001860/  3.322198, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-314 lr=['0.0100000'], tr/val_loss:  0.001643/  3.294923, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-315 lr=['0.0100000'], tr/val_loss:  0.001442/  3.340255, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-316 lr=['0.0100000'], tr/val_loss:  0.001884/  3.386324, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-317 lr=['0.0100000'], tr/val_loss:  0.002036/  3.378483, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-318 lr=['0.0100000'], tr/val_loss:  0.001895/  3.424201, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-319 lr=['0.0100000'], tr/val_loss:  0.002127/  3.376228, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-320 lr=['0.0100000'], tr/val_loss:  0.002991/  3.271121, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-321 lr=['0.0100000'], tr/val_loss:  0.002426/  3.290180, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-322 lr=['0.0100000'], tr/val_loss:  0.002697/  3.380586, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-323 lr=['0.0100000'], tr/val_loss:  0.002812/  3.295018, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-324 lr=['0.0100000'], tr/val_loss:  0.002699/  3.336912, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-325 lr=['0.0100000'], tr/val_loss:  0.002069/  3.325762, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-326 lr=['0.0100000'], tr/val_loss:  0.001837/  3.341132, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-327 lr=['0.0100000'], tr/val_loss:  0.002338/  3.361917, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-328 lr=['0.0100000'], tr/val_loss:  0.002290/  3.391362, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-329 lr=['0.0100000'], tr/val_loss:  0.003940/  3.322716, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-330 lr=['0.0100000'], tr/val_loss:  0.003171/  3.340804, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-331 lr=['0.0100000'], tr/val_loss:  0.002520/  3.328384, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-332 lr=['0.0100000'], tr/val_loss:  0.002630/  3.312866, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-333 lr=['0.0100000'], tr/val_loss:  0.001844/  3.328162, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-334 lr=['0.0100000'], tr/val_loss:  0.001793/  3.346393, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-335 lr=['0.0100000'], tr/val_loss:  0.002407/  3.314034, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-336 lr=['0.0100000'], tr/val_loss:  0.001852/  3.351252, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-337 lr=['0.0100000'], tr/val_loss:  0.001446/  3.380672, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-338 lr=['0.0100000'], tr/val_loss:  0.001422/  3.375099, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-339 lr=['0.0100000'], tr/val_loss:  0.001399/  3.370143, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-340 lr=['0.0100000'], tr/val_loss:  0.001343/  3.428654, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-341 lr=['0.0100000'], tr/val_loss:  0.001246/  3.376480, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-342 lr=['0.0100000'], tr/val_loss:  0.001065/  3.385759, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-343 lr=['0.0100000'], tr/val_loss:  0.001064/  3.353051, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-344 lr=['0.0100000'], tr/val_loss:  0.001292/  3.377473, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-345 lr=['0.0100000'], tr/val_loss:  0.001539/  3.442464, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-346 lr=['0.0100000'], tr/val_loss:  0.001355/  3.401642, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-347 lr=['0.0100000'], tr/val_loss:  0.001740/  3.365288, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-348 lr=['0.0100000'], tr/val_loss:  0.001315/  3.377371, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-349 lr=['0.0100000'], tr/val_loss:  0.001403/  3.381247, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-350 lr=['0.0100000'], tr/val_loss:  0.001454/  3.392277, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-351 lr=['0.0100000'], tr/val_loss:  0.001419/  3.419457, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-352 lr=['0.0100000'], tr/val_loss:  0.001691/  3.334550, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-353 lr=['0.0100000'], tr/val_loss:  0.001396/  3.417281, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-354 lr=['0.0100000'], tr/val_loss:  0.001139/  3.408638, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-355 lr=['0.0100000'], tr/val_loss:  0.001563/  3.369860, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-356 lr=['0.0100000'], tr/val_loss:  0.001533/  3.394713, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-357 lr=['0.0100000'], tr/val_loss:  0.001378/  3.412621, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-358 lr=['0.0100000'], tr/val_loss:  0.001207/  3.460326, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-359 lr=['0.0100000'], tr/val_loss:  0.001217/  3.409971, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-360 lr=['0.0100000'], tr/val_loss:  0.001355/  3.460539, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-361 lr=['0.0100000'], tr/val_loss:  0.001386/  3.417049, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-362 lr=['0.0100000'], tr/val_loss:  0.001184/  3.429550, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-363 lr=['0.0100000'], tr/val_loss:  0.001113/  3.419056, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-364 lr=['0.0100000'], tr/val_loss:  0.001088/  3.446023, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-365 lr=['0.0100000'], tr/val_loss:  0.001125/  3.414900, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-366 lr=['0.0100000'], tr/val_loss:  0.001611/  3.460197, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-367 lr=['0.0100000'], tr/val_loss:  0.001150/  3.472701, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-368 lr=['0.0100000'], tr/val_loss:  0.001004/  3.445625, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-369 lr=['0.0100000'], tr/val_loss:  0.000982/  3.488676, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-370 lr=['0.0100000'], tr/val_loss:  0.000943/  3.476399, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-371 lr=['0.0100000'], tr/val_loss:  0.000924/  3.450181, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-372 lr=['0.0100000'], tr/val_loss:  0.000958/  3.427591, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-373 lr=['0.0100000'], tr/val_loss:  0.000959/  3.430904, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-374 lr=['0.0100000'], tr/val_loss:  0.000919/  3.427165, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-375 lr=['0.0100000'], tr/val_loss:  0.000944/  3.422638, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-376 lr=['0.0100000'], tr/val_loss:  0.000999/  3.422774, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-377 lr=['0.0100000'], tr/val_loss:  0.000928/  3.446768, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-378 lr=['0.0100000'], tr/val_loss:  0.001088/  3.413052, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-379 lr=['0.0100000'], tr/val_loss:  0.001050/  3.413257, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-380 lr=['0.0100000'], tr/val_loss:  0.000930/  3.421155, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.0 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"2\",\n",
    "                single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = decay, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 6, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 9, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 5, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "#         \"epoch_num\": {\"values\": [100]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [5]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [100_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [True]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [1]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"0\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  unique_name_hyper,\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#                         ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = '6pj3lh8j'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
