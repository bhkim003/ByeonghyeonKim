{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2649/2361114005.py:45: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from spikingjelly.datasets import split_to_train_test_set\n",
    "from spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import torchneuromorphic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:79: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:81: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:99: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:101: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:79: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:81: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:99: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:101: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # dvs 데이터 시각화 코드\n",
    " ##############################################################################################\n",
    "            # mapping = {\n",
    "            #     0: 'Hand Clapping',\n",
    "            #     1: 'Right Hand Wave',\n",
    "            #     2: 'Left Hand Wave',\n",
    "            #     3: 'Right Arm CW',\n",
    "            #     4: 'Right Arm CCW',\n",
    "            #     5: 'Left Arm CW',\n",
    "            #     6: 'Left Arm CCW',\n",
    "            #     7: 'Arm Roll',\n",
    "            #     8: 'Air Drums',\n",
    "            #     9: 'Air Guitar',\n",
    "            #     10: 'Other'\n",
    "            # }\n",
    "def dvs_visualization(inputs, labels, TIME, BATCH):\n",
    "            \n",
    "    what_input = random.randint(0, BATCH - 1)\n",
    "    inputs_for_view = inputs.permute(1, 0, 2, 3, 4)\n",
    "    for i in range(TIME):\n",
    "        # 예시 데이터 생성\n",
    "        data1 = inputs_for_view[what_input][i][0].numpy()  # torch tensor를 numpy 배열로 변환\n",
    "        data2 = inputs_for_view[what_input][i][1].numpy()  # torch tensor를 numpy 배열로 변환\n",
    "\n",
    "        # 데이터 플로팅\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # 1행 2열의 subplot 생성\n",
    "\n",
    "        # 첫 번째 subplot에 데이터1 플로팅\n",
    "        im1 = axs[0].imshow(data1, cmap='viridis', interpolation='nearest')\n",
    "        axs[0].set_title(f'Input 1\\nLabel: {labels[what_input]}')  # 라벨값 맵핑하여 제목에 추가\n",
    "        axs[0].set_xlabel('X axis')\n",
    "        axs[0].set_ylabel('Y axis')\n",
    "        axs[0].grid(False)\n",
    "        fig.colorbar(im1, ax=axs[0])  # Color bar 추가\n",
    "\n",
    "        # 두 번째 subplot에 데이터2 플로팅\n",
    "        im2 = axs[1].imshow(data2, cmap='viridis', interpolation='nearest')\n",
    "        axs[1].set_title(f'Input 2\\nLabel: {labels[what_input]}')  # 라벨값 맵핑하여 제목에 추가\n",
    "        axs[1].set_xlabel('X axis')\n",
    "        axs[1].set_ylabel('Y axis')\n",
    "        axs[1].grid(False)\n",
    "        fig.colorbar(im2, ax=axs[1])  # Color bar 추가\n",
    "\n",
    "        plt.tight_layout()  # subplot 간 간격 조정\n",
    "        plt.show()\n",
    "    sys.exit(\"종료\")\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    weight_count_print = False, # True # False\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "\n",
    "                    nda_net = False,\n",
    "                    \n",
    "                    domain_il_epoch = 0, # over 0, then domain il mode on\n",
    "                  ):\n",
    "    # 함수 내 모든 로컬 변수 저장\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "\n",
    "\n",
    "    \n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME)\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "\n",
    "    ## parameter number calculator ##########################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    classifier_making = False\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (classifier_making == False):\n",
    "                if (type(kernel) == list):\n",
    "                    for residual_kernel in kernel:\n",
    "                        if (residual_kernel >= 10000 and residual_kernel < 20000): # separable\n",
    "                            residual_kernel -= 10000\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            params_num += (1**2 * past_kernel + bias_param) * residual_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        elif (residual_kernel >= 20000 and residual_kernel < 30000): # depthwise\n",
    "                            residual_kernel -= 20000\n",
    "                            # 'past_kernel' should be same with 'kernel'\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        else:\n",
    "                            params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                            past_kernel = residual_kernel\n",
    "                elif (kernel == 'P' or kernel == 'M'):\n",
    "                    img_size = img_size // 2\n",
    "                elif (kernel == 'D'):\n",
    "                    img_size = 1\n",
    "                elif (kernel == 'L'):\n",
    "                    classifier_making = True\n",
    "                    past_kernel = past_kernel * (img_size**2)\n",
    "                else:\n",
    "                    if (kernel >= 10000 and kernel < 20000): # separable\n",
    "                        kernel -= 10000\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        params_num += (1**2 * past_kernel + bias_param) * kernel\n",
    "                        past_kernel = kernel  \n",
    "                    elif (kernel >= 20000 and kernel < 30000): # depthwise\n",
    "                        kernel -= 20000\n",
    "                        # 'past_kernel' should be same with 'kernel'\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        past_kernel = kernel  \n",
    "                    else:\n",
    "                        params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                        past_kernel = kernel    \n",
    "            else: # classifier making\n",
    "                params_num += (past_kernel + bias_param) * kernel\n",
    "                past_kernel = kernel\n",
    "        \n",
    "        \n",
    "        if classifier_making == False:\n",
    "            past_kernel = past_kernel*img_size*img_size\n",
    "\n",
    "        params_num += (past_kernel + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            # elif (in_channel == 'M'): #it's a holy FC layer!\n",
    "            #     img_size = img_size // 2\n",
    "            else:\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ## parameter number calculator ##########################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        \n",
    "        if (nda_net == True):\n",
    "            net = VGG(cfg = cfg, num_classes=10, batch_norm = tdBN_on, in_c = synapse_conv_in_channels, \n",
    "                      lif_layer_v_threshold=lif_layer_v_threshold, lif_layer_v_decay=lif_layer_v_decay, lif_layer_sg_width=lif_layer_sg_width)\n",
    "            net.T = TIME\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "\n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)\n",
    "    \n",
    "    ## param num and memory estimation except BN at MY calculation ##########################################\n",
    "    real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    if (weight_count_print == True):\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f'Layer: {name} | Number of parameters: {param.numel()}')\n",
    "    # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "    # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "    print('='*50)\n",
    "    print(f\"My Num of PARAMS: {params_num:,}, system's param_num : {real_param_num:,}\")\n",
    "    memory = params_num / 8 / 1024 / 1024 # MB\n",
    "    precision = 32\n",
    "    memory = memory * precision \n",
    "    print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "    print('='*50)\n",
    "    ############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## loss ##########################################\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    ## loss ##########################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ##########################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=100)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ##########################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    val_acc = 0\n",
    "    val_acc_now = 0\n",
    "    elapsed_time_val = 0\n",
    "    iter_acc_array = np.array([])\n",
    "    tr_acc_array = np.array([])\n",
    "    val_acc_now_array = np.array([])\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    for epoch in range(epoch_num):\n",
    "        print('EPOCH', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # if (domain_il_epoch>0 and which_data == 'PMNIST'):\n",
    "        #     k = epoch // domain_il_epoch\n",
    "        #     xtrain=data[k]['train']['x']\n",
    "        #     ytrain=data[k]['train']['y']\n",
    "        #     xtest =data[k]['test']['x']\n",
    "        #     ytest =data[k]['test']['y']\n",
    "\n",
    "        \n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if (ddp_on == True):\n",
    "            if torch.distributed.get_rank() == 0:   \n",
    "                iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        else:\n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        \n",
    "        validation_interval2 = validation_interval\n",
    "        if (validation_interval > len(iterator)):\n",
    "            validation_interval2 = len(iterator)\n",
    "            \n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train()\n",
    "\n",
    "            ## data loading #################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                # print('x_len',x_len)\n",
    "                # mask = padded_sequence_mask(x_len)\n",
    "                max_time_step = x_len.max()\n",
    "                min_time_step = x_len.min()\n",
    "                B, T, *spatial_dims = inputs.shape\n",
    "\n",
    "                # inputs.size(1)를 TIME으로 맞추기\n",
    "                if T > TIME:\n",
    "                    inputs = inputs[:, :TIME, :, :, :]\n",
    "                else:\n",
    "                    inputs = torch.cat([inputs, torch.zeros(B, TIME - T, *spatial_dims)], dim=1)\n",
    "                \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ################################################# \n",
    "                \n",
    "\n",
    "                \n",
    "#  # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "#  ##############################################################################################\n",
    "#             dvs_visualization(inputs, labels, TIME, BATCH\n",
    "# ######################################################################################################\n",
    "\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "            # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "            # print(inputs.size())\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            batch = BATCH \n",
    "            if labels.size(0) != BATCH: \n",
    "                batch = labels.size(0)\n",
    "            \n",
    "\n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = (predicted[0:batch] == labels).sum().item()\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc = correct / total\n",
    "            if i % verbose_interval == verbose_interval-1:\n",
    "                print(f'{epoch}-{i} training acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "            iter_acc_string = f'{epoch}-{i}/{len(train_loader)} iter_acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs[0:batch,:], labels)\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            ### gradinet verbose ##########################################\n",
    "            if (gradient_verbose == True):\n",
    "                if (i % verbose_interval == verbose_interval-1):\n",
    "                    print('\\n\\nepoch', epoch, 'iter', i)\n",
    "                    for name, param in net.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            print('\\n\\n\\n\\n' , name, param.grad)\n",
    "            ################################################################\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "            if (i % verbose_interval == verbose_interval-1):\n",
    "                print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\\n\")\n",
    "\n",
    "            ##### validation ##############################################\n",
    "            if i % validation_interval2 == validation_interval2-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    for data in test_loader:\n",
    "                        ## data loading #################################\n",
    "                        if len(data) == 2:\n",
    "                            inputs, labels = data\n",
    "                            # 처리 로직 작성\n",
    "                        elif len(data) == 3:\n",
    "                            inputs, labels, x_len = data\n",
    "                            # print('x_len',x_len)\n",
    "                            # mask = padded_sequence_mask(x_len)\n",
    "                            max_time_step = x_len.max()\n",
    "                            min_time_step = x_len.min()\n",
    "                            B, T, *spatial_dims = inputs.shape\n",
    "\n",
    "                            # inputs.size(1)를 TIME으로 맞추기\n",
    "                            if T > TIME:\n",
    "                                inputs = inputs[:, :TIME, :, :, :]\n",
    "                            else:\n",
    "                                inputs = torch.cat([inputs, torch.zeros(B, TIME - T, *spatial_dims)], dim=1)\n",
    "\n",
    "                        \n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ################################################# \n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        batch = BATCH \n",
    "                        if labels.size(0) != BATCH: \n",
    "                            batch = labels.size(0)\n",
    "                        correct += (predicted[0:batch] == labels).sum().item()\n",
    "                        val_loss = criterion(outputs[0:batch,:], labels)\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc < val_acc_now:\n",
    "                    val_acc = val_acc_now\n",
    "                    torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                    torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                    torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                    torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "            ################################################################\n",
    "            iterator.set_description(f\"iter_acc: {iter_acc_string}, iter_loss: {loss}, val_acc: {100 * val_acc_now:.2f}%\")  \n",
    "\n",
    "            iter_acc_array = np.append(iter_acc_array, iter_acc)\n",
    "            tr_acc_array = np.append(tr_acc_array, tr_acc)\n",
    "            val_acc_now_array = np.append(val_acc_now_array, val_acc_now)\n",
    "            base_name = f'{current_time}'\n",
    "            iter_acc_file_name = f'result_save/{base_name}_iter_acc_array.npy'\n",
    "            tr_acc_file_name = f'result_save/{base_name}_tr_acc_array.npy'\n",
    "            val_acc_file_name = f'result_save/{base_name}_val_acc_now_array.npy'\n",
    "            hyperparameters_file_name = f'result_save/{base_name}_hyperparameters.json'\n",
    "\n",
    "            hyperparameters['current epoch'] = epoch\n",
    "\n",
    "            # 덮어쓰기 하기 싫으면 주석 풀어서 사용 (시간마다 새로 쓰기)\n",
    "            # np.save(iter_acc_file_name, iter_acc_array)\n",
    "            # np.save(val_acc_file_name, val_acc_now_array)\n",
    "            # with open(hyperparameters_file_name, 'w') as f:\n",
    "            #     json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "            np.save('result_save/iter_acc_array.npy', iter_acc_array)\n",
    "            np.save('result_save/tr_acc_array.npy', tr_acc_array)\n",
    "            np.save('result_save/val_acc_now_array.npy', val_acc_now_array)\n",
    "            with open('result_save/hyperparameters.json', 'w') as f:\n",
    "                json.dump(hyperparameters, f, indent=4)\n",
    "            \n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        \n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "        \n",
    "        print(f\"epoch_time: {epoch_time} seconds\")\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "DataParallel(\n",
      "  (module): MY_SNN_FC(\n",
      "    (layers): Sequential(\n",
      "      (0): SYNAPSE_FC()\n",
      "      (1): tdBatchNorm_FC(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LIF_layer()\n",
      "      (3): SYNAPSE_FC()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 2,097,866, system's param_num : 2,097,994\n",
      "Memory: 8.00MiB at 32-bit\n",
      "==================================================\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-215/216 iter_acc: 33.33%, lr=['1.953125e-05'], iter_loss: 2.1305997371673584, val_acc: 51.52%: 100%|██████████| 216/216 [00:12<00:00, 17.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_time: 12.449183940887451 seconds\n",
      "\n",
      "\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "iter_acc: 1-152/216 iter_acc: 20.00%, lr=['1.9530045239078716e-05'], iter_loss: 1.8245553970336914, val_acc: 51.52%:  71%|███████   | 153/216 [00:06<00:02, 24.64it/s] "
     ]
    }
   ],
   "source": [
    "### my_snn control board ########################\n",
    "decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "my_snn_system(  devices = \"1\",\n",
    "                my_seed = 42,\n",
    "                TIME = 8 , # dvscifar 10 # ottt 6 or 10 # nda 10\n",
    "                BATCH = 5, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 128, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "                #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = True, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 1.0, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                cfg = [64],\n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [], \n",
    "                \n",
    "                net_print = True, # True # False\n",
    "                weight_count_print = False, # True # False\n",
    "                pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                learning_rate = 0.001, # default 0.001  # ottt 0.1  # nda 0.001\n",
    "                epoch_num = 200,\n",
    "                verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "                validation_interval = 999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "                tdBN_on = True,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'rough_rectangle', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "                gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                optimizer_what = 'Adam', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False,   # True # False\n",
    "\n",
    "                nda_net = False,   # True # False\n",
    "\n",
    "                domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                ) \n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling이 낫다.\n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def pad_array_to_match_length(array1, array2):\n",
    "    if len(array1) > len(array2):\n",
    "        padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "        return array1, padded_array2\n",
    "    elif len(array2) > len(array1):\n",
    "        padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "        return padded_array1, array2\n",
    "    else:\n",
    "        return array1, array2\n",
    "def load_hyperparameters(filename='hyperparameters.json'):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_time = '20240628_110116'\n",
    "base_name = f'{current_time}'\n",
    "iter_acc_file_name = f'result_save/{base_name}_iter_acc_array.npy'\n",
    "val_acc_file_name = f'result_save/{base_name}_val_acc_now_array.npy'\n",
    "hyperparameters_file_name = f'result_save/{base_name}_hyperparameters.json'\n",
    "\n",
    "### if you want to just see most recent train and val acc###########################\n",
    "iter_acc_file_name = 'result_save/iter_acc_array.npy'\n",
    "tr_acc_file_name = 'result_save/tr_acc_array.npy'\n",
    "val_acc_file_name = 'result_save/val_acc_now_array.npy'\n",
    "hyperparameters_file_name = 'result_save/hyperparameters.json'\n",
    "\n",
    "loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "which_data = hyperparameters['which_data']\n",
    "BPTT_on = hyperparameters['BPTT_on']\n",
    "current_epoch = hyperparameters['current epoch']\n",
    "surrogate = hyperparameters['surrogate']\n",
    "cfg = hyperparameters['cfg']\n",
    "tdBN_on = hyperparameters['tdBN_on']\n",
    "BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# 텍스트 추가\n",
    "plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "plt.title(title)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0)  # x축을 0부터 시작\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch.spikevision import spikedata\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# root, train=True, transform=None, target_transform=None, download_and_create=True, num_steps=1000, ds=1, dt=1000)\n",
    "train_ds = spikedata.SHD(\"/data2/Heidelberg\", train=True)\n",
    "test_ds = spikedata.SHD(\"/data2/Heidelberg\", train=False)\n",
    "\n",
    "# create dataloaders\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=64) # 8156x2x1000x700\n",
    "test_dl = DataLoader(test_ds, shuffle=False, batch_size=64) # 2264x2x1000x700\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "\n",
    "# choose a random sample\n",
    "n = 6295\n",
    "\n",
    "# initialize figure and axes\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# use spikeplot to generate a raster\n",
    "splt.raster(train_dl.dataset[n][0], ax, s=1.5, c=\"black\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
