{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    my_seed = 42,\n",
    "                    TIME = 8,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "\n",
    "                    nda_net = False,\n",
    "                  ):\n",
    "\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "\n",
    "\n",
    "    \n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on)\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "\n",
    "    ## parameter number calculator ##########################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    classifier_making = False\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (classifier_making == False):\n",
    "                if (type(kernel) == list):\n",
    "                    for residual_kernel in kernel:\n",
    "                        params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                        past_kernel = residual_kernel\n",
    "                elif (kernel == 'P' or kernel == 'M'):\n",
    "                    img_size = img_size // 2\n",
    "                elif (kernel == 'L'):\n",
    "                    classifier_making = True\n",
    "                    past_kernel = past_kernel * (img_size**2)\n",
    "                else:\n",
    "                    params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                    past_kernel = kernel    \n",
    "            else: # classifier making\n",
    "                params_num += (past_kernel + bias_param) * kernel\n",
    "                past_kernel = kernel\n",
    "        \n",
    "        \n",
    "        if classifier_making == False:\n",
    "            past_kernel = past_kernel*img_size*img_size\n",
    "\n",
    "        params_num += (past_kernel + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            # elif (in_channel == 'M'): #it's a holy FC layer!\n",
    "            #     img_size = img_size // 2\n",
    "            else:\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ## parameter number calculator ##########################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        \n",
    "        if (nda_net == True):\n",
    "            net = VGG(cfg = cfg, num_classes=10, batch_norm = tdBN_on, in_c = synapse_conv_in_channels, \n",
    "                      lif_layer_v_threshold=lif_layer_v_threshold, lif_layer_v_decay=lif_layer_v_decay, lif_layer_sg_width=lif_layer_sg_width)\n",
    "            net.T = TIME\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "\n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)\n",
    "    \n",
    "    ## param num and memory estimation except BN at MY calculation ##########################################\n",
    "    real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "    # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "    print('='*50)\n",
    "    print(f\"My Num of PARAMS: {params_num:,}, system's param_num : {real_param_num:,}\")\n",
    "    memory = params_num / 8 / 1024 / 1024 # MB\n",
    "    precision = 32\n",
    "    memory = memory * precision \n",
    "    print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "    print('='*50)\n",
    "    ##########################################################################\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=100)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "\n",
    "    val_acc = 0\n",
    "    val_acc_now = 0\n",
    "    elapsed_time_val = 0\n",
    "    for epoch in range(epoch_num):\n",
    "        print('EPOCH', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if (ddp_on == True):\n",
    "            if torch.distributed.get_rank() == 0:   \n",
    "                iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        else:\n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "\n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train()\n",
    "\n",
    "            ## data loading #################################\n",
    "            inputs, labels = data\n",
    "\n",
    "            \n",
    "            if (which_data == 'DVS-CIFAR10'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ################################################# \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "            # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        \n",
    "            outputs = net(inputs)\n",
    "\n",
    "            batch = BATCH \n",
    "            if labels.size(0) != BATCH: \n",
    "                batch = labels.size(0)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            ####### training accruacy print ###############################\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted[0:batch] == labels).sum().item()\n",
    "            if i % verbose_interval == verbose_interval-1:\n",
    "                print(f'{epoch}-{i} training acc: {100 * correct / total:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "            training_acc_string = f'{epoch}-{i}/{len(train_loader)} tr_acc: {100 * correct / total:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            \n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs[0:batch,:], labels)\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            ### gradinet verbose ##########################################\n",
    "            if (gradient_verbose == True):\n",
    "                if (i % verbose_interval == verbose_interval-1):\n",
    "                    print('\\n\\nepoch', epoch, 'iter', i)\n",
    "                    for name, param in net.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            print('\\n\\n\\n\\n' , name, param.grad)\n",
    "            ################################################################\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "            if (i % verbose_interval == verbose_interval-1):\n",
    "                print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\\n\")\n",
    "\n",
    "            ##### validation ##############################################\n",
    "            if i % validation_interval == validation_interval-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    for data in test_loader:\n",
    "                        ## data loading #################################\n",
    "                        inputs, labels = data\n",
    "\n",
    "                        \n",
    "                        if (which_data == 'DVS-CIFAR10'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ################################################# \n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        batch = BATCH \n",
    "                        if labels.size(0) != BATCH: \n",
    "                            batch = labels.size(0)\n",
    "                        correct += (predicted[0:batch] == labels).sum().item()\n",
    "                        val_loss = criterion(outputs[0:batch,:], labels)\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc < val_acc_now:\n",
    "                    val_acc = val_acc_now\n",
    "                    torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                    torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                    torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                    torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "            ################################################################\n",
    "            iterator.set_description(f\"train: {training_acc_string}, tr_loss: {loss}, val_acc: {100 * val_acc_now:.2f}%\")     \n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "        \n",
    "        print(f\"epoch_time: {epoch_time} seconds\")\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==================================================\n",
      "My Num of PARAMS: 9,302,410, system's param_num : 9,357,450\n",
      "Memory: 35.49MiB at 32-bit\n",
      "==================================================\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-48/391 tr_acc: 71.09%, lr=['1e-05'], tr_loss: 0.9262285828590393, val_acc: 0.00%:  13%|█▎        | 49/391 [01:46<11:11,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-49 training acc: 71.09%, lr=['1e-05'], val_acc: 0.00%\n",
      "iter_one_train_time: 1.9332501888275146 seconds, last one_val_time: 0 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-98/391 tr_acc: 77.34%, lr=['1e-05'], tr_loss: 0.6875458359718323, val_acc: 71.53%:  25%|██▌       | 99/391 [03:46<09:40,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-99 training acc: 71.09%, lr=['1e-05'], val_acc: 71.53%\n",
      "iter_one_train_time: 1.9791066646575928 seconds, last one_val_time: 20.28554654121399 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-148/391 tr_acc: 71.88%, lr=['1e-05'], tr_loss: 0.7376105189323425, val_acc: 72.29%:  38%|███▊      | 149/391 [05:46<07:59,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-149 training acc: 74.22%, lr=['1e-05'], val_acc: 72.29%\n",
      "iter_one_train_time: 1.957078456878662 seconds, last one_val_time: 20.793298482894897 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-198/391 tr_acc: 75.00%, lr=['1e-05'], tr_loss: 0.7603843212127686, val_acc: 71.76%:  51%|█████     | 199/391 [07:47<06:19,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-199 training acc: 74.22%, lr=['1e-05'], val_acc: 71.76%\n",
      "iter_one_train_time: 2.1313276290893555 seconds, last one_val_time: 20.9477801322937 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-248/391 tr_acc: 72.66%, lr=['1e-05'], tr_loss: 0.7450255155563354, val_acc: 72.18%:  64%|██████▎   | 249/391 [09:45<04:40,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-249 training acc: 70.31%, lr=['1e-05'], val_acc: 72.18%\n",
      "iter_one_train_time: 1.9848682880401611 seconds, last one_val_time: 19.678719520568848 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-298/391 tr_acc: 76.56%, lr=['1e-05'], tr_loss: 0.6481726765632629, val_acc: 71.51%:  76%|███████▋  | 299/391 [11:45<03:02,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-299 training acc: 78.12%, lr=['1e-05'], val_acc: 71.51%\n",
      "iter_one_train_time: 2.0938613414764404 seconds, last one_val_time: 20.545332193374634 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-348/391 tr_acc: 69.53%, lr=['1e-05'], tr_loss: 0.8496976494789124, val_acc: 72.04%:  89%|████████▉ | 349/391 [13:44<01:22,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-349 training acc: 76.56%, lr=['1e-05'], val_acc: 72.04%\n",
      "iter_one_train_time: 2.0022199153900146 seconds, last one_val_time: 19.865952968597412 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-390/391 tr_acc: 76.25%, lr=['1e-05'], tr_loss: 0.6589891314506531, val_acc: 71.99%: 100%|██████████| 391/391 [15:28<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_time: 928.2498273849487 seconds\n",
      "\n",
      "\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "train: 1-48/391 tr_acc: 75.00%, lr=['1e-05'], tr_loss: 0.7466093897819519, val_acc: 71.99%:  13%|█▎        | 49/391 [01:37<11:30,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-49 training acc: 74.22%, lr=['1e-05'], val_acc: 71.99%\n",
      "iter_one_train_time: 1.9708943367004395 seconds, last one_val_time: 20.73065972328186 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 1-98/391 tr_acc: 73.44%, lr=['1e-05'], tr_loss: 0.7200464010238647, val_acc: 72.15%:  25%|██▌       | 99/391 [03:36<08:57,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-99 training acc: 75.00%, lr=['1e-05'], val_acc: 72.15%\n",
      "iter_one_train_time: 2.064702272415161 seconds, last one_val_time: 20.413851737976074 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 1-148/391 tr_acc: 71.09%, lr=['1e-05'], tr_loss: 0.9740023612976074, val_acc: 71.86%:  38%|███▊      | 149/391 [05:35<08:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-149 training acc: 71.09%, lr=['1e-05'], val_acc: 71.86%\n",
      "iter_one_train_time: 1.9602372646331787 seconds, last one_val_time: 19.706886768341064 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 1-198/391 tr_acc: 78.91%, lr=['1e-05'], tr_loss: 0.6933053135871887, val_acc: 71.67%:  51%|█████     | 199/391 [07:34<06:19,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-199 training acc: 77.34%, lr=['1e-05'], val_acc: 71.67%\n",
      "iter_one_train_time: 1.9951119422912598 seconds, last one_val_time: 20.57272171974182 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 1-248/391 tr_acc: 73.44%, lr=['1e-05'], tr_loss: 0.7800415754318237, val_acc: 71.75%:  64%|██████▎   | 249/391 [09:34<04:40,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-249 training acc: 75.00%, lr=['1e-05'], val_acc: 71.75%\n",
      "iter_one_train_time: 2.0802550315856934 seconds, last one_val_time: 20.576533317565918 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 1-298/391 tr_acc: 72.66%, lr=['1e-05'], tr_loss: 0.7782691121101379, val_acc: 71.86%:  76%|███████▋  | 299/391 [11:32<03:02,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-299 training acc: 71.09%, lr=['1e-05'], val_acc: 71.86%\n",
      "iter_one_train_time: 1.9525201320648193 seconds, last one_val_time: 19.740546703338623 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 1-348/391 tr_acc: 69.53%, lr=['1e-05'], tr_loss: 0.8394355773925781, val_acc: 72.02%:  89%|████████▉ | 349/391 [13:32<01:22,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-349 training acc: 76.56%, lr=['1e-05'], val_acc: 72.02%\n",
      "iter_one_train_time: 1.9534072875976562 seconds, last one_val_time: 20.468198776245117 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 1-390/391 tr_acc: 75.00%, lr=['1e-05'], tr_loss: 0.7161511778831482, val_acc: 71.73%: 100%|██████████| 391/391 [15:15<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_time: 915.4095420837402 seconds\n",
      "\n",
      "\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "train: 2-48/391 tr_acc: 75.78%, lr=['1e-05'], tr_loss: 0.7930952310562134, val_acc: 71.73%:  13%|█▎        | 49/391 [01:37<11:20,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-49 training acc: 78.12%, lr=['1e-05'], val_acc: 71.73%\n",
      "iter_one_train_time: 1.967228889465332 seconds, last one_val_time: 20.563231706619263 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 2-98/391 tr_acc: 75.00%, lr=['1e-05'], tr_loss: 0.7610330581665039, val_acc: 71.75%:  25%|██▌       | 99/391 [03:36<09:43,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-99 training acc: 80.47%, lr=['1e-05'], val_acc: 71.75%\n",
      "iter_one_train_time: 2.0106725692749023 seconds, last one_val_time: 20.63790988922119 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 2-148/391 tr_acc: 70.31%, lr=['1e-05'], tr_loss: 0.7788671255111694, val_acc: 72.05%:  38%|███▊      | 149/391 [05:35<07:59,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-149 training acc: 67.97%, lr=['1e-05'], val_acc: 72.05%\n",
      "iter_one_train_time: 1.9604003429412842 seconds, last one_val_time: 19.939858436584473 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 2-198/391 tr_acc: 73.44%, lr=['1e-05'], tr_loss: 0.8102719783782959, val_acc: 72.39%:  51%|█████     | 199/391 [07:34<06:19,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-199 training acc: 72.66%, lr=['1e-05'], val_acc: 72.39%\n",
      "iter_one_train_time: 1.994488000869751 seconds, last one_val_time: 19.77862286567688 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 2-198/391 tr_acc: 73.44%, lr=['1e-05'], tr_loss: 0.8102719783782959, val_acc: 72.39%:  51%|█████     | 199/391 [07:56<07:39,  2.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;66;03m# 0.875 0.25 0.125 0.75 0.5\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nda 0.25 # ottt 0.5\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmy_snn_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0,1,2,3,4,5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmy_seed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mTIME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dvscifar 10 # ottt 6 or 10\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBATCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dvscifar 48 # MNIST 28 # CIFAR10 32\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# DVS-CIFAR10 할거면 time 10으로 해라\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwhich_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCIFAR10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mCLASS_NUM\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# YOU NEED TO CHANGE THIS\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrate_coding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# nda 0.5  # ottt 1.0\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_reset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#현재 안씀. 걍 빼기 해버림\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_sg_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# surrogate sigmoid 쓸 때는 의미없음\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_kernel_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_padding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_trace_const1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_trace_const2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lif_layer_v_decay\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# synapse_fc_out_features = CLASS_NUM,\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_fc_trace_const1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_fc_trace_const2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lif_layer_v_decay\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpre_trained\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconvTrue_fcFalse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# 'P' for average pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64],\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# ottt \u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnet_print\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpre_trained_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnet_save/save_now_net.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ottt 0.1  # nda 0.001\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepoch_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#숫자 크게 하면 꺼짐\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#숫자 크게 하면 꺼짐\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtdBN_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBN_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                \u001b[49m\u001b[43msurrogate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'rectangle' 'sigmoid' 'rough_rectangle'\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgradient_verbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False  # weight gradient 각 layer마다 띄워줌\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBPTT_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False # True이면 BPTT, False이면 OTTT\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[43m                \u001b[49m\u001b[43mscheduler_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReduceLROnPlateau\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                \u001b[49m\u001b[43mddp_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnda_net\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 303\u001b[0m, in \u001b[0;36mmy_snn_system\u001b[0;34m(devices, my_seed, TIME, BATCH, IMAGE_SIZE, which_data, CLASS_NUM, data_path, rate_coding, lif_layer_v_init, lif_layer_v_decay, lif_layer_v_threshold, lif_layer_v_reset, lif_layer_sg_width, synapse_conv_kernel_size, synapse_conv_stride, synapse_conv_padding, synapse_conv_trace_const1, synapse_conv_trace_const2, synapse_fc_trace_const1, synapse_fc_trace_const2, pre_trained, convTrue_fcFalse, cfg, net_print, pre_trained_path, learning_rate, epoch_num, verbose_interval, validation_interval, tdBN_on, BN_on, surrogate, gradient_verbose, BPTT_on, scheduler_name, ddp_on, nda_net)\u001b[0m\n\u001b[1;32m    301\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    302\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 303\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    305\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:78\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m---> 78\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### my_snn control board ########################\n",
    "decay = 0.5 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "my_snn_system(  devices = \"0,1,2,3,4,5\",\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10\n",
    "                BATCH = 128, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32\n",
    "\n",
    "                # DVS-CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'CIFAR10',# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\n",
    "                CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.0, # nda 0.5  # ottt 1.0\n",
    "                lif_layer_v_reset = 0.0, #현재 안씀. 걍 빼기 해버림\n",
    "                lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = True, # True # False\n",
    "                convTrue_fcFalse = True, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # cfg = [64],\n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "\n",
    "\n",
    "                net_print = False, # True # False\n",
    "                pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                learning_rate = 0.00001,  # ottt 0.1  # nda 0.001\n",
    "                epoch_num = 200,\n",
    "                verbose_interval = 50, #숫자 크게 하면 꺼짐\n",
    "                validation_interval = 50, #숫자 크게 하면 꺼짐\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = True,  # True # False\n",
    "                \n",
    "                surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "                gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT\n",
    "\n",
    "                scheduler_name = 'ReduceLROnPlateau', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False,   # True # False\n",
    "\n",
    "                nda_net = False,   # True # False\n",
    "                ) \n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling이 낫다.\n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_snn_system(  devices = \"2,3,4,5\",\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 8,\n",
    "#                 BATCH = 128, # batch norm 할거면 2이상으로 해야함\n",
    "#                 IMAGE_SIZE = 32,\n",
    "#                 which_data = 'CIFAR10',# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\n",
    "#                 CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 1.2,\n",
    "#                 lif_layer_v_reset = 0.0, #현재 안씀. 걍 빼기 해버림\n",
    "#                 lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1,\n",
    "#                 synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1,\n",
    "#                 synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = True, # True # False\n",
    "#                 # cfg = [64],\n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "#                 pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "#                 learning_rate = 0.00001,\n",
    "#                 epoch_num = 200,\n",
    "#                 verbose_interval = 10000, #숫자 크게 하면 꺼짐\n",
    "#                 validation_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = True,  # True # False\n",
    "                \n",
    "#                 surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False\n",
    "\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,\n",
    "#                 )\n",
    "\n",
    "\n",
    "# Files already downloaded and verified\n",
    "# Files already downloaded and verified\n",
    "# ==================================================\n",
    "# Num of PARAMS: 9,302,410\n",
    "# Memory: 35.49MiB at 32-bit\n",
    "# ==================================================\n",
    "# EPOCH 0\n",
    "# train: 0-390/391 tr_acc: 27.50%, lr=['1e-05'], val_acc: 25.65%: 100%|██████████| 391/391 [08:39<00:00,  1.33s/it]\n",
    "# epoch_time: 519.6511018276215 seconds\n",
    "\n",
    "\n",
    "# EPOCH 1\n",
    "\n",
    "# train: 1-390/391 tr_acc: 32.50%, lr=['1e-05'], val_acc: 32.00%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.4912984371185 seconds\n",
    "\n",
    "\n",
    "# EPOCH 2\n",
    "\n",
    "# train: 2-390/391 tr_acc: 33.75%, lr=['1e-05'], val_acc: 35.14%: 100%|██████████| 391/391 [08:33<00:00,  1.31s/it]\n",
    "# epoch_time: 513.511162519455 seconds\n",
    "\n",
    "\n",
    "# EPOCH 3\n",
    "\n",
    "# train: 3-390/391 tr_acc: 28.75%, lr=['1e-05'], val_acc: 37.17%: 100%|██████████| 391/391 [08:30<00:00,  1.30s/it]\n",
    "# epoch_time: 510.2808663845062 seconds\n",
    "\n",
    "\n",
    "# EPOCH 4\n",
    "\n",
    "# train: 4-390/391 tr_acc: 58.75%, lr=['1e-05'], val_acc: 40.37%: 100%|██████████| 391/391 [08:21<00:00,  1.28s/it]\n",
    "# epoch_time: 501.48126435279846 seconds\n",
    "\n",
    "\n",
    "# EPOCH 5\n",
    "\n",
    "# train: 5-390/391 tr_acc: 37.50%, lr=['1e-05'], val_acc: 42.59%: 100%|██████████| 391/391 [08:18<00:00,  1.28s/it]\n",
    "# epoch_time: 498.8543312549591 seconds\n",
    "\n",
    "\n",
    "# EPOCH 6\n",
    "\n",
    "# train: 6-390/391 tr_acc: 35.00%, lr=['1e-05'], val_acc: 43.45%: 100%|██████████| 391/391 [08:15<00:00,  1.27s/it]\n",
    "# epoch_time: 495.6830530166626 seconds\n",
    "\n",
    "\n",
    "# EPOCH 7\n",
    "\n",
    "# train: 7-390/391 tr_acc: 48.75%, lr=['1e-05'], val_acc: 44.97%: 100%|██████████| 391/391 [08:16<00:00,  1.27s/it]\n",
    "# epoch_time: 496.7004475593567 seconds\n",
    "\n",
    "\n",
    "# EPOCH 8\n",
    "\n",
    "# train: 8-390/391 tr_acc: 47.50%, lr=['1e-05'], val_acc: 46.19%: 100%|██████████| 391/391 [08:16<00:00,  1.27s/it]\n",
    "# epoch_time: 496.552987575531 seconds\n",
    "\n",
    "\n",
    "# EPOCH 9\n",
    "\n",
    "# train: 9-390/391 tr_acc: 46.25%, lr=['1e-05'], val_acc: 46.52%: 100%|██████████| 391/391 [08:37<00:00,  1.32s/it]\n",
    "# epoch_time: 517.5774216651917 seconds\n",
    "\n",
    "\n",
    "# EPOCH 10\n",
    "\n",
    "# train: 10-390/391 tr_acc: 48.75%, lr=['1e-05'], val_acc: 48.67%: 100%|██████████| 391/391 [08:57<00:00,  1.37s/it]\n",
    "# epoch_time: 537.5675616264343 seconds\n",
    "\n",
    "\n",
    "# EPOCH 11\n",
    "\n",
    "# train: 11-390/391 tr_acc: 48.75%, lr=['1e-05'], val_acc: 48.40%: 100%|██████████| 391/391 [08:53<00:00,  1.37s/it]\n",
    "# epoch_time: 533.9970047473907 seconds\n",
    "\n",
    "\n",
    "# EPOCH 12\n",
    "\n",
    "# train: 12-390/391 tr_acc: 43.75%, lr=['1e-05'], val_acc: 50.58%: 100%|██████████| 391/391 [08:57<00:00,  1.38s/it]\n",
    "# epoch_time: 537.9225826263428 seconds\n",
    "\n",
    "\n",
    "# EPOCH 13\n",
    "\n",
    "# train: 13-390/391 tr_acc: 42.50%, lr=['1e-05'], val_acc: 50.98%: 100%|██████████| 391/391 [08:58<00:00,  1.38s/it]\n",
    "# epoch_time: 538.5700080394745 seconds\n",
    "\n",
    "\n",
    "# EPOCH 14\n",
    "\n",
    "# train: 14-390/391 tr_acc: 48.75%, lr=['1e-05'], val_acc: 52.68%: 100%|██████████| 391/391 [08:59<00:00,  1.38s/it]\n",
    "# epoch_time: 539.7910151481628 seconds\n",
    "\n",
    "\n",
    "# EPOCH 15\n",
    "\n",
    "# train: 15-390/391 tr_acc: 55.00%, lr=['1e-05'], val_acc: 54.05%: 100%|██████████| 391/391 [08:52<00:00,  1.36s/it]\n",
    "# epoch_time: 532.8848164081573 seconds\n",
    "\n",
    "\n",
    "# EPOCH 16\n",
    "\n",
    "# train: 16-390/391 tr_acc: 52.50%, lr=['1e-05'], val_acc: 53.80%: 100%|██████████| 391/391 [08:58<00:00,  1.38s/it]\n",
    "# epoch_time: 538.2881193161011 seconds\n",
    "\n",
    "\n",
    "# EPOCH 17\n",
    "\n",
    "# train: 17-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 54.73%: 100%|██████████| 391/391 [09:00<00:00,  1.38s/it]\n",
    "# epoch_time: 540.2721989154816 seconds\n",
    "\n",
    "\n",
    "# EPOCH 18\n",
    "\n",
    "# train: 18-390/391 tr_acc: 60.00%, lr=['1e-05'], val_acc: 55.54%: 100%|██████████| 391/391 [09:00<00:00,  1.38s/it]\n",
    "# epoch_time: 540.7067131996155 seconds\n",
    "\n",
    "\n",
    "# EPOCH 19\n",
    "\n",
    "# train: 19-390/391 tr_acc: 61.25%, lr=['1e-05'], val_acc: 56.20%: 100%|██████████| 391/391 [08:53<00:00,  1.36s/it]\n",
    "# epoch_time: 533.882045507431 seconds\n",
    "\n",
    "\n",
    "# EPOCH 20\n",
    "\n",
    "# train: 20-390/391 tr_acc: 61.25%, lr=['1e-05'], val_acc: 57.64%: 100%|██████████| 391/391 [08:55<00:00,  1.37s/it]\n",
    "# epoch_time: 535.4623730182648 seconds\n",
    "\n",
    "\n",
    "# EPOCH 21\n",
    "\n",
    "# train: 21-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 57.87%: 100%|██████████| 391/391 [08:57<00:00,  1.37s/it]\n",
    "# epoch_time: 537.7847683429718 seconds\n",
    "\n",
    "\n",
    "# EPOCH 22\n",
    "\n",
    "# train: 22-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 58.46%: 100%|██████████| 391/391 [08:55<00:00,  1.37s/it]\n",
    "# epoch_time: 535.7902438640594 seconds\n",
    "\n",
    "\n",
    "# EPOCH 23\n",
    "\n",
    "# train: 23-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 57.95%: 100%|██████████| 391/391 [08:57<00:00,  1.37s/it]\n",
    "# epoch_time: 537.5092451572418 seconds\n",
    "\n",
    "\n",
    "# EPOCH 24\n",
    "\n",
    "# train: 24-390/391 tr_acc: 55.00%, lr=['1e-05'], val_acc: 57.98%: 100%|██████████| 391/391 [08:44<00:00,  1.34s/it]\n",
    "# epoch_time: 524.3707118034363 seconds\n",
    "\n",
    "\n",
    "# EPOCH 25\n",
    "\n",
    "# train: 25-390/391 tr_acc: 50.00%, lr=['1e-05'], val_acc: 60.33%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.3296625614166 seconds\n",
    "\n",
    "\n",
    "# EPOCH 26\n",
    "\n",
    "# train: 26-390/391 tr_acc: 52.50%, lr=['1e-05'], val_acc: 60.02%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.8723931312561 seconds\n",
    "\n",
    "\n",
    "# EPOCH 27\n",
    "\n",
    "# train: 27-390/391 tr_acc: 60.00%, lr=['1e-05'], val_acc: 60.80%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.6468982696533 seconds\n",
    "\n",
    "\n",
    "# EPOCH 28\n",
    "\n",
    "# train: 28-390/391 tr_acc: 56.25%, lr=['1e-05'], val_acc: 60.99%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 524.2045466899872 seconds\n",
    "\n",
    "\n",
    "# EPOCH 29\n",
    "\n",
    "# train: 29-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 62.56%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 523.102609872818 seconds\n",
    "\n",
    "\n",
    "# EPOCH 30\n",
    "\n",
    "# train: 30-390/391 tr_acc: 65.00%, lr=['1e-05'], val_acc: 62.23%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.3475232124329 seconds\n",
    "\n",
    "\n",
    "# EPOCH 31\n",
    "\n",
    "# train: 31-390/391 tr_acc: 70.00%, lr=['1e-05'], val_acc: 61.05%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.611293554306 seconds\n",
    "\n",
    "\n",
    "# EPOCH 32\n",
    "\n",
    "# train: 32-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 63.73%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.550628900528 seconds\n",
    "\n",
    "\n",
    "# EPOCH 33\n",
    "\n",
    "# train: 33-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 63.01%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.7382657527924 seconds\n",
    "\n",
    "\n",
    "# EPOCH 34\n",
    "\n",
    "# train: 34-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 63.66%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 523.2350842952728 seconds\n",
    "\n",
    "\n",
    "# EPOCH 35\n",
    "\n",
    "# train: 35-390/391 tr_acc: 65.00%, lr=['1e-05'], val_acc: 64.20%: 100%|██████████| 391/391 [08:39<00:00,  1.33s/it]\n",
    "# epoch_time: 519.9772260189056 seconds\n",
    "\n",
    "\n",
    "# EPOCH 36\n",
    "\n",
    "# train: 36-390/391 tr_acc: 61.25%, lr=['1e-05'], val_acc: 64.67%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 522.0173165798187 seconds\n",
    "\n",
    "\n",
    "# EPOCH 37\n",
    "\n",
    "# train: 37-390/391 tr_acc: 62.50%, lr=['1e-05'], val_acc: 64.44%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 522.151460647583 seconds\n",
    "\n",
    "\n",
    "# EPOCH 38\n",
    "\n",
    "# train: 38-390/391 tr_acc: 62.50%, lr=['1e-05'], val_acc: 64.77%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.8787703514099 seconds\n",
    "\n",
    "\n",
    "# EPOCH 39\n",
    "\n",
    "# train: 39-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 65.22%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.234944820404 seconds\n",
    "\n",
    "\n",
    "# EPOCH 40\n",
    "\n",
    "# train: 40-390/391 tr_acc: 68.75%, lr=['1e-05'], val_acc: 64.49%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.9287745952606 seconds\n",
    "\n",
    "\n",
    "# EPOCH 41\n",
    "\n",
    "# train: 41-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 66.06%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.8189256191254 seconds\n",
    "\n",
    "\n",
    "# EPOCH 42\n",
    "\n",
    "# train: 42-390/391 tr_acc: 76.25%, lr=['1e-05'], val_acc: 65.47%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.2214741706848 seconds\n",
    "\n",
    "\n",
    "# EPOCH 43\n",
    "\n",
    "# train: 43-390/391 tr_acc: 78.75%, lr=['1e-05'], val_acc: 66.97%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.9151771068573 seconds\n",
    "\n",
    "\n",
    "# EPOCH 44\n",
    "\n",
    "# train: 44-390/391 tr_acc: 72.50%, lr=['1e-05'], val_acc: 66.76%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.2902402877808 seconds\n",
    "\n",
    "\n",
    "# EPOCH 45\n",
    "\n",
    "# train: 45-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 66.30%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.3684139251709 seconds\n",
    "\n",
    "\n",
    "# EPOCH 46\n",
    "\n",
    "# train: 46-390/391 tr_acc: 76.25%, lr=['1e-05'], val_acc: 66.27%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.4130618572235 seconds\n",
    "\n",
    "\n",
    "# EPOCH 47\n",
    "\n",
    "# train: 47-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 66.20%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.4402587413788 seconds\n",
    "\n",
    "\n",
    "# EPOCH 48\n",
    "\n",
    "# train: 48-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 67.45%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 521.1478617191315 seconds\n",
    "\n",
    "\n",
    "# EPOCH 49\n",
    "\n",
    "# train: 49-390/391 tr_acc: 77.50%, lr=['1e-05'], val_acc: 68.14%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.3577303886414 seconds\n",
    "\n",
    "\n",
    "# EPOCH 50\n",
    "\n",
    "# train: 50-390/391 tr_acc: 63.75%, lr=['1e-05'], val_acc: 67.95%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.6194558143616 seconds\n",
    "\n",
    "\n",
    "# EPOCH 51\n",
    "\n",
    "# train: 51-390/391 tr_acc: 75.00%, lr=['1e-05'], val_acc: 68.41%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.8790509700775 seconds\n",
    "\n",
    "\n",
    "# EPOCH 52\n",
    "\n",
    "# train: 52-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 67.84%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 521.1223595142365 seconds\n",
    "\n",
    "\n",
    "# EPOCH 53\n",
    "\n",
    "# train: 53-390/391 tr_acc: 68.75%, lr=['1e-05'], val_acc: 68.00%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.3871352672577 seconds\n",
    "\n",
    "\n",
    "# EPOCH 54\n",
    "\n",
    "# train: 54-390/391 tr_acc: 73.75%, lr=['1e-05'], val_acc: 68.75%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.407853603363 seconds\n",
    "\n",
    "\n",
    "# EPOCH 55\n",
    "\n",
    "# train: 55-390/391 tr_acc: 71.25%, lr=['1e-05'], val_acc: 69.21%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.9369058609009 seconds\n",
    "\n",
    "\n",
    "# EPOCH 56\n",
    "\n",
    "# train: 56-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 69.28%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.430141210556 seconds\n",
    "\n",
    "\n",
    "# EPOCH 57\n",
    "\n",
    "# train: 57-390/391 tr_acc: 65.00%, lr=['1e-05'], val_acc: 69.16%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.3304183483124 seconds\n",
    "\n",
    "\n",
    "# EPOCH 58\n",
    "\n",
    "# train: 58-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 68.30%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.6461317539215 seconds\n",
    "\n",
    "\n",
    "# EPOCH 59\n",
    "\n",
    "# train: 59-390/391 tr_acc: 70.00%, lr=['1e-05'], val_acc: 69.50%: 100%|██████████| 391/391 [08:39<00:00,  1.33s/it]\n",
    "# epoch_time: 520.0395126342773 seconds\n",
    "\n",
    "\n",
    "# EPOCH 60\n",
    "\n",
    "# train: 60-390/391 tr_acc: 68.75%, lr=['1e-05'], val_acc: 69.68%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.3599922657013 seconds\n",
    "\n",
    "\n",
    "# EPOCH 61\n",
    "\n",
    "# train: 61-390/391 tr_acc: 63.75%, lr=['1e-05'], val_acc: 70.64%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.446202993393 seconds\n",
    "\n",
    "\n",
    "# EPOCH 62\n",
    "\n",
    "# train: 62-390/391 tr_acc: 75.00%, lr=['1e-05'], val_acc: 70.57%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.744499206543 seconds\n",
    "\n",
    "\n",
    "# EPOCH 63\n",
    "\n",
    "# train: 63-390/391 tr_acc: 65.00%, lr=['1e-05'], val_acc: 70.47%: 100%|██████████| 391/391 [08:39<00:00,  1.33s/it]\n",
    "# epoch_time: 519.3143074512482 seconds\n",
    "\n",
    "\n",
    "# EPOCH 64\n",
    "\n",
    "# train: 64-390/391 tr_acc: 63.75%, lr=['1e-05'], val_acc: 70.40%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.6692788600922 seconds\n",
    "\n",
    "\n",
    "# EPOCH 65\n",
    "\n",
    "# train: 65-390/391 tr_acc: 80.00%, lr=['1e-05'], val_acc: 70.64%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.667236328125 seconds\n",
    "\n",
    "\n",
    "# EPOCH 66\n",
    "\n",
    "# train: 66-390/391 tr_acc: 72.50%, lr=['1e-05'], val_acc: 69.29%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 522.0747861862183 seconds\n",
    "\n",
    "\n",
    "# EPOCH 67\n",
    "\n",
    "# train: 67-390/391 tr_acc: 73.75%, lr=['1e-05'], val_acc: 70.66%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.6984555721283 seconds\n",
    "\n",
    "\n",
    "# EPOCH 68\n",
    "\n",
    "# train: 68-390/391 tr_acc: 71.25%, lr=['1e-05'], val_acc: 70.71%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 523.1850342750549 seconds\n",
    "\n",
    "\n",
    "# EPOCH 69\n",
    "\n",
    "# train: 69-390/391 tr_acc: 86.25%, lr=['1e-05'], val_acc: 70.95%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 522.0473079681396 seconds\n",
    "\n",
    "\n",
    "# EPOCH 70\n",
    "\n",
    "# train: 70-390/391 tr_acc: 70.00%, lr=['1e-05'], val_acc: 71.07%: 100%|██████████| 391/391 [08:44<00:00,  1.34s/it]\n",
    "# epoch_time: 524.3250975608826 seconds\n",
    "\n",
    "\n",
    "# EPOCH 71\n",
    "\n",
    "# train: 71-390/391 tr_acc: 71.25%, lr=['1e-05'], val_acc: 71.03%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 524.1436457633972 seconds\n",
    "\n",
    "\n",
    "# EPOCH 72\n",
    "\n",
    "# train: 72-390/391 tr_acc: 72.50%, lr=['1e-05'], val_acc: 70.97%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.3500926494598 seconds\n",
    "\n",
    "\n",
    "# EPOCH 73\n",
    "\n",
    "# train: 73-390/391 tr_acc: 81.25%, lr=['1e-05'], val_acc: 69.71%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.6104423999786 seconds\n",
    "\n",
    "\n",
    "# EPOCH 74\n",
    "\n",
    "# train: 74-390/391 tr_acc: 72.50%, lr=['1e-05'], val_acc: 71.35%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 523.1159906387329 seconds\n",
    "\n",
    "\n",
    "# EPOCH 75\n",
    "\n",
    "# train: 75-390/391 tr_acc: 70.00%, lr=['1e-05'], val_acc: 72.26%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.9588918685913 seconds\n",
    "\n",
    "\n",
    "# EPOCH 76\n",
    "\n",
    "# train: 76-105/391 tr_acc: 75.78%, lr=['1e-05'], val_acc: 71.69%:  27%|██▋       | 106/391 [02:24<06:28,  1.36s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board ########################\n",
    "# epoch 6 acc 36\n",
    "\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75\n",
    "\n",
    "# my_snn_system(  devices = \"0,1,2,3,4,5\",\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 8, # dvscifar 10\n",
    "#                 BATCH = 128, # batch norm 할거면 2이상으로 해야함\n",
    "#                 IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32\n",
    "#                 which_data = 'CIFAR10',# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\n",
    "#                 CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,\n",
    "#                 lif_layer_v_reset = 0.0, #현재 안씀. 걍 빼기 해버림\n",
    "#                 lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1,\n",
    "#                 synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1,\n",
    "#                 synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = True, # True # False\n",
    "#                 # cfg = [64],\n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "#                 cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512],\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 net_print = False, # True # False\n",
    "#                 pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "#                 learning_rate = 0.00001,\n",
    "#                 epoch_num = 200,\n",
    "#                 verbose_interval = 10, #숫자 크게 하면 꺼짐\n",
    "#                 validation_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = True,  # True # False\n",
    "                \n",
    "#                 surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = True,  # True # False # True이면 BPTT, False이면 OTTT\n",
    "\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### my_snn control board ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75\n",
    "\n",
    "# my_snn_system(  devices = \"0,1,2,3,4,5\",\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 8, # dvscifar 10\n",
    "#                 BATCH = 128, # batch norm 할거면 2이상으로 해야함\n",
    "#                 IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32\n",
    "#                 which_data = 'CIFAR10',# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\n",
    "#                 CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,\n",
    "#                 lif_layer_v_reset = 0.0, #현재 안씀. 걍 빼기 해버림\n",
    "#                 lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1,\n",
    "#                 synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1,\n",
    "#                 synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = True, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # cfg = [64],\n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "#                 cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512],\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "\n",
    "\n",
    "#                 net_print = False, # True # False\n",
    "#                 pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "#                 learning_rate = 0.00001,\n",
    "#                 epoch_num = 200,\n",
    "#                 verbose_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 validation_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = True,  # True # False\n",
    "                \n",
    "#                 surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT\n",
    "\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,\n",
    "#                 )\n",
    "# # sigmoid와 BN이 있어야 잘되는건가? average pooling도 중요?\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid\n",
    "\n",
    "\n",
    "# EPOCH 0\n",
    "# train: 0-8/391 tr_acc: 16.41%, lr=['1e-05'], val_acc: 0.00%:   2%|▏         | 9/391 [00:24<07:31,  1.18s/it]  \n",
    "# 0-9 training acc: 8.59%, lr=['1e-05'], val_acc: 0.00%\n",
    "# train: 0-9/391 tr_acc: 8.59%, lr=['1e-05'], val_acc: 0.00%:   3%|▎         | 10/391 [00:25<06:58,  1.10s/it]\n",
    "# iter_one_train_time: 0.9021003246307373 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-18/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 0.00%:   5%|▍         | 19/391 [00:36<08:32,  1.38s/it]\n",
    "# 0-19 training acc: 7.81%, lr=['1e-05'], val_acc: 0.00%\n",
    "# train: 0-19/391 tr_acc: 7.81%, lr=['1e-05'], val_acc: 0.00%:   5%|▌         | 20/391 [00:37<08:56,  1.45s/it] \n",
    "# iter_one_train_time: 1.6002912521362305 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-28/391 tr_acc: 9.38%, lr=['1e-05'], val_acc: 0.00%:   7%|▋         | 29/391 [00:51<09:12,  1.53s/it] \n",
    "# 0-29 training acc: 11.72%, lr=['1e-05'], val_acc: 0.00%\n",
    "# train: 0-29/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 0.00%:   8%|▊         | 30/391 [00:52<09:10,  1.53s/it]\n",
    "# iter_one_train_time: 1.5201919078826904 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-38/391 tr_acc: 14.06%, lr=['1e-05'], val_acc: 0.00%:  10%|▉         | 39/391 [01:06<08:56,  1.52s/it]\n",
    "# 0-39 training acc: 11.72%, lr=['1e-05'], val_acc: 0.00%\n",
    "# train: 0-39/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 0.00%:  10%|█         | 40/391 [01:08<08:56,  1.53s/it]\n",
    "# iter_one_train_time: 1.529097318649292 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-48/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 0.00%:  13%|█▎        | 49/391 [01:22<08:52,  1.56s/it]\n",
    "# 0-49 training acc: 10.16%, lr=['1e-05'], val_acc: 0.00%\n",
    "# iter_one_train_time: 1.5265469551086426 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-58/391 tr_acc: 14.06%, lr=['1e-05'], val_acc: 11.72%:  15%|█▌        | 59/391 [01:55<09:53,  1.79s/it]\n",
    "# 0-59 training acc: 9.38%, lr=['1e-05'], val_acc: 11.72%\n",
    "# train: 0-59/391 tr_acc: 9.38%, lr=['1e-05'], val_acc: 11.72%:  15%|█▌        | 60/391 [01:56<09:26,  1.71s/it] \n",
    "# iter_one_train_time: 1.5334508419036865 seconds, last one_val_time: 16.866817474365234 seconds\n",
    "\n",
    "# train: 0-68/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 11.72%:  18%|█▊        | 69/391 [02:10<08:27,  1.57s/it]\n",
    "# 0-69 training acc: 12.50%, lr=['1e-05'], val_acc: 11.72%\n",
    "# train: 0-69/391 tr_acc: 12.50%, lr=['1e-05'], val_acc: 11.72%:  18%|█▊        | 70/391 [02:12<08:21,  1.56s/it]\n",
    "# iter_one_train_time: 1.5315065383911133 seconds, last one_val_time: 16.866817474365234 seconds\n",
    "\n",
    "# train: 0-78/391 tr_acc: 15.62%, lr=['1e-05'], val_acc: 11.72%:  20%|██        | 79/391 [02:26<08:02,  1.55s/it]\n",
    "# 0-79 training acc: 12.50%, lr=['1e-05'], val_acc: 11.72%\n",
    "# train: 0-79/391 tr_acc: 12.50%, lr=['1e-05'], val_acc: 11.72%:  20%|██        | 80/391 [02:27<07:59,  1.54s/it]\n",
    "# iter_one_train_time: 1.5302252769470215 seconds, last one_val_time: 16.866817474365234 seconds\n",
    "\n",
    "# train: 0-88/391 tr_acc: 10.94%, lr=['1e-05'], val_acc: 11.72%:  23%|██▎       | 89/391 [02:41<07:48,  1.55s/it]\n",
    "# 0-89 training acc: 12.50%, lr=['1e-05'], val_acc: 11.72%\n",
    "# train: 0-89/391 tr_acc: 12.50%, lr=['1e-05'], val_acc: 11.72%:  23%|██▎       | 90/391 [02:43<07:45,  1.55s/it]\n",
    "# iter_one_train_time: 1.5380234718322754 seconds, last one_val_time: 16.866817474365234 seconds\n",
    "\n",
    "# train: 0-98/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 11.72%:  25%|██▌       | 99/391 [02:57<07:42,  1.58s/it]\n",
    "# 0-99 training acc: 9.38%, lr=['1e-05'], val_acc: 11.72%\n",
    "# iter_one_train_time: 1.5485491752624512 seconds, last one_val_time: 16.866817474365234 seconds\n",
    "\n",
    "# train: 0-108/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 13.49%:  28%|██▊       | 109/391 [03:29<07:57,  1.69s/it]\n",
    "# 0-109 training acc: 10.16%, lr=['1e-05'], val_acc: 13.49%\n",
    "# train: 0-109/391 tr_acc: 10.16%, lr=['1e-05'], val_acc: 13.49%:  28%|██▊       | 110/391 [03:30<07:20,  1.57s/it]\n",
    "# iter_one_train_time: 1.2722880840301514 seconds, last one_val_time: 17.4084632396698 seconds\n",
    "\n",
    "# train: 0-118/391 tr_acc: 8.59%, lr=['1e-05'], val_acc: 13.49%:  30%|███       | 119/391 [03:42<06:55,  1.53s/it] \n",
    "# 0-119 training acc: 14.84%, lr=['1e-05'], val_acc: 13.49%\n",
    "# train: 0-119/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 13.49%:  31%|███       | 120/391 [03:44<06:57,  1.54s/it]\n",
    "# iter_one_train_time: 1.5578389167785645 seconds, last one_val_time: 17.4084632396698 seconds\n",
    "\n",
    "# train: 0-128/391 tr_acc: 10.94%, lr=['1e-05'], val_acc: 13.49%:  33%|███▎      | 129/391 [03:58<06:53,  1.58s/it]\n",
    "# 0-129 training acc: 10.94%, lr=['1e-05'], val_acc: 13.49%\n",
    "# train: 0-129/391 tr_acc: 10.94%, lr=['1e-05'], val_acc: 13.49%:  33%|███▎      | 130/391 [04:00<06:50,  1.57s/it]\n",
    "# iter_one_train_time: 1.5546276569366455 seconds, last one_val_time: 17.4084632396698 seconds\n",
    "\n",
    "# train: 0-138/391 tr_acc: 10.94%, lr=['1e-05'], val_acc: 13.49%:  36%|███▌      | 139/391 [04:14<06:38,  1.58s/it]\n",
    "# 0-139 training acc: 10.94%, lr=['1e-05'], val_acc: 13.49%\n",
    "# train: 0-139/391 tr_acc: 10.94%, lr=['1e-05'], val_acc: 13.49%:  36%|███▌      | 140/391 [04:15<06:34,  1.57s/it]\n",
    "# iter_one_train_time: 1.5492908954620361 seconds, last one_val_time: 17.4084632396698 seconds\n",
    "\n",
    "# train: 0-148/391 tr_acc: 15.62%, lr=['1e-05'], val_acc: 13.49%:  38%|███▊      | 149/391 [04:29<06:17,  1.56s/it]\n",
    "# 0-149 training acc: 15.62%, lr=['1e-05'], val_acc: 13.49%\n",
    "# iter_one_train_time: 1.5452179908752441 seconds, last one_val_time: 17.4084632396698 seconds\n",
    "\n",
    "# train: 0-158/391 tr_acc: 10.16%, lr=['1e-05'], val_acc: 14.48%:  41%|████      | 159/391 [05:02<06:51,  1.77s/it]\n",
    "# 0-159 training acc: 17.97%, lr=['1e-05'], val_acc: 14.48%\n",
    "# train: 0-159/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 14.48%:  41%|████      | 160/391 [05:04<06:40,  1.74s/it]\n",
    "# iter_one_train_time: 1.6436526775360107 seconds, last one_val_time: 17.05498194694519 seconds\n",
    "\n",
    "# train: 0-168/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 14.48%:  43%|████▎     | 169/391 [05:18<05:54,  1.60s/it]\n",
    "# 0-169 training acc: 16.41%, lr=['1e-05'], val_acc: 14.48%\n",
    "# train: 0-169/391 tr_acc: 16.41%, lr=['1e-05'], val_acc: 14.48%:  43%|████▎     | 170/391 [05:20<05:50,  1.59s/it]\n",
    "# iter_one_train_time: 1.5567233562469482 seconds, last one_val_time: 17.05498194694519 seconds\n",
    "\n",
    "# train: 0-178/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 14.48%:  46%|████▌     | 179/391 [05:34<05:30,  1.56s/it]\n",
    "# 0-179 training acc: 18.75%, lr=['1e-05'], val_acc: 14.48%\n",
    "# train: 0-179/391 tr_acc: 18.75%, lr=['1e-05'], val_acc: 14.48%:  46%|████▌     | 180/391 [05:35<05:28,  1.56s/it]\n",
    "# iter_one_train_time: 1.546079158782959 seconds, last one_val_time: 17.05498194694519 seconds\n",
    "\n",
    "# train: 0-188/391 tr_acc: 9.38%, lr=['1e-05'], val_acc: 14.48%:  48%|████▊     | 189/391 [05:49<05:15,  1.56s/it] \n",
    "# 0-189 training acc: 13.28%, lr=['1e-05'], val_acc: 14.48%\n",
    "# train: 0-189/391 tr_acc: 13.28%, lr=['1e-05'], val_acc: 14.48%:  49%|████▊     | 190/391 [05:51<05:12,  1.56s/it]\n",
    "# iter_one_train_time: 1.5421521663665771 seconds, last one_val_time: 17.05498194694519 seconds\n",
    "\n",
    "# train: 0-198/391 tr_acc: 22.66%, lr=['1e-05'], val_acc: 14.48%:  51%|█████     | 199/391 [06:05<04:58,  1.56s/it]\n",
    "# 0-199 training acc: 14.84%, lr=['1e-05'], val_acc: 14.48%\n",
    "# iter_one_train_time: 1.6056420803070068 seconds, last one_val_time: 17.05498194694519 seconds\n",
    "\n",
    "# train: 0-208/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 15.04%:  53%|█████▎    | 209/391 [06:39<05:28,  1.80s/it]\n",
    "# 0-209 training acc: 20.31%, lr=['1e-05'], val_acc: 15.04%\n",
    "# train: 0-209/391 tr_acc: 20.31%, lr=['1e-05'], val_acc: 15.04%:  54%|█████▎    | 210/391 [06:40<05:13,  1.73s/it]\n",
    "# iter_one_train_time: 1.5574557781219482 seconds, last one_val_time: 17.079707384109497 seconds\n",
    "\n",
    "# train: 0-218/391 tr_acc: 14.06%, lr=['1e-05'], val_acc: 15.04%:  56%|█████▌    | 219/391 [06:54<04:29,  1.57s/it]\n",
    "# 0-219 training acc: 17.97%, lr=['1e-05'], val_acc: 15.04%\n",
    "# train: 0-219/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 15.04%:  56%|█████▋    | 220/391 [06:56<04:26,  1.56s/it]\n",
    "# iter_one_train_time: 1.5369503498077393 seconds, last one_val_time: 17.079707384109497 seconds\n",
    "\n",
    "# train: 0-228/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 15.04%:  59%|█████▊    | 229/391 [07:10<04:16,  1.58s/it]\n",
    "# 0-229 training acc: 10.16%, lr=['1e-05'], val_acc: 15.04%\n",
    "# train: 0-229/391 tr_acc: 10.16%, lr=['1e-05'], val_acc: 15.04%:  59%|█████▉    | 230/391 [07:12<04:12,  1.57s/it]\n",
    "# iter_one_train_time: 1.5427579879760742 seconds, last one_val_time: 17.079707384109497 seconds\n",
    "\n",
    "# train: 0-238/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 15.04%:  61%|██████    | 239/391 [07:26<03:56,  1.55s/it]\n",
    "# 0-239 training acc: 20.31%, lr=['1e-05'], val_acc: 15.04%\n",
    "# train: 0-239/391 tr_acc: 20.31%, lr=['1e-05'], val_acc: 15.04%:  61%|██████▏   | 240/391 [07:27<03:54,  1.55s/it]\n",
    "# iter_one_train_time: 1.5445988178253174 seconds, last one_val_time: 17.079707384109497 seconds\n",
    "\n",
    "# train: 0-248/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 15.04%:  64%|██████▎   | 249/391 [07:41<03:41,  1.56s/it]\n",
    "# 0-249 training acc: 10.94%, lr=['1e-05'], val_acc: 15.04%\n",
    "# iter_one_train_time: 1.5501785278320312 seconds, last one_val_time: 17.079707384109497 seconds\n",
    "\n",
    "# train: 0-258/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 15.79%:  66%|██████▌   | 259/391 [08:14<03:54,  1.78s/it]\n",
    "# 0-259 training acc: 17.97%, lr=['1e-05'], val_acc: 15.79%\n",
    "# train: 0-259/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 15.79%:  66%|██████▋   | 260/391 [08:16<03:44,  1.71s/it]\n",
    "# iter_one_train_time: 1.5513510704040527 seconds, last one_val_time: 16.802847623825073 seconds\n",
    "\n",
    "# train: 0-268/391 tr_acc: 18.75%, lr=['1e-05'], val_acc: 15.79%:  69%|██████▉   | 269/391 [08:30<03:13,  1.59s/it]\n",
    "# 0-269 training acc: 10.94%, lr=['1e-05'], val_acc: 15.79%\n",
    "# train: 0-269/391 tr_acc: 10.94%, lr=['1e-05'], val_acc: 15.79%:  69%|██████▉   | 270/391 [08:32<03:10,  1.58s/it]\n",
    "# iter_one_train_time: 1.545609712600708 seconds, last one_val_time: 16.802847623825073 seconds\n",
    "\n",
    "# train: 0-278/391 tr_acc: 16.41%, lr=['1e-05'], val_acc: 15.79%:  71%|███████▏  | 279/391 [08:46<02:56,  1.58s/it]\n",
    "# 0-279 training acc: 13.28%, lr=['1e-05'], val_acc: 15.79%\n",
    "# train: 0-279/391 tr_acc: 13.28%, lr=['1e-05'], val_acc: 15.79%:  72%|███████▏  | 280/391 [08:47<02:54,  1.58s/it]\n",
    "# iter_one_train_time: 1.5714669227600098 seconds, last one_val_time: 16.802847623825073 seconds\n",
    "\n",
    "# train: 0-288/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 15.79%:  74%|███████▍  | 289/391 [09:01<02:38,  1.55s/it]\n",
    "# 0-289 training acc: 21.88%, lr=['1e-05'], val_acc: 15.79%\n",
    "# train: 0-289/391 tr_acc: 21.88%, lr=['1e-05'], val_acc: 15.79%:  74%|███████▍  | 290/391 [09:03<02:37,  1.56s/it]\n",
    "# iter_one_train_time: 1.5540351867675781 seconds, last one_val_time: 16.802847623825073 seconds\n",
    "\n",
    "# train: 0-298/391 tr_acc: 13.28%, lr=['1e-05'], val_acc: 15.79%:  76%|███████▋  | 299/391 [09:14<01:36,  1.05s/it]\n",
    "# 0-299 training acc: 19.53%, lr=['1e-05'], val_acc: 15.79%\n",
    "# iter_one_train_time: 0.8877673149108887 seconds, last one_val_time: 16.802847623825073 seconds\n",
    "\n",
    "# train: 0-308/391 tr_acc: 12.50%, lr=['1e-05'], val_acc: 16.33%:  79%|███████▉  | 309/391 [09:40<01:30,  1.10s/it]\n",
    "# 0-309 training acc: 18.75%, lr=['1e-05'], val_acc: 16.33%\n",
    "# train: 0-309/391 tr_acc: 18.75%, lr=['1e-05'], val_acc: 16.33%:  79%|███████▉  | 310/391 [09:41<01:23,  1.04s/it]\n",
    "# iter_one_train_time: 0.8837041854858398 seconds, last one_val_time: 16.87713122367859 seconds\n",
    "\n",
    "# train: 0-318/391 tr_acc: 18.75%, lr=['1e-05'], val_acc: 16.33%:  82%|████████▏ | 319/391 [09:49<01:06,  1.09it/s]\n",
    "# 0-319 training acc: 14.06%, lr=['1e-05'], val_acc: 16.33%\n",
    "# train: 0-319/391 tr_acc: 14.06%, lr=['1e-05'], val_acc: 16.33%:  82%|████████▏ | 320/391 [09:50<01:05,  1.09it/s]\n",
    "# iter_one_train_time: 0.9021422863006592 seconds, last one_val_time: 16.87713122367859 seconds\n",
    "\n",
    "# train: 0-328/391 tr_acc: 21.88%, lr=['1e-05'], val_acc: 16.33%:  84%|████████▍ | 329/391 [09:58<00:55,  1.12it/s]\n",
    "# 0-329 training acc: 12.50%, lr=['1e-05'], val_acc: 16.33%\n",
    "# train: 0-329/391 tr_acc: 12.50%, lr=['1e-05'], val_acc: 16.33%:  84%|████████▍ | 330/391 [09:59<00:54,  1.11it/s]\n",
    "# iter_one_train_time: 0.8932549953460693 seconds, last one_val_time: 16.87713122367859 seconds\n",
    "\n",
    "# train: 0-338/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 16.33%:  87%|████████▋ | 339/391 [10:07<00:46,  1.12it/s]\n",
    "# 0-339 training acc: 17.97%, lr=['1e-05'], val_acc: 16.33%\n",
    "# train: 0-339/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 16.33%:  87%|████████▋ | 340/391 [10:08<00:45,  1.11it/s]\n",
    "# iter_one_train_time: 0.9081242084503174 seconds, last one_val_time: 16.87713122367859 seconds\n",
    "\n",
    "# train: 0-348/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 16.33%:  89%|████████▉ | 349/391 [10:17<00:38,  1.08it/s]\n",
    "# 0-349 training acc: 17.19%, lr=['1e-05'], val_acc: 16.33%\n",
    "# iter_one_train_time: 0.8888015747070312 seconds, last one_val_time: 16.87713122367859 seconds\n",
    "\n",
    "# train: 0-358/391 tr_acc: 15.62%, lr=['1e-05'], val_acc: 17.23%:  92%|█████████▏| 359/391 [10:43<00:35,  1.11s/it]\n",
    "# 0-359 training acc: 17.19%, lr=['1e-05'], val_acc: 17.23%\n",
    "# train: 0-359/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 17.23%:  92%|█████████▏| 360/391 [10:44<00:32,  1.04s/it]\n",
    "# iter_one_train_time: 0.889817476272583 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 0-368/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 17.23%:  94%|█████████▍| 369/391 [10:52<00:19,  1.12it/s]\n",
    "# 0-369 training acc: 10.16%, lr=['1e-05'], val_acc: 17.23%\n",
    "# train: 0-369/391 tr_acc: 10.16%, lr=['1e-05'], val_acc: 17.23%:  95%|█████████▍| 370/391 [10:53<00:19,  1.09it/s]\n",
    "# iter_one_train_time: 0.9528336524963379 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 0-378/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 17.23%:  97%|█████████▋| 379/391 [11:01<00:10,  1.11it/s]\n",
    "# 0-379 training acc: 16.41%, lr=['1e-05'], val_acc: 17.23%\n",
    "# train: 0-379/391 tr_acc: 16.41%, lr=['1e-05'], val_acc: 17.23%:  97%|█████████▋| 380/391 [11:02<00:09,  1.11it/s]\n",
    "# iter_one_train_time: 0.8962624073028564 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 0-388/391 tr_acc: 12.50%, lr=['1e-05'], val_acc: 17.23%:  99%|█████████▉| 389/391 [11:10<00:01,  1.10it/s]\n",
    "# 0-389 training acc: 16.41%, lr=['1e-05'], val_acc: 17.23%\n",
    "# train: 0-389/391 tr_acc: 16.41%, lr=['1e-05'], val_acc: 17.23%: 100%|█████████▉| 390/391 [11:11<00:00,  1.11it/s]\n",
    "# iter_one_train_time: 0.8882513046264648 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 0-390/391 tr_acc: 16.25%, lr=['1e-05'], val_acc: 17.23%: 100%|██████████| 391/391 [11:12<00:00,  1.72s/it]\n",
    "# epoch_time: 672.5876622200012 seconds\n",
    "\n",
    "\n",
    "# EPOCH 1\n",
    "\n",
    "# train: 1-8/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 17.23%:   2%|▏         | 9/391 [00:08<05:46,  1.10it/s]\n",
    "# 1-9 training acc: 11.72%, lr=['1e-05'], val_acc: 17.23%\n",
    "# train: 1-9/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 17.23%:   3%|▎         | 10/391 [00:09<05:45,  1.10it/s]\n",
    "# iter_one_train_time: 0.9011766910552979 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 1-18/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 17.23%:   5%|▍         | 19/391 [00:17<05:45,  1.08it/s]\n",
    "# 1-19 training acc: 14.06%, lr=['1e-05'], val_acc: 17.23%\n",
    "# train: 1-19/391 tr_acc: 14.06%, lr=['1e-05'], val_acc: 17.23%:   5%|▌         | 20/391 [00:18<05:40,  1.09it/s]\n",
    "# iter_one_train_time: 0.8899838924407959 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 1-28/391 tr_acc: 18.75%, lr=['1e-05'], val_acc: 17.23%:   7%|▋         | 29/391 [00:26<05:33,  1.09it/s]\n",
    "# 1-29 training acc: 13.28%, lr=['1e-05'], val_acc: 17.23%\n",
    "# train: 1-29/391 tr_acc: 13.28%, lr=['1e-05'], val_acc: 17.23%:   8%|▊         | 30/391 [00:27<05:32,  1.09it/s]\n",
    "# iter_one_train_time: 0.913830041885376 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 1-38/391 tr_acc: 15.62%, lr=['1e-05'], val_acc: 17.23%:  10%|▉         | 39/391 [00:35<05:17,  1.11it/s]\n",
    "# 1-39 training acc: 12.50%, lr=['1e-05'], val_acc: 17.23%\n",
    "# train: 1-39/391 tr_acc: 12.50%, lr=['1e-05'], val_acc: 17.23%:  10%|█         | 40/391 [00:36<05:16,  1.11it/s]\n",
    "# iter_one_train_time: 0.9035923480987549 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 1-48/391 tr_acc: 15.62%, lr=['1e-05'], val_acc: 17.23%:  13%|█▎        | 49/391 [00:44<05:15,  1.08it/s]\n",
    "# 1-49 training acc: 14.06%, lr=['1e-05'], val_acc: 17.23%\n",
    "# iter_one_train_time: 0.8847906589508057 seconds, last one_val_time: 17.09144353866577 seconds\n",
    "\n",
    "# train: 1-58/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 17.51%:  15%|█▌        | 59/391 [01:10<06:04,  1.10s/it]\n",
    "# 1-59 training acc: 15.62%, lr=['1e-05'], val_acc: 17.51%\n",
    "# train: 1-59/391 tr_acc: 15.62%, lr=['1e-05'], val_acc: 17.51%:  15%|█▌        | 60/391 [01:11<05:43,  1.04s/it]\n",
    "# iter_one_train_time: 0.8940188884735107 seconds, last one_val_time: 16.755418300628662 seconds\n",
    "\n",
    "# train: 1-68/391 tr_acc: 25.00%, lr=['1e-05'], val_acc: 17.51%:  18%|█▊        | 69/391 [01:20<04:57,  1.08it/s]\n",
    "# 1-69 training acc: 17.97%, lr=['1e-05'], val_acc: 17.51%\n",
    "# train: 1-69/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 17.51%:  18%|█▊        | 70/391 [01:20<04:53,  1.09it/s]\n",
    "# iter_one_train_time: 0.8915848731994629 seconds, last one_val_time: 16.755418300628662 seconds\n",
    "\n",
    "# train: 1-78/391 tr_acc: 24.22%, lr=['1e-05'], val_acc: 17.51%:  20%|██        | 79/391 [01:29<04:44,  1.10it/s]\n",
    "# 1-79 training acc: 18.75%, lr=['1e-05'], val_acc: 17.51%\n",
    "# train: 1-79/391 tr_acc: 18.75%, lr=['1e-05'], val_acc: 17.51%:  20%|██        | 80/391 [01:30<04:40,  1.11it/s]\n",
    "# iter_one_train_time: 0.8839564323425293 seconds, last one_val_time: 16.755418300628662 seconds\n",
    "\n",
    "# train: 1-88/391 tr_acc: 18.75%, lr=['1e-05'], val_acc: 17.51%:  23%|██▎       | 89/391 [01:38<04:33,  1.10it/s]\n",
    "# 1-89 training acc: 11.72%, lr=['1e-05'], val_acc: 17.51%\n",
    "# train: 1-89/391 tr_acc: 11.72%, lr=['1e-05'], val_acc: 17.51%:  23%|██▎       | 90/391 [01:39<04:33,  1.10it/s]\n",
    "# iter_one_train_time: 0.9085617065429688 seconds, last one_val_time: 16.755418300628662 seconds\n",
    "\n",
    "# train: 1-98/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 17.51%:  25%|██▌       | 99/391 [01:47<04:50,  1.00it/s]\n",
    "# 1-99 training acc: 17.19%, lr=['1e-05'], val_acc: 17.51%\n",
    "# iter_one_train_time: 1.5453689098358154 seconds, last one_val_time: 16.755418300628662 seconds\n",
    "\n",
    "# train: 1-108/391 tr_acc: 15.62%, lr=['1e-05'], val_acc: 18.53%:  28%|██▊       | 109/391 [02:21<08:22,  1.78s/it]\n",
    "# 1-109 training acc: 22.66%, lr=['1e-05'], val_acc: 18.53%\n",
    "# train: 1-109/391 tr_acc: 22.66%, lr=['1e-05'], val_acc: 18.53%:  28%|██▊       | 110/391 [02:22<08:01,  1.71s/it]\n",
    "# iter_one_train_time: 1.5485820770263672 seconds, last one_val_time: 16.960761070251465 seconds\n",
    "\n",
    "# train: 1-118/391 tr_acc: 10.16%, lr=['1e-05'], val_acc: 18.53%:  30%|███       | 119/391 [02:36<07:08,  1.57s/it]\n",
    "# 1-119 training acc: 21.09%, lr=['1e-05'], val_acc: 18.53%\n",
    "# train: 1-119/391 tr_acc: 21.09%, lr=['1e-05'], val_acc: 18.53%:  31%|███       | 120/391 [02:38<07:12,  1.60s/it]\n",
    "# iter_one_train_time: 1.6461992263793945 seconds, last one_val_time: 16.960761070251465 seconds\n",
    "\n",
    "# train: 1-128/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 18.53%:  33%|███▎      | 129/391 [02:52<06:57,  1.59s/it]\n",
    "# 1-129 training acc: 17.97%, lr=['1e-05'], val_acc: 18.53%\n",
    "# train: 1-129/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 18.53%:  33%|███▎      | 130/391 [02:54<06:55,  1.59s/it]\n",
    "# iter_one_train_time: 1.5823149681091309 seconds, last one_val_time: 16.960761070251465 seconds\n",
    "\n",
    "# train: 1-138/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 18.53%:  36%|███▌      | 139/391 [03:08<06:37,  1.58s/it]\n",
    "# 1-139 training acc: 21.88%, lr=['1e-05'], val_acc: 18.53%\n",
    "# train: 1-139/391 tr_acc: 21.88%, lr=['1e-05'], val_acc: 18.53%:  36%|███▌      | 140/391 [03:10<06:34,  1.57s/it]\n",
    "# iter_one_train_time: 1.558037281036377 seconds, last one_val_time: 16.960761070251465 seconds\n",
    "\n",
    "# train: 1-148/391 tr_acc: 18.75%, lr=['1e-05'], val_acc: 18.53%:  38%|███▊      | 149/391 [03:24<06:20,  1.57s/it]\n",
    "# 1-149 training acc: 23.44%, lr=['1e-05'], val_acc: 18.53%\n",
    "# iter_one_train_time: 1.5661795139312744 seconds, last one_val_time: 16.960761070251465 seconds\n",
    "\n",
    "# train: 1-158/391 tr_acc: 22.66%, lr=['1e-05'], val_acc: 18.44%:  41%|████      | 159/391 [03:57<06:58,  1.80s/it]\n",
    "# 1-159 training acc: 23.44%, lr=['1e-05'], val_acc: 18.44%\n",
    "# train: 1-159/391 tr_acc: 23.44%, lr=['1e-05'], val_acc: 18.44%:  41%|████      | 160/391 [03:58<06:38,  1.72s/it]\n",
    "# iter_one_train_time: 1.539853811264038 seconds, last one_val_time: 16.89861512184143 seconds\n",
    "\n",
    "# train: 1-168/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 18.44%:  43%|████▎     | 169/391 [04:12<05:52,  1.59s/it]\n",
    "# 1-169 training acc: 23.44%, lr=['1e-05'], val_acc: 18.44%\n",
    "# train: 1-169/391 tr_acc: 23.44%, lr=['1e-05'], val_acc: 18.44%:  43%|████▎     | 170/391 [04:14<05:49,  1.58s/it]\n",
    "# iter_one_train_time: 1.5540361404418945 seconds, last one_val_time: 16.89861512184143 seconds\n",
    "\n",
    "# train: 1-178/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 18.44%:  46%|████▌     | 179/391 [04:28<05:34,  1.58s/it]\n",
    "# 1-179 training acc: 24.22%, lr=['1e-05'], val_acc: 18.44%\n",
    "# train: 1-179/391 tr_acc: 24.22%, lr=['1e-05'], val_acc: 18.44%:  46%|████▌     | 180/391 [04:30<05:33,  1.58s/it]\n",
    "# iter_one_train_time: 1.5785167217254639 seconds, last one_val_time: 16.89861512184143 seconds\n",
    "\n",
    "# train: 1-188/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 18.44%:  48%|████▊     | 189/391 [04:44<05:18,  1.58s/it]\n",
    "# 1-189 training acc: 21.88%, lr=['1e-05'], val_acc: 18.44%\n",
    "# train: 1-189/391 tr_acc: 21.88%, lr=['1e-05'], val_acc: 18.44%:  49%|████▊     | 190/391 [04:45<05:17,  1.58s/it]\n",
    "# iter_one_train_time: 1.5851466655731201 seconds, last one_val_time: 16.89861512184143 seconds\n",
    "\n",
    "# train: 1-198/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 18.44%:  51%|█████     | 199/391 [05:00<05:06,  1.60s/it]\n",
    "# 1-199 training acc: 21.88%, lr=['1e-05'], val_acc: 18.44%\n",
    "# iter_one_train_time: 1.5574944019317627 seconds, last one_val_time: 16.89861512184143 seconds\n",
    "\n",
    "# train: 1-208/391 tr_acc: 19.53%, lr=['1e-05'], val_acc: 18.39%:  53%|█████▎    | 209/391 [05:26<03:25,  1.13s/it]\n",
    "# 1-209 training acc: 15.62%, lr=['1e-05'], val_acc: 18.39%\n",
    "# train: 1-209/391 tr_acc: 15.62%, lr=['1e-05'], val_acc: 18.39%:  54%|█████▎    | 210/391 [05:27<03:11,  1.06s/it]\n",
    "# iter_one_train_time: 0.878171443939209 seconds, last one_val_time: 16.62509536743164 seconds\n",
    "\n",
    "# train: 1-218/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 18.39%:  56%|█████▌    | 219/391 [05:35<02:35,  1.11it/s]\n",
    "# 1-219 training acc: 19.53%, lr=['1e-05'], val_acc: 18.39%\n",
    "# train: 1-219/391 tr_acc: 19.53%, lr=['1e-05'], val_acc: 18.39%:  56%|█████▋    | 220/391 [05:36<02:33,  1.12it/s]\n",
    "# iter_one_train_time: 0.8779330253601074 seconds, last one_val_time: 16.62509536743164 seconds\n",
    "\n",
    "# train: 1-228/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 18.39%:  59%|█████▊    | 229/391 [05:44<02:24,  1.12it/s]\n",
    "# 1-229 training acc: 13.28%, lr=['1e-05'], val_acc: 18.39%\n",
    "# train: 1-229/391 tr_acc: 13.28%, lr=['1e-05'], val_acc: 18.39%:  59%|█████▉    | 230/391 [05:45<02:22,  1.13it/s]\n",
    "# iter_one_train_time: 0.8615732192993164 seconds, last one_val_time: 16.62509536743164 seconds\n",
    "\n",
    "# train: 1-238/391 tr_acc: 21.88%, lr=['1e-05'], val_acc: 18.39%:  61%|██████    | 239/391 [05:53<02:17,  1.10it/s]\n",
    "# 1-239 training acc: 24.22%, lr=['1e-05'], val_acc: 18.39%\n",
    "# train: 1-239/391 tr_acc: 24.22%, lr=['1e-05'], val_acc: 18.39%:  61%|██████▏   | 240/391 [05:54<02:16,  1.11it/s]\n",
    "# iter_one_train_time: 0.8894679546356201 seconds, last one_val_time: 16.62509536743164 seconds\n",
    "\n",
    "# train: 1-248/391 tr_acc: 20.31%, lr=['1e-05'], val_acc: 18.39%:  64%|██████▎   | 249/391 [06:02<02:07,  1.11it/s]\n",
    "# 1-249 training acc: 17.19%, lr=['1e-05'], val_acc: 18.39%\n",
    "# iter_one_train_time: 0.9120488166809082 seconds, last one_val_time: 16.62509536743164 seconds\n",
    "\n",
    "# train: 1-258/391 tr_acc: 20.31%, lr=['1e-05'], val_acc: 19.09%:  66%|██████▌   | 259/391 [06:28<02:26,  1.11s/it]\n",
    "# 1-259 training acc: 16.41%, lr=['1e-05'], val_acc: 19.09%\n",
    "# train: 1-259/391 tr_acc: 16.41%, lr=['1e-05'], val_acc: 19.09%:  66%|██████▋   | 260/391 [06:29<02:17,  1.05s/it]\n",
    "# iter_one_train_time: 0.8989126682281494 seconds, last one_val_time: 16.488541841506958 seconds\n",
    "\n",
    "# train: 1-268/391 tr_acc: 17.97%, lr=['1e-05'], val_acc: 19.09%:  69%|██████▉   | 269/391 [06:39<02:36,  1.28s/it]\n",
    "# 1-269 training acc: 16.41%, lr=['1e-05'], val_acc: 19.09%\n",
    "# train: 1-269/391 tr_acc: 16.41%, lr=['1e-05'], val_acc: 19.09%:  69%|██████▉   | 270/391 [06:40<02:44,  1.36s/it]\n",
    "# iter_one_train_time: 1.5264859199523926 seconds, last one_val_time: 16.488541841506958 seconds\n",
    "\n",
    "# train: 1-278/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 19.09%:  71%|███████▏  | 279/391 [06:54<02:54,  1.56s/it]\n",
    "# 1-279 training acc: 21.09%, lr=['1e-05'], val_acc: 19.09%\n",
    "# train: 1-279/391 tr_acc: 21.09%, lr=['1e-05'], val_acc: 19.09%:  72%|███████▏  | 280/391 [06:56<02:53,  1.56s/it]\n",
    "# iter_one_train_time: 1.564835786819458 seconds, last one_val_time: 16.488541841506958 seconds\n",
    "\n",
    "# train: 1-288/391 tr_acc: 14.84%, lr=['1e-05'], val_acc: 19.09%:  74%|███████▍  | 289/391 [07:10<02:39,  1.57s/it]\n",
    "# 1-289 training acc: 23.44%, lr=['1e-05'], val_acc: 19.09%\n",
    "# train: 1-289/391 tr_acc: 23.44%, lr=['1e-05'], val_acc: 19.09%:  74%|███████▍  | 290/391 [07:11<02:37,  1.56s/it]\n",
    "# iter_one_train_time: 1.5514628887176514 seconds, last one_val_time: 16.488541841506958 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75\n",
    "\n",
    "# my_snn_system(  devices = \"0,1,2,3,4,5\",\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 8, # dvscifar 10\n",
    "#                 BATCH = 128, # batch norm 할거면 2이상으로 해야함\n",
    "#                 IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32\n",
    "#                 which_data = 'CIFAR10',# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\n",
    "#                 CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,\n",
    "#                 lif_layer_v_reset = 0.0, #현재 안씀. 걍 빼기 해버림\n",
    "#                 lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1,\n",
    "#                 synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1,\n",
    "#                 synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = True, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # cfg = [64],\n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512],\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "\n",
    "\n",
    "#                 net_print = False, # True # False\n",
    "#                 pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "#                 learning_rate = 0.00001,\n",
    "#                 epoch_num = 200,\n",
    "#                 verbose_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 validation_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = True,  # True # False\n",
    "                \n",
    "#                 surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT\n",
    "\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,\n",
    "#                 )\n",
    "# # sigmoid와 BN이 있어야 잘되는건가? average pooling도 중요?\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid\n",
    "\n",
    "\n",
    "\n",
    "# Files already downloaded and verified\n",
    "# Files already downloaded and verified\n",
    "# ==================================================\n",
    "# My Num of PARAMS: 9,302,410, system's param_num : 9,346,442\n",
    "# Memory: 35.49MiB at 32-bit\n",
    "# ==================================================\n",
    "# EPOCH 0\n",
    "# train: 0-48/391 tr_acc: 12.50%, lr=['1e-05'], val_acc: 0.00%:  13%|█▎        | 49/391 [01:31<05:53,  1.03s/it]\n",
    "# 0-49 training acc: 10.16%, lr=['1e-05'], val_acc: 0.00%\n",
    "# iter_one_train_time: 1.548201084136963 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-98/391 tr_acc: 19.53%, lr=['1e-05'], val_acc: 16.11%:  25%|██▌       | 99/391 [03:07<07:44,  1.59s/it]\n",
    "# 0-99 training acc: 14.84%, lr=['1e-05'], val_acc: 16.11%\n",
    "# iter_one_train_time: 1.5574898719787598 seconds, last one_val_time: 16.978580951690674 seconds\n",
    "\n",
    "# train: 0-148/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 18.27%:  38%|███▊      | 149/391 [04:44<06:22,  1.58s/it]\n",
    "# 0-149 training acc: 14.06%, lr=['1e-05'], val_acc: 18.27%\n",
    "# iter_one_train_time: 1.5439555644989014 seconds, last one_val_time: 16.892942667007446 seconds\n",
    "\n",
    "# train: 0-198/391 tr_acc: 26.56%, lr=['1e-05'], val_acc: 19.09%:  51%|█████     | 199/391 [05:53<02:52,  1.11it/s]\n",
    "# 0-199 training acc: 25.00%, lr=['1e-05'], val_acc: 19.09%\n",
    "# iter_one_train_time: 0.9714524745941162 seconds, last one_val_time: 16.94005250930786 seconds\n",
    "\n",
    "# train: 0-248/391 tr_acc: 20.31%, lr=['1e-05'], val_acc: 21.53%:  64%|██████▎   | 249/391 [07:28<03:42,  1.57s/it]\n",
    "# 0-249 training acc: 21.09%, lr=['1e-05'], val_acc: 21.53%\n",
    "# iter_one_train_time: 1.5712730884552002 seconds, last one_val_time: 16.967337608337402 seconds\n",
    "\n",
    "# train: 0-298/391 tr_acc: 21.88%, lr=['1e-05'], val_acc: 22.16%:  76%|███████▋  | 299/391 [09:03<02:23,  1.56s/it]\n",
    "# 0-299 training acc: 20.31%, lr=['1e-05'], val_acc: 22.16%\n",
    "# iter_one_train_time: 1.0139188766479492 seconds, last one_val_time: 16.793952465057373 seconds\n",
    "\n",
    "# train: 0-348/391 tr_acc: 23.44%, lr=['1e-05'], val_acc: 22.73%:  89%|████████▉ | 349/391 [10:38<01:06,  1.57s/it]\n",
    "# 0-349 training acc: 21.09%, lr=['1e-05'], val_acc: 22.73%\n",
    "# iter_one_train_time: 1.538879156112671 seconds, last one_val_time: 17.2120463848114 seconds\n",
    "\n",
    "# train: 0-390/391 tr_acc: 26.25%, lr=['1e-05'], val_acc: 23.78%: 100%|██████████| 391/391 [12:01<00:00,  1.84s/it]\n",
    "# epoch_time: 721.2595112323761 seconds\n",
    "\n",
    "\n",
    "# EPOCH 1\n",
    "\n",
    "# train: 1-48/391 tr_acc: 22.66%, lr=['1e-05'], val_acc: 23.78%:  13%|█▎        | 49/391 [01:15<08:49,  1.55s/it]\n",
    "# 1-49 training acc: 22.66%, lr=['1e-05'], val_acc: 23.78%\n",
    "# iter_one_train_time: 1.547459363937378 seconds, last one_val_time: 17.239224672317505 seconds\n",
    "\n",
    "# train: 1-98/391 tr_acc: 22.66%, lr=['1e-05'], val_acc: 23.63%:  25%|██▌       | 99/391 [02:47<04:46,  1.02it/s]\n",
    "# 1-99 training acc: 19.53%, lr=['1e-05'], val_acc: 23.63%\n",
    "# iter_one_train_time: 0.9713468551635742 seconds, last one_val_time: 17.197083711624146 seconds\n",
    "\n",
    "# train: 1-148/391 tr_acc: 17.19%, lr=['1e-05'], val_acc: 24.43%:  38%|███▊      | 149/391 [03:49<03:38,  1.11it/s]\n",
    "# 1-149 training acc: 19.53%, lr=['1e-05'], val_acc: 24.43%\n",
    "# iter_one_train_time: 0.8979551792144775 seconds, last one_val_time: 16.401078462600708 seconds\n",
    "\n",
    "# train: 1-198/391 tr_acc: 28.91%, lr=['1e-05'], val_acc: 25.07%:  51%|█████     | 199/391 [04:51<02:52,  1.12it/s]\n",
    "# 1-199 training acc: 21.09%, lr=['1e-05'], val_acc: 25.07%\n",
    "# iter_one_train_time: 0.886080265045166 seconds, last one_val_time: 17.012481927871704 seconds\n",
    "\n",
    "# train: 1-248/391 tr_acc: 24.22%, lr=['1e-05'], val_acc: 25.92%:  64%|██████▎   | 249/391 [05:59<03:39,  1.55s/it]\n",
    "# 1-249 training acc: 22.66%, lr=['1e-05'], val_acc: 25.92%\n",
    "# iter_one_train_time: 1.5489864349365234 seconds, last one_val_time: 16.566229104995728 seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conv fc weight gradient 방향바꾸는거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board ########################\n",
    "# decay = 0.5 # 0.875 0.25 0.125 0.75 0.5\n",
    "\n",
    "# my_snn_system(  devices = \"0,1,2,3,4,5\",\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 8, # dvscifar 10\n",
    "#                 BATCH = 128, # batch norm 할거면 2이상으로 해야함\n",
    "#                 IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32\n",
    "#                 which_data = 'CIFAR10',# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\n",
    "#                 CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 1.0,\n",
    "#                 lif_layer_v_reset = 0.0, #현재 안씀. 걍 빼기 해버림\n",
    "#                 lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1,\n",
    "#                 synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1,\n",
    "#                 synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = True, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # cfg = [64],\n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512],\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "\n",
    "\n",
    "#                 net_print = False, # True # False\n",
    "#                 pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "#                 learning_rate = 0.001,\n",
    "#                 epoch_num = 200,\n",
    "#                 verbose_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 validation_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = True,  # True # False\n",
    "                \n",
    "#                 surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT\n",
    "\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,\n",
    "#                 )\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling이 낫다.\n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid\n",
    "\n",
    "# Files already downloaded and verified\n",
    "# Files already downloaded and verified\n",
    "# ==================================================\n",
    "# My Num of PARAMS: 9,302,410, system's param_num : 9,346,442\n",
    "# Memory: 35.49MiB at 32-bit\n",
    "# # \n",
    "# EPOCH 0\n",
    "# train: 0-48/391 tr_acc: 22.66%, lr=['0.001'], tr_loss: 290.5330505371094, val_acc: 0.00%:  13%|█▎        | 49/391 [00:59<05:11,  1.10it/s] \n",
    "# 0-49 training acc: 21.09%, lr=['0.001'], val_acc: 0.00%\n",
    "# iter_one_train_time: 0.8913624286651611 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-98/391 tr_acc: 24.22%, lr=['0.001'], tr_loss: 228.34286499023438, val_acc: 24.43%:  25%|██▌       | 99/391 [02:02<04:22,  1.11it/s]\n",
    "# 0-99 training acc: 17.97%, lr=['0.001'], val_acc: 24.43%\n",
    "# iter_one_train_time: 0.9158523082733154 seconds, last one_val_time: 16.69316840171814 seconds\n",
    "\n",
    "# train: 0-148/391 tr_acc: 32.03%, lr=['0.001'], tr_loss: 210.59719848632812, val_acc: 27.37%:  38%|███▊      | 149/391 [03:04<03:40,  1.10it/s]\n",
    "# 0-149 training acc: 28.91%, lr=['0.001'], val_acc: 27.37%\n",
    "# iter_one_train_time: 0.9076290130615234 seconds, last one_val_time: 16.61735248565674 seconds\n",
    "\n",
    "# train: 0-198/391 tr_acc: 35.16%, lr=['0.001'], tr_loss: 195.4486846923828, val_acc: 26.79%:  51%|█████     | 199/391 [04:06<02:57,  1.08it/s] \n",
    "# 0-199 training acc: 30.47%, lr=['0.001'], val_acc: 26.79%\n",
    "# iter_one_train_time: 0.8877060413360596 seconds, last one_val_time: 16.78444528579712 seconds\n",
    "\n",
    "# train: 0-248/391 tr_acc: 25.00%, lr=['0.001'], tr_loss: 209.00848388671875, val_acc: 30.89%:  64%|██████▎   | 249/391 [05:08<02:10,  1.09it/s]\n",
    "# 0-249 training acc: 27.34%, lr=['0.001'], val_acc: 30.89%\n",
    "# iter_one_train_time: 0.8717858791351318 seconds, last one_val_time: 16.502493143081665 seconds\n",
    "\n",
    "# train: 0-298/391 tr_acc: 30.47%, lr=['0.001'], tr_loss: 220.2567138671875, val_acc: 31.96%:  76%|███████▋  | 299/391 [06:11<01:23,  1.10it/s] \n",
    "# 0-299 training acc: 29.69%, lr=['0.001'], val_acc: 31.96%\n",
    "# iter_one_train_time: 0.9970495700836182 seconds, last one_val_time: 16.796446800231934 seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv만 이상하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files already downloaded and verified\n",
    "# Files already downloaded and verified\n",
    "# ==================================================\n",
    "# My Num of PARAMS: 9,302,410, system's param_num : 9,346,442\n",
    "# Memory: 35.49MiB at 32-bit\n",
    "# ==================================================\n",
    "# EPOCH 0\n",
    "# train: 0-48/391 tr_acc: 8.59%, lr=['0.001'], tr_loss: 1039.622802734375, val_acc: 0.00%:  13%|█▎        | 49/391 [00:59<04:57,  1.15it/s]  \n",
    "# 0-49 training acc: 6.25%, lr=['0.001'], val_acc: 0.00%\n",
    "# iter_one_train_time: 0.8593673706054688 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-98/391 tr_acc: 7.81%, lr=['0.001'], tr_loss: 339.25616455078125, val_acc: 7.53%:  25%|██▌       | 99/391 [02:01<04:22,  1.11it/s] \n",
    "# 0-99 training acc: 5.47%, lr=['0.001'], val_acc: 7.53%\n",
    "# iter_one_train_time: 0.8696670532226562 seconds, last one_val_time: 17.034608364105225 seconds\n",
    "\n",
    "# train: 0-148/391 tr_acc: 4.69%, lr=['0.001'], tr_loss: 80.27715301513672, val_acc: 6.74%:  38%|███▊      | 149/391 [03:02<03:39,  1.10it/s]  \n",
    "# 0-149 training acc: 7.81%, lr=['0.001'], val_acc: 6.74%\n",
    "# iter_one_train_time: 0.8959760665893555 seconds, last one_val_time: 16.716905117034912 seconds\n",
    "\n",
    "# train: 0-198/391 tr_acc: 9.38%, lr=['0.001'], tr_loss: 41.17329788208008, val_acc: 7.03%:  51%|█████     | 199/391 [04:04<02:53,  1.10it/s]  \n",
    "# 0-199 training acc: 9.38%, lr=['0.001'], val_acc: 7.03%\n",
    "# iter_one_train_time: 0.8900871276855469 seconds, last one_val_time: 17.06007432937622 seconds\n",
    "\n",
    "# train: 0-217/391 tr_acc: 7.81%, lr=['0.001'], tr_loss: 38.29032516479492, val_acc: 8.52%:  56%|█████▌    | 218/391 [04:39<02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv fc 둘다 이상하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files already downloaded and verified\n",
    "# Files already downloaded and verified\n",
    "# ==================================================\n",
    "# My Num of PARAMS: 9,302,410, system's param_num : 9,346,442\n",
    "# Memory: 35.49MiB at 32-bit\n",
    "# ==================================================\n",
    "# EPOCH 0\n",
    "# train: 0-48/391 tr_acc: 7.03%, lr=['0.001'], tr_loss: 603.5956420898438, val_acc: 0.00%:  13%|█▎        | 49/391 [00:59<05:09,  1.10it/s]  \n",
    "# 0-49 training acc: 8.59%, lr=['0.001'], val_acc: 0.00%\n",
    "# iter_one_train_time: 0.8527345657348633 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-98/391 tr_acc: 5.47%, lr=['0.001'], tr_loss: 75.4881362915039, val_acc: 10.00%:  25%|██▌       | 99/391 [02:01<04:21,  1.12it/s]   \n",
    "# 0-99 training acc: 14.06%, lr=['0.001'], val_acc: 10.00%\n",
    "# iter_one_train_time: 0.8860585689544678 seconds, last one_val_time: 16.647587537765503 seconds\n",
    "\n",
    "# train: 0-148/391 tr_acc: 8.59%, lr=['0.001'], tr_loss: 77.6851577758789, val_acc: 10.00%:  38%|███▊      | 149/391 [03:03<03:37,  1.11it/s]   \n",
    "# 0-149 training acc: 11.72%, lr=['0.001'], val_acc: 10.00%\n",
    "# iter_one_train_time: 0.8722302913665771 seconds, last one_val_time: 16.754177808761597 seconds\n",
    "\n",
    "# train: 0-157/391 tr_acc: 12.50%, lr=['0.001'], tr_loss: 84.20757293701172, val_acc: 10.00%:  40%|████      | 158/391 [03:28<04:36,  1.18s/it]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fc만 이상하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files already downloaded and verified\n",
    "# Files already downloaded and verified\n",
    "# ==================================================\n",
    "# My Num of PARAMS: 9,302,410, system's param_num : 9,346,442\n",
    "# Memory: 35.49MiB at 32-bit\n",
    "# ==================================================\n",
    "# EPOCH 0\n",
    "# train: 0-48/391 tr_acc: 22.66%, lr=['0.001'], tr_loss: 341.66009521484375, val_acc: 0.00%:  13%|█▎        | 49/391 [00:59<05:13,  1.09it/s]\n",
    "# 0-49 training acc: 21.88%, lr=['0.001'], val_acc: 0.00%\n",
    "# iter_one_train_time: 0.870579719543457 seconds, last one_val_time: 0 seconds\n",
    "\n",
    "# train: 0-98/391 tr_acc: 27.34%, lr=['0.001'], tr_loss: 248.8526153564453, val_acc: 24.19%:  25%|██▌       | 99/391 [02:00<04:21,  1.11it/s] \n",
    "# 0-99 training acc: 30.47%, lr=['0.001'], val_acc: 24.19%\n",
    "# iter_one_train_time: 0.8846514225006104 seconds, last one_val_time: 16.405277729034424 seconds\n",
    "\n",
    "# train: 0-148/391 tr_acc: 29.69%, lr=['0.001'], tr_loss: 226.84280395507812, val_acc: 25.52%:  38%|███▊      | 149/391 [03:03<03:39,  1.10it/s]\n",
    "# 0-149 training acc: 25.00%, lr=['0.001'], val_acc: 25.52%\n",
    "# iter_one_train_time: 0.8858129978179932 seconds, last one_val_time: 16.496455907821655 seconds\n",
    "\n",
    "# train: 0-198/391 tr_acc: 33.59%, lr=['0.001'], tr_loss: 160.5304718017578, val_acc: 28.07%:  51%|█████     | 199/391 [04:05<02:49,  1.13it/s] \n",
    "# 0-199 training acc: 24.22%, lr=['0.001'], val_acc: 28.07%\n",
    "# iter_one_train_time: 0.9705793857574463 seconds, last one_val_time: 16.123810291290283 seconds\n",
    "\n",
    "# train: 0-248/391 tr_acc: 33.59%, lr=['0.001'], tr_loss: 206.28160095214844, val_acc: 28.51%:  64%|██████▎   | 249/391 [05:07<02:12,  1.07it/s]\n",
    "# 0-249 training acc: 32.81%, lr=['0.001'], val_acc: 28.51%\n",
    "# iter_one_train_time: 0.8938279151916504 seconds, last one_val_time: 16.548280239105225 seconds\n",
    "\n",
    "# train: 0-298/391 tr_acc: 32.03%, lr=['0.001'], tr_loss: 214.91668701171875, val_acc: 30.11%:  76%|███████▋  | 299/391 [06:10<01:26,  1.07it/s]\n",
    "# 0-299 training acc: 32.03%, lr=['0.001'], val_acc: 30.11%\n",
    "# iter_one_train_time: 0.8992974758148193 seconds, last one_val_time: 16.498471975326538 seconds\n",
    "\n",
    "# train: 0-348/391 tr_acc: 37.50%, lr=['0.001'], tr_loss: 199.61183166503906, val_acc: 29.93%:  89%|████████▉ | 349/391 [07:11<00:37,  1.11it/s]\n",
    "# 0-349 training acc: 37.50%, lr=['0.001'], val_acc: 29.93%\n",
    "# iter_one_train_time: 0.9911439418792725 seconds, last one_val_time: 16.14445948600769 seconds\n",
    "\n",
    "# train: 0-390/391 tr_acc: 47.50%, lr=['0.001'], tr_loss: 126.8869400024414, val_acc: 34.47%: 100%|██████████| 391/391 [08:06<00:00,  1.24s/it] \n",
    "# epoch_time: 486.4687490463257 seconds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
