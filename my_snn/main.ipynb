{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17250/2809884579.py:45: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from spikingjelly.datasets import split_to_train_test_set\n",
    "from spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # dvs 데이터 시각화 코드\n",
    " ##############################################################################################\n",
    "            # mapping = {\n",
    "            #     0: 'Hand Clapping',\n",
    "            #     1: 'Right Hand Wave',\n",
    "            #     2: 'Left Hand Wave',\n",
    "            #     3: 'Right Arm CW',\n",
    "            #     4: 'Right Arm CCW',\n",
    "            #     5: 'Left Arm CW',\n",
    "            #     6: 'Left Arm CCW',\n",
    "            #     7: 'Arm Roll',\n",
    "            #     8: 'Air Drums',\n",
    "            #     9: 'Air Guitar',\n",
    "            #     10: 'Other'\n",
    "            # }\n",
    "def dvs_visualization(inputs, labels, TIME, BATCH):\n",
    "            \n",
    "    what_input = random.randint(0, BATCH - 1)\n",
    "    inputs_for_view = inputs.permute(1, 0, 2, 3, 4)\n",
    "    for i in range(TIME):\n",
    "        # 예시 데이터 생성\n",
    "        data1 = inputs_for_view[what_input][i][0].numpy()  # torch tensor를 numpy 배열로 변환\n",
    "        data2 = inputs_for_view[what_input][i][1].numpy()  # torch tensor를 numpy 배열로 변환\n",
    "\n",
    "        # 데이터 플로팅\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # 1행 2열의 subplot 생성\n",
    "\n",
    "        # 첫 번째 subplot에 데이터1 플로팅\n",
    "        im1 = axs[0].imshow(data1, cmap='viridis', interpolation='nearest')\n",
    "        axs[0].set_title(f'Channel 0\\nLabel: {labels[what_input]}  Time: {i}')  # 라벨값 맵핑하여 제목에 추가\n",
    "        axs[0].set_xlabel('X axis')\n",
    "        axs[0].set_ylabel('Y axis')\n",
    "        axs[0].grid(False)\n",
    "        fig.colorbar(im1, ax=axs[0])  # Color bar 추가\n",
    "\n",
    "        # 두 번째 subplot에 데이터2 플로팅\n",
    "        im2 = axs[1].imshow(data2, cmap='viridis', interpolation='nearest')\n",
    "        axs[1].set_title(f'Channel 1\\nLabel: {labels[what_input]}  Time: {i}')  # 라벨값 맵핑하여 제목에 추가\n",
    "        axs[1].set_xlabel('X axis')\n",
    "        axs[1].set_ylabel('Y axis')\n",
    "        axs[1].grid(False)\n",
    "        fig.colorbar(im2, ax=axs[1])  # Color bar 추가\n",
    "\n",
    "        plt.tight_layout()  # subplot 간 간격 조정\n",
    "        plt.show()\n",
    "    sys.exit(\"종료\")\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    weight_count_print = False, # True # False\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "\n",
    "                    nda_net = False,\n",
    "                    \n",
    "                    domain_il_epoch = 0, # over 0, then domain il mode on\n",
    "\n",
    "                    dvs_clipping = True, \n",
    "                    dvs_duration = 1000000,\n",
    "\n",
    "                    OTTT_sWS_on = True, # True # False\n",
    "                  ):\n",
    "    if OTTT_sWS_on == True:\n",
    "        assert BPTT_on == False and tdBN_on == False and convTrue_fcFalse == True\n",
    "\n",
    "\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "    \n",
    "    \n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    torch.manual_seed(my_seed)\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME,\n",
    "            dvs_clipping,\n",
    "            dvs_duration)\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "    \n",
    "    ## parameter number calculator (안 중요함) ##################################################################################################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    classifier_making = False\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (classifier_making == False):\n",
    "                if (type(kernel) == list):\n",
    "                    for residual_kernel in kernel:\n",
    "                        if (residual_kernel >= 10000 and residual_kernel < 20000): # separable\n",
    "                            residual_kernel -= 10000\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            params_num += (1**2 * past_kernel + bias_param) * residual_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        elif (residual_kernel >= 20000 and residual_kernel < 30000): # depthwise\n",
    "                            residual_kernel -= 20000\n",
    "                            # 'past_kernel' should be same with 'kernel'\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        else:\n",
    "                            params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                            past_kernel = residual_kernel\n",
    "                elif (kernel == 'P' or kernel == 'M'):\n",
    "                    img_size = img_size // 2\n",
    "                elif (kernel == 'D'):\n",
    "                    img_size = 1\n",
    "                elif (kernel == 'L'):\n",
    "                    classifier_making = True\n",
    "                    past_kernel = past_kernel * (img_size**2)\n",
    "                else:\n",
    "                    if (kernel >= 10000 and kernel < 20000): # separable\n",
    "                        kernel -= 10000\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        params_num += (1**2 * past_kernel + bias_param) * kernel\n",
    "                        past_kernel = kernel  \n",
    "                    elif (kernel >= 20000 and kernel < 30000): # depthwise\n",
    "                        kernel -= 20000\n",
    "                        # 'past_kernel' should be same with 'kernel'\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        past_kernel = kernel  \n",
    "                    else:\n",
    "                        params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                        past_kernel = kernel    \n",
    "            else: # classifier making\n",
    "                params_num += (past_kernel + bias_param) * kernel\n",
    "                past_kernel = kernel\n",
    "        \n",
    "        \n",
    "        if classifier_making == False:\n",
    "            past_kernel = past_kernel*img_size*img_size\n",
    "\n",
    "        params_num += (past_kernel + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            # elif (in_channel == 'M'): #it's a holy FC layer!\n",
    "            #     img_size = img_size // 2\n",
    "            else:\n",
    "                print('past_in_channel', past_in_channel)\n",
    "                print('bias_param', bias_param)\n",
    "                print('in_channel', in_channel)\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on,\n",
    "                     OTTT_sWS_on).to(device)\n",
    "        \n",
    "        if (nda_net == True):\n",
    "            net = VGG(cfg = cfg, num_classes=10, batch_norm = tdBN_on, in_c = synapse_conv_in_channels, \n",
    "                      lif_layer_v_threshold=lif_layer_v_threshold, lif_layer_v_decay=lif_layer_v_decay, lif_layer_sg_width=lif_layer_sg_width)\n",
    "            net.T = TIME\n",
    "        net = torch.nn.DataParallel(net) #나중에풀어줘\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)        \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "    ## param num and memory estimation except BN with MY own calculation some lines above ##########################################\n",
    "    real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    if (weight_count_print == True):\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f'Layer: {name} | Number of parameters: {param.numel()}')\n",
    "    # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "    # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "    print('='*50)\n",
    "    print(f\"My Num of PARAMS: {params_num:,}, system's param_num : {real_param_num:,}\")\n",
    "    memory = params_num / 8 / 1024 / 1024 # MB\n",
    "    precision = 32\n",
    "    memory = memory * precision \n",
    "    print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "    print('='*50)\n",
    "    ##############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    if (OTTT_sWS_on == True):\n",
    "        # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=100)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    val_acc = 0\n",
    "    val_acc_now = 0\n",
    "    elapsed_time_val = 0\n",
    "    iter_acc_array = np.array([])\n",
    "    tr_acc_array = np.array([])\n",
    "    val_acc_now_array = np.array([])\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        print('EPOCH', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # if (domain_il_epoch>0 and which_data == 'PMNIST'):\n",
    "        #     k = epoch // domain_il_epoch\n",
    "        #     xtrain=data[k]['train']['x']\n",
    "        #     ytrain=data[k]['train']['y']\n",
    "        #     xtest =data[k]['test']['x']\n",
    "        #     ytest =data[k]['test']['y']\n",
    "\n",
    "        \n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if (ddp_on == True):\n",
    "            if torch.distributed.get_rank() == 0:   \n",
    "                iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        else:\n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "        \n",
    "        #### validation_interval이 batch size보다 작을 시 validation_interval을 batch size로 맞춰줌#############\n",
    "        validation_interval2 = validation_interval\n",
    "        if (validation_interval > len(iterator)):\n",
    "            validation_interval2 = len(iterator)\n",
    "        ##################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                # print('x_len',x_len)\n",
    "                # mask = padded_sequence_mask(x_len)\n",
    "                # max_time_step = x_len.max()\n",
    "                # min_time_step = x_len.min()\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "                    \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                \n",
    "            # # DVS에서 time duration으로 잘랐을 때는 timestep 맞춰주자 --> data 가져올 때, 그 함수 안에서 처리함.\n",
    "            # if (dvs_duration > 0): \n",
    "            #     # inputs.size(1)를 TIME으로 맞추기\n",
    "            #     T, *spatial_dims = inputs.shape\n",
    "            #     if T > TIME:\n",
    "            #         inputs = inputs[:TIME]\n",
    "            #     else:\n",
    "            #         inputs = torch.cat([inputs, torch.zeros(TIME - T, *spatial_dims)], dim=0)\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "                \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH)\n",
    "            # ######################################################################################################\n",
    "\n",
    "\n",
    "            ## device로 보내주기 ######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "            # inputs: [Batch, Time, Channel, Height, Width] \n",
    "            #################################################################################################\n",
    "\n",
    "\n",
    "            ### input --> net --> output #####################################################\n",
    "            outputs = net(inputs)\n",
    "            ##################################################################################\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            batch = BATCH \n",
    "            if labels.size(0) != BATCH: \n",
    "                batch = labels.size(0)\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = (predicted[0:batch] == labels).sum().item()\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc = correct / total\n",
    "            if i % verbose_interval == verbose_interval-1:\n",
    "                print(f'{epoch}-{i} training acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "            iter_acc_string = f'{epoch}-{i}/{len(train_loader)} iter_acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ## loss, backward ##########################################\n",
    "            loss = criterion(outputs[0:batch,:], labels)\n",
    "            loss.backward()\n",
    "            ############################################################\n",
    "\n",
    "            ### gradinet verbose ##########################################\n",
    "            if (gradient_verbose == True):\n",
    "                if (i % verbose_interval == verbose_interval-1):\n",
    "                    print('\\n\\nepoch', epoch, 'iter', i)\n",
    "                    for name, param in net.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            print('\\n\\n\\n\\n' , name, param.grad)\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ## weight 업데이트!! ##################################\n",
    "            optimizer.step()\n",
    "            ################################################################\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "            if (i % verbose_interval == verbose_interval-1):\n",
    "                print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\\n\")\n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i % validation_interval2 == validation_interval2-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data in test_loader:\n",
    "                        ## data loading & semi-pre-processing ##########################################################\n",
    "                        if len(data) == 2:\n",
    "                            inputs, labels = data\n",
    "                            # 처리 로직 작성\n",
    "                        elif len(data) == 3:\n",
    "                            inputs, labels, x_len = data\n",
    "                            # print('x_len',x_len)\n",
    "                            # mask = padded_sequence_mask(x_len)\n",
    "                            # max_time_step = x_len.max()\n",
    "                            # min_time_step = x_len.min()\n",
    "                            # B, T, *spatial_dims = inputs.shape\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        batch = BATCH \n",
    "                        if labels.size(0) != BATCH: \n",
    "                            batch = labels.size(0)\n",
    "                        correct += (predicted[0:batch] == labels).sum().item()\n",
    "                        val_loss = criterion(outputs[0:batch,:], labels)\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc < val_acc_now:\n",
    "                    val_acc = val_acc_now\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "                    # torch.save(net, f\"net_save/save_now_net_{unique_name}.pth\")\n",
    "                    # torch.save(net.module.state_dict(), f\"net_save/save_now_net_weights2_{unique_name}.pth\")\n",
    "                    # torch.save(net.module, f\"net_save/save_now_net2_{unique_name}.pth\")\n",
    "            ####################################################################################################################################################\n",
    "            iterator.set_description(f\"iter_acc: {iter_acc_string}, iter_loss: {loss}, val_acc: {100 * val_acc_now:.2f}%\")  \n",
    "            wandb.log({\"iter_acc\": iter_acc}, step=i+epoch*len(train_loader))\n",
    "            wandb.log({\"tr_acc\": tr_acc}, step=i+epoch*len(train_loader))\n",
    "            wandb.log({\"val_acc_now\": val_acc_now}, step=i+epoch*len(train_loader))\n",
    "            wandb.define_metric(\"val_acc_now\", summary=\"max\")\n",
    "            iter_acc_array = np.append(iter_acc_array, iter_acc)\n",
    "            tr_acc_array = np.append(tr_acc_array, tr_acc)\n",
    "            val_acc_now_array = np.append(val_acc_now_array, val_acc_now)\n",
    "            base_name = f'{current_time}'\n",
    "            iter_acc_file_name_time = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "            tr_acc_file_name_time = f'result_save/{base_name}_tr_acc_array_{unique_name}.npy'\n",
    "            val_acc_file_name_time = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "            hyperparameters_file_name_time = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "            hyperparameters['current epoch'] = epoch\n",
    "\n",
    "            ### # 덮어쓰기 하기 싫으면 주석 풀어서 사용 (시간마다 새로 쓰기) 비추천 ########################\n",
    "            # np.save(iter_acc_file_name_time, iter_acc_array)\n",
    "            # np.save(tr_acc_file_name_time, iter_acc_array)\n",
    "            # np.save(val_acc_file_name_time, val_acc_now_array)\n",
    "            # with open(hyperparameters_file_name_time, 'w') as f:\n",
    "            #     json.dump(hyperparameters, f, indent=4)\n",
    "            #######################################################################################\n",
    "\n",
    "            # np.save(f'result_save/iter_acc_array_{unique_name}.npy', iter_acc_array)\n",
    "            # np.save(f'result_save/tr_acc_array_{unique_name}.npy', tr_acc_array)\n",
    "            # np.save(f'result_save/val_acc_now_array_{unique_name}.npy', val_acc_now_array)\n",
    "            with open(f'result_save/hyperparameters_{unique_name}.json', 'w') as f:\n",
    "                json.dump(hyperparameters, f, indent=4)\n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "                \n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "        # 실행 시간 계산\n",
    "        epoch_time_end = time.time()\n",
    "        print(f\"epoch_time: {epoch_time_end - epoch_start_time} seconds\\n\") \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240724_111422-ezap8m8t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ezap8m8t' target=\"_blank\">major-star-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ezap8m8t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ezap8m8t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "DataParallel(\n",
      "  (module): MY_SNN_CONV(\n",
      "    (layers): OTTTSequential(\n",
      "      (0): SYNAPSE_CONV_trace()\n",
      "      (1): LIF_layer_trace()\n",
      "      (2): Scale()\n",
      "      (3): SYNAPSE_CONV_trace()\n",
      "      (4): LIF_layer_trace()\n",
      "      (5): Scale()\n",
      "      (6): DimChanger_for_pooling(\n",
      "        (ann_module): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (7): SYNAPSE_CONV_trace()\n",
      "      (8): LIF_layer_trace()\n",
      "      (9): Scale()\n",
      "      (10): SYNAPSE_CONV_trace()\n",
      "      (11): LIF_layer_trace()\n",
      "      (12): Scale()\n",
      "      (13): DimChanger_for_pooling(\n",
      "        (ann_module): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (14): SYNAPSE_CONV_trace()\n",
      "      (15): LIF_layer_trace()\n",
      "      (16): Scale()\n",
      "      (17): SYNAPSE_CONV_trace()\n",
      "      (18): LIF_layer_trace()\n",
      "      (19): Scale()\n",
      "      (20): DimChanger_for_pooling(\n",
      "        (ann_module): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (21): SYNAPSE_CONV_trace()\n",
      "      (22): LIF_layer_trace()\n",
      "      (23): Scale()\n",
      "      (24): SYNAPSE_CONV_trace()\n",
      "      (25): LIF_layer_trace()\n",
      "      (26): Scale()\n",
      "      (27): DimChanger_for_pooling(\n",
      "        (ann_module): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      )\n",
      "      (28): DimChanger_for_FC()\n",
      "      (29): SYNAPSE_FC_trace()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 9,225,610, system's param_num : 9,228,362\n",
      "Memory: 35.19MiB at 32-bit\n",
      "==================================================\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-243/391 iter_acc: 20.31%, lr=['0.6'], iter_loss: 0.35282763838768005, val_acc: 0.00%:  62%|██████▏   | 244/391 [01:50<01:06,  2.21it/s]"
     ]
    }
   ],
   "source": [
    "### my_snn control board ########################\n",
    "decay = 0.5 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "wandb.init(project= f'my_snn {unique_name}')\n",
    "my_snn_system(  devices = \"2\",\n",
    "                unique_name = unique_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 6 , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 128, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "                #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'CIFAR10',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.0,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 1.0, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = True, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = [64],\n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], #ottt\n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [], \n",
    "                \n",
    "                net_print = True, # True # False\n",
    "                weight_count_print = False, # True # False\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_{unique_name}.pth\",\n",
    "                learning_rate = 0.6, # default 0.001  # ottt 0.1 0.00001 # nda 0.001 \n",
    "                epoch_num = 300,\n",
    "                verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "                validation_interval = 999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "                gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False,   # True # False\n",
    "\n",
    "                nda_net = False,   # True # False\n",
    "\n",
    "                domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "                dvs_clipping = True, # dvs zero&one  # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                dvs_duration = 1000000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                #있는 데이터들 #gesture 1000000 #nmnist 10000\n",
    "\n",
    "                OTTT_sWS_on = True, # True # False # BPTT끄고, CONV에만 적용됨.\n",
    "                \n",
    "                ) \n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling\n",
    "# 이 낫다. \n",
    " \n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 02uhuz1p\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/02uhuz1p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hghhiqdy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.1911562407482998\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240724_111153-hghhiqdy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hghhiqdy' target=\"_blank\">lyric-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/02uhuz1p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/02uhuz1p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/02uhuz1p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/02uhuz1p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hghhiqdy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hghhiqdy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "DataParallel(\n",
      "  (module): MY_SNN_CONV(\n",
      "    (layers): OTTTSequential(\n",
      "      (0): SYNAPSE_CONV_trace()\n",
      "      (1): LIF_layer_trace()\n",
      "      (2): Scale()\n",
      "      (3): SYNAPSE_CONV_trace()\n",
      "      (4): LIF_layer_trace()\n",
      "      (5): Scale()\n",
      "      (6): DimChanger_for_pooling(\n",
      "        (ann_module): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (7): SYNAPSE_CONV_trace()\n",
      "      (8): LIF_layer_trace()\n",
      "      (9): Scale()\n",
      "      (10): SYNAPSE_CONV_trace()\n",
      "      (11): LIF_layer_trace()\n",
      "      (12): Scale()\n",
      "      (13): DimChanger_for_pooling(\n",
      "        (ann_module): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (14): SYNAPSE_CONV_trace()\n",
      "      (15): LIF_layer_trace()\n",
      "      (16): Scale()\n",
      "      (17): SYNAPSE_CONV_trace()\n",
      "      (18): LIF_layer_trace()\n",
      "      (19): Scale()\n",
      "      (20): DimChanger_for_pooling(\n",
      "        (ann_module): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (21): SYNAPSE_CONV_trace()\n",
      "      (22): LIF_layer_trace()\n",
      "      (23): Scale()\n",
      "      (24): SYNAPSE_CONV_trace()\n",
      "      (25): LIF_layer_trace()\n",
      "      (26): Scale()\n",
      "      (27): DimChanger_for_pooling(\n",
      "        (ann_module): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      )\n",
      "      (28): DimChanger_for_FC()\n",
      "      (29): SYNAPSE_FC_trace()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 9,225,610, system's param_num : 9,228,362\n",
      "Memory: 35.19MiB at 32-bit\n",
      "==================================================\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-258/391 iter_acc: 10.94%, lr=['1.1911562407482998'], iter_loss: 0.3571101129055023, val_acc: 0.00%:  66%|██████▌   | 258/391 [01:56<00:59,  2.23it/s] "
     ]
    }
   ],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes',\n",
    "#     'name': 'my_snn_sweep',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_now'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         \"learning_rate\": {\"min\": 0.1, \"max\": 2.0},\n",
    "#         \"batch_size\": {\"values\": [64, 96, 128]},\n",
    "#         \"decay\": {\"values\": [0.3,0.4,0.5,0.6,0.7,0.8,0.875,0.9]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     unique_name = unique_name_hyper ## 이거 설정하면 새로운 경로에 모두 save\n",
    "    \n",
    "#     wandb.init()\n",
    "#     learning_rate  =  wandb.config.learning_rate\n",
    "#     batch_size  =  wandb.config.batch_size\n",
    "#     decay  =  wandb.config.decay\n",
    "\n",
    "#     my_snn_system(  devices = \"3\",\n",
    "#                     unique_name = unique_name,\n",
    "#                     my_seed = 42,\n",
    "#                     TIME = 6 , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                     BATCH = batch_size, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                     IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\n",
    "#                     # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                     #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "#                     # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                     which_data = 'CIFAR10',\n",
    "#     # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "#     # 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                     # CLASS_NUM = 10,\n",
    "#                     data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                     rate_coding = False, # True # False\n",
    "\n",
    "#                     lif_layer_v_init = 0.0,\n",
    "#                     lif_layer_v_decay = decay,\n",
    "#                     lif_layer_v_threshold = 1.0,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "#                     lif_layer_v_reset = 0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                     lif_layer_sg_width = 1.0, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                     # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                     synapse_conv_kernel_size = 3,\n",
    "#                     synapse_conv_stride = 1,\n",
    "#                     synapse_conv_padding = 1,\n",
    "#                     synapse_conv_trace_const1 = 1,\n",
    "#                     synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                     # synapse_fc_out_features = CLASS_NUM,\n",
    "#                     synapse_fc_trace_const1 = 1,\n",
    "#                     synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                     pre_trained = False, # True # False\n",
    "#                     convTrue_fcFalse = True, # True # False\n",
    "\n",
    "#                     # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                     # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                     # cfg = [64],\n",
    "#                     # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                     cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                     # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], #ottt\n",
    "#                     # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                     # cfg = [20001,10001], # depthwise, separable\n",
    "#                     # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                     # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                     # cfg = [], \n",
    "                    \n",
    "#                     net_print = True, # True # False\n",
    "#                     weight_count_print = False, # True # False\n",
    "                    \n",
    "#                     pre_trained_path = f\"net_save/save_now_net_{unique_name}.pth\",\n",
    "#                     learning_rate = learning_rate, # default 0.001  # ottt 0.1 0.00001 # nda 0.001 \n",
    "#                     epoch_num = 4,\n",
    "#                     verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "#                     validation_interval = 999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "#                     tdBN_on = False,  # True # False\n",
    "#                     BN_on = False,  # True # False\n",
    "                    \n",
    "#                     surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                    \n",
    "#                     gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                     BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "#                     optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                     scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                    \n",
    "#                     ddp_on = False,   # True # False\n",
    "\n",
    "#                     nda_net = False,   # True # False\n",
    "\n",
    "#                     domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                    \n",
    "#                     dvs_clipping = True, # dvs zero&one  # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                     dvs_duration = 1000000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                     #있는 데이터들 #gesture 1000000 #nmnist 10000\n",
    "\n",
    "#                     OTTT_sWS_on = True, # True # False # BPTT끄고, CONV에만 적용됨.\n",
    "                    \n",
    "#                     ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_array_to_match_length(array1, array2):\n",
    "    if len(array1) > len(array2):\n",
    "        padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "        return array1, padded_array2\n",
    "    elif len(array2) > len(array1):\n",
    "        padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "        return padded_array1, array2\n",
    "    else:\n",
    "        return array1, array2\n",
    "def load_hyperparameters(filename=f'result_save/hyperparameters_{unique_name}.json'):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_time = '20240628_110116'\n",
    "base_name = f'{current_time}'\n",
    "iter_acc_file_name = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "val_acc_file_name = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "hyperparameters_file_name = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "### if you want to just see most recent train and val acc###########################\n",
    "iter_acc_file_name = f'result_save/iter_acc_array_{unique_name}.npy'\n",
    "tr_acc_file_name = f'result_save/tr_acc_array_{unique_name}.npy'\n",
    "val_acc_file_name = f'result_save/val_acc_now_array_{unique_name}.npy'\n",
    "hyperparameters_file_name = f'result_save/hyperparameters_{unique_name}.json'\n",
    "\n",
    "loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "which_data = hyperparameters['which_data']\n",
    "BPTT_on = hyperparameters['BPTT_on']\n",
    "current_epoch = hyperparameters['current epoch']\n",
    "surrogate = hyperparameters['surrogate']\n",
    "cfg = hyperparameters['cfg']\n",
    "tdBN_on = hyperparameters['tdBN_on']\n",
    "BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# 텍스트 추가\n",
    "plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "plt.title(title)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0)  # x축을 0부터 시작\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
