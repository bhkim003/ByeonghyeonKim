{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21167/2361114005.py:45: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from spikingjelly.datasets import split_to_train_test_set\n",
    "from spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import torchneuromorphic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:79: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:81: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:99: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:101: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:79: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:81: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:99: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchneuromorphic-0.3.7-py3.8.egg/torchneuromorphic/ntidigits/ntidigits_dataloaders.py:101: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # dvs 데이터 시각화 코드\n",
    " ##############################################################################################\n",
    "            # mapping = {\n",
    "            #     0: 'Hand Clapping',\n",
    "            #     1: 'Right Hand Wave',\n",
    "            #     2: 'Left Hand Wave',\n",
    "            #     3: 'Right Arm CW',\n",
    "            #     4: 'Right Arm CCW',\n",
    "            #     5: 'Left Arm CW',\n",
    "            #     6: 'Left Arm CCW',\n",
    "            #     7: 'Arm Roll',\n",
    "            #     8: 'Air Drums',\n",
    "            #     9: 'Air Guitar',\n",
    "            #     10: 'Other'\n",
    "            # }\n",
    "def dvs_visualization(inputs, labels, TIME, BATCH):\n",
    "            \n",
    "    what_input = random.randint(0, BATCH - 1)\n",
    "    inputs_for_view = inputs.permute(1, 0, 2, 3, 4)\n",
    "    for i in range(TIME):\n",
    "        # 예시 데이터 생성\n",
    "        data1 = inputs_for_view[what_input][i][0].numpy()  # torch tensor를 numpy 배열로 변환\n",
    "        data2 = inputs_for_view[what_input][i][1].numpy()  # torch tensor를 numpy 배열로 변환\n",
    "\n",
    "        # 데이터 플로팅\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # 1행 2열의 subplot 생성\n",
    "\n",
    "        # 첫 번째 subplot에 데이터1 플로팅\n",
    "        im1 = axs[0].imshow(data1, cmap='viridis', interpolation='nearest')\n",
    "        axs[0].set_title(f'Channel 0\\nLabel: {labels[what_input]}  Time: {i}')  # 라벨값 맵핑하여 제목에 추가\n",
    "        axs[0].set_xlabel('X axis')\n",
    "        axs[0].set_ylabel('Y axis')\n",
    "        axs[0].grid(False)\n",
    "        fig.colorbar(im1, ax=axs[0])  # Color bar 추가\n",
    "\n",
    "        # 두 번째 subplot에 데이터2 플로팅\n",
    "        im2 = axs[1].imshow(data2, cmap='viridis', interpolation='nearest')\n",
    "        axs[1].set_title(f'Channel 1\\nLabel: {labels[what_input]}  Time: {i}')  # 라벨값 맵핑하여 제목에 추가\n",
    "        axs[1].set_xlabel('X axis')\n",
    "        axs[1].set_ylabel('Y axis')\n",
    "        axs[1].grid(False)\n",
    "        fig.colorbar(im2, ax=axs[1])  # Color bar 추가\n",
    "\n",
    "        plt.tight_layout()  # subplot 간 간격 조정\n",
    "        plt.show()\n",
    "    sys.exit(\"종료\")\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    weight_count_print = False, # True # False\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "\n",
    "                    nda_net = False,\n",
    "                    \n",
    "                    domain_il_epoch = 0, # over 0, then domain il mode on\n",
    "\n",
    "                    dvs_clipping = True, \n",
    "                    dvs_duration = 1000000,\n",
    "                  ):\n",
    "    # 함수 내 모든 로컬 변수 저장\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "\n",
    "\n",
    "    \n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME,\n",
    "            dvs_clipping,\n",
    "            dvs_duration)\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    \n",
    "    ## parameter number calculator (안 중요함) ##################################################################################################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    classifier_making = False\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (classifier_making == False):\n",
    "                if (type(kernel) == list):\n",
    "                    for residual_kernel in kernel:\n",
    "                        if (residual_kernel >= 10000 and residual_kernel < 20000): # separable\n",
    "                            residual_kernel -= 10000\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            params_num += (1**2 * past_kernel + bias_param) * residual_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        elif (residual_kernel >= 20000 and residual_kernel < 30000): # depthwise\n",
    "                            residual_kernel -= 20000\n",
    "                            # 'past_kernel' should be same with 'kernel'\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        else:\n",
    "                            params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                            past_kernel = residual_kernel\n",
    "                elif (kernel == 'P' or kernel == 'M'):\n",
    "                    img_size = img_size // 2\n",
    "                elif (kernel == 'D'):\n",
    "                    img_size = 1\n",
    "                elif (kernel == 'L'):\n",
    "                    classifier_making = True\n",
    "                    past_kernel = past_kernel * (img_size**2)\n",
    "                else:\n",
    "                    if (kernel >= 10000 and kernel < 20000): # separable\n",
    "                        kernel -= 10000\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        params_num += (1**2 * past_kernel + bias_param) * kernel\n",
    "                        past_kernel = kernel  \n",
    "                    elif (kernel >= 20000 and kernel < 30000): # depthwise\n",
    "                        kernel -= 20000\n",
    "                        # 'past_kernel' should be same with 'kernel'\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        past_kernel = kernel  \n",
    "                    else:\n",
    "                        params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                        past_kernel = kernel    \n",
    "            else: # classifier making\n",
    "                params_num += (past_kernel + bias_param) * kernel\n",
    "                past_kernel = kernel\n",
    "        \n",
    "        \n",
    "        if classifier_making == False:\n",
    "            past_kernel = past_kernel*img_size*img_size\n",
    "\n",
    "        params_num += (past_kernel + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            # elif (in_channel == 'M'): #it's a holy FC layer!\n",
    "            #     img_size = img_size // 2\n",
    "            else:\n",
    "                print('past_in_channel', past_in_channel)\n",
    "                print('bias_param', bias_param)\n",
    "                print('in_channel', in_channel)\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        \n",
    "        if (nda_net == True):\n",
    "            net = VGG(cfg = cfg, num_classes=10, batch_norm = tdBN_on, in_c = synapse_conv_in_channels, \n",
    "                      lif_layer_v_threshold=lif_layer_v_threshold, lif_layer_v_decay=lif_layer_v_decay, lif_layer_sg_width=lif_layer_sg_width)\n",
    "            net.T = TIME\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)        \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## param num and memory estimation except BN with MY own calculation some lines above ##########################################\n",
    "    real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    if (weight_count_print == True):\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f'Layer: {name} | Number of parameters: {param.numel()}')\n",
    "    # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "    # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "    print('='*50)\n",
    "    print(f\"My Num of PARAMS: {params_num:,}, system's param_num : {real_param_num:,}\")\n",
    "    memory = params_num / 8 / 1024 / 1024 # MB\n",
    "    precision = 32\n",
    "    memory = memory * precision \n",
    "    print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "    print('='*50)\n",
    "    ##############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## criterion ##########################################\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    ## criterion ##########################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=100)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    val_acc = 0\n",
    "    val_acc_now = 0\n",
    "    elapsed_time_val = 0\n",
    "    iter_acc_array = np.array([])\n",
    "    tr_acc_array = np.array([])\n",
    "    val_acc_now_array = np.array([])\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        print('EPOCH', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # if (domain_il_epoch>0 and which_data == 'PMNIST'):\n",
    "        #     k = epoch // domain_il_epoch\n",
    "        #     xtrain=data[k]['train']['x']\n",
    "        #     ytrain=data[k]['train']['y']\n",
    "        #     xtest =data[k]['test']['x']\n",
    "        #     ytest =data[k]['test']['y']\n",
    "\n",
    "        \n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if (ddp_on == True):\n",
    "            if torch.distributed.get_rank() == 0:   \n",
    "                iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        else:\n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "        \n",
    "        #### validation_interval이 batch size보다 작을 시 validation_interval을 batch size로 맞춰줌#############\n",
    "        validation_interval2 = validation_interval\n",
    "        if (validation_interval > len(iterator)):\n",
    "            validation_interval2 = len(iterator)\n",
    "        ##################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                # print('x_len',x_len)\n",
    "                # mask = padded_sequence_mask(x_len)\n",
    "                # max_time_step = x_len.max()\n",
    "                # min_time_step = x_len.min()\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "                    \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                \n",
    "            # # DVS에서 time duration으로 잘랐을 때는 timestep 맞춰주자 --> data 가져올 때, 그 함수 안에서 처리함.\n",
    "            # if (dvs_duration > 0): \n",
    "            #     # inputs.size(1)를 TIME으로 맞추기\n",
    "            #     T, *spatial_dims = inputs.shape\n",
    "            #     if T > TIME:\n",
    "            #         inputs = inputs[:TIME]\n",
    "            #     else:\n",
    "            #         inputs = torch.cat([inputs, torch.zeros(TIME - T, *spatial_dims)], dim=0)\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "                \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH)\n",
    "            # ######################################################################################################\n",
    "\n",
    "\n",
    "            ## device로 보내주기 ######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "            # inputs: [Batch, Time, Channel, Height, Width] \n",
    "            #################################################################################################\n",
    "\n",
    "\n",
    "            ### input --> net --> output #####################################################\n",
    "            outputs = net(inputs)\n",
    "            ##################################################################################\n",
    "\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            batch = BATCH \n",
    "            if labels.size(0) != BATCH: \n",
    "                batch = labels.size(0)\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = (predicted[0:batch] == labels).sum().item()\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc = correct / total\n",
    "            if i % verbose_interval == verbose_interval-1:\n",
    "                print(f'{epoch}-{i} training acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "            iter_acc_string = f'{epoch}-{i}/{len(train_loader)} iter_acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ## loss, backward ##########################################\n",
    "            loss = criterion(outputs[0:batch,:], labels)\n",
    "            loss.backward()\n",
    "            ############################################################\n",
    "\n",
    "\n",
    "            ### gradinet verbose ##########################################\n",
    "            if (gradient_verbose == True):\n",
    "                if (i % verbose_interval == verbose_interval-1):\n",
    "                    print('\\n\\nepoch', epoch, 'iter', i)\n",
    "                    for name, param in net.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            print('\\n\\n\\n\\n' , name, param.grad)\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ## weight 업데이트!! ##################################\n",
    "            optimizer.step()\n",
    "            ################################################################\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "            if (i % verbose_interval == verbose_interval-1):\n",
    "                print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\\n\")\n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i % validation_interval2 == validation_interval2-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data in test_loader:\n",
    "                        ## data loading & semi-pre-processing ##########################################################\n",
    "                        if len(data) == 2:\n",
    "                            inputs, labels = data\n",
    "                            # 처리 로직 작성\n",
    "                        elif len(data) == 3:\n",
    "                            inputs, labels, x_len = data\n",
    "                            # print('x_len',x_len)\n",
    "                            # mask = padded_sequence_mask(x_len)\n",
    "                            # max_time_step = x_len.max()\n",
    "                            # min_time_step = x_len.min()\n",
    "                            # B, T, *spatial_dims = inputs.shape\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        batch = BATCH \n",
    "                        if labels.size(0) != BATCH: \n",
    "                            batch = labels.size(0)\n",
    "                        correct += (predicted[0:batch] == labels).sum().item()\n",
    "                        val_loss = criterion(outputs[0:batch,:], labels)\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc < val_acc_now:\n",
    "                    val_acc = val_acc_now\n",
    "                    torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                    torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                    torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                    torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "            ####################################################################################################################################################\n",
    "            iterator.set_description(f\"iter_acc: {iter_acc_string}, iter_loss: {loss}, val_acc: {100 * val_acc_now:.2f}%\")  \n",
    "\n",
    "            iter_acc_array = np.append(iter_acc_array, iter_acc)\n",
    "            tr_acc_array = np.append(tr_acc_array, tr_acc)\n",
    "            val_acc_now_array = np.append(val_acc_now_array, val_acc_now)\n",
    "            base_name = f'{current_time}'\n",
    "            iter_acc_file_name = f'result_save/{base_name}_iter_acc_array.npy'\n",
    "            tr_acc_file_name = f'result_save/{base_name}_tr_acc_array.npy'\n",
    "            val_acc_file_name = f'result_save/{base_name}_val_acc_now_array.npy'\n",
    "            hyperparameters_file_name = f'result_save/{base_name}_hyperparameters.json'\n",
    "\n",
    "            hyperparameters['current epoch'] = epoch\n",
    "\n",
    "            # 덮어쓰기 하기 싫으면 주석 풀어서 사용 (시간마다 새로 쓰기)\n",
    "            # np.save(iter_acc_file_name, iter_acc_array)\n",
    "            # np.save(val_acc_file_name, val_acc_now_array)\n",
    "            # with open(hyperparameters_file_name, 'w') as f:\n",
    "            #     json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "            np.save('result_save/iter_acc_array.npy', iter_acc_array)\n",
    "            np.save('result_save/tr_acc_array.npy', tr_acc_array)\n",
    "            np.save('result_save/val_acc_now_array.npy', val_acc_now_array)\n",
    "            with open('result_save/hyperparameters.json', 'w') as f:\n",
    "                json.dump(hyperparameters, f, indent=4)\n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "                \n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "        # 실행 시간 계산\n",
    "        epoch_time_end = time.time()\n",
    "        print(f\"epoch_time: {epoch_time_end - epoch_start_time} seconds\\n\") \n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==================================================\n",
      "My Num of PARAMS: 9,302,410, system's param_num : 9,335,434\n",
      "Memory: 35.49MiB at 32-bit\n",
      "==================================================\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-390/391 iter_acc: 13.75%, lr=['0.1'], iter_loss: 2.3015778064727783, val_acc: 22.15%: 100%|██████████| 391/391 [09:21<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_time: 562.1537971496582 seconds\n",
      "\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "iter_acc: 1-390/391 iter_acc: 25.00%, lr=['0.09999383162408304'], iter_loss: 2.1133553981781006, val_acc: 26.96%: 100%|██████████| 391/391 [09:39<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_time: 579.8633217811584 seconds\n",
      "\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "iter_acc: 2-47/391 iter_acc: 19.53%, lr=['0.09997532801828658'], iter_loss: 2.3999505043029785, val_acc: 26.96%:  12%|█▏        | 48/391 [01:08<08:07,  1.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;66;03m# 0.875 0.25 0.125 0.75 0.5\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nda 0.25 # ottt 0.5\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmy_snn_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2,3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmy_seed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mTIME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBATCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# DVS_CIFAR10 할거면 time 10으로 해라\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwhich_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCIFAR10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;43;03m# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;43;03m# 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# CLASS_NUM = 10,\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# YOU NEED TO CHANGE THIS\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrate_coding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_reset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_sg_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# # surrogate sigmoid 쓸 때는 의미없음\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_kernel_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_padding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_trace_const1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_trace_const2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lif_layer_v_decay\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# synapse_fc_out_features = CLASS_NUM,\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_fc_trace_const1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_fc_trace_const2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lif_layer_v_decay\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpre_trained\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconvTrue_fcFalse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64],\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [20001,10001], # depthwise, separable\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64,20064,10001], # vanilla conv, depthwise, separable\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [], \u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnet_print\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweight_count_print\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpre_trained_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnet_save/save_now_net.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# default 0.001  # ottt 0.1  # nda 0.001\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepoch_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m999999999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m999999999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#숫자 크게 하면 에포크 마지막 iter 때 val 함\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtdBN_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBN_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                \u001b[49m\u001b[43msurrogate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'rectangle' 'sigmoid' 'rough_rectangle'\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgradient_verbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False  # weight gradient 각 layer마다 띄워줌\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBPTT_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer_what\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSGD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'SGD' 'Adam', 'RMSprop'\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m                \u001b[49m\u001b[43mscheduler_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCosineAnnealingLR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[43mddp_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnda_net\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdomain_il_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdvs_clipping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dvs zero&one  # gesture, cifar-dvs2, nmnist, ncaltech101\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdvs_duration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#있는 데이터들 #gesture 1000000 #nmnist 10000\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 395\u001b[0m, in \u001b[0;36mmy_snn_system\u001b[0;34m(devices, my_seed, TIME, BATCH, IMAGE_SIZE, which_data, data_path, rate_coding, lif_layer_v_init, lif_layer_v_decay, lif_layer_v_threshold, lif_layer_v_reset, lif_layer_sg_width, synapse_conv_kernel_size, synapse_conv_stride, synapse_conv_padding, synapse_conv_trace_const1, synapse_conv_trace_const2, synapse_fc_trace_const1, synapse_fc_trace_const2, pre_trained, convTrue_fcFalse, cfg, net_print, weight_count_print, pre_trained_path, learning_rate, epoch_num, verbose_interval, validation_interval, tdBN_on, BN_on, surrogate, gradient_verbose, BPTT_on, optimizer_what, scheduler_name, ddp_on, nda_net, domain_il_epoch, dvs_clipping, dvs_duration)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m################################################################\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m## loss, backward ##########################################\u001b[39;00m\n\u001b[1;32m    394\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs[\u001b[38;5;241m0\u001b[39m:batch,:], labels)\n\u001b[0;32m--> 395\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m############################################################\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m### gradinet verbose ##########################################\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (gradient_verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### my_snn control board ########################\n",
    "decay = 0.5 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "my_snn_system(  devices = \"2,3\",\n",
    "                my_seed = 42,\n",
    "                TIME = 6 , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 128, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "                #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'CIFAR10',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.0,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 1.0, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = True, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = [64],\n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [], \n",
    "                \n",
    "                net_print = False, # True # False\n",
    "                weight_count_print = False, # True # False\n",
    "                pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                learning_rate = 0.1, # default 0.001  # ottt 0.1  # nda 0.001\n",
    "                epoch_num = 200,\n",
    "                verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "                validation_interval = 999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = True,  # True # False\n",
    "                \n",
    "                surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "                gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False,   # True # False\n",
    "\n",
    "                nda_net = False,   # True # False\n",
    "\n",
    "                domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "                dvs_clipping = True, # dvs zero&one  # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                dvs_duration = 1000000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                #있는 데이터들 #gesture 1000000 #nmnist 10000\n",
    "                ) \n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def pad_array_to_match_length(array1, array2):\n",
    "    if len(array1) > len(array2):\n",
    "        padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "        return array1, padded_array2\n",
    "    elif len(array2) > len(array1):\n",
    "        padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "        return padded_array1, array2\n",
    "    else:\n",
    "        return array1, array2\n",
    "def load_hyperparameters(filename='hyperparameters.json'):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_time = '20240628_110116'\n",
    "base_name = f'{current_time}'\n",
    "iter_acc_file_name = f'result_save/{base_name}_iter_acc_array.npy'\n",
    "val_acc_file_name = f'result_save/{base_name}_val_acc_now_array.npy'\n",
    "hyperparameters_file_name = f'result_save/{base_name}_hyperparameters.json'\n",
    "\n",
    "### if you want to just see most recent train and val acc###########################\n",
    "iter_acc_file_name = 'result_save/iter_acc_array.npy'\n",
    "tr_acc_file_name = 'result_save/tr_acc_array.npy'\n",
    "val_acc_file_name = 'result_save/val_acc_now_array.npy'\n",
    "hyperparameters_file_name = 'result_save/hyperparameters.json'\n",
    "\n",
    "loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "which_data = hyperparameters['which_data']\n",
    "BPTT_on = hyperparameters['BPTT_on']\n",
    "current_epoch = hyperparameters['current epoch']\n",
    "surrogate = hyperparameters['surrogate']\n",
    "cfg = hyperparameters['cfg']\n",
    "tdBN_on = hyperparameters['tdBN_on']\n",
    "BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# 텍스트 추가\n",
    "plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "plt.title(title)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0)  # x축을 0부터 시작\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch.spikevision import spikedata\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# root, train=True, transform=None, target_transform=None, download_and_create=True, num_steps=1000, ds=1, dt=1000)\n",
    "train_ds = spikedata.SHD(\"/data2/Heidelberg\", train=True)\n",
    "test_ds = spikedata.SHD(\"/data2/Heidelberg\", train=False)\n",
    "\n",
    "# create dataloaders\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=64) # 8156x2x1000x700\n",
    "test_dl = DataLoader(test_ds, shuffle=False, batch_size=64) # 2264x2x1000x700\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "\n",
    "# choose a random sample\n",
    "n = 6295\n",
    "\n",
    "# initialize figure and axes\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# use spikeplot to generate a raster\n",
    "splt.raster(train_dl.dataset[n][0], ax, s=1.5, c=\"black\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
