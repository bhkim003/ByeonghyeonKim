{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    my_seed = 42,\n",
    "                    TIME = 8,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "                  ):\n",
    "\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "\n",
    "\n",
    "    \n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on)\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "\n",
    "    ## parameter number calculator ##########################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (type(kernel) == list):\n",
    "                for residual_kernel in kernel:\n",
    "                    params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                    past_kernel = residual_kernel\n",
    "            elif (kernel == 'P'):\n",
    "                img_size = img_size // 2\n",
    "            else:\n",
    "                params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                past_kernel = kernel    \n",
    "        params_num += (past_kernel * (img_size**2) + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            # elif (in_channel == 'M'): #it's a holy FC layer!\n",
    "            #     img_size = img_size // 2\n",
    "            else:\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ## parameter number calculator ##########################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                     tdBN_on,\n",
    "                     BN_on, TIME,\n",
    "                     surrogate,\n",
    "                     BPTT_on).to(device)\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "\n",
    "    net = net.to(device)\n",
    "    # print(net)\n",
    "    \n",
    "    ## param num and memory estimation ##########################################\n",
    "    real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "    # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "    print('='*50)\n",
    "    print(f\"Num of PARAMS: {params_num:,}\")\n",
    "    memory = params_num / 8 / 1024 / 1024 # MB\n",
    "    precision = 32\n",
    "    memory = memory * precision \n",
    "    print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "    print('='*50)\n",
    "    ##########################################################################\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=100)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "\n",
    "    val_acc = 0\n",
    "    val_acc_now = 0\n",
    "    elapsed_time_val = 0\n",
    "    for epoch in range(epoch_num):\n",
    "        print('EPOCH', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if (ddp_on == True):\n",
    "            if torch.distributed.get_rank() == 0:   \n",
    "                iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        else:\n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "\n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train()\n",
    "\n",
    "            ## data loading #################################\n",
    "            inputs, labels = data\n",
    "\n",
    "            \n",
    "            if (which_data == 'DVS-CIFAR10'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ################################################# \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "            # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        \n",
    "            outputs = net(inputs)\n",
    "\n",
    "            batch = BATCH \n",
    "            if labels.size(0) != BATCH: \n",
    "                batch = labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ####### training accruacy print ###############################\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted[0:batch] == labels).sum().item()\n",
    "            if i % verbose_interval == verbose_interval-1:\n",
    "                print(f'{epoch}-{i} training acc: {100 * correct / total:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "            training_acc_string = f'{epoch}-{i}/{len(train_loader)} tr_acc: {100 * correct / total:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            \n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs[0:batch,:], labels)\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            ### gradinet verbose ##########################################\n",
    "            if (gradient_verbose == True):\n",
    "                if (i % verbose_interval == verbose_interval-1):\n",
    "                    print('\\n\\nepoch', epoch, 'iter', i)\n",
    "                    for name, param in net.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            print('\\n\\n\\n\\n' , name, param.grad)\n",
    "            ################################################################\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "            if (i % verbose_interval == verbose_interval-1):\n",
    "                print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "            ##### validation ##############################################\n",
    "            if i % validation_interval == validation_interval-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    for data in test_loader:\n",
    "                        ## data loading #################################\n",
    "                        inputs, labels = data\n",
    "\n",
    "                        \n",
    "                        if (which_data == 'DVS-CIFAR10'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ################################################# \n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        batch = BATCH \n",
    "                        if labels.size(0) != BATCH: \n",
    "                            batch = labels.size(0)\n",
    "                        correct += (predicted[0:batch] == labels).sum().item()\n",
    "                        val_loss = criterion(outputs[0:batch,:], labels)\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc < val_acc_now:\n",
    "                    val_acc = val_acc_now\n",
    "                    torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                    torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                    torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                    torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "            ################################################################\n",
    "            iterator.set_description(f\"train: {training_acc_string}, val_acc: {100 * val_acc_now:.2f}%\")     \n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "        \n",
    "        print(f\"epoch_time: {epoch_time} seconds\")\n",
    "        print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Num of PARAMS: 9,301,834\n",
      "Memory: 35.48MiB at 32-bit\n",
      "==================================================\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0-5/282 tr_acc: 18.75%, lr=['1e-05'], val_acc: 0.00%:   2%|▏         | 6/282 [00:15<11:57,  2.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### my_snn control board ########################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.875\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmy_snn_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2,3,4,5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmy_seed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mTIME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 10 dvscifar\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBATCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# batch norm 할거면 2이상으로 해야함\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 48 dvscufar\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwhich_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDVS-CIFAR10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mCLASS_NUM\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# YOU NEED TO CHANGE THIS\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrate_coding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_v_reset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#현재 안씀. 걍 빼기 해버림\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlif_layer_sg_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# surrogate sigmoid 쓸 때는 의미없음\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_kernel_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_padding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_trace_const1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_conv_trace_const2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lif_layer_v_decay\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# synapse_fc_out_features = CLASS_NUM,\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_fc_trace_const1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[43msynapse_fc_trace_const2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lif_layer_v_decay\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpre_trained\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconvTrue_fcFalse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64],\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpre_trained_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnet_save/save_now_net.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepoch_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#숫자 크게 하면 꺼짐\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#숫자 크게 하면 꺼짐\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtdBN_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBN_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                \u001b[49m\u001b[43msurrogate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'rectangle' 'sigmoid' 'rough_rectangle'\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgradient_verbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False  # weight gradient 각 layer마다 띄워줌\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[43mBPTT_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True # False # True이면 BPTT, False이면 OTTT\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[43m                \u001b[49m\u001b[43mscheduler_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                \u001b[49m\u001b[43mddp_on\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# sigmoid와 BN이 있어야 잘되는건가?\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mcfg 종류 = {\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m[64, 64]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m} \u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 232\u001b[0m, in \u001b[0;36mmy_snn_system\u001b[0;34m(devices, my_seed, TIME, BATCH, IMAGE_SIZE, which_data, CLASS_NUM, data_path, rate_coding, lif_layer_v_init, lif_layer_v_decay, lif_layer_v_threshold, lif_layer_v_reset, lif_layer_sg_width, synapse_conv_kernel_size, synapse_conv_stride, synapse_conv_padding, synapse_conv_trace_const1, synapse_conv_trace_const2, synapse_fc_trace_const1, synapse_fc_trace_const2, pre_trained, convTrue_fcFalse, cfg, pre_trained_path, learning_rate, epoch_num, verbose_interval, validation_interval, tdBN_on, BN_on, surrogate, gradient_verbose, BPTT_on, scheduler_name, ddp_on)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m################################################################\u001b[39;00m\n\u001b[1;32m    231\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs[\u001b[38;5;241m0\u001b[39m:batch,:], labels)\n\u001b[0;32m--> 232\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m### gradinet verbose ##########################################\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (gradient_verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### my_snn control board ########################\n",
    "decay = 0.875\n",
    "\n",
    "my_snn_system(  devices = \"2,3,4,5\",\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10\n",
    "                BATCH = 32, # batch norm 할거면 2이상으로 해야함\n",
    "                IMAGE_SIZE = 48, # dvscifar 48 # MNIST 28 # CIFAR10 32\n",
    "                which_data = 'DVS-CIFAR10',# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\n",
    "                CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.2,\n",
    "                lif_layer_v_reset = 0.0, #현재 안씀. 걍 빼기 해버림\n",
    "                lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = True, # True # False\n",
    "                # cfg = [64],\n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "                pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                learning_rate = 0.00001,\n",
    "                epoch_num = 200,\n",
    "                verbose_interval = 10000, #숫자 크게 하면 꺼짐\n",
    "                validation_interval = 50, #숫자 크게 하면 꺼짐\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = True,  # True # False\n",
    "                \n",
    "                surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "                gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT\n",
    "\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False,\n",
    "                )\n",
    "# sigmoid와 BN이 있어야 잘되는건가?\n",
    "\n",
    "'''\n",
    "cfg 종류 = {\n",
    "[64, 64]\n",
    "[64, 64, 64, 64]\n",
    "[64, [64, 64], 64]\n",
    "[64, 128, 256]\n",
    "[64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512]\n",
    "[64, 64]\n",
    "[64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512]\n",
    "} \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_snn_system(  devices = \"2,3,4,5\",\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 8,\n",
    "#                 BATCH = 128, # batch norm 할거면 2이상으로 해야함\n",
    "#                 IMAGE_SIZE = 32,\n",
    "#                 which_data = 'CIFAR10',# 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS-CIFAR10'\n",
    "#                 CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 1.2,\n",
    "#                 lif_layer_v_reset = 0.0, #현재 안씀. 걍 빼기 해버림\n",
    "#                 lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1,\n",
    "#                 synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1,\n",
    "#                 synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = True, # True # False\n",
    "#                 # cfg = [64],\n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "#                 pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "#                 learning_rate = 0.00001,\n",
    "#                 epoch_num = 200,\n",
    "#                 verbose_interval = 10000, #숫자 크게 하면 꺼짐\n",
    "#                 validation_interval = 50, #숫자 크게 하면 꺼짐\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = True,  # True # False\n",
    "                \n",
    "#                 surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False\n",
    "\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,\n",
    "#                 )\n",
    "\n",
    "\n",
    "# Files already downloaded and verified\n",
    "# Files already downloaded and verified\n",
    "# ==================================================\n",
    "# Num of PARAMS: 9,302,410\n",
    "# Memory: 35.49MiB at 32-bit\n",
    "# ==================================================\n",
    "# EPOCH 0\n",
    "# train: 0-390/391 tr_acc: 27.50%, lr=['1e-05'], val_acc: 25.65%: 100%|██████████| 391/391 [08:39<00:00,  1.33s/it]\n",
    "# epoch_time: 519.6511018276215 seconds\n",
    "\n",
    "\n",
    "# EPOCH 1\n",
    "\n",
    "# train: 1-390/391 tr_acc: 32.50%, lr=['1e-05'], val_acc: 32.00%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.4912984371185 seconds\n",
    "\n",
    "\n",
    "# EPOCH 2\n",
    "\n",
    "# train: 2-390/391 tr_acc: 33.75%, lr=['1e-05'], val_acc: 35.14%: 100%|██████████| 391/391 [08:33<00:00,  1.31s/it]\n",
    "# epoch_time: 513.511162519455 seconds\n",
    "\n",
    "\n",
    "# EPOCH 3\n",
    "\n",
    "# train: 3-390/391 tr_acc: 28.75%, lr=['1e-05'], val_acc: 37.17%: 100%|██████████| 391/391 [08:30<00:00,  1.30s/it]\n",
    "# epoch_time: 510.2808663845062 seconds\n",
    "\n",
    "\n",
    "# EPOCH 4\n",
    "\n",
    "# train: 4-390/391 tr_acc: 58.75%, lr=['1e-05'], val_acc: 40.37%: 100%|██████████| 391/391 [08:21<00:00,  1.28s/it]\n",
    "# epoch_time: 501.48126435279846 seconds\n",
    "\n",
    "\n",
    "# EPOCH 5\n",
    "\n",
    "# train: 5-390/391 tr_acc: 37.50%, lr=['1e-05'], val_acc: 42.59%: 100%|██████████| 391/391 [08:18<00:00,  1.28s/it]\n",
    "# epoch_time: 498.8543312549591 seconds\n",
    "\n",
    "\n",
    "# EPOCH 6\n",
    "\n",
    "# train: 6-390/391 tr_acc: 35.00%, lr=['1e-05'], val_acc: 43.45%: 100%|██████████| 391/391 [08:15<00:00,  1.27s/it]\n",
    "# epoch_time: 495.6830530166626 seconds\n",
    "\n",
    "\n",
    "# EPOCH 7\n",
    "\n",
    "# train: 7-390/391 tr_acc: 48.75%, lr=['1e-05'], val_acc: 44.97%: 100%|██████████| 391/391 [08:16<00:00,  1.27s/it]\n",
    "# epoch_time: 496.7004475593567 seconds\n",
    "\n",
    "\n",
    "# EPOCH 8\n",
    "\n",
    "# train: 8-390/391 tr_acc: 47.50%, lr=['1e-05'], val_acc: 46.19%: 100%|██████████| 391/391 [08:16<00:00,  1.27s/it]\n",
    "# epoch_time: 496.552987575531 seconds\n",
    "\n",
    "\n",
    "# EPOCH 9\n",
    "\n",
    "# train: 9-390/391 tr_acc: 46.25%, lr=['1e-05'], val_acc: 46.52%: 100%|██████████| 391/391 [08:37<00:00,  1.32s/it]\n",
    "# epoch_time: 517.5774216651917 seconds\n",
    "\n",
    "\n",
    "# EPOCH 10\n",
    "\n",
    "# train: 10-390/391 tr_acc: 48.75%, lr=['1e-05'], val_acc: 48.67%: 100%|██████████| 391/391 [08:57<00:00,  1.37s/it]\n",
    "# epoch_time: 537.5675616264343 seconds\n",
    "\n",
    "\n",
    "# EPOCH 11\n",
    "\n",
    "# train: 11-390/391 tr_acc: 48.75%, lr=['1e-05'], val_acc: 48.40%: 100%|██████████| 391/391 [08:53<00:00,  1.37s/it]\n",
    "# epoch_time: 533.9970047473907 seconds\n",
    "\n",
    "\n",
    "# EPOCH 12\n",
    "\n",
    "# train: 12-390/391 tr_acc: 43.75%, lr=['1e-05'], val_acc: 50.58%: 100%|██████████| 391/391 [08:57<00:00,  1.38s/it]\n",
    "# epoch_time: 537.9225826263428 seconds\n",
    "\n",
    "\n",
    "# EPOCH 13\n",
    "\n",
    "# train: 13-390/391 tr_acc: 42.50%, lr=['1e-05'], val_acc: 50.98%: 100%|██████████| 391/391 [08:58<00:00,  1.38s/it]\n",
    "# epoch_time: 538.5700080394745 seconds\n",
    "\n",
    "\n",
    "# EPOCH 14\n",
    "\n",
    "# train: 14-390/391 tr_acc: 48.75%, lr=['1e-05'], val_acc: 52.68%: 100%|██████████| 391/391 [08:59<00:00,  1.38s/it]\n",
    "# epoch_time: 539.7910151481628 seconds\n",
    "\n",
    "\n",
    "# EPOCH 15\n",
    "\n",
    "# train: 15-390/391 tr_acc: 55.00%, lr=['1e-05'], val_acc: 54.05%: 100%|██████████| 391/391 [08:52<00:00,  1.36s/it]\n",
    "# epoch_time: 532.8848164081573 seconds\n",
    "\n",
    "\n",
    "# EPOCH 16\n",
    "\n",
    "# train: 16-390/391 tr_acc: 52.50%, lr=['1e-05'], val_acc: 53.80%: 100%|██████████| 391/391 [08:58<00:00,  1.38s/it]\n",
    "# epoch_time: 538.2881193161011 seconds\n",
    "\n",
    "\n",
    "# EPOCH 17\n",
    "\n",
    "# train: 17-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 54.73%: 100%|██████████| 391/391 [09:00<00:00,  1.38s/it]\n",
    "# epoch_time: 540.2721989154816 seconds\n",
    "\n",
    "\n",
    "# EPOCH 18\n",
    "\n",
    "# train: 18-390/391 tr_acc: 60.00%, lr=['1e-05'], val_acc: 55.54%: 100%|██████████| 391/391 [09:00<00:00,  1.38s/it]\n",
    "# epoch_time: 540.7067131996155 seconds\n",
    "\n",
    "\n",
    "# EPOCH 19\n",
    "\n",
    "# train: 19-390/391 tr_acc: 61.25%, lr=['1e-05'], val_acc: 56.20%: 100%|██████████| 391/391 [08:53<00:00,  1.36s/it]\n",
    "# epoch_time: 533.882045507431 seconds\n",
    "\n",
    "\n",
    "# EPOCH 20\n",
    "\n",
    "# train: 20-390/391 tr_acc: 61.25%, lr=['1e-05'], val_acc: 57.64%: 100%|██████████| 391/391 [08:55<00:00,  1.37s/it]\n",
    "# epoch_time: 535.4623730182648 seconds\n",
    "\n",
    "\n",
    "# EPOCH 21\n",
    "\n",
    "# train: 21-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 57.87%: 100%|██████████| 391/391 [08:57<00:00,  1.37s/it]\n",
    "# epoch_time: 537.7847683429718 seconds\n",
    "\n",
    "\n",
    "# EPOCH 22\n",
    "\n",
    "# train: 22-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 58.46%: 100%|██████████| 391/391 [08:55<00:00,  1.37s/it]\n",
    "# epoch_time: 535.7902438640594 seconds\n",
    "\n",
    "\n",
    "# EPOCH 23\n",
    "\n",
    "# train: 23-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 57.95%: 100%|██████████| 391/391 [08:57<00:00,  1.37s/it]\n",
    "# epoch_time: 537.5092451572418 seconds\n",
    "\n",
    "\n",
    "# EPOCH 24\n",
    "\n",
    "# train: 24-390/391 tr_acc: 55.00%, lr=['1e-05'], val_acc: 57.98%: 100%|██████████| 391/391 [08:44<00:00,  1.34s/it]\n",
    "# epoch_time: 524.3707118034363 seconds\n",
    "\n",
    "\n",
    "# EPOCH 25\n",
    "\n",
    "# train: 25-390/391 tr_acc: 50.00%, lr=['1e-05'], val_acc: 60.33%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.3296625614166 seconds\n",
    "\n",
    "\n",
    "# EPOCH 26\n",
    "\n",
    "# train: 26-390/391 tr_acc: 52.50%, lr=['1e-05'], val_acc: 60.02%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.8723931312561 seconds\n",
    "\n",
    "\n",
    "# EPOCH 27\n",
    "\n",
    "# train: 27-390/391 tr_acc: 60.00%, lr=['1e-05'], val_acc: 60.80%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.6468982696533 seconds\n",
    "\n",
    "\n",
    "# EPOCH 28\n",
    "\n",
    "# train: 28-390/391 tr_acc: 56.25%, lr=['1e-05'], val_acc: 60.99%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 524.2045466899872 seconds\n",
    "\n",
    "\n",
    "# EPOCH 29\n",
    "\n",
    "# train: 29-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 62.56%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 523.102609872818 seconds\n",
    "\n",
    "\n",
    "# EPOCH 30\n",
    "\n",
    "# train: 30-390/391 tr_acc: 65.00%, lr=['1e-05'], val_acc: 62.23%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.3475232124329 seconds\n",
    "\n",
    "\n",
    "# EPOCH 31\n",
    "\n",
    "# train: 31-390/391 tr_acc: 70.00%, lr=['1e-05'], val_acc: 61.05%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.611293554306 seconds\n",
    "\n",
    "\n",
    "# EPOCH 32\n",
    "\n",
    "# train: 32-390/391 tr_acc: 57.50%, lr=['1e-05'], val_acc: 63.73%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.550628900528 seconds\n",
    "\n",
    "\n",
    "# EPOCH 33\n",
    "\n",
    "# train: 33-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 63.01%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.7382657527924 seconds\n",
    "\n",
    "\n",
    "# EPOCH 34\n",
    "\n",
    "# train: 34-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 63.66%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 523.2350842952728 seconds\n",
    "\n",
    "\n",
    "# EPOCH 35\n",
    "\n",
    "# train: 35-390/391 tr_acc: 65.00%, lr=['1e-05'], val_acc: 64.20%: 100%|██████████| 391/391 [08:39<00:00,  1.33s/it]\n",
    "# epoch_time: 519.9772260189056 seconds\n",
    "\n",
    "\n",
    "# EPOCH 36\n",
    "\n",
    "# train: 36-390/391 tr_acc: 61.25%, lr=['1e-05'], val_acc: 64.67%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 522.0173165798187 seconds\n",
    "\n",
    "\n",
    "# EPOCH 37\n",
    "\n",
    "# train: 37-390/391 tr_acc: 62.50%, lr=['1e-05'], val_acc: 64.44%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 522.151460647583 seconds\n",
    "\n",
    "\n",
    "# EPOCH 38\n",
    "\n",
    "# train: 38-390/391 tr_acc: 62.50%, lr=['1e-05'], val_acc: 64.77%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.8787703514099 seconds\n",
    "\n",
    "\n",
    "# EPOCH 39\n",
    "\n",
    "# train: 39-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 65.22%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.234944820404 seconds\n",
    "\n",
    "\n",
    "# EPOCH 40\n",
    "\n",
    "# train: 40-390/391 tr_acc: 68.75%, lr=['1e-05'], val_acc: 64.49%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.9287745952606 seconds\n",
    "\n",
    "\n",
    "# EPOCH 41\n",
    "\n",
    "# train: 41-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 66.06%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.8189256191254 seconds\n",
    "\n",
    "\n",
    "# EPOCH 42\n",
    "\n",
    "# train: 42-390/391 tr_acc: 76.25%, lr=['1e-05'], val_acc: 65.47%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.2214741706848 seconds\n",
    "\n",
    "\n",
    "# EPOCH 43\n",
    "\n",
    "# train: 43-390/391 tr_acc: 78.75%, lr=['1e-05'], val_acc: 66.97%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.9151771068573 seconds\n",
    "\n",
    "\n",
    "# EPOCH 44\n",
    "\n",
    "# train: 44-390/391 tr_acc: 72.50%, lr=['1e-05'], val_acc: 66.76%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.2902402877808 seconds\n",
    "\n",
    "\n",
    "# EPOCH 45\n",
    "\n",
    "# train: 45-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 66.30%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.3684139251709 seconds\n",
    "\n",
    "\n",
    "# EPOCH 46\n",
    "\n",
    "# train: 46-390/391 tr_acc: 76.25%, lr=['1e-05'], val_acc: 66.27%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.4130618572235 seconds\n",
    "\n",
    "\n",
    "# EPOCH 47\n",
    "\n",
    "# train: 47-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 66.20%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.4402587413788 seconds\n",
    "\n",
    "\n",
    "# EPOCH 48\n",
    "\n",
    "# train: 48-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 67.45%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 521.1478617191315 seconds\n",
    "\n",
    "\n",
    "# EPOCH 49\n",
    "\n",
    "# train: 49-390/391 tr_acc: 77.50%, lr=['1e-05'], val_acc: 68.14%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.3577303886414 seconds\n",
    "\n",
    "\n",
    "# EPOCH 50\n",
    "\n",
    "# train: 50-390/391 tr_acc: 63.75%, lr=['1e-05'], val_acc: 67.95%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.6194558143616 seconds\n",
    "\n",
    "\n",
    "# EPOCH 51\n",
    "\n",
    "# train: 51-390/391 tr_acc: 75.00%, lr=['1e-05'], val_acc: 68.41%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 520.8790509700775 seconds\n",
    "\n",
    "\n",
    "# EPOCH 52\n",
    "\n",
    "# train: 52-390/391 tr_acc: 66.25%, lr=['1e-05'], val_acc: 67.84%: 100%|██████████| 391/391 [08:40<00:00,  1.33s/it]\n",
    "# epoch_time: 521.1223595142365 seconds\n",
    "\n",
    "\n",
    "# EPOCH 53\n",
    "\n",
    "# train: 53-390/391 tr_acc: 68.75%, lr=['1e-05'], val_acc: 68.00%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.3871352672577 seconds\n",
    "\n",
    "\n",
    "# EPOCH 54\n",
    "\n",
    "# train: 54-390/391 tr_acc: 73.75%, lr=['1e-05'], val_acc: 68.75%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.407853603363 seconds\n",
    "\n",
    "\n",
    "# EPOCH 55\n",
    "\n",
    "# train: 55-390/391 tr_acc: 71.25%, lr=['1e-05'], val_acc: 69.21%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.9369058609009 seconds\n",
    "\n",
    "\n",
    "# EPOCH 56\n",
    "\n",
    "# train: 56-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 69.28%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.430141210556 seconds\n",
    "\n",
    "\n",
    "# EPOCH 57\n",
    "\n",
    "# train: 57-390/391 tr_acc: 65.00%, lr=['1e-05'], val_acc: 69.16%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.3304183483124 seconds\n",
    "\n",
    "\n",
    "# EPOCH 58\n",
    "\n",
    "# train: 58-390/391 tr_acc: 67.50%, lr=['1e-05'], val_acc: 68.30%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.6461317539215 seconds\n",
    "\n",
    "\n",
    "# EPOCH 59\n",
    "\n",
    "# train: 59-390/391 tr_acc: 70.00%, lr=['1e-05'], val_acc: 69.50%: 100%|██████████| 391/391 [08:39<00:00,  1.33s/it]\n",
    "# epoch_time: 520.0395126342773 seconds\n",
    "\n",
    "\n",
    "# EPOCH 60\n",
    "\n",
    "# train: 60-390/391 tr_acc: 68.75%, lr=['1e-05'], val_acc: 69.68%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.3599922657013 seconds\n",
    "\n",
    "\n",
    "# EPOCH 61\n",
    "\n",
    "# train: 61-390/391 tr_acc: 63.75%, lr=['1e-05'], val_acc: 70.64%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.446202993393 seconds\n",
    "\n",
    "\n",
    "# EPOCH 62\n",
    "\n",
    "# train: 62-390/391 tr_acc: 75.00%, lr=['1e-05'], val_acc: 70.57%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.744499206543 seconds\n",
    "\n",
    "\n",
    "# EPOCH 63\n",
    "\n",
    "# train: 63-390/391 tr_acc: 65.00%, lr=['1e-05'], val_acc: 70.47%: 100%|██████████| 391/391 [08:39<00:00,  1.33s/it]\n",
    "# epoch_time: 519.3143074512482 seconds\n",
    "\n",
    "\n",
    "# EPOCH 64\n",
    "\n",
    "# train: 64-390/391 tr_acc: 63.75%, lr=['1e-05'], val_acc: 70.40%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 521.6692788600922 seconds\n",
    "\n",
    "\n",
    "# EPOCH 65\n",
    "\n",
    "# train: 65-390/391 tr_acc: 80.00%, lr=['1e-05'], val_acc: 70.64%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.667236328125 seconds\n",
    "\n",
    "\n",
    "# EPOCH 66\n",
    "\n",
    "# train: 66-390/391 tr_acc: 72.50%, lr=['1e-05'], val_acc: 69.29%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 522.0747861862183 seconds\n",
    "\n",
    "\n",
    "# EPOCH 67\n",
    "\n",
    "# train: 67-390/391 tr_acc: 73.75%, lr=['1e-05'], val_acc: 70.66%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.6984555721283 seconds\n",
    "\n",
    "\n",
    "# EPOCH 68\n",
    "\n",
    "# train: 68-390/391 tr_acc: 71.25%, lr=['1e-05'], val_acc: 70.71%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 523.1850342750549 seconds\n",
    "\n",
    "\n",
    "# EPOCH 69\n",
    "\n",
    "# train: 69-390/391 tr_acc: 86.25%, lr=['1e-05'], val_acc: 70.95%: 100%|██████████| 391/391 [08:41<00:00,  1.33s/it]\n",
    "# epoch_time: 522.0473079681396 seconds\n",
    "\n",
    "\n",
    "# EPOCH 70\n",
    "\n",
    "# train: 70-390/391 tr_acc: 70.00%, lr=['1e-05'], val_acc: 71.07%: 100%|██████████| 391/391 [08:44<00:00,  1.34s/it]\n",
    "# epoch_time: 524.3250975608826 seconds\n",
    "\n",
    "\n",
    "# EPOCH 71\n",
    "\n",
    "# train: 71-390/391 tr_acc: 71.25%, lr=['1e-05'], val_acc: 71.03%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 524.1436457633972 seconds\n",
    "\n",
    "\n",
    "# EPOCH 72\n",
    "\n",
    "# train: 72-390/391 tr_acc: 72.50%, lr=['1e-05'], val_acc: 70.97%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 522.3500926494598 seconds\n",
    "\n",
    "\n",
    "# EPOCH 73\n",
    "\n",
    "# train: 73-390/391 tr_acc: 81.25%, lr=['1e-05'], val_acc: 69.71%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.6104423999786 seconds\n",
    "\n",
    "\n",
    "# EPOCH 74\n",
    "\n",
    "# train: 74-390/391 tr_acc: 72.50%, lr=['1e-05'], val_acc: 71.35%: 100%|██████████| 391/391 [08:42<00:00,  1.34s/it]\n",
    "# epoch_time: 523.1159906387329 seconds\n",
    "\n",
    "\n",
    "# EPOCH 75\n",
    "\n",
    "# train: 75-390/391 tr_acc: 70.00%, lr=['1e-05'], val_acc: 72.26%: 100%|██████████| 391/391 [08:43<00:00,  1.34s/it]\n",
    "# epoch_time: 523.9588918685913 seconds\n",
    "\n",
    "\n",
    "# EPOCH 76\n",
    "\n",
    "# train: 76-105/391 tr_acc: 75.78%, lr=['1e-05'], val_acc: 71.69%:  27%|██▋       | 106/391 [02:24<06:28,  1.36s/it]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
