{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAIhCAYAAACfVbSSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA720lEQVR4nO3deXxU1f3/8fckIROWJKwJQUKIW41EDSaobP5wIZUCYl2gqCyyFAyLLFVIsaJQiaAirQiKbCKLEQFBRTSVKqggMSJYN1SQBCVGFgkiBDJzf39Q8u2QgGScOZeZeT0fj/t4mJM7534ygHx43zPnOizLsgQAAAC/C7O7AAAAgFBB4wUAAGAIjRcAAIAhNF4AAACG0HgBAAAYQuMFAABgCI0XAACAITReAAAAhtB4AQAAGELjBXhh/vz5cjgcFUdERIQSEhL0pz/9SV999ZVtdT344INyOBy2Xf9kBQUFGjJkiC655BJFR0crPj5e119/vdauXVvp3L59+3q8p7Vr11bz5s114403at68eSorK6v29UeNGiWHw6EuXbr44scBgN+Mxgv4DebNm6cNGzboX//6l4YOHapVq1apXbt22r9/v92lnRWWLFmiTZs2qV+/flq5cqVmz54tp9Op6667TgsWLKh0fs2aNbVhwwZt2LBBr776qiZMmKDatWtr4MCBSk9P165du8742seOHdPChQslSWvWrNF3333ns58LALxmAai2efPmWZKs/Px8j/GHHnrIkmTNnTvXlrrGjx9vnU1/rH/44YdKY+Xl5dall15qnXfeeR7jffr0sWrXrl3lPG+88YZVo0YN68orrzzjay9dutSSZHXu3NmSZD388MNn9LqjR49ax44dq/J7hw4dOuPrA0BVSLwAH8rIyJAk/fDDDxVjR44c0ejRo5WWlqbY2FjVr19frVu31sqVKyu93uFwaOjQoXr++eeVkpKiWrVq6bLLLtOrr75a6dzXXntNaWlpcjqdSk5O1mOPPVZlTUeOHFF2draSk5MVGRmpc845R0OGDNFPP/3kcV7z5s3VpUsXvfrqq2rZsqVq1qyplJSUimvPnz9fKSkpql27tq644gp9+OGHv/p+xMXFVRoLDw9Xenq6ioqKfvX1J2RmZmrgwIH64IMPtG7dujN6zZw5cxQZGal58+YpMTFR8+bNk2VZHue8/fbbcjgcev755zV69Gidc845cjqd+vrrr9W3b1/VqVNHn3zyiTIzMxUdHa3rrrtOkpSXl6du3bqpadOmioqK0vnnn69BgwZpz549FXOvX79eDodDS5YsqVTbggUL5HA4lJ+ff8bvAYDgQOMF+NCOHTskSRdeeGHFWFlZmfbt26e//OUvevnll7VkyRK1a9dON998c5W321577TVNnz5dEyZM0LJly1S/fn398Y9/1Pbt2yvOeeutt9StWzdFR0frhRde0KOPPqoXX3xR8+bN85jLsizddNNNeuyxx9SrVy+99tprGjVqlJ577jlde+21ldZNbdmyRdnZ2RozZoyWL1+u2NhY3XzzzRo/frxmz56tSZMmadGiRTpw4IC6dOmiw4cPV/s9Ki8v1/r169WiRYtqve7GG2+UpDNqvHbt2qU333xT3bp1U6NGjdSnTx99/fXXp3xtdna2CgsL9fTTT+uVV16paBiPHj2qG2+8Uddee61Wrlyphx56SJL0zTffqHXr1po5c6befPNNPfDAA/rggw/Url07HTt2TJLUvn17tWzZUk899VSl602fPl2tWrVSq1atqvUeAAgCdkduQCA6catx48aN1rFjx6yDBw9aa9assRo3bmxdffXVp7xVZVnHb7UdO3bM6t+/v9WyZUuP70my4uPjrdLS0oqx4uJiKywszMrJyakYu/LKK60mTZpYhw8frhgrLS216tev73Grcc2aNZYka8qUKR7Xyc3NtSRZs2bNqhhLSkqyatasae3atati7OOPP7YkWQkJCR632V5++WVLkrVq1aozebs8jBs3zpJkvfzyyx7jp7vVaFmW9fnnn1uSrLvvvvtXrzFhwgRLkrVmzRrLsixr+/btlsPhsHr16uVx3r///W9LknX11VdXmqNPnz5ndNvY7XZbx44ds3bu3GlJslauXFnxvRO/TzZv3lwxtmnTJkuS9dxzz/3qzwEg+JB4Ab/BVVddpRo1aig6Olo33HCD6tWrp5UrVyoiIsLjvKVLl6pt27aqU6eOIiIiVKNGDc2ZM0eff/55pTmvueYaRUdHV3wdHx+vuLg47dy5U5J06NAh5efn6+abb1ZUVFTFedHR0eratavHXCc+Pdi3b1+P8dtuu021a9fWW2+95TGelpamc845p+LrlJQUSVKHDh1Uq1atSuMnajpTs2fP1sMPP6zRo0erW7du1XqtddJtwtOdd+L2YseOHSVJycnJ6tChg5YtW6bS0tJKr7nllltOOV9V3yspKdHgwYOVmJhY8euZlJQkSR6/pj179lRcXJxH6vXkk0+qUaNG6tGjxxn9PACCC40X8BssWLBA+fn5Wrt2rQYNGqTPP/9cPXv29Dhn+fLl6t69u8455xwtXLhQGzZsUH5+vvr166cjR45UmrNBgwaVxpxOZ8Vtvf3798vtdqtx48aVzjt5bO/evYqIiFCjRo08xh0Ohxo3bqy9e/d6jNevX9/j68jIyNOOV1X/qcybN0+DBg3Sn//8Zz366KNn/LoTTjR5TZo0Oe15a9eu1Y4dO3TbbbeptLRUP/30k3766Sd1795dv/zyS5VrrhISEqqcq1atWoqJifEYc7vdyszM1PLly3Xffffprbfe0qZNm7Rx40ZJ8rj96nQ6NWjQIC1evFg//fSTfvzxR7344osaMGCAnE5ntX5+AMEh4tdPAXAqKSkpFQvqr7nmGrlcLs2ePVsvvfSSbr31VknSwoULlZycrNzcXI89trzZl0qS6tWrJ4fDoeLi4krfO3msQYMGKi8v148//ujRfFmWpeLiYmNrjObNm6cBAwaoT58+evrpp73aa2zVqlWSjqdvpzNnzhxJ0tSpUzV16tQqvz9o0CCPsVPVU9X4f/7zH23ZskXz589Xnz59Ksa//vrrKue4++679cgjj2ju3Lk6cuSIysvLNXjw4NP+DACCF4kX4ENTpkxRvXr19MADD8jtdks6/pd3ZGSkx1/ixcXFVX6q8Uyc+FTh8uXLPRKngwcP6pVXXvE498Sn8E7sZ3XCsmXLdOjQoYrv+9P8+fM1YMAA3XnnnZo9e7ZXTVdeXp5mz56tNm3aqF27dqc8b//+/VqxYoXatm2rf//735WOO+64Q/n5+frPf/7j9c9zov6TE6tnnnmmyvMTEhJ02223acaMGXr66afVtWtXNWvWzOvrAwhsJF6AD9WrV0/Z2dm67777tHjxYt15553q0qWLli9frqysLN16660qKirSxIkTlZCQ4PUu9xMnTtQNN9ygjh07avTo0XK5XJo8ebJq166tffv2VZzXsWNH/f73v9eYMWNUWlqqtm3bauvWrRo/frxatmypXr16+epHr9LSpUvVv39/paWladCgQdq0aZPH91u2bOnRwLjd7opbdmVlZSosLNTrr7+uF198USkpKXrxxRdPe71FixbpyJEjGj58eJXJWIMGDbRo0SLNmTNHTzzxhFc/00UXXaTzzjtPY8eOlWVZql+/vl555RXl5eWd8jX33HOPrrzySkmq9MlTACHG3rX9QGA61QaqlmVZhw8ftpo1a2ZdcMEFVnl5uWVZlvXII49YzZs3t5xOp5WSkmI9++yzVW52KskaMmRIpTmTkpKsPn36eIytWrXKuvTSS63IyEirWbNm1iOPPFLlnIcPH7bGjBljJSUlWTVq1LASEhKsu+++29q/f3+la3Tu3LnStauqaceOHZYk69FHHz3le2RZ//fJwFMdO3bsOOW5NWvWtJo1a2Z17drVmjt3rlVWVnbaa1mWZaWlpVlxcXGnPfeqq66yGjZsaJWVlVV8qnHp0qVV1n6qT1l+9tlnVseOHa3o6GirXr161m233WYVFhZakqzx48dX+ZrmzZtbKSkpv/ozAAhuDss6w48KAQC8snXrVl122WV66qmnlJWVZXc5AGxE4wUAfvLNN99o586d+utf/6rCwkJ9/fXXHttyAAg9LK4HAD+ZOHGiOnbsqJ9//llLly6l6QJA4gUAAGAKiRcAAIAhNF4AAACG0HgBAAAYEtAbqLrdbn3//feKjo72ajdsAABCiWVZOnjwoJo0aaKwMPPZy5EjR3T06FG/zB0ZGamoqCi/zO1LAd14ff/990pMTLS7DAAAAkpRUZGaNm1q9JpHjhxRclIdFZe4/DJ/48aNtWPHjrO++Qroxis6OlqSdOX/G6uIiLP7jT5Z1OZv7S7BK5e/WmJ3CV776Ib6dpfglb7vbLG7BK/kPNPT7hK8ZgXoIow23T+2uwSvvLs8ze4SvNZoq3cPu7dLeXmZNr77SMXfnyYdPXpUxSUu7Sxorpho3/4hKz3oVlL6tzp69CiNlz+duL0YEREVcI1XRFik3SV4xVmnht0leC3CEZjvea3ocLtL8Eq4M7D+TP6vQG28IgP0z2cg/16JiAjMZS52Ls+pE+1QnWjfXt+twPl1COjGCwAABBaX5ZbLxzuIuiy3byf0owD9dx0AAEDgIfECAADGuGXJLd9GXr6ez59IvAAAAAwh8QIAAMa45ZavV2T5fkb/IfECAAAwhMQLAAAY47IsuSzfrsny9Xz+ROIFAABgCIkXAAAwJtQ/1UjjBQAAjHHLkiuEGy9uNQIAABhC4gUAAIwJ9VuNJF4AAACGkHgBAABj2E4CAAAARpB4AQAAY9z/PXw9Z6CwPfGaMWOGkpOTFRUVpfT0dK1fv97ukgAAAPzC1sYrNzdXI0aM0Lhx47R582a1b99enTp1UmFhoZ1lAQAAP3H9dx8vXx+BwtbGa+rUqerfv78GDBiglJQUTZs2TYmJiZo5c6adZQEAAD9xWf45AoVtjdfRo0dVUFCgzMxMj/HMzEy9//77Vb6mrKxMpaWlHgcAAECgsK3x2rNnj1wul+Lj4z3G4+PjVVxcXOVrcnJyFBsbW3EkJiaaKBUAAPiI209HoLB9cb3D4fD42rKsSmMnZGdn68CBAxVHUVGRiRIBAAB8wrbtJBo2bKjw8PBK6VZJSUmlFOwEp9Mpp9NpojwAAOAHbjnkUtUBy2+ZM1DYlnhFRkYqPT1deXl5HuN5eXlq06aNTVUBAAD4j60bqI4aNUq9evVSRkaGWrdurVmzZqmwsFCDBw+2sywAAOAnbuv44es5A4WtjVePHj20d+9eTZgwQbt371ZqaqpWr16tpKQkO8sCAADwC9sfGZSVlaWsrCy7ywAAAAa4/LDGy9fz+ZPtjRcAAAgdod542b6dBAAAQKgg8QIAAMa4LYfclo+3k/DxfP5E4gUAAGAIiRcAADCGNV4AAAAwgsQLAAAY41KYXD7OfVw+nc2/SLwAAAAMIfECAADGWH74VKMVQJ9qpPECAADGsLgeAAAARpB4AQAAY1xWmFyWjxfXWz6dzq9IvAAAAAwh8QIAAMa45ZDbx7mPW4ETeZF4AQAAGBIUiVf4iBKF13baXUa1WDeW2V2CVxbkt7a7BK/VvCfS7hK88sDTqXaX4JX19z1udwle63ldL7tL8Mr2bg3sLsErYe32212C17ZfXNvuEqrFfdghvW1vDXyqEQAAAEYEReIFAAACg38+1Rg4a7xovAAAgDHHF9f79tagr+fzJ241AgAAGELiBQAAjHErTC62kwAAAIC/kXgBAABjQn1xPYkXAACAISReAADAGLfCeGQQAAAA/I/ECwAAGOOyHHJZPn5kkI/n8ycaLwAAYIzLD9tJuLjVCAAAgJOReAEAAGPcVpjcPt5Ows12EgAAADgZiRcAADCGNV4AAAAwgsQLAAAY45bvt39w+3Q2/yLxAgAAMITECwAAGOOfRwYFTo5E4wUAAIxxWWFy+Xg7CV/P50+BUykAAECAI/ECAADGuOWQW75eXB84z2ok8QIAADCExAsAABjDGi8AAAAYQeIFAACM8c8jgwInRwqcSgEAAAIciRcAADDGbTnk9vUjg3w8nz+ReAEAABhC4gUAAIxx+2GNF48MAgAAqILbCpPbx9s/+Ho+fwqcSgEAAAIciRcAADDGJYdcPn7Ej6/n8ycSLwAAAENIvAAAgDGs8QIAAIARJF4AAMAYl3y/Jsvl09n8i8QLAADAEBIvAABgDGu8AAAADHFZYX45vDFjxgwlJycrKipK6enpWr9+/WnPX7RokS677DLVqlVLCQkJuuuuu7R3795qXZPGCwAAhJzc3FyNGDFC48aN0+bNm9W+fXt16tRJhYWFVZ7/7rvvqnfv3urfv78+/fRTLV26VPn5+RowYEC1rkvjBQAAjLHkkNvHh+XFYv2pU6eqf//+GjBggFJSUjRt2jQlJiZq5syZVZ6/ceNGNW/eXMOHD1dycrLatWunQYMG6cMPP6zWdWm8AABAUCgtLfU4ysrKqjzv6NGjKigoUGZmpsd4Zmam3n///Spf06ZNG+3atUurV6+WZVn64Ycf9NJLL6lz587VqpHGCwAAGOPPNV6JiYmKjY2tOHJycqqsYc+ePXK5XIqPj/cYj4+PV3FxcZWvadOmjRYtWqQePXooMjJSjRs3Vt26dfXkk09W6+en8QIAAEGhqKhIBw4cqDiys7NPe77D4XmL0rKsSmMnfPbZZxo+fLgeeOABFRQUaM2aNdqxY4cGDx5crRqDYjuJw8dqKOJYDbvLqJae+VUv3jvbffF2C7tL8FrEL3ZX4J2aP1p2l+CVqx8fbXcJXjv0wGG7S/DKyuTn7S7BK/e1vcXuErzmKvnG7hKqpdw6pl021+C2HHJbvt1A9cR8MTExiomJ+dXzGzZsqPDw8ErpVklJSaUU7IScnBy1bdtW9957ryTp0ksvVe3atdW+fXv9/e9/V0JCwhnVSuIFAABCSmRkpNLT05WXl+cxnpeXpzZt2lT5ml9++UVhYZ5tU3h4uKTjSdmZCorECwAABAaXwuTyce7jzXyjRo1Sr169lJGRodatW2vWrFkqLCysuHWYnZ2t7777TgsWLJAkde3aVQMHDtTMmTP1+9//Xrt379aIESN0xRVXqEmTJmd8XRovAABgjD9vNVZHjx49tHfvXk2YMEG7d+9WamqqVq9eraSkJEnS7t27Pfb06tu3rw4ePKjp06dr9OjRqlu3rq699lpNnjy5Wtel8QIAACEpKytLWVlZVX5v/vz5lcaGDRumYcOG/aZr0ngBAABj3AqT28e3Gn09nz8FTqUAAAABjsQLAAAY47Iccvl4jZev5/MnEi8AAABDSLwAAIAxZ8unGu1C4gUAAGAIiRcAADDGssLktnyb+1g+ns+faLwAAIAxLjnkko8X1/t4Pn8KnBYRAAAgwJF4AQAAY9yW7xfDu8/8GdW2I/ECAAAwhMQLAAAY4/bD4npfz+dPgVMpAABAgCPxAgAAxrjlkNvHn0L09Xz+ZGvilZOTo1atWik6OlpxcXG66aab9OWXX9pZEgAAgN/Y2ni98847GjJkiDZu3Ki8vDyVl5crMzNThw4dsrMsAADgJyceku3rI1DYeqtxzZo1Hl/PmzdPcXFxKigo0NVXX21TVQAAwF9CfXH9WbXG68CBA5Kk+vXrV/n9srIylZWVVXxdWlpqpC4AAABfOGtaRMuyNGrUKLVr106pqalVnpOTk6PY2NiKIzEx0XCVAADgt3DLIbfl44PF9dU3dOhQbd26VUuWLDnlOdnZ2Tpw4EDFUVRUZLBCAACA3+asuNU4bNgwrVq1SuvWrVPTpk1PeZ7T6ZTT6TRYGQAA8CXLD9tJWAGUeNnaeFmWpWHDhmnFihV6++23lZycbGc5AAAAfmVr4zVkyBAtXrxYK1euVHR0tIqLiyVJsbGxqlmzpp2lAQAAPzixLsvXcwYKW9d4zZw5UwcOHFCHDh2UkJBQceTm5tpZFgAAgF/YfqsRAACEDvbxAgAAMIRbjQAAADCCxAsAABjj9sN2EmygCgAAgEpIvAAAgDGs8QIAAIARJF4AAMAYEi8AAAAYQeIFAACMCfXEi8YLAAAYE+qNF7caAQAADCHxAgAAxljy/YangfTkZxIvAAAAQ0i8AACAMazxAgAAgBEkXgAAwJhQT7yCovFyRpQrIiLc7jKq5aVRN9hdgldqDTxkdwleS8w5YHcJXvnynqZ2l+CVh7q+aHcJXlv5Y5rdJXjl5g8G2V2CV8pH17K7BK9d+Fys3SVUi8NVJn1idxWhLSgaLwAAEBhIvAAAAAwJ9caLxfUAAACGkHgBAABjLMshy8cJla/n8ycSLwAAAENIvAAAgDFuOXz+yCBfz+dPJF4AAACGkHgBAABj+FQjAAAAjCDxAgAAxvCpRgAAABhB4gUAAIwJ9TVeNF4AAMAYbjUCAADACBIvAABgjOWHW40kXgAAAKiExAsAABhjSbIs388ZKEi8AAAADCHxAgAAxrjlkIOHZAMAAMDfSLwAAIAxob6PF40XAAAwxm055Ajhneu51QgAAGAIiRcAADDGsvywnUQA7SdB4gUAAGAIiRcAADAm1BfXk3gBAAAYQuIFAACMIfECAACAESReAADAmFDfx4vGCwAAGMN2EgAAADCCxAsAABhzPPHy9eJ6n07nVyReAAAAhpB4AQAAY9hOAgAAAEaQeAEAAGOs/x6+njNQkHgBAAAYQuIFAACMCfU1XjReAADAnBC/18itRgAAEJJmzJih5ORkRUVFKT09XevXrz/t+WVlZRo3bpySkpLkdDp13nnnae7cudW6JokXAAAwxw+3GuXFfLm5uRoxYoRmzJihtm3b6plnnlGnTp302WefqVmzZlW+pnv37vrhhx80Z84cnX/++SopKVF5eXm1rkvjBQAAQs7UqVPVv39/DRgwQJI0bdo0vfHGG5o5c6ZycnIqnb9mzRq988472r59u+rXry9Jat68ebWvy61GAABgzImHZPv6kKTS0lKPo6ysrMoajh49qoKCAmVmZnqMZ2Zm6v3336/yNatWrVJGRoamTJmic845RxdeeKH+8pe/6PDhw9X6+Um8AABAUEhMTPT4evz48XrwwQcrnbdnzx65XC7Fx8d7jMfHx6u4uLjKubdv3653331XUVFRWrFihfbs2aOsrCzt27evWuu8gqLx2rf6HIU7o+wuo1q6T3nL7hK88u6tqXaX4LUfMhN//aSzUK3vA+dj0v/r2VE3212C16KKf7G7BK/kvvSs3SV45YgVuH8VDdo5zO4SqsVVdkT6xN4a/LmdRFFRkWJiYirGnU7naV/ncHjWYVlWpbET3G63HA6HFi1apNjYWEnHb1feeuuteuqpp1SzZs0zqpVbjQAAICjExMR4HKdqvBo2bKjw8PBK6VZJSUmlFOyEhIQEnXPOORVNlySlpKTIsizt2rXrjGuk8QIAAOZYDv8c1RAZGan09HTl5eV5jOfl5alNmzZVvqZt27b6/vvv9fPPP1eMbdu2TWFhYWratOkZX5vGCwAAGOPPxfXVMWrUKM2ePVtz587V559/rpEjR6qwsFCDBw+WJGVnZ6t3794V599+++1q0KCB7rrrLn322Wdat26d7r33XvXr1++MbzNKQbLGCwAAoDp69OihvXv3asKECdq9e7dSU1O1evVqJSUlSZJ2796twsLCivPr1KmjvLw8DRs2TBkZGWrQoIG6d++uv//979W6Lo0XAAAw5yx6ZFBWVpaysrKq/N78+fMrjV100UWVbk9WF7caAQAADCHxAgAAxvhzO4lAQOIFAABgCIkXAAAwy9drvAIIiRcAAIAhJF4AAMCYUF/jReMFAADMOYu2k7ADtxoBAAAMIfECAAAGOf57+HrOwEDiBQAAYAiJFwAAMIc1XgAAADCBxAsAAJhD4gUAAAATzprGKycnRw6HQyNGjLC7FAAA4C+Wwz9HgDgrbjXm5+dr1qxZuvTSS+0uBQAA+JFlHT98PWegsD3x+vnnn3XHHXfo2WefVb169ewuBwAAwG9sb7yGDBmizp076/rrr//Vc8vKylRaWupxAACAAGL56QgQtt5qfOGFF/TRRx8pPz//jM7PycnRQw895OeqAAAA/MO2xKuoqEj33HOPFi5cqKioqDN6TXZ2tg4cOFBxFBUV+blKAADgUyyut0dBQYFKSkqUnp5eMeZyubRu3TpNnz5dZWVlCg8P93iN0+mU0+k0XSoAAIBP2NZ4XXfddfrkk088xu666y5ddNFFGjNmTKWmCwAABD6Hdfzw9ZyBwrbGKzo6WqmpqR5jtWvXVoMGDSqNAwAABINqr/F67rnn9Nprr1V8fd9996lu3bpq06aNdu7c6dPiAABAkAnxTzVWu/GaNGmSatasKUnasGGDpk+frilTpqhhw4YaOXLkbyrm7bff1rRp037THAAA4CzG4vrqKSoq0vnnny9Jevnll3Xrrbfqz3/+s9q2basOHTr4uj4AAICgUe3Eq06dOtq7d68k6c0336zY+DQqKkqHDx/2bXUAACC4hPitxmonXh07dtSAAQPUsmVLbdu2TZ07d5Ykffrpp2revLmv6wMAAAga1U68nnrqKbVu3Vo//vijli1bpgYNGkg6vi9Xz549fV4gAAAIIiRe1VO3bl1Nnz690jiP8gEAADi9M2q8tm7dqtTUVIWFhWnr1q2nPffSSy/1SWEAACAI+SOhCrbEKy0tTcXFxYqLi1NaWpocDocs6/9+yhNfOxwOuVwuvxULAAAQyM6o8dqxY4caNWpU8d8AAABe8ce+W8G2j1dSUlKV/32y/03BAAAA4Knan2rs1auXfv7550rj3377ra6++mqfFAUAAILTiYdk+/oIFNVuvD777DNdcskleu+99yrGnnvuOV122WWKj4/3aXEAACDIsJ1E9XzwwQe6//77de2112r06NH66quvtGbNGv3jH/9Qv379/FEjAABAUKh24xUREaFHHnlETqdTEydOVEREhN555x21bt3aH/UBAAAEjWrfajx27JhGjx6tyZMnKzs7W61bt9Yf//hHrV692h/1AQAABI1qJ14ZGRn65Zdf9Pbbb+uqq66SZVmaMmWKbr75ZvXr108zZszwR50AACAIOOT7xfCBs5mEl43XP//5T9WuXVvS8c1Tx4wZo9///ve68847fV7gmWh/e4Ei69Sw5dreeq/bRXaX4JVLln9jdwlee+WlwPzwxznvHra7BK98c7PT7hK8VqM0xu4SvNIistr/Sz8rfH2szO4SvBa1L4BWdUtyHQ2seoNRtf+Uzpkzp8rxtLQ0FRQU/OaCAABAEGMDVe8dPnxYx44d8xhzOgP3X7kAAAD+VO3F9YcOHdLQoUMVFxenOnXqqF69eh4HAADAKYX4Pl7Vbrzuu+8+rV27VjNmzJDT6dTs2bP10EMPqUmTJlqwYIE/agQAAMEixBuvat9qfOWVV7RgwQJ16NBB/fr1U/v27XX++ecrKSlJixYt0h133OGPOgEAAAJetROvffv2KTk5WZIUExOjffv2SZLatWundevW+bY6AAAQVHhWYzWde+65+vbbbyVJF198sV588UVJx5OwunXr+rI2AACAoFLtxuuuu+7Sli1bJEnZ2dkVa71Gjhype++91+cFAgCAIMIar+oZOXJkxX9fc801+uKLL/Thhx/qvPPO02WXXebT4gAAAILJb97muFmzZmrWrJkvagEAAMHOHwlVACVe1b7VCAAAAO8E5oO9AABAQPLHpxCD8lONu3bt8mcdAAAgFJx4VqOvjwBxxo1Xamqqnn/+eX/WAgAAENTOuPGaNGmShgwZoltuuUV79+71Z00AACBYhfh2EmfceGVlZWnLli3av3+/WrRooVWrVvmzLgAAgKBTrcX1ycnJWrt2raZPn65bbrlFKSkpiojwnOKjjz7yaYEAACB4hPri+mp/qnHnzp1atmyZ6tevr27dulVqvAAAAFC1anVNzz77rEaPHq3rr79e//nPf9SoUSN/1QUAAIJRiG+gesaN1w033KBNmzZp+vTp6t27tz9rAgAACEpn3Hi5XC5t3bpVTZs29Wc9AAAgmPlhjVdQJl55eXn+rAMAAISCEL/VyLMaAQAADOEjiQAAwBwSLwAAAJhA4gUAAIwJ9Q1USbwAAAAMofECAAAwhMYLAADAENZ4AQAAc0L8U400XgAAwBgW1wMAAMAIEi8AAGBWACVUvkbiBQAAYAiJFwAAMCfEF9eTeAEAABhC4gUAAIzhU40AAAAwgsQLAACYE+JrvGi8AACAMdxqBAAAgBE0XgAAwBzLT4cXZsyYoeTkZEVFRSk9PV3r168/o9e99957ioiIUFpaWrWvSeMFAABCTm5urkaMGKFx48Zp8+bNat++vTp16qTCwsLTvu7AgQPq3bu3rrvuOq+uS+MFAADMOUsSr6lTp6p///4aMGCAUlJSNG3aNCUmJmrmzJmnfd2gQYN0++23q3Xr1tW/qGi8AABAkCgtLfU4ysrKqjzv6NGjKigoUGZmpsd4Zmam3n///VPOP2/ePH3zzTcaP3681zXSeAEAAGNOfKrR14ckJSYmKjY2tuLIycmpsoY9e/bI5XIpPj7eYzw+Pl7FxcVVvuarr77S2LFjtWjRIkVEeL8pRFBsJ/H6lxcrrFaU3WVUS6cXP7O7BK+MaPCu3SV4re/AU/8r5mw2/NoedpfglYiS+naX4LU1Nz1ldwleaTX5PrtL8EqT16v+iy4Q7HvosN0lVIv7lyPSC3ZX4T9FRUWKiYmp+NrpdJ72fIfD4fG1ZVmVxiTJ5XLp9ttv10MPPaQLL7zwN9UYFI0XAAAIEH7cQDUmJsaj8TqVhg0bKjw8vFK6VVJSUikFk6SDBw/qww8/1ObNmzV06FBJktvtlmVZioiI0Jtvvqlrr732jEql8QIAAOacBTvXR0ZGKj09XXl5efrjH/9YMZ6Xl6du3bpVOj8mJkaffPKJx9iMGTO0du1avfTSS0pOTj7ja9N4AQCAkDNq1Cj16tVLGRkZat26tWbNmqXCwkINHjxYkpSdna3vvvtOCxYsUFhYmFJTUz1eHxcXp6ioqErjv4bGCwAAGHO2PDKoR48e2rt3ryZMmKDdu3crNTVVq1evVlJSkiRp9+7dv7qnlzdovAAAQEjKyspSVlZWld+bP3/+aV/74IMP6sEHH6z2NWm8AACAOWfBGi87sY8XAACAISReAADAmLNljZddSLwAAAAMIfECAADmhPgaLxovAABgTog3XtxqBAAAMITECwAAGOP47+HrOQMFiRcAAIAhJF4AAMAc1ngBAADABBIvAABgDBuoAgAAwAjbG6/vvvtOd955pxo0aKBatWopLS1NBQUFdpcFAAD8wfLTESBsvdW4f/9+tW3bVtdcc41ef/11xcXF6ZtvvlHdunXtLAsAAPhTADVKvmZr4zV58mQlJiZq3rx5FWPNmze3ryAAAAA/svVW46pVq5SRkaHbbrtNcXFxatmypZ599tlTnl9WVqbS0lKPAwAABI4Ti+t9fQQKWxuv7du3a+bMmbrgggv0xhtvaPDgwRo+fLgWLFhQ5fk5OTmKjY2tOBITEw1XDAAA4D1bGy+3263LL79ckyZNUsuWLTVo0CANHDhQM2fOrPL87OxsHThwoOIoKioyXDEAAPhNQnxxva2NV0JCgi6++GKPsZSUFBUWFlZ5vtPpVExMjMcBAAAQKGxdXN+2bVt9+eWXHmPbtm1TUlKSTRUBAAB/YgNVG40cOVIbN27UpEmT9PXXX2vx4sWaNWuWhgwZYmdZAAAAfmFr49WqVSutWLFCS5YsUWpqqiZOnKhp06bpjjvusLMsAADgLyG+xsv2ZzV26dJFXbp0sbsMAAAAv7O98QIAAKEj1Nd40XgBAABz/HFrMIAaL9sfkg0AABAqSLwAAIA5JF4AAAAwgcQLAAAYE+qL60m8AAAADCHxAgAA5rDGCwAAACaQeAEAAGMcliWH5duIytfz+RONFwAAMIdbjQAAADCBxAsAABjDdhIAAAAwgsQLAACYwxovAAAAmBAUide69rMVEx1YPWSHR0bbXYJX2mak2l2C1xwRbrtL8ErybIfdJXglrmENu0vw2tq08+0uwSv9B71mdwleeX1Bc7tL8FqzuHK7S6iW8kNl+tbmGljjBQAAACOCIvECAAABIsTXeNF4AQAAY7jVCAAAACNIvAAAgDkhfquRxAsAAMAQEi8AAGBUIK3J8jUSLwAAAENIvAAAgDmWdfzw9ZwBgsQLAADAEBIvAABgTKjv40XjBQAAzGE7CQAAAJhA4gUAAIxxuI8fvp4zUJB4AQAAGELiBQAAzGGNFwAAAEwg8QIAAMaE+nYSJF4AAACGkHgBAABzQvyRQTReAADAGG41AgAAwAgSLwAAYA7bSQAAAMAEEi8AAGAMa7wAAABgBIkXAAAwJ8S3kyDxAgAAMITECwAAGBPqa7xovAAAgDlsJwEAAAATSLwAAIAxoX6rkcQLAADAEBIvAABgjts6fvh6zgBB4gUAAGAIiRcAADCHTzUCAADABBIvAABgjEN++FSjb6fzKxovAABgDs9qBAAAgAkkXgAAwBg2UAUAAAhBM2bMUHJysqKiopSenq7169ef8tzly5erY8eOatSokWJiYtS6dWu98cYb1b4mjRcAADDH8tNRTbm5uRoxYoTGjRunzZs3q3379urUqZMKCwurPH/dunXq2LGjVq9erYKCAl1zzTXq2rWrNm/eXK3r0ngBAICQM3XqVPXv318DBgxQSkqKpk2bpsTERM2cObPK86dNm6b77rtPrVq10gUXXKBJkybpggsu0CuvvFKt67LGCwAAGOOwLDl8/CnEE/OVlpZ6jDudTjmdzkrnHz16VAUFBRo7dqzHeGZmpt5///0zuqbb7dbBgwdVv379atUaFI3X2O+uVWSdSLvLqJbFf3nM7hK8MvCLO+0uwWvOKfXsLsErP50fWL+3TyirF0g763h6MaWx3SV4pfT18+wuwSt7suPsLsFryX/7xe4SqsUqP2J3CX6VmJjo8fX48eP14IMPVjpvz549crlcio+P9xiPj49XcXHxGV3r8ccf16FDh9S9e/dq1RgUjRcAAAgQ7v8evp5TUlFRkWJiYiqGq0q7/pfD4fkPRMuyKo1VZcmSJXrwwQe1cuVKxcVV7x8ONF4AAMAYf95qjImJ8Wi8TqVhw4YKDw+vlG6VlJRUSsFOlpubq/79+2vp0qW6/vrrq10ri+sBAEBIiYyMVHp6uvLy8jzG8/Ly1KZNm1O+bsmSJerbt68WL16szp07e3VtEi8AAGCOl9s//Oqc1TRq1Cj16tVLGRkZat26tWbNmqXCwkINHjxYkpSdna3vvvtOCxYskHS86erdu7f+8Y9/6KqrrqpIy2rWrKnY2Ngzvi6NFwAACDk9evTQ3r17NWHCBO3evVupqalavXq1kpKSJEm7d+/22NPrmWeeUXl5uYYMGaIhQ4ZUjPfp00fz588/4+vSeAEAAHPOoodkZ2VlKSsrq8rvndxMvf32215d42Ss8QIAADCExAsAABjDQ7IBAABgBIkXAAAw5yxa42UHEi8AAABDSLwAAIAxDvfxw9dzBgoaLwAAYA63GgEAAGACiRcAADDnLHlkkF1IvAAAAAwh8QIAAMY4LEsOH6/J8vV8/kTiBQAAYAiJFwAAMIdPNdqnvLxc999/v5KTk1WzZk2de+65mjBhgtzuANqQAwAA4AzZmnhNnjxZTz/9tJ577jm1aNFCH374oe666y7FxsbqnnvusbM0AADgD5YkX+crgRN42dt4bdiwQd26dVPnzp0lSc2bN9eSJUv04YcfVnl+WVmZysrKKr4uLS01UicAAPANFtfbqF27dnrrrbe0bds2SdKWLVv07rvv6g9/+EOV5+fk5Cg2NrbiSExMNFkuAADAb2Jr4jVmzBgdOHBAF110kcLDw+VyufTwww+rZ8+eVZ6fnZ2tUaNGVXxdWlpK8wUAQCCx5IfF9b6dzp9sbbxyc3O1cOFCLV68WC1atNDHH3+sESNGqEmTJurTp0+l851Op5xOpw2VAgAA/Ha2Nl733nuvxo4dqz/96U+SpEsuuUQ7d+5UTk5OlY0XAAAIcGwnYZ9ffvlFYWGeJYSHh7OdBAAACEq2Jl5du3bVww8/rGbNmqlFixbavHmzpk6dqn79+tlZFgAA8Be3JIcf5gwQtjZeTz75pP72t78pKytLJSUlatKkiQYNGqQHHnjAzrIAAAD8wtbGKzo6WtOmTdO0adPsLAMAABgS6vt48axGAABgDovrAQAAYAKJFwAAMIfECwAAACaQeAEAAHNIvAAAAGACiRcAADAnxDdQJfECAAAwhMQLAAAYwwaqAAAAprC4HgAAACaQeAEAAHPcluTwcULlJvECAADASUi8AACAOazxAgAAgAkkXgAAwCA/JF4KnMQrKBqvPb2jFRHmtLuMaol6L4C22f0fNXNi7S7Ba1/fEZgB744uM+0uwSvthw6yuwSvOTJS7S7BK/WGHbK7BK9EXBKY/z+UJLcz3O4SqsUdHlj1BqOgaLwAAECACPE1XjReAADAHLcln98aZDsJAAAAnIzECwAAmGO5jx++njNAkHgBAAAYQuIFAADMCfHF9SReAAAAhpB4AQAAc/hUIwAAAEwg8QIAAOaE+BovGi8AAGCOJT80Xr6dzp+41QgAAGAIiRcAADAnxG81kngBAAAYQuIFAADMcbsl+fgRP24eGQQAAICTkHgBAABzWOMFAAAAE0i8AACAOSGeeNF4AQAAc3hWIwAAAEwg8QIAAMZYlluW5dvtH3w9nz+ReAEAABhC4gUAAMyxLN+vyQqgxfUkXgAAAIaQeAEAAHMsP3yqkcQLAAAAJyPxAgAA5rjdksPHn0IMoE810ngBAABzuNUIAAAAE0i8AACAMZbbLcvHtxrZQBUAAACVkHgBAABzWOMFAAAAE0i8AACAOW5LcpB4AQAAwM9IvAAAgDmWJcnXG6iSeAEAAOAkJF4AAMAYy23J8vEaLyuAEi8aLwAAYI7llu9vNbKBKgAAAE5C4gUAAIwJ9VuNJF4AAACGkHgBAABzQnyNV0A3XieixXL3UZsrqb6DBwPnN8n/Ki8/YncJXnMfDszf7qWB+nvlWOD+Xil3BWbtYa4yu0vwSiD/XgkvP2Z3CdVSXn7894idt+bKdcznj2osV+D8OjisQLoxepJdu3YpMTHR7jIAAAgoRUVFatq0qdFrHjlyRMnJySouLvbL/I0bN9aOHTsUFRXll/l9JaAbL7fbre+//17R0dFyOBw+nbu0tFSJiYkqKipSTEyMT+dG1XjPzeL9Nov32zze88osy9LBgwfVpEkThYWZX+Z95MgRHT3qn7tUkZGRZ33TJQX4rcawsDC/d+wxMTH8gTWM99ws3m+zeL/N4z33FBsba9u1o6KiAqI58ic+1QgAAGAIjRcAAIAhNF6n4HQ6NX78eDmdTrtLCRm852bxfpvF+20e7znORgG9uB4AACCQkHgBAAAYQuMFAABgCI0XAACAITReAAAAhtB4ncKMGTOUnJysqKgopaena/369XaXFJRycnLUqlUrRUdHKy4uTjfddJO+/PJLu8sKGTk5OXI4HBoxYoTdpQS17777TnfeeacaNGigWrVqKS0tTQUFBXaXFZTKy8t1//33Kzk5WTVr1tS5556rCRMmyO0OzGeeIvjQeFUhNzdXI0aM0Lhx47R582a1b99enTp1UmFhod2lBZ133nlHQ4YM0caNG5WXl6fy8nJlZmbq0KFDdpcW9PLz8zVr1ixdeumldpcS1Pbv36+2bduqRo0aev311/XZZ5/p8ccfV926de0uLShNnjxZTz/9tKZPn67PP/9cU6ZM0aOPPqonn3zS7tIASWwnUaUrr7xSl19+uWbOnFkxlpKSoptuukk5OTk2Vhb8fvzxR8XFxemdd97R1VdfbXc5Qevnn3/W5ZdfrhkzZujvf/+70tLSNG3aNLvLCkpjx47Ve++9R2puSJcuXRQfH685c+ZUjN1yyy2qVauWnn/+eRsrA44j8TrJ0aNHVVBQoMzMTI/xzMxMvf/++zZVFToOHDggSapfv77NlQS3IUOGqHPnzrr++uvtLiXorVq1ShkZGbrtttsUFxenli1b6tlnn7W7rKDVrl07vfXWW9q2bZskacuWLXr33Xf1hz/8webKgOMC+iHZ/rBnzx65XC7Fx8d7jMfHx6u4uNimqkKDZVkaNWqU2rVrp9TUVLvLCVovvPCCPvroI+Xn59tdSkjYvn27Zs6cqVGjRumvf/2rNm3apOHDh8vpdKp37952lxd0xowZowMHDuiiiy5SeHi4XC6XHn74YfXs2dPu0gBJNF6n5HA4PL62LKvSGHxr6NCh2rp1q9599127SwlaRUVFuueee/Tmm28qKirK7nJCgtvtVkZGhiZNmiRJatmypT799FPNnDmTxssPcnNztXDhQi1evFgtWrTQxx9/rBEjRqhJkybq06eP3eUBNF4na9iwocLDwyulWyUlJZVSMPjOsGHDtGrVKq1bt05Nmza1u5ygVVBQoJKSEqWnp1eMuVwurVu3TtOnT1dZWZnCw8NtrDD4JCQk6OKLL/YYS0lJ0bJly2yqKLjde++9Gjt2rP70pz9Jki655BLt3LlTOTk5NF44K7DG6ySRkZFKT09XXl6ex3heXp7atGljU1XBy7IsDR06VMuXL9fatWuVnJxsd0lB7brrrtMnn3yijz/+uOLIyMjQHXfcoY8//pimyw/atm1baYuUbdu2KSkpyaaKgtsvv/yisDDPv9rCw8PZTgJnDRKvKowaNUq9evVSRkaGWrdurVmzZqmwsFCDBw+2u7SgM2TIEC1evFgrV65UdHR0RdIYGxurmjVr2lxd8ImOjq60fq527dpq0KAB6+r8ZOTIkWrTpo0mTZqk7t27a9OmTZo1a5ZmzZpld2lBqWvXrnr44YfVrFkztWjRQps3b9bUqVPVr18/u0sDJLGdxCnNmDFDU6ZM0e7du5WamqonnniC7Q384FTr5ubNm6e+ffuaLSZEdejQge0k/OzVV19Vdna2vvrqKyUnJ2vUqFEaOHCg3WUFpYMHD+pvf/ubVqxYoZKSEjVp0kQ9e/bUAw88oMjISLvLA2i8AAAATGGNFwAAgCE0XgAAAIbQeAEAABhC4wUAAGAIjRcAAIAhNF4AAACG0HgBAAAYQuMFAABgCI0XANs5HA69/PLLdpcBAH5H4wVALpdLbdq00S233OIxfuDAASUmJur+++/36/V3796tTp06+fUaAHA24JFBACRJX331ldLS0jRr1izdcccdkqTevXtry5Ytys/P5zl3AOADJF4AJEkXXHCBcnJyNGzYMH3//fdauXKlXnjhBT333HOnbboWLlyojIwMRUdHq3Hjxrr99ttVUlJS8f0JEyaoSZMm2rt3b8XYjTfeqKuvvlput1uS563Go0ePaujQoUpISFBUVJSaN2+unJwc//zQAGAYiReACpZl6dprr1V4eLg++eQTDRs27FdvM86dO1cJCQn63e9+p5KSEo0cOVL16tXT6tWrJR2/jdm+fXvFx8drxYoVevrppzV27Fht2bJFSUlJko43XitWrNBNN92kxx57TP/85z+1aNEiNWvWTEVFRSoqKlLPnj39/vMDgL/ReAHw8MUXXyglJUWXXHKJPvroI0VERFTr9fn5+briiit08OBB1alTR5K0fft2paWlKSsrS08++aTH7UzJs/EaPny4Pv30U/3rX/+Sw+Hw6c8GAHbjViMAD3PnzlWtWrW0Y8cO7dq161fP37x5s7p166akpCRFR0erQ4cOkqTCwsKKc84991w99thjmjx5srp27erRdJ2sb9+++vjjj/W73/1Ow4cP15tvvvmbfyYAOFvQeAGosGHDBj3xxBNauXKlWrdurf79++t0ofihQ4eUmZmpOnXqaOHChcrPz9eKFSskHV+r9b/WrVun8PBwffvttyovLz/lnJdffrl27NihiRMn6vDhw+revbtuvfVW3/yAAGAzGi8AkqTDhw+rT58+GjRokK6//nrNnj1b+fn5euaZZ075mi+++EJ79uzRI488ovbt2+uiiy7yWFh/Qm5urpYvX663335bRUVFmjhx4mlriYmJUY8ePfTss88qNzdXy5Yt0759+37zzwgAdqPxAiBJGjt2rNxutyZPnixJatasmR5//HHde++9+vbbb6t8TbNmzRQZGaknn3xS27dv16pVqyo1Vbt27dLdd9+tyZMnq127dpo/f75ycnK0cePGKud84okn9MILL+iLL77Qtm3btHTpUjVu3Fh169b15Y8LALag8QKgd955R0899ZTmz5+v2rVrV4wPHDhQbdq0OeUtx0aNGmn+/PlaunSpLr74Yj3yyCN67LHHKr5vWZb69u2rK664QkOHDpUkdezYUUOHDtWdd96pn3/+udKcderU0eTJk5WRkaFWrVrp22+/1erVqxUWxv+uAAQ+PtUIAABgCP+EBAAAMITGCwAAwBAaLwAAAENovAAAAAyh8QIAADCExgsAAMAQGi8AAABDaLwAAAAMofECAAAwhMYLAADAEBovAAAAQ/4/qxL52A4LbCsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 9l47sl93\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vits5f81 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.8454659787656174\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.02026688773173016\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 3.99798662987355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: MNIST\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_191157-vits5f81</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/vits5f81' target=\"_blank\">ruby-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/vits5f81' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/vits5f81</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /data2/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38265dbe56d5472db1586c7e5f37a610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90bd783b9964aada7400e74da11c60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.725 MB of 0.725 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/vits5f81' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/vits5f81</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240726_191157-vits5f81/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run vits5f81 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_16326/3863419927.py\", line 21, in sweep_cover\n",
      "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
      "  File \"/tmp/ipykernel_16326/404782496.py\", line 12, in main\n",
      "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
      "  File \"/tmp/ipykernel_16326/2240228058.py\", line 12, in data_loader\n",
      "    trainset = torchvision.datasets.MNIST(root=data_path,\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 87, in __init__\n",
      "    self.download()\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 176, in download\n",
      "    download_and_extract_archive(\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchvision/datasets/utils.py\", line 427, in download_and_extract_archive\n",
      "    download_url(url, download_root, filename, md5)\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchvision/datasets/utils.py\", line 152, in download_url\n",
      "    raise RuntimeError(\"File not found or corrupted.\")\n",
      "RuntimeError: File not found or corrupted.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run vits5f81 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_16326/3863419927.py\", line 21, in sweep_cover\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_16326/404782496.py\", line 12, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_16326/2240228058.py\", line 12, in data_loader\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainset = torchvision.datasets.MNIST(root=data_path,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 87, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.download()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 176, in download\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     download_and_extract_archive(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchvision/datasets/utils.py\", line 427, in download_and_extract_archive\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     download_url(url, download_root, filename, md5)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torchvision/datasets/utils.py\", line 152, in download_url\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise RuntimeError(\"File not found or corrupted.\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: File not found or corrupted.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v5cjj46a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.43359240763836254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0832245510356748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 2.2305995697421417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: MNIST\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_191256-v5cjj46a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/v5cjj46a' target=\"_blank\">zany-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/9l47sl93</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/v5cjj46a' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/v5cjj46a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '7', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'MNIST' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} fc_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [28]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
