{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: vz1kbys1\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/vz1kbys1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hvxizywt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 7.338504673816841\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: MNIST\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_221227-hvxizywt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/hvxizywt' target=\"_blank\">electric-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/vz1kbys1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/vz1kbys1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/vz1kbys1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/vz1kbys1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/hvxizywt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/hvxizywt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (reservoir): Linear(in_features=784, out_features=784, bias=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '7', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'MNIST' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} fc_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"values\": [0.09]},\n",
    "        \"batch_size\": {\"values\": [128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"values\": [0.7]},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [3]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [28]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
